---
title: 특화 클라우드 아키텍처
categories:
- Cloud Computing
feature_text: |
  ## 13. 특화 클라우드 아키텍처
feature_image: "https://picsum.photos/2560/600?image=733"
image: "https://picsum.photos/2560/600?image=733"
---
<style>
	thead td { text-align: center; }
	td { border: 1px solid #444444; }
</style>

### 13.1. 직접 I/O 접근 아키텍처
<br/>
물리 서버에 설치된 물리 I/O 카드에 대한 접근은 보통 I/O 가상화라 불리는 하이퍼바이저 기반 처리를 통해 운영중인 가상 서버에 제공된다. 하지만, 때로 가상 서버는 하이퍼바이저의 개입이나 에뮬레이션 없이 I/O 카드에 연결하고 사용해야 할 필요가 있다.  

직접 I/O 접근 아키텍처를 통해 가상 서버는 하이퍼바이저로 연결을 에뮬레이션하지 않고 하이퍼바이저를 우회하여 직접 물리 서버의 I/O 카드에 접근할 수 있다.  

이 솔루션을 완성하여 하이퍼바이저와의 상호작용 없이 물리 I/O 카드에 접근하려면 호스트 CPU가 이런 종류의 접근을 지원할 필요가 있고 가상 서버상에 적절한 드라이버를 설치해야 한다. 가상 서버는 드라이버가 설치된 후 이제 I/O 카드를 하드웨어 장치로 인식할 수 있다.  

<img src="/assets/images/Cloud_Computing/Fig13-01.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-02.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-03.jpg" width="100%" height="100%"/>
<br/>

가상 서버와 하이퍼바이저에 추가로 이 아키텍처를 구성하는 데 다음을 포함한 타 메커니즘들이 사용될 수 있다.  

+ 클라우드 사용 모니터  
런타임 모니터를 통해 수집되는 클라우드 서비스 사용 데이터는 직접 I/O 접근을 포함할 뿐 아니라 별도로 분류할 수 있다.  

+ 논리 네트워크 경계  
논리 네트워크 경계는 할당된 물리 I/O 카드가 클라우드 소비자의 다른 클라우드 소비자에게 속한 IT 자원에 대한 접근을 허용하지 않음을 보장한다.  

+ 사용량당 과금 모니터  
이 모니터는 핟당된 물리 I/O 카드의 사용 비용 정보를 수집한다.  

+ 자원 복제  
복제 기술은 가상 I/O 카드를 물리 I/O 카드로 교체하는 데 사용된다.  

### 13.2. 직접 LUN 접근 아키텍처
<br/>
스토리지 LUN은 일반적으로 호스트 버스 어댑터(HBA)를 통해 하이퍼바이저상에 가상 서버에 파일 기반 스토리지로 에뮬레이션되는 스토리지 공간으로 사상된다. 하지만, 가상 서버는 때로 저 수준의 블록 기반 스토리지에 직접 접근해야 할 필요가 있다. 예를 들어 클러스터가 구현되거나, 두 가상 서버 간 LUN이 공유 클러스터 스토리지 장치로 사용되는 경우 에뮬레이션된 어댑터를 통한 접근만으로는 불충분하다.  

직접 LUN 접근 아키턱체는 가상 서버에 물리 HBA 카드를 통한 LUN 접근을 제공하는데, 이는 동일한 클러스터 내에 있는 가상 서버들이 클러스터화된 데이터베이스용으로 해당 LUN을 공유 볼륨으로 활용할 수 있어 효과적이다. 이 솔루션을 구현한 후 가상 서버의 LUN에 대한 물리 연결과 클라우드 스토리지 장치가 물리 호스트에 의해 활성화된다.  

하이퍼바이저를 위한 LUN 표시를 위해 클라우드 스토리지 장치상에 LUN이 생성 및 구성된다. 클라우드 스토리지 장치는 가상 서버에게 해당 LUN이 포맷되지 않았고 파티션되지 않은 초기 상태의 블록 기반 저 수준 SAN LUN으로 보이도록 저 수준 장치 사상을 사용해 구성될 필요가 있다. LUN은 모든 가상 서버가 공유 스토리지로 활용할 수 있도록 고유한 LUN ID와 함께 표시될 필요가 있다.  

<img src="/assets/images/Cloud_Computing/Fig13-04.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-05.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-06.jpg" width="100%" height="100%"/>
<br/>

가상 서버, 하이퍼바이저, 클라우드 스토리지 장치 이외에 다음과 같은 메커니즘들이 이 아키텍처에 포함될 수 있다.  

+ 클라우드 사용 모니터  
이 모니터는 LUN의 직접 사용과 관계된 스토리지 사용 정보를 추적 및 수집한다.  

+ 사용량당 과금 모니터  
사용량당 과금 모니터는 직접 LUN 접근에 대한 사용 비용 정보를 수집하고 별도로 분류한다.  

+ 자원 복제  
이 메커니즘은 가상 서버가 파링 기반 스토리지 대신 블록 기반 스토리지에 직접 접근하는 방식과 관련이 있다.  

### 13.3. 동적 데이터 정규화 아키텍처
<br/>
중복 데이터는 클라우드 기반 환경에서 다음과 같은 다양한 이슈를 유발할 수 있다.  

+ 파일을 저장하고 카탈로그를 작성하는 데 필요한 시간의 증가
+ 필요한 스토리지 및 백업 공간의 증가
+ 늘어난 데이터 볼륭에 따른 비용의 증가
+ 보조 스토리지 장치에 복제하는 데 필요한 시간의 증가
+ 데이터 백업에 필요한 시간의 증가  

예를 들어 클라우드 소비자가 100MB 크기의 파일을 클라우드 스토리지 장치에 복사하고, 해당 데이터가 10번 반복하여 복사된다면, 이로 인한 영향은 상당히 심각할 수 있다.  

+ 클라우드 소비자는 실제로는 단 100MB의 고유한 데이터만 저장됨에도 불구하고 10 × 100MB의 스토리지 공간을 사용한 만큼의 비용을 지불하게 될 것이다.  

+ 클라우드 제공자는 온라인 클라우드 스토리지 장치 및 기타 백업 스토리지 시스템에 불필요한 900MB의 공간을 제공해야 할 필요가 있다.  

+ 데이터를 저장하고 카탈로그를 작성하는 데 훨씬 많은 시간이 걸린다.  

+ 클라우드 제공자가 사이트 복구를 수행할 때마다 100MB 대신 1000MB가 복제될 필요가 있으므로 데이터 복제 시간 및 성능이 불필요하게 늘어난다.  

이러한 영향은 멀티테넌시 방식 퍼블릭 클라우드에서는 심각하게 확대될 수 있다.  

동적 데이터 정규화 아키텍처는 중복 제거 시스템을 확립하는데, 이는 클라우드 스토리지 장치상의 중복된 데이터를 탐지 및 제거해서 클라우드 소비자가 우연히 중복된 데이터 사본을 저장하는 사태를 방지한다. 이 시스템은 블록 기반 스토리지 장치에서 훨씬 효과적이지만 블록 및 파일 기반 스토리지 장치 모두에 적용될 수 있다. 이 중복 제거 시스템은 이미 저장해 둔 블록과 새로 받은 각 블록이 중복되는지 여부를 확인한다. 중복된 블록은 이미 스토리지 내 저장된 동일한 블록에 대한 포인터로 대체된다.  

중복 제거 시스템은 스토리지 컨트롤러로 전달받은 데이터를 넘기기 전에 이를 조사한다. 조사 프로세스의 일부로 해시 코드가 처리 및 저장된 모든 데이터 조각에 각각 할당된다. 해시와 데이터 조각들의 색인 또한 유지된다. 그 결과, 새로 전달받은 데이터 블록에 생성된 해시가 스토리지 내의 해시와 비교되어 이 데이터 블록이 새로운 데이터를 포함하는지, 중복된 데이터 블록인지를 판별한다. 신규 블록은 저장되는 반면, 중복된 데이터는 제거되고 대신 원본 데이터 블록에 대한 포인터가 생성 및 저장된다.  

<img src="/assets/images/Cloud_Computing/Fig13-07.jpg" width="100%" height="100%"/>
<br/>

이 아키텍처 모델은 디스크 스토리지 및 백업 테이프 드라이브에 모두 사용될 수 있다. 어떤 클라우드 제공자는 백업 클라우드 스토리지 장치상의 중복 데이터만 제거하도록 결정할 수 있고, 또 다른 제공자는 좀 더 공격적으로 데이터 중복 제거 시스템을 전 클라우드 스토리지 장치에 모두 구현할 수도 있다. 데이터 블록이 다른 블록과 중복되었는지 확인하기 위한 데이터 블록 비교 알고리즘에는 다양한 방식이 존재한다.  

### 13.4. 탄력적 네트워크 용량 아키텍처
<br/>
IT 자원이 클라우드 플랫폼을 통해 필요에 따라 확장된다 하더라도, IT 자원에 대한 원격 접근은 네트워크 대역폭 제약에 영향을 받아 여전히 성능 및 확장성이 저해될 수 있다.  

탄력적 네트워크 용량 아키텍처는 런타임 병목 현상을 방지하기 위해 동적으로 네트워크에 추가적인 대역폭이 할당되는 시스템을 확립한다. 이 시스템은 각 클라우드 소비자가 개개의 클라우드 소비자 트래픽의 흐름을 분리하기 위해 서로 다른 네트워크 포트의 집합을 사용하도록 보장한다.  

<img src="/assets/images/Cloud_Computing/Fig13-08.jpg" width="100%" height="100%"/>
<br/>

자동 확장 라스너와 지능형 자동화 엔진 스크립트가 트래픽이 대역폭 임계치에 도달할 때 이를 감지하고, 동적으로 추가적인 대역폭 및 네트워크 포트를 필요한 경우 할당하는 데 사용된다.  

이 클라우드 아키텍처는 공유 용도로 사용할 수 있도록 만들어진 네트워크 포트를 포함하는 네트워크 자원 풀을 형성할 수 있다. 자동 확장 리스너가 워크로드와 네트워크 트래픽을 감시하다가 지능형 자동화 엔진의 사용 변동에 대응하여 할당된 네트워크 포트의 수나 대역폭을 변경하도록 신호를 보낸다.  

단, 이 아키텍처 모델이 가상 스위치 수준에서 구현된 경우에는 지능형 자동화 엔진이 특별히 가상 스위치에 물리 업링크를 추가하는 별도의 스크립트를 실행할 필요가 있을 수 있다는 것에 주의하라. 또는 가상 서버에 할당된 네트워크 대역폭을 늘리는 데 직접 I/O 접근 아키텍처도 포함될 수 있다.  

자동 확장 리스너에 덧붙여 다음과 같은 메커니즘이 이 아키텍처의 일부가 될 수 있다.  

+ 클라우드 사용 모니터  
이 모니터는 확장 이전, 도중 및 이후에 탄력적 네트워크 용량을 추적할 책임이 있다.  

+ 하이퍼바이저  
하이퍼바이저는 가상 서버에 가상 스위치와 물리 업링크를 통한 물리 네트워크로의 접근을 제공한다.  

+ 논리 네트워크 경계  
이 메커니즘은 각 클라우드 소비자에게 할당된 네트워크 용량을 제공하는 데 필요한 경계를 형성한다.  

+ 사용량당 과금 모니터  
이 모니터는 동적 네트워크 대역폭 소비와 연관된 과금 관련 데이터를 지속적으로 추적한다.  

+ 자원 복제  
자원 복제는 워크로드 수요에 대응하여 물리 및 가상 서버에 네트워크 포트를 추가하는 데 사용된다.  

+ 가상 서버  
가상 서버는 네트워크 자원이 할당되고, 그 자체로 네트워크 용량의 확장에 영향을 받는 IT 자원 및 클라우드 서비스를 제공한다.  

### 13.5. 교차 스토리지 장치 수직 계층화 아키텍처
<br/>
클라우드 스토리지 장치는 때로 클라우드 소비자의 섣능 요구사항을 수용할 수 없어 IOPS를 개선하기 위해 데이터 프로세싱 파워나 대역폭을 추가해야 한다. 이런한 전형적인 수직 확장 방식은 일반적으로 구현하기 비효율적이고 시간 소모적이며, 추가된 용량이 더 이상 필요하지 않을 때 낭비가 심해질 수 있다.  

<img src="/assets/images/Cloud_Computing/Fig13-09.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-10.jpg" width="100%" height="100%"/>
<br/>

교차 스토리지 장치 수직 계층화 아키텍처는 서로 다른 용량을 가진 스토리지 장치간 수직 확장을 통해 대역폭 및 데이터 프로세싱 파워 제약 사항을 극복하는 시스템을 확립한다. LUN은 이 시스템 내 여러 장치들간 자동으로 확장 및 축소될 수 있어, 클라우드 소비자 작업을 수행하는 데 적절한 스토리지 장치 수준을 활용할 수 있다.  

자동 계층화 기술이 데이터를 동일한 스토리지 처리 용량을 가진 클라우드 스토리지 장치 로 이관할 수 있음에도 불구하고 용량이 늘어난 신규 클라우드 스토리지 장치를 쓸 수도 있다. 예를 들어 SSD(Solid-State Drives)가 데이터 프로세싱 파워 업그레이드용으로 적합한 장치일 수 있다.  

자동 확장 리스너가 특정 LUN에 전달되는 요청을 감시하고 선 정의된 임계치에 도달한 경우 더 많은 용량을 가진 장치로 LUN을 이관하도록 스토리지 관리 프로그램에 신호를 보낸다. 이전하는 도중에 연결의 단절이 없기 때문에 서비스 중단이 예방된다. 기존 장치는 여전히 정상적으로 수행되는 상태에서 LUN 데이터는 또 다른 장치로 확장된다. 클라우드 소비자 요청은 확장 프로세스가 완료되자 마자 신규 클라우드 스토리지 장치로 자동 전용된다.  

<img src="/assets/images/Cloud_Computing/Fig13-11.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-12.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-13.jpg" width="100%" height="100%"/>
<br/>

자동 확장 리스너 및 클라우드 스토리지 장치에 더불어, 다음을 포함하는 기타 메커니즘들이 이 기술 아키텍처에 포함될 수 있다.  

+ 감사 모니터  
이 모니터가 수행하는 감사 작업은 클라우드 소비자의 데이터 재배치가 법적, 혹은 개인 정보 보호 규제나 정책에 어긋나지 않는지 여부를 확인한다.  

+ 클라우드 사용 모니터  
이 인프라 메커니즘은 원본 및 목적지 스토리지 위치에서 데이터 이전과 사용르 추적 및 기록하기 위한 다양한 런타임 모니터링 요구사항을 나타낸다.  

+ 사욜량당 과금 모니터  
이 아키텍처의 맥락에서 사용량당 과금 모니터는 양 원본 및 목적지 위치상에서의 스토리지 사용 정보뿐 아니라, 교차 스토리지 계층화 기능을 수행하는 데 쓰인 IT 자원 사용 정보를 수집한다.  

### 13.6. 내부 스토리지 장치 수직 데이터 계층화
<br/>
일부 클라우드 소비자들은 데이터의 물리적 위치를 단일 클라우드 스토리지 장치에 한정하는 차별화된 데이터 스토리지 요구사항을 가질 수도 있다. 다른 클라우드 스토리지 장치에 걸친 워크로드 분배는 보안, 개인 정보 보호, 기타 다양한 법적인 이유로 허가되지 않을 수 있다. 이런 종류의 제한은 장치의 스토리지 및 성능 용량에 심각한 확장성의 제약을 초래할 수 있다. 이 제약 사항은 나아가 클라우드 스토리지 장치의 활용에 의존적인 클라우드 서비스나 애플리케이션에 파급효과를 미칠 수 있다.  

내부 스토리지 장치 수직 데이터 계층화 아키텍처는 단일 클라우드 스토리지 장치 내 수직 확장을 지원하는 시스템을 확립한다. 이 내부 장치 확장 시스템은 다양한 용량의 서로 다른 디스크 종류의 가용성을 최적화시킨다.  

<img src="/assets/images/Cloud_Computing/Fig13-14.jpg" width="100%" height="100%"/>
<br/>

이 클라우드 스토리지 아키텍처는 다양한 종류의 하드 디스크를 지원하는 복잡한 스토리지 장치의 활용을 필요로 하는데, 이는 특히 SATA, SAS, SSD와 같은 고성능 디스크들을 포함한다. 처리 및 용량 요구사항에 따른 디스크 유형의 할당을 기반으로 LUN 이관 프포그램이 장치를 수직적으로 확장할 수 있도록 이러한 디스크 종류들이 등급화된 계층에 조직화되어 위치한다.  

데이터 로드 조건 및 정의가 디스크 분류 이후에 설정되어 선 정의된 조건에 부합하는 경우 상위 혹은 하위 등급으로 LUN이 이동할 수 있도록 한다. 이러한 임계치와 조건들은 런타임 데이터 처리 트래픽을 감시할 때 자동 확장 리스너가 활용한다.  

<img src="/assets/images/Cloud_Computing/Fig13-15.jpg" width="100%" height="100%"/>
<br/>

클라우드 스토리지 장치 영역 내 여러 다른 종류의 하드 디스크들이 설치된다(1). 유사한 디스크 종류들은 I/O 성능을 기반으로 다양한 등급의 디스크 그룹을 생성하도록 여러 계층으로 그룹화된다.  

<img src="/assets/images/Cloud_Computing/Fig13-16.jpg" width="100%" height="100%"/>
<br/>

디스크 그룹 1에 두 개의 LUN이 생성되었다(3). 자동 확장 리스너는 선 정의된 임계치와 관련하여 사용자의 요청을 감시한다(4). 사용량당 과금 모니터는 여유 공간 및 디스크 그룹 성능을 토대로 실제 디스크 사용량을 추적한다(5). 자동 확장 리스너는 요청의 수가 임계치에 도달했는지 파악하고, 스토리지 관리 프로그램에 고성능 디스크 그룹으로 해당 LUN이 이관될 필요가 있음을 알린다(6). 스토리지 관리 프로그램은 LUN 이관 프로그램에 요청된 이관을 수행하도록 신호를 보낸다(7). LUN 이관 프로그램은 고용량 디시크 그룹 2로 해당 LUN을 이관하기 위해 스토리지 컨트롤러와 합작한다(8).  

<img src="/assets/images/Cloud_Computing/Fig13-17.jpg" width="100%" height="100%"/>
<br/>

디스크 그룹 2로 이관된 LUN의 사용 요금은 이제 더 높은 성능을 내는 디스크 그룹이 사용되므로 이전보다 높아진다(9).  

### 13.7. 부하 분산 가상 스위치 아키텍처
<br/>
가상 서버들은 가상 스위치를 통해 외부 세계와 연결되며, 동일한 업링크를 통해 트래픽을 주고받는다. 대역폭 병목 현상은 업링크 포트상의 네트워크 트래픽이 전송 지연, 성능 이슈, 패킷 손실 및 지체 시간(lag time)을 유발하는 어느 시점까지 증가할 때 형성된다.  

<img src="/assets/images/Cloud_Computing/Fig13-18.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-19.jpg" width="100%" height="100%"/>
<br/>

부하 분산 가상 스위치 아키텍처는 다중 업링크나 이중화 경로에 걸쳐 네트워크 트래픽 워크로드를 분산시키기 위해 다중 업링크가 제공되는 부하 분산 시스템을 형성하며, 이는 전송 속도가 느려지거나 데이터가 손실되는 것을 방지할 수 있다. 링크 티밍(Link aggregation)이 수행되어 동시에 다중 업링크에 걸쳐 워크로드가 분산되는 것을 허용할 수 있도록 트래픽을 분배하며, 이에 따라 네트워크 카드에 과부하가 걸리지 않는다.  

가상 스위치는 다중 물리 업링크를 지원할 수 있도록 구성될 필요가 있고, 이들은 보통 트래픽 형성 정책을 정의하는 하나의 NIC 팀으로서 구성된다.  

다음 메커니즘들이 이 아키텍처에 포함될 수 있다.  

+ 클라우드 사용 모니터  
클라우드 사용 모니터는 네트워크 트래픽과 대역폭 사용을 감시하는 데 사용된다.  

+ 하이퍼바이저  
이 메커니즘은 가상 서버에 가상 스위치와 외부 네트워크에 대한 접근을 제공 및 운영한다.  

+ 로드 밸런서  
로드 밸런서는 서로 다른 업링크에 걸쳐 네트워크 워크로드를 분배한다.  

+ 논리 네트워크 경계  
논리 네트워크 경계는 각 클라우드 소비자용 대역폭 사용을 보호 및 제한하기 위해 경계를 형성한다.  

+ 자원 복제  
이 메커니즘은 가상 스위치로 추가적인 업링크를 생성하기 위해 사용된다.  

+ 가상 서버  
가상 서버는 가상 스위치를 통해 추가적인 업링크 및 대역폭으로부터 혜택을 받는 IT 자원들을 제공한다.  

<img src="/assets/images/Cloud_Computing/Fig13-20.jpg" width="100%" height="100%"/>
<br/>

### 13.8. 다중 경로 자원 접근 아키텍처
<br/>
특정 IT 자원은 자원이 잇는 정확한 위치로 안내하는 할당 경로(혹은 하이퍼링크)를 통해서만 접근할 수 있다. 이 경로는 손실될 수도 있고, 클라우드 소비자가 잘못 정의하거나 클라우드 제공자에 의해 변경될 수도 있다. 클라우드 소비자가 더 이상 소유하지 않은 하이퍼링크를 가진 IT 자원은 접근할 수 없고 사용할 수 없게 된다. IT 자원의 비가용성으로 인한 예외 상황은 해당 IT 자원에 의존하는 더 큰 규모의 클라우드 솔루션의 안정성에 악영향을 미칠 수 있다.  

<img src="/assets/images/Cloud_Computing/Fig13-21.jpg" width="100%" height="100%"/>
<br/>

다중 자원 접근 아키텍처는 IT 자원으로 연결되는 대안 경로들로 다중 경로 시스템을 형성하여, 클라우드 소비자가 특정 경로에 장애가 생기더라도 프로그램상, 혹은 수동으로 이를 극복할 수 있는 수단을 마련해 준다.  

이 기술 아키텍처는 다중 경로 시스템의 활용과 특정 IT 자원에 할당된 대안적 물리, 혹은 가상 하이퍼링크의 생성을 필요로 한다. 다중 경로 시스템은 서버나 하이퍼바이저 상에 상주하면서 각 대안 경로에도 동일하게 해당 IT 자원이 보이게 해준다.  

<img src="/assets/images/Cloud_Computing/Fig13-22.jpg" width="100%" height="100%"/>
<br/>

이 아키텍처는 다음 메커니즘들을 포함할 수 있다.  

+ 클라우드 스토리지 장치  
클라우드 스토리지 장치는 데이터 접근에 의존하는 솔루션에 지속적으로 접근 가능하도록 하기 위하여 대안 경로를 생성할 필요가 있는 일반적인 IT 자원이다.  

+ 하이퍼바이저  
운영중인 가상 서버들로 연결되는 이중화 링크를 확보하기 위해 하이퍼바이저로 연결되는 대안 경로들이 필요하다.  

+ 논리 네트워크 경계  
이 메커니즘은 동일한 IT 자원에 다중 경로가 생겼을 때도 클라우드 소비자의 개인 정보를 지속적으로 보호한다.  

+ 자원 복제  
자원 복제 메커니즘은 IT 자원의 신규 인스턴스가 대안 경로를 만들기 위해 생성될 필요가 있는 경우 필요하다.  

+ 가상 서버  
이 서버들은 다양한 링크 혹은 가상 스위치를 통해 다중 경로에 대한 접근을 허용하는 IT 자원을 제공한다.  

<img src="/assets/images/Cloud_Computing/Fig13-23.jpg" width="100%" height="100%"/>
<br/>

### 13.9. 영구적 가상 네트워크 구성 아키텍처
<br/>
가상 서버용 네트워크 구성 및 포트 할당은 가상 서버를 운영하는 호스트 물리 서버와 하이퍼바이저상의 가상 스위치 생성 시 진행된다. 이러한 구성 및 할당 작업은 가상 서버와 직결된 호스팅 환경 내 적용되는데, 이는 곧 가상 서버가 다른 호스트로 옮겨가거나 이관되면 목적지의 호스팅 환경에는 필요한 포트 할당과 네트워크 구성 정보가 없기 때문에 네트워크 연결을 잃게 될 것이라는 의미다.  

영구적 가상 네트워크 구성 아키텍처에서는 네트워크 구성 정보가 중앙 집중화된 위치에 저장되고 물리 서버 호스트에 복제된다. 이는 목적지 호스트로 하여금 가상 서버가 한 호스트에서 또 다른 호스트로 옮겨갈 때 구성 정보에 접근할 수 있도록 해준다.  

<img src="/assets/images/Cloud_Computing/Fig13-24.jpg" width="100%" height="100%"/>
<br/>

이 아키텍처로 형성되는 시스템은 중앙 집중형 가상 스위치, VIM, 구성 복제 기술을 포함한다. 중앙 집중형 가상 스위치는 물리 서버 간 공유되며, VIM을 통해 구성된다. VIM은 물리 서버에 구성 설정의 복제 작업을 개시한다.  

<img src="/assets/images/Cloud_Computing/Fig13-25.jpg" width="100%" height="100%"/>
<br/>

가상 서버의 네트워크 설정은 양 물리 서버가 공유하는 중앙 집중형 가상 스위치에 저장되기 때문에 조회할 수 있다. 가상 서버 A는 신규 호스트인 물리 서버 B상에서도 네트워크 연결을 유지한다.  

이관 시스템을 제공하는 이러한 아키텍처를 위한 가상 서버 메커니즘에 덧붙여 다음 메커니즘들이 포함될 수 있다.  

+ 하이퍼바이저  
하이퍼바이저는 물리 호스트 간 복제될 구성 설정을 필요로 하는 가상 서버들을 운영 및 제공한다.  

+ 논리 네트워크 경계  
논리 네트워크 경계는 가상 서버와 IT 자원으로의 접근이 가상 서버가 이관되기 전후에 적절한 클라우드 소비자에게만 한하여 분리되어 허용됨을 보장한다.  

+ 자원 복제   
자원 복제 메커니즘은 중앙 집중형 가상 스위치를 통해 하이퍼바이저 간 가상 스위치 구성 및 네트워크 용량 할당을 복제하는 데 사용된다.  

### 13.10. 가상 서버용 이중화 물리 연결 아키텍처
<br/>
가상 서버는 가상 스위치 업링크 포트를 통해 외부 네트워크와 연결되는데, 이는 곧 업링크에 장애가 발생하면 가상 서버가 외부 네트웤로부터 고립되고 연결이 단절될 것임을 의미한다.  

<img src="/assets/images/Cloud_Computing/Fig13-26.jpg" width="100%" height="100%"/>
<br/>

가상 서버용 이중화 물리 연결 아키텍처는 하나 이상의 이중화 업링크 연결을 맺고 이들을 스탠바이 모드로 둔다. 이 아키텍처는 주 업링크 연결이 사용할 수 없게 되었을 때 이중화 업링크 연결을 사용하여 활성화된 업링크에 연결할 수 있음을 보장한다.  

<img src="/assets/images/Cloud_Computing/Fig13-27.jpg" width="100%" height="100%"/>
<br/>

가상 서버와 사용자들에게 모두 투명한 프로세스로, 주 업링크에 장애가 생기는 즉시 스탠바이 업링크가 자동으로 활성 업링크가 되고 가상 서버들은 새롭게 활성화된 업링크를 사용하여 외부로 패킷을 보낸다.  

보조 NIC은 주 업링크가 활성 상태인 동안에는 가상 서버의 패킷을 받는다 할지라도 트래픽을 전달하지 않는다. 하지만 주 업링크에 장애가 생기면 즉시 보조 업링크가 패킷을 전송하기 시작할 것이다. 장애가 발생한 업링크가 복구되어 정상 운영되기 시작하면 다시 주 업링크가 되고, 보조 NIC은 스탠바이 모드로 돌아간다.  

<img src="/assets/images/Cloud_Computing/Fig13-28.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-29.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-30.jpg" width="100%" height="100%"/>
<br/>

가상 서버 이외에 다음 메커니즘들이 일반적으로 이 아키텍처를 이루는 일부가 된다.  

+ 대체 작동 시스템  
대체 작동 시스템은 사용할 수 없는 업링크를 스탠바이 업링크로 전환한다.  

+ 하이퍼바이저  
이 메커니즘은 가상 서버와 일부 가상 스위치를 운영하며, 가상 서버로의 접근을 포함하는 가상 네트워크 및 가상 스위치를 제공한다.  

+ 논리 네트워크 경계  
논리 네트워크 경계는 각 클라우드 소비자에게 할당되거나 정의된 가상 스위치가 별개로 유지됨을 보장한다.  

+ 자원 복제  
자원 복제는 네트워크 연결을 유지하기 위해 활성 업링크의 현재 상태를 스탠바이 업링크로 복제하는 데 사용된다.  

### 13.11 스토리지 유지보수 기간 관리 아키텍처
<br/>
클라우드 스토리지 장치는 땨로 일시적으로 다운타임을 필요로 하는 유지보수 및 관리 작업의 영향을 받는데, 이는 곧 결과적으로 클라우드 서비스 소비자들과 IT 자원들이 이러한 장치 및 저장된 데이터에 대한 접근을 잃게 됨을 의미한다.  

+ 실시간 스토리지 이관  
실시간 스토리지 이관 프로그램은 LUN 이관 컴포넌트를 활용하여 목적지 복제본이 완벽히 정상적으로 운영됨을 확인할 때까지 원본을 활성화 상태로 유지하면서 안전하게 LUN을 이동시키는 정교한 시스템이다.  

<img src="/assets/images/Cloud_Computing/Fig13-31.jpg" width="100%" height="100%"/>
<br/>

클라우드 자원 관리자는 클라우드 스토리지 장치에 다운타임을 초래하는 미리 계획된 유지보수 작업을 수행하고, 클라우드 서비스 소비자들은 해당 클라우드 스토리지 장치를 사용할 수 없게 된다. 클라우드 소비자들은 미리 다운타임에 대한 공지를 받았기 때문에 해당 기간에 데이터 접근에 시도하지 않는다.  

유지보수 다운타임의 영향을 받게 될 클라우드 스토리지 장치의 데이터는 일시적으로 이중화된 보조 클라우드 스토리지 장치에 옮겨질 수 있다. 스토리지 유지 보수 기간 관리 아키텍처는 클라우드 서비스 소비자가 자동으로, 투명하게 주 스토리지 장치가 다운되었음을 인지하지 못한 채로 보조 클라우드 스토리지 장치로 전용될 수 있도록 한다.  

이 아키텍처는 실시간 스토리지 이관 프로그램을 활용한다.  

<img src="/assets/images/Cloud_Computing/Fig13-32.jpg" width="100%" height="100%"/>
<br/>

클라우드 스토리지 장치에는 유지보수를 위한 다운타임이 계획되어 있지만, 클라우드 소비자는 다운타임에 대한 공지를 받지 못했으며 해당 기간에 클라우드 스토리지 장치에 접근한다.  

<img src="/assets/images/Cloud_Computing/Fig13-33.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-34.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-35.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-36.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig13-37.jpg" width="100%" height="100%"/>
<br/>

이 아키텍처의 중심이라 할 수 있는 클라우드 스토리지 장치 메커니즘에 추가로 자원 복제 메커니즘이 주 스토리지 미치 보조 스토리지 장치를 지속적으로 동기화하는 데 사용된다. 수동 및 자동 대체 작동 역시 이관이 미리 계획도이ㅓ 있다 할지라도 대체 작동 시스템 메커니즘을 통해 이 클라우드 아키텍처에 포함될 수 있다.

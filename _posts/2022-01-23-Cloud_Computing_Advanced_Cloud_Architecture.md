---
title: 심화 클라우드 아키텍처
categories:
- Cloud Computing
feature_text: |
  ## 12. 심화 클라우드 아키텍처
feature_image: "https://picsum.photos/2560/600?image=733"
image: "https://picsum.photos/2560/600?image=733"
---
<style>
	thead td { text-align: center; }
	td { border: 1px solid #444444; }
</style>

### 12.1. 하이퍼바이저 클러스터링 아키텍처
<br/>
하이퍼바이저는 다중 가상 서버를 생성 및 제공할 책임이 있을 수 있다. 이러한 의존성 때문에 하이퍼바이저에 영향을 미치는 장애 조건들은 하이퍼바이저상의 가상 서버에도 파급효과를 미칠 수 있다.  

<img src="/assets/images/Cloud_Computing/Fig12-01.jpg" width="100%" height="100%"/>
<br/>

+ 핫빗(heartbeats)  
하이퍼바이저간, 하이퍼바이저간 가상 서버간, 하이퍼바이저와 VIM간 교환되는 시스템 수준의 메시지다.  

하이퍼바이저 클러스터링 아키텍처는 다중 물리 서버간 하이퍼바이저의 고가용성 클러스터를 형성한다. 주어진 하이퍼바이저나 기저 물리 서버가 사용할 수 없게 되면, 제공되는 가상 서버는 런타임 동작을 유지하기 위해 이외의 물리 서버나 하이퍼바이저로 이관될 수 있다.  

<img src="/assets/images/Cloud_Computing/Fig12-02.jpg" width="100%" height="100%"/>
<br/>

하이퍼바이저 클러스터는 하이퍼바이저가 정상으로 운영되고 있음을 확인하기 위해 정기적으로 핫빗 메시지를 보내는 중앙 VIM을 통해 통제된다. 응답을 받지 못한 핫빗 메시지는 VIM으로 하여금 동적으로 영향력 하에 있는 가상 서버들을 신규 호스트로 이관하기 위해 실시간 VM 이관 프로그램을 초기화하도록 한다.  

하이퍼바이저 클러스터는 실시간으로 가상 서버를 이관하기 위해 공유된 클라우드 스토리지 장치를 사용한다.  

+ 실시간 VM 이관  
실시간 VM 이관(Live VM Migration)이란 가상 서버나 가상 서버 인스턴스를 런타임에 재배치할 수 있는 시스템이다.  

<img src="/assets/images/Cloud_Computing/Fig12-03.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-04.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-05.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-06.jpg" width="100%" height="100%"/>
<br/>

클러스터 환경 하에서 보호되는 이러한 아키텍처 모델과 가상 서버들의 핵심은 하이퍼바이저와 자원 클러스터 메커니즘이며, 이에 덧붙여 다음 메커니즘들이 포함될 수 있다.  

+ 논리 네트워크 경계  
이 메커니즘에 의해 생성된 논리적 경계는 타 클라우드 소비자의 하이퍼바이저가 우연히 주어진 클러스터 내에 포함되지 않도록 보장한다.  

+ 자원 복제  
동일한 클러스터 내 하이퍼바이저들은 서로 그들의 상태와 가용성에 대한 정보를 주고받는다. 가상 스위치의 생서이나 삭제와 같은 클러스터 내 발생한 변경 사항에 대한 갱신 정보는 VIM을 통해 모든 하이퍼바이저에 복제될 필요가 있다.  

### 12.2. 부하 분산 가상 서버 인스턴스 아키텍처
<br/>
운영 및 관리 작업이 분리된 물리 서버들간에 교차 서버 워크로드가 균등하게 분산되도록 유지하는 일은 까다로울 수 있다. 특정 물리 서버가 이웃한 물리 서버들보다 더 많은 가상 서버를 제공하거나, 더 많은 워크로드를 분배받는 결과가 쉽사리 이루어질 수 있다. 활용도가 적정 수준을 초과하거나 이에 미달하는 물리 서버들은 시간이 지남에 따라 엄청나게 많아질 수 있는데, 이는 초과 활용되는 서버들에게는 실존하는 성능 문제를, 활용도가 수준 이하인 서버들의 경우네는 잠재적인 처리 용량의 손실로 지속적인 낭비를 초래할 수 있다.  

<img src="/assets/images/Cloud_Computing/Fig12-07.jpg" width="100%" height="100%"/>
<br/>

부하 분산 가상 서버 인스턴스 아키텍처는 가용 물리 서버 호스트 간 처리를 분산시키기 전에 동적으로 가상 서버 인스턴스와 관련 워크로드를 계산하는 용량 관제 시스템을 형성한다.  

<img src="/assets/images/Cloud_Computing/Fig12-08.jpg" width="100%" height="100%"/>
<br/>

용량 관제 시스템은 용량 관제 클라우드 사용 모니터, 실시간 VM 이관 프로그램, 용량 계획 도구로 구성된다. 용량 관제 모니터는 물리 및 가상 서버 사용량을 추적하고 가상 서버 용량 요구사항에 대하여 동적으로 물리 서버 컴퓨팅 자원의 용량을 계산할 책임이 있는 용량 계획 두구에 의미 있는 변동 사항을 보고한다. 용량 계획 도구가 워크로드를 분배하기 위해 가상 서버를 또 다른 호스트로 이관하도록 결정하면 실시간 VM 이관 프로그램에 가상 서버를 이관하도록 신호를 보낸다.  

<img src="/assets/images/Cloud_Computing/Fig12-09.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-10.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-11.jpg" width="100%" height="100%"/>
<br/>

이러한 아키텍처를 구성하는 데 있어 하이퍼바이저, 자원 클러스터링, 가상 서버, (욜량 관제)클라우드 사용 모니터에 추가로 다음과 같은 메커니즘들이 포함될 수 있다.  

+ 자동 확장 리스너  
자동 확장 리스너는 부하 분산 프로세스를 초기화하고 하이퍼바이저를 통해 가상 서버들에게 오는 워크로드를 동적으로 감시하는 데 사용될 것이다.  

+ 로드 밸런서  
로드 밸런서 메커니즘은 하이퍼바이저 간 가상 서버들의 워크로드를 분배할 책임이 있다.  

+ 논리 네트워크 경계  
논리 네트워크 경계는 주어진 재배치된 가상 서버의 목적지가 SLA와 개인 정보 보호 규제를 준수함을 보장한다.  

+ 자원 복제  
가상 서버 인스턴스의 복제가 부하 분산 기능의 일부로서 필요하게 될 것이다.  

### 12.3. 무중단 서비스 재배치 아키텍처  
<br/>
클라우드 서비스는 다음과 같은 다양한 이유로 사용할 수 없게 될 수 있다.  

+ 런타임 사용 수요가 처리 용량을 초과하는 경우
+ 일시적인 다운 타임을 필수적으로 요구하는 유지 보수 갱신 작업
+ 신규 물리 서버 호스트로의 영구적인 이관  

클라우드 서비스 소비자 요청은 클라우드 서비스가 사용 불가능해지는 경우 보통 처리되지 않는데, 이는 곧 잠재적으로 예외적인 상황을 초래할 수 있다. 다운 타임이 계획되어 있을 때 조차도 클라우드 서비스를 일시적으로 사용할 수 없다고 표시하는 것은 피해야 한다.  

무중단 서비스 재배치 아키텍처는 선 정의된 이벤트 발생 시 자동으로 런타임에 클라우드 서비스 구현체르르 복제 또는 이관하여 다운 타임을 방지하는 시스템을 확립한다. 이중화된 구현체를 활용하여 클라우드 서비스를 확장하거나 축소하는 대신, 일시적으로 신규 호스트에 복제된 구현체를 추가해서 클라우드 서비스 활동이 런타임에 나머지 호스팅 환경으로 우회될 수 있다. 유사하게 원본 구현체에서 계획된 다운 타임이 필요한 유지 보수 작업을 진행해야 할 경우 클라우드 서비스 소비자 요청을 일지시적으로 복제된 구현체로 재전송할 수 있다. 클라우드 서비스 구현체와 클라우드 서비스 활동의 재배치는 신규 물리 서버 호스트로 클라욷 서비스의 이관을 수용하기 위해 영구적이 될 수도 있다.  

기저 아키텍처의 핵심적인 특성은 기존의 원본 클라우드 서비스 구현체가 비활성화되거나 제거되기 전에 신규 클라우드 서비스 구현체가 클라우드 서비스 소비자의 요청을 성공적으로 수용 및 대응할 수 있도록 보장한다는 것이다. 일반적인 접근법은 클라우드 서비스를 제공하고 있는 전체 가상 서버 인스턴스를 실시간 VM 이관 프로그램이 이관하도록 하는 것이다. 자동 확장 리스너와 로드 밸런서 메커니즘이 확장 및 워크로드 분배 요구사항에 대응하여 일시적으로 클라우드 서비스 소비자 요청의 재전송을 개시하는 데 사용될 수 있다. 두 메커니즘 모두 실시간 VM 이관 프로세스를 개시하기 위해 VIM에 접근할 수 있다.  

<img src="/assets/images/Cloud_Computing/Fig12-12.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-13.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-14.jpg" width="100%" height="100%"/>
<br/>

가상 서버의 이관은 가상 서버 디스크의 위치와 구성에 따라 다음 두 가지 방식 중 한 가지로 진행할 수 있다.  

+ 가상 서버 디스크가 로컬 스토리지 장치에 저장되거나, 비공유 원격 스토리지 장치가 소스 호스트에 장착되어 있는 경우에는 가상 서버 디스크의 복제본이 목적지 호스트상에 생성된다. 복제본이 생성되고 나면, 양 가상 서버 인스턴스는 동기화되고 가상 서버 파일이 기존 호스트에서 삭제된다.  

+ 가상 서버의 파일들이 기존 및 목적지 호스트 간 공유된 원격 스토리지 장치에 저장되는 경우 가상 서버 디스크를 복제할 필요는 없다. 단순히 가상 서버의 소유권이 기존에서 목적지 물리 서버 호스트로 이전되고, 가상 서버의 상태가 자동으로 동기화된다.  

이 아키텍처는 클라우드 서비스 소비자와의 연결을 유지하기 위해 이관된 가상 서버의 기존에 정으된 네트워크 구성 환경이 보존되도록 하는 영구적인 가상 네트워크 구성 아키텍처의 지원을 받을 수 있다.  

자동 확장 리스너, 로드 밸런서, 클라우드 스토리지 장치, 하이퍼바이저, 가상 서버 이외에 기타 다음을 포함한 메커니즘들 역시 이 아키텍처의 일부를 이룰 수 있다.  

+ 클라우드 사용 모니터  
여러 종류의 클라우드 사용 모니터가 지속적으로 IT 자원 사용량과 시스템 활동을 추적하는 데 사용될 수 있다.  

+ 사용량당 과금 모니터  
사용량당 과금 모니터는 기존 및 목적지 양 위치의 IT 자원에 대한 서비스 사용 비용을 계산하기 위해 데이터를 수집하는 데 사용된다.  

+ 자원 복제  
자원 복제 메커니즘은 목적지의 클라우드 서비스 새도우 복제본을 인스턴스화하는 데 사용된다.  

+ SLA 관리 시스템  
이 관리 시스템은 클라우드 서비스 복제나 재배치 도중 및 이후에 클라우드 서비스의 가용성을 보장하기 위해 SLA 모니터에 의해 제공되는 SLA 데이터를 처리할 책임이 있다.  

+ SLA 모니터  
이 모니터링 메커니즘은 가용성 보장을 위해 이 아키텍처에 의존하고 있는 경우 연관성이 있을 수 있는 SLA 관리 시스템이 필요로 하는 SLA 정보를 수집한다.  

무중단 서비스 재배치 기술 아키텍처는 직접 I/O 접근 아키텍처와 충돌하며 양립할 수 없다. 직접 I/O 접근이 적용된 가상 서버는 해당 물리 서버에 고정되며 이러한 방식으로 다른 호스트에 이관될 수 없다.  

### 12.4. 무정지 아키텍처
<br/>
물리 서버는 자연스럽게 자신이 제공하는 가상 서버에 대한 단일 장애점으로 작용한다. 그 결과, 물리 서버가 중단되거나 장애가 발생하는 경우 해당 물리 서버가 제공하는 가상 서버의 가용성은 영향을 받을 수 있다. 이 점이 클라우드 제공자가 소비자에게 무정지 서비스를 보장하는 일을 어렵게 만든다.  

무정지 아키텍처는 기존 물리 서버호스트가 중단되는 이벤트가 발생했을 때, 다른 무리 서버호스트로 가상서버들을 동적으로 이관할 수 있도록 정교한 대체 작동 시스템(failover system)을 확립한다.  

<img src="/assets/images/Cloud_Computing/Fig12-15.jpg" width="100%" height="100%"/>
<br/>

여러 물리 서버들이 하나의 그룹을 이루어 중단 없이 한 물리 서버에서 다른 물리 서버로 전환하는 활동을 수행할 수 있는 장애 방지 시스템을 통해 통제된다. 실시간 VM 이관 컴포넌트는 일반적으로 이런 형태의 고가용성 클라우드 아키텍처의 핵심적인 부분이 된다.  

그 결과 장애 방지 시스템은 물리 서버가 중단되는 경우 해당 서버에서 제공되던 가상 서버들이 보조 물리 서버로 이관될 것을 보장한다. 모든 가상 서버들은 공유된 스토리지 볼륨(영구적인 가상 네트워크 구성 아키텍처에 따라)에 저장되어 같은 그룹 내에 속한 다른 물리 서버들이 관련 파일에 접근할 수 있다.  

대체 작동 시스템, 클라우드 스토리지 장치, 가상 서버 메커니즘 이외에 다음과 같은 메커니즘들이 이 아키텍처의 일부가 될 수 있다.  

+ 감사 모니터  
이 메커니즘은 가상 서버의 재배치로 인해 제공되던 데이터들 역시 금지된 영역에 재배치되는지의 여부를 확인할 필요가 있을 수 있다.  

+ 클라우드 감사 모니터  
이 메커니즘의 다양한 변형은 가상 서버 용량이 초과되지 않도록 보장하기 위해 클라우드 소바자의 실제 IT 자원 사용량을 감시하는 데 사용된다.  

+ 하이퍼바이저  
장애의 영향을 받는 물리 서버의 하이퍼바이저가 물리 서버의 장애에 영향을 받는 가상 서버들을 제공한다.  

+ 논리 네트워크 경계  
논리 네트워크 경계가 가상 서버의 재배치 이후 각 클라우드 소비자가 고유한 논리적 경계 내에 머물러 있도록 보장하는 데 필요한 분리 메커니즘을 제공 및 유지한다.  

+ 자원 클러스터  
자원 클러스터 메커니즘은 가상 서버가 제공하는 IT 자원의 가용성을 협력 하에 개선하는 다양한 종류의 액티브-액티브 클러스터 그룹을 생성하는 데 적용된다.  

+ 자원 복제  
이 메커니즘은 주 가상 서버가 중단될 때 신규 가상 서버와 클라우드 서비스 인스턴스를 생성할 수 있다.  

### 12.5. 클라우드 밸런싱 아키텍처
<br/>
IT 자원은 다중 클라우드간에 걸쳐서도 부하가 분산될 수 있다. 클라우드 서비스 소비자의 요청을 다중 클라우드 간 교차 분산시키는 것에는 다음과 같은 장점이 있다.  

+ IT 자원의 성능과 확장성을 개선한다.
+ IT 자원의 가용성과 신뢰성을 향상한다.
+ 부하 분산과 IT 자원 최적화를 개선한다.  

클라우드 밸런싱 기능은 주로 자동 확장 리스너와 대체 작동 시스템 메커니즘의 조합에 기반하고 있다. 더 많은 컴포넌트들이(그리고 가능한 다른 메커니즘들도) 전체적인 클라우드 밸런싱 아키텍처의 일부가 될 수 있다.  

<img src="/assets/images/Cloud_Computing/Fig12-16.jpg" width="100%" height="100%"/>
<br/>

시작점으로 다음과 같은 두 가지 메커니즘이 활용된다.  

+ 자동 확장 리스너는 현재의 확장 및 성능 요구사항을 토대로 여러 이중화 IT 자원 구현체 중 하나에 클라우드 서비스 소비자 요청을 전용한다.  
+ 대체 작동 시스템은 이중화 IT 자원들이 특정 IT 자원이나 기저 호스팅 환경 내의 장애 이벤트 발생 시 클라우드 간 교차 대체 작동이 가능함을 보장해준다. IT 자원의 장애가 보고되면 자동 확장 리스너가 우연히 클라우드 서비스 소비자의 요청을 사용할 수 없거나 불안정한 IT 자원으로 라우팅하는 것을 방지해준다. 클라우드 밸런싱 아키텍처가 효율적으로 작동하려면, 자동 확장 리스너는 클라우드 밸런싱을 이루는 아키텍처 범위 내의 모든 이중화 IT 자원 구현체를 인지하고 있을 필요가 있다. 다중 클라우드에 걸친 IT 자원 구현체들 간 수동 동기화가 불가능할 경우, 자원 복제 메커니즘이 동기화를 자동화하기 위해 포함될 필요가 있을 수 있다.  

### 12.6. 자원 예약 아키텍처
<br/>
IT 자원이 공유된 활용을 위해 설계된 방식과 용량의 가용 수준에 따라 동시 사용자 접근이 자원 제약이라 불리는 런타임 예외 상황에 도달할 수 있다. 자원 제약이란 둘 이상의 클라우드 소비자들이 그들의 총 처리 요구사항을 수용할 수 없는 용량을 가진 IT 자원을 공유하도록 할당된 경우 발생하는 상태다. 그 결과, 하나 이상의 클라우드 소비자들이 성능 저하를 경험하거나 모두 처리 거부 상태가 될 수 있다. 클라우드 서비스 자체가 중단되어 모든 클라우드 소비자들의 요청이 거부되는 결과에 이를 수도 있다. 특정 IT 자원(특히 공유를 받아들이도록 특별히 설계되지 않은 것)이 다른 클라우드 서비스 소비자에 의해 동시에 접근될 경우에 기타 다른 종류의 런타임 충돌이 일어날 수 있다. 예를 들어 중첩 및 형제 자원 풀은 하나의 풀이 임시로 다른 풀로부터 IT 자원을 빌려 쓸 수 있는 자원 임대의 개념을 도입한다. 런타임 충돌은 빌런 IT 자원을 해당 자원을 빌려 쓰고 있는 클라우드 서비스 소비자가 오랫동안 점유해서 반납하지 않는 경우 발생할 수 있다. 이는 불가피하게 자원 제약의 발생으로 이어질 수 있다.  

자원 예약 아키텍처는 주어진 클라우드 소비자를 위해 다음 항목들 중 하나를 전용으로 예비해두는 시스템을 형성한다.  

+ 단일 IT 자원
+ IT 자원의 일부
+ 다중 IT 자원들  

이는 클라우드 소비자들이 서로 앞서 언급한 자원 제약 및 자원 임대 상황에 빠지는 것을 방지하도록 보호한다.  

<img src="/assets/images/Cloud_Computing/Fig12-17.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-18.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-19.jpg" width="100%" height="100%"/>
<br/>

IT 자원 예약 시스템의 생성은 각 독립된 IT 자원과 자원 풀에 대한 사용 임계치를 정의하는데 사용되는 자원 관리 시스템 메커니즘을 포함하도록 요구할 수 있다. 자원 예약약은 해당 풀의 IT 자원을 여전히 공유 및 임대용으로 사용할 수 있도록 균형을 이루면서도 각 풀이 유지해야 할 필요가 있는 IT 자원의 양을 고정하여 잠금 장치를 해 둔다. 원격 관리 시스템 메커니즘 또한 클라우드 소비자가 예약한 IT 자원의 할당을 관리하기 위해 관리 통제권을 가질 수 있도록 프론트 엔드 맞춤 제공을 위해 활용된다.  

이 아키텍처 내에서 일반적으로 예약되는 메커니즘의 종류는 클라우드 스토리지 장치와 가상 서버들이다. 아래를 포함한 다른 메커니즘들 또한 이 아키텍처의 일부를 이룰 수도 있다.  

+ 감사 모니터  
감사 모니터는 자원 예약 시스템들이 클라우드 소비자 감사, 개인 정보 보호, 기타 규제 요구사항을 준사하는지 여부를 확인하는 데 사용된다. 예를 들어 예약된 IT 자원의 지리적 위치를 추적할 수 있다.  

+ 클라우드 사용 모니터  
클라우드 사용 모니터는 예약된 IT 자원의 할당을 유발하는 임계치를 관제할 수 있다.  

+ 하이퍼바이저  
하이퍼바이저 메커니즘은 여러 클라우드 소비자들이 예약한 IT 자원을 그대로 정확히 할당받을 수 있도록 보장하기 위해 예약 설정을 적용할 수 있다.  

+ 논리 네트워크 경계  
이 메커니즘은 예약된 IT 자원이 각 클라우드 소비자에게 전용으로 쓰일 수 있도록 보장하는 데 필요한 경계를 정립한다.  

+ 자원 복제  
이 컴포넌트는 편의상 신규 IT 자원 인스턴스를 복제 및 프로비저닝하기 위해 IT 자원 소비에 대한 각 클라우드 소비자의 한계점에 대한 정보를 지속적으로 제공받을 필요가 있다.  

### 12.7. 동적 장애 감지 및 복구 아키텍처
<br/>
클라우드 기반 환경은 수많은 클라우드 소비자가 동시 다발적으로 접근하는 방대한 양의 IT 자원들로 구성될 수 있다. 이들 IT 자원들 중 어떤 것이라도 해결을 위해 인간의 수동적 개입 이상이 필요한 장애 상황에 처할 수 있다. IT 자원의 장애를 수동으로 관리 및 해결하는 것은 일반적으로 비효율적이고 비실용적인 일이다.  

동적 장애 감지 및 복구 시스템은 광범위한 선 정의된 장애 시나리오를 감시 및 대응하는 자동 복구 능력이 있는 관제 시스템을 확립한다. 이 시스템은 직접 자동으로 해결할 수 없는 장애 상황이 발생하면 통지 및 에스컬레이션한다. 이는 적극적으로 활발하게 IT 자원을 추적하고 선 정의된 이벤트에 대응하여 선 정의된 활동을 실행하는 지능형 관제 모니터라 불리는 특화된 클라우드 사용 모니터에 의존한다.  

<img src="/assets/images/Cloud_Computing/Fig12-20.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-21.jpg" width="100%" height="100%"/>
<br/>

자동 복구 관제 시스템은 다음과 같은 다섯 가지의 핵심 기능을 수행한다.  

+ 관찰 및 감시
+ 이벤트 발생 시 대응 행위를 결정
+ 이벤트 발생 시 대응 행위를 수행
+ 보고
+ 에스컬레이션  

장애 상황이 발생한 경우 지능형 관제 모니터가 취해야 할 필요가 있는 단계적 행동을 결정하기 위해 순차적인 복구 정책이 각 IT 자원용으로 정의될 수 있다. 예를 들어 복구 정책은 장애 통지를 발행하기 이전에 한 가지 복구 시도가 자동으로 수행될 필요가 있음을 명시할 수 있다.  

<img src="/assets/images/Cloud_Computing/Fig12-22.jpg" width="100%" height="100%"/>
<br/>

일반적으로 지능형 관제 모니터가 특정 이슈를 에스컬레이션할 때는 다음과 같은 일련의 활동을 수행한다.  

+ 배치 파일 실행
+ 콘솔 메시지 전송
+ 텍스트 메시지 전송
+ 이메일 메시지 전송
+ SNMP 트랩 전송
+ 티켓 로깅  

지능형 관제 모니터로 활용될 수 있는 프로그램과 제품은 다양한 것들이 있다. 대부분 표준 티켓 및 이벤트 관리 시스템과 통합될 수 있다. 이 아키텍처 모델은 다음 메커니즘들을 더 포함할 수 있다.  

+ 감사 모니터  
이 메커니즘은 데이터 복구가 법적, 정책적 요구사항을 준수하여 수행되는지의 여부를 추적하는 데 사용된다.  

+ 대체 작동 시스템  
대체 작동 시스템 메커니즘은 보통 장애가 발생한 IT 자원을 복구하기 위한 초기 시도를 실행하는 도중에 사용된다.  

+ SLA 관리 시스템 및 SLA 모니터  
이 아키텍처를 적용해서 성공적으로 수행되는 기능은 SLA 보장 요건을 밀접한 관련이 있기 때문에, 해당 시스템은 일반적으로 이러한 메커니즘들에 의해 관리 및 처리되는 정보에 의존하게 된다.  

### 12.8. 베어 메탈 프로비저닝 아키텍처
<br/>
원격으로 서버를 프로비저닝하는 일은 보통 대부분의 물리 서버 운영체제에 원격 관리 시스템 소프트웨어가 본래 탑재되어 있으므로 일반적인 일이다. 그렇지만 베어 메탈 서버, 즉 미리 운영체제나 기타 다른 소프트웨어가 설치되지 않은 물리 서베에는 이와 같이 전형적인 원격 관리 프로그램에 접근하는 것 자체가 불가능하다. 대부분의 현대적인 물리 서버들은 원격 관리 지원을 해당 서버의 ROM에 설치하는 옵션을 제공한다. 어떤 업체들은 확장 카드를 통해서 제공하고, 기타 업체들은 미리 칩셋에 통합해 둔다. 베어 메탈 프로비저닝 어키텍처는 효과적으로 전체 운영체제를 원격으로 식별 및 프로비저닝하는 데 사용되는 특화된 서비스 에이전트를 통해 이 기능을 활용하는 시스템을 형성한다.  

서버의 ROM에 통합된 원격 관리 소프트웨어는 서버가 시작할 때 사용 가능한 상태가 된다. 원격 관리 시스템을 통해 제공되는 포털과 같은 웹 기반, 혹은 사설 사용자 인터페이스가 일반적으로 물리 서버의 원시적(native) 원격 관리 인터페이스에 연결하는 데 사용된다. 원격 관리 인터페이스의 IP 주소는 기본 IP를 통해 수동으로 구성되거나, DHCP 서비스의 구성을 통해 대체 설정될 수 있다. IaaS 플랫폼 내 IP 주소는 클라우드 소비자들에게 직접적으로 전달되어 클라우드 소비자들이 독립적으로 베어 메탈 운영체제 설치를 수행할 수 있도록 할 수 있다. 원격 관리 소프트웨어가 물리 서버 콘솔에의 연결과 운영체제의 배치를 가능케 하는 데 사용되기는 하지만, 이러한 활용에 있어 두 가지 일반적인 고려 사항이 있다.  

+ 여러 서버에 수동으로 배치하는 것은 우연한 인적 및 구성 오류에 취약할 수 있다.  
+ 원격 관리 소프트웨어는 시간 소모적일 수 있고, 상당한 런타임 IT 자원 처리를 요구할 수 있다.  

베어 메탈 프로비저닝 시스템은 다음과 같은 컴포넌트를 사용하여 이러한 이슈를 해결한다.  

+ 식별 에이전트  
클라우드 소비자에게 할당될 가용한 물리 서버를 탐색하고 발견하는 모니터링 에이전트의 한 종류  

+ 배치 에이전트  
베어 메탈 프로비저닝 배치 시스템의 클라이언트로서 물리 서버의 메모리에 설치되는 관리 에이전트  

+ 식별 영역  
네트워크를 정밀 검사하고 연결할 가용한 무릴 서버의 위치를 확인하는 소프트웨어 컴포넌트  

+ 관리 로더  
물리 서버에 연결하고 클라우드 소비자용 관리 옵션을 로드하는 컴포넌트  

+ 배치 컴포넌트  
선택된 물리 서버 위에 운영체제를 설치할 책임이 있는 컴포넌트  

베어 메탈 프로비저닝 시스템은 클라우드 소비자가 배치 소프트웨어에 연결하여 동시에 하나 이상의 서버나 운영체제를 프로비저닝할 수 있도록 하는 자동 배치 기능을 제공한다. 중앙 배치 시스템은 관리 인터페이스를 통해 서버로 연결하며, 물리 서버의 RAM 안에 업로드하고 에이전트로서 동작하는 데 동일한 프로토콜을 사용한다. 베어 메탈 서버는 이제 관리 에이전트가 설치된 저 수준 클라이언트가 되고, 배치 소프트웨어는 운영체제를 배치하기 위해 필요한 설정 파일들을 업로드한다. 이 기능을 확장 제공하기 위해 지능형 자동화 엔진과 셀프 서비스 포털을 통해 배치 이미지, 운영체제 배치 자동화, 의도하지 않은 배치 및 설치 후 구성 스크립트가 활용될 수 있다. 다음과 같은 추가적인 메커니즘이 이 아키텍처의 일부가 될 수 있다.  

+ 클라우드 스토리지 장치  
이 메커니즘은 운영체제 템플릿과 설치 파일을 저장할 뿐 아니라, 프로비저닝 시스템을 위해 배치 에이전트와 배치 패키지를 저장한다.  

+ 하이퍼바이저  
운영체제 프로비저닝의 일부로서 물리 서버상에 하이퍼바이저를 배치하는 것이 필요할 수 있다.  

+ 논리 네트워크 경계  
논리 네트워크 경계가 저 수준의 물리 서버들에 인가받은 클라우드 소비자만 접근할 수 있도록 보장한다.  

+ 자원 복제  
이 메커니즘은 프로비저닝 도중 혹은 이후에 하이퍼바이저 워크로드를 분배하기 위해 물리 서버상에 신규 하이퍼바이저를 배치해서 IT 자원 복제용으로 구현된다.  

+ SLA 관리 시스템  
이 관리 시스템은 물리 베어 메탈 서버의 가용성이 선 정의된 SLA 조항을 따른다는 것을 보장한다.  

<img src="/assets/images/Cloud_Computing/Fig12-23.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-24.jpg" width="100%" height="100%"/>
<br/>

### 12.9. 신속한 프로비저닝 아키텍처  
<br/>
전형적인 프로비저닝 프로세스는 기존에는 관습적으로 관리자와 기술 전문가들이 수동으로 완료하던 다수의 작업들, 즉 미리 패키징된 명세나 고객 맞춤 요구사항에 따라 필요한 IT 자원을 준비하는 작업을 포함할 수 있다. 더 많은 수의 고객을 대상으로 서비스하고, 평균적으로 고객들이 더 많은 양의 IT 자원을 요청하는 클라우드 환경에서 수동 프로비저닝 프로세스는 부적절하며, 나아가 인적 오류 및 비효율적인 대응 시간으로 인해 비합리적인 위험을 감당하게 만들 수 있다.  

예를 들어 여러 애플리케이션이 수행되는 25개의 윈도우 서버들을 설치 및 구성, 갱신하도록 요청하는 클라우드 소비자가 애플리케이션의 절반은 동일한 설치 구성으로 이루어져야 하고, 나머지 절반은 맞춤 제공되어야 한다고 요구한다 가정하자. 각 운영체제 배치는 최대 30분이 소요되고, 서버 재시작이 필요한 보안 패치와 운영체제 업데이트를 위해 추가적인 시간이 어느 정도 필요하다. 이제 본격적으로 애플리케이션이 배치 및 구성되어야 하는데, 수동 혹은 반자동화된 접근법은 막대한 시간을 필요로 하며, 각각 설치하는 도중에 인적 사고의 발생 확률을 증가시킨다.  

신속한 프로비저닝 아키텍처는 독자적으로, 혹은 협력 하에 광범위한 IT 자원의 프로비저닝을 자동화하는 시스템을 형성한다. 신속한 IT 자원 프로비저닝의 기저에 있는 기술적 아키텍처는 정교하고 복잡할 수 있고, 자동화 프로비저닝 프로그램, 신속한 프로비저닝 엔진, 온디맨드 포르비저닝용 스크립트와 템플릿으로 구성된 시스템에 의존할 수 있다.  

시스템은 많은 추가적인 아키텍처의 산물이 IT 자원 프로비저닝의 다양한 측면을 조정 및 자동화하는 데 사용될 수 있는데, 예는 다음과 같다.  

+ 서버 템플릿  
신규 가상 서버의 인스턴스화를 자동화하는 데 사용되는 가상 이미지 파일의 템플릿  

+ 서버 이미지  
이 이미지들은 가상 서버 템플릿과 유사하지만 물리 서버를 프로비저닝하는 데 사용된다.  

+ 애플리케이션 패키지  
자동 배치용으로 패키징된 애플리케이션 및 기타 소프트웨어의 모음  

+ 맞춤 스크립트  
지능형 자동화 엔진의 일부로 관리 작업을 자동화하는 스크립트  

+ 순서 배열 관리자  
자동화 프로비저닝 작업의 순서를 조직화하는 프로그램  

+ 순서 이력 기록 장치  
자동화 프로비저닝 작업 순서의 실행 이력을 기록하는 컴포넌트  

+ 운영체제 베이스라인  
운영체제가 설치된 후에 이를 신속하게 활용 가능하도록 준비하기 위해 적용되는 구성 템플릿  

+ 애플리케이션 구성 베이스라인  
신규 애플리케이션 사용 준비에 필요한 설정 및 환경 변수가 포함된 구성 템플릿  

+ 배치 데이터 스토어  
가상 아미지, 템플릿, 스크립트, 베이스라인 구성 및 기타 관련 데이터를 저장하는 저장소  

<img src="/assets/images/Cloud_Computing/Fig12-25.jpg" width="100%" height="100%"/>
<br/>

다음 단계별 설명은 앞서 언급된 다수의 시스템 컴포넌트들을 포함하여 신속한 프로비저닝 엔진의 내부 원리를 묘사한다.  

1. 클라우드 소비자가 셀프 서비스 포털을 통해 신규 서버를 요청한다.  
2. 순서 배열 관리자가 운영체제의 준비를 위해 해당 요청을 배치 엔진으로 전달한다.  
3. 배치 엔진은 해당 요청이 가상 서버용인 경우 프로비저닝을 위해 가상 서버 템플릿을 사용한다. 가상 서버용이 아닌 경우, 배치 엔진은 물리 서버를 프로비저닝하도록 요청을 보낸다.  
4. 사용할 수 있는 경우, 요청된 종류의 운영체제용으로 제작된 선 정의된 이미지가 운영체제의 프로비저닝에 사용된다. 선 정의된 이미지를 사용할 수 없는 경우에는 운영체제를 설치하기 위해 정규 배치 프로세스가 수행된다.  
5. 핫빗은 하이퍼바이저 간, 하이버바이저와 가상 서버 간, 하이퍼바이저와 VIM 간 교환되는 시스템 수준의 메시지다. 배치 엔진은 순서 배열 관리자에게 운영체제의 사용 준비가 완료되면 이를 통지한다.  
6. 순서 배열 관리자는 저장을 위해 순서 이력 기록 장치에 로그를 갱신 및 전송한다.  
7. 순서 배열 관리자는 배치 엔진이 프로비저닝된 운영체제에 운영체제 베이스라인을 적용하도록 요청한다.  
8. 배치 엔진은 요청된 운영체제 베이스라인을 적용한다.  
9. 배치 엔진은 순서 배열 관리자에게 운영체제 베이스라인이 적용되었음을 알린다.  
10. 순서 배열 관리자는 저장을 위해 순서 이력 기록 장치에 완료된 단계의 로그를 갱신 및 전송한다.  
11. 순서 배열 관리자는 배치 엔진이 애플리케이션을 설치하도록 요청한다.  
12. 배치 엔진은 해당 애플리케이션을 프로비저닝된 서버에 배치한다.  
13. 배치 엔진은 순서 배열 관리자에게 애플리케이션이 설치되었음을 알린다.  
14. 순서 배열 관리자는 저장을 위해 순서 이력 기록 장치에 완료된 단계의 로그를 갱신 및 전송한다.  
15. 순서 배열 관리자는 배치 엔진이 애플리케이션 구성 베이스라인을 적용하도록 요청한다.  
16. 배치 엔진은 해당 구성 베이스라인을 적용한다.  
17. 배치 엔진은 구성 베이스라인이 적용되었음을 순서 배열 관리자에게 알린다.  
18. 순서 배열 관리자는 저장을 위해 순서 이력 기록 장치에 완료된 단계의 로그를 갱신 및 전송한다.  

클라우드 스토리지 장치 메커니즘이 애플리케이션 배이스라인 정보, 템플릿, 스크립트의 저장을 위해 사용되며, 이때 하이퍼바이저는 자체 프로비저닝되거나 기타 다른 프로비저닝된 IT 자원을 제공하는 가상 서버들을 신속하게 생성, 배치, 운영한다. 자원 복제 메커니즘은 일반적으로 신속한 프로비저닝 요구사항에 대응하여 복제된 IT 자원의 인스턴스를 생성하는데 사용된다.  

### 12.10. 스토리지 워크로드 관리 아키텍처  
<br/>
사용 수준이 적정 수준을 초과한 클라우드 스토리지 장치는 스토리지 컨트롤러상의 워크로드를 증가시키고, 다양한 성능 이슈를 야기할 수 있다. 역으로 사용 수준이 미달된 클라우드 스토리지 장치는 잠재적인 처리 및 저장 용량의 손실로 인해 낭비가 심하다.  

스토리지 워크로드 관리 아키텍처는 쓸 수 있는 클라우드 스토리지 장치에 걸쳐 LUN이 균등하게 분배되도록 하며, 동시에 스토리지 용량 시스템이 확립되어 런타임 워크로드가 여러 LUN에 걸쳐 균등하게 분배되도록 보장한다.  

+ LUN 이관  
LUN 이관은 하나의 스토리지 장치에서 다른 스토리지 장치로 LUN을 중단 없이, 또 클라우드 소비자에게는 투명하게 이관하는 데 사용되는 특화된 스토리지 프로그램이다.  

<img src="/assets/images/Cloud_Computing/Fig12-26.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-27.jpg" width="100%" height="100%"/>
<br/>

클라우드 스토리지 장치를 하나의 그룹으로 묶으면 LUN 데이터가 사용할 수 있는 스토리지 호스트에 균등하게 분배될 수 있게 된다. 스토리지 관리 시스템이 구성되고 자동 확장 리스너가 그룹화된 클라우드 스토리지 장치들 간 배분되는 런타임 워크로드를 감시 및 균등하게 하는 역할을 수행한다.  

<img src="/assets/images/Cloud_Computing/Fig12-28.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-29.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Cloud_Computing/Fig12-30.jpg" width="100%" height="100%"/>
<br/>

스토리지 용량 시스템은 LUN의 접근 빈도가 낮거나 유휴 상태인 기간에 운영중인 스토리지 장치를 전원 절약 모드로 유지할 수 있다.  

클라우드 스토리지 장치와 더불어 스토리지 워크로드 관리 아키텍처 내에 포함될 수 있는 일부 다른 메커니즘들은 다음과 같다.  

+ 감사 모니터  
이러한 아키텍처에 의해 형성된 시스템이 물리적으로 데이터를 재배치할 수 있기 때문에 이 모니터링 메커니즘이 규제, 개인 정보 및 보안 요구사항을 준수하는지 확인하는 데 사용된다.  

+ 자동 확장 리스너  
자동 확장 리스너가 워크로드의 변동을 관제 및 대응하는 데 사용된다.  

+ 클라우드 사용 모니터  
용량 워크로드 모니터에 추가로, 특화된 클라우드 사용 모니터가 LUN의 이동을 추적하고 워크로드 분배 통계치를 수집하는 데 사용된다.  

+ 로드 밸런서  
이 메커니즘이 사용할 수 있는 클라우드 스토리지 장치들 간 워크로드를 수평적으로 분배하는 데 추가될 수 있다.  

+ 논리 네트워크 경계  
논리 네트워크 경계는 다양한 수준의 분리 기능을 제공하여 재배치의 영향을 받는 클라우드 소비자의 데이터에 허가되지 않은 사용자가 접근할 수 없도록 보호한다.

---
title: Kubernetes Best Practices
categories:
- Kubernetes
feature_text: |
  ## Kubernetes Best Practices
feature_image: "https://picsum.photos/2560/600?image=733"
image: "https://picsum.photos/2560/600?image=733"
---
<style>
	thead td { text-align: center; }
	td { border: 1px solid #444444; }
	.align-center { text-align: center; }
</style>

### 2. 개발자 워크플로  
<br/>

#### 2.1. 목표  
<br/>

개발자와 클러스터 사이의 상호작용 단계를 이해해봅시다.  

첫 번재 단계는 온보딩입니다. 팀에 새로운 개발자가 들어온 상황입니다. 이 단계에서는 개발자에게 계정을 생성해주고 첫 배포까지 지원해줍니다. 개발자가 최대한 이른 시일 안에 적응하도록 돕는 것이 목표입니다. 핵심 성과 지표(key performance indicator, KPI)도 세워야 합니다. '빈손으로 시작한 사용자가 30분 내에 애플리케이션의 최신 버전을 실행하도록 만들기'가 좋은 예입니다. 새로운 팀원이 추가될 때마다 이런 목표에 부합하는지 시험해보세요.  

두 번째 단계는 개발입니다. 개발자가 매일 개발을 합니다. 이 단계의 목표는 빠른 반복과 디버그입니다. 개발자는 클러스터에 코드를 빠르고 반복적으로 푸시(push)합니다. 문제가 발생했을 때는 쉽게 코드를 테스트하고 디버그하길 원합니다. 이 단계에서 KPI를 측정하기는 어렵지만 풀 리퀘스트를 보내거나 클러스터에 변경을 반영하고 실행하는 데 걸린 시간, 사용자가 느끼는 생산성에 대한 조사, 혹은 그 모두를 평가에 이용할 수 있습니다. 아니면 팀의 전반적인 생산성을 측정하는 방법도 좋습니다.  

세 번째 단계는 테스트입니다. 이는 개발 단계와 번갈아 일어납니다. 여기서는 코드 제출과 병합 전 검증 작업을 진행합니다. 목표는 두 가지입니다. 첫 번째, 개발자가 풀 리퀘스트를 제출하기 전에 자신의 환경에서 이를 테스트할 수 있어야 합니다. 두 번째, 코드가 리포티터리에 병합되기 전에 모든 테스트가 자동으로 실행돼야 합니다. 테스트 실행 시간 또한 KPI로 설정해야 합니다. 프로젝트가 복잡해지면 테스트 시간이 늘어나는 것은 당연합니다. 이때는 풀 리퀘스트를 제출하기 전에 작은 스모크 테스트(smoke test)로 초기 검증을 하면 도움이 됩니다. 더불어 산발적 테스트 실패(test flakiness)에 대해 매우 엄격한 KPI를 설정해야 합니다. 산발적 테스트 실패는 말그대로 가끔 혹은 종종 발생합니다. 어느 정도 활성화된 프로젝트에서는 고작 천 분의 일 정도의 산발적 실패율로 인해 개발자와 충돌하고는 합니다. 클러스터 환경 자체로 인해 산발적 테스트 실패가 일어나면 안 됩니다. 산발적 테스트 실패는 코드 자체의 문제가 원인이 되기도 하지만 개발 환경의 문제로 인해 발생하기도 합니다(예를 들어 리소스 부족과 주변 노이즈). 이러한 문제가 없는지 테스트의 산발성을 측정하고 빠르게 고쳐야 합니다.  

#### 2.2. 개발 클러스터 구축  
<br/>

쿠버네티스에서 개발을 시작할 때, 먼저 대규모 단일 개발 클러스터와 개발자별 클러스터 중 하나를 선택해야 합니다. 이러한 선택은 공개 클라우드와 같이 클러스터를 동적으로 생성할 수 있는 환경에서 필요합니다. 물리적인 환경세너는 하나의 커다란 클러스터만 선택할 수 있습니다.  

각각의 장단점을 파악해야 합니다. 먼저 개발자별 클러스터의 가장 치명적인 단점은 큰 비용과 낮은 효율성, 관리해야 할 클러스터의 수가 많다는 것입니다. 추가로 각 클러스터의 사용률이 굉장히 저조할 가능성이 존재합니다. 또한 리소스를 추적하여 사용하지 않는 리소스를 삭제하는 것이 어렵습니다. 개발자별 클러스터의 장점은 단순하다는 것입니다. 각 사용자는 고립된 클러스터를 독립적으로 관리하게 되므로 서로에게 피해를 주지 않습니다.  

반면 대규모 단일 개발 클러스터는 굉장히 효율적입니다. 개발자 수가 같을 때 1/3 가격(혹은 더 적은)의 공용 클러스터로 처리가 가능합니다. 또한 모니터링과 로깅처럼 개발자에게 필요한 공용 클러스터 서비스를 설치하는 것이 더욱 수월합니다. 단점은 사용자 관리 프로세스와 잠재적인 개발자 간의 간섭입니다. 새로운 사용자와 네임스페이스를 추가하는 과정은 간단하지 않기 때문에, 새로운 개발자를 위한 온보딩 프로세스를 만들어야 합니다. 한 사용자가 너무 많은 리소스를 사용하는 경우, 개발 클러스터가 먹통이 되어 다른 애플리케이션과 개발자에게 리소스가 할당되지 않는다는 문제도 있습니다. 이때 쿠버네티스 리소스 관리나 역할 기반 접근 제어(role-based access control, RBAC)로 개발자 사이의 충돌을 줄일 수 있습니다. 또한 개발자가 리소스를 생성해야 하며 누수를 일으키거나 방치되지 않았는지 신경을 써야 합니다. 그래도 개발자가 직접 클러스터를 생성하는 것보다는 쉽습니다.  

대규모 단일 클러스터를 추천합니다. 개발자들 사이의 간섭 문제가 있지만 충분히 관리가 가능합니다. 궁극적으로 비용 대비 효율성이 높으며 클러스터에 조직 단위 기능을 쉽게 추가할 수 있다는 것이 큰 장점입니다. 그 대신 개발자 온보딩 프로세스, 리소스 관리, 가비지 컬렉션에 투자해야 합니다. 처음에는 대규모 단일 클러스터를 시도해보고 추후 조직이 거대해지면(혹은 이미 거대하다면) 수백 명의 사용자를 가진 거대 클러스터 대신 팀이나 그룹 단위(10~20명) 클러스터를 고려해보세요. 비용과 관리 측면에서 좋은 방법입니다.  

#### 2.3. 다중 개발자를 위한 공용 클러스터 구축  
<br/>

대규모 클러스터를 구축할 때의 주된 목표는 함께 클러스터를 사용하는 개발자들이 서로 피해를 주고받지 않도록 만드는 겁니다. 개발자를 분리하는 간단한 방법은 쿠버네티스 네임스페이스를 이용하는 것입니다. 네임스페이스는 서비스 배포 범위를 제한하여 특정 사용자의 프런트엔드 서비스가 다른 사용자의 프런트엔드 서비스에 피해를 주지 않도록 만듭니다. 또한 RBAC의 범위도 제한하기 때문에 특정 개발자가 실수로 다른 개발자의 작업 결과를 삭제하는 것을 막을 수 있습니다. 따라서 공용 클러스터에서 개발자의 작업 공간으로 네임스페이스를 사용하는 것은 좋은 생각입니다.  

##### 2.3.1. 사용자 온보딩  
<br/>

사용자가 네임스페이스를 할당받기 위해서는 먼저 쿠버네티스 클러스터 자체에 접근할 수 있어야 합니다. 이를 위한 두 가지 옵션이 있습니다. 첫 번째로 증명(certificate) 기반 인증을 사용할 수 있습니다. 사용자를 위한 새로운 증명을 생성하여 로그인을 사용할 수 있는 kubeconfig 파일을 전달해주는 겁니다. 두 번째로 외부 신원 시스템, 예를 들면 마이크로소프트 애저 액티브 디렉터리(Azure Active Directory) 또는 AWS 아이덴티티 앤드 액세스 매니지먼트(AWS Identity and Access Management, IAM)를 사용해 클러스터에 접근하도록 설정할 수 있습니다.  

일반적으로 신원 데이터를 한곳에서 관리할 수 있는 외부 신원 시스템을 사용하는 것이 모범 사례입니다. 외부 신원 시스템을 사용할 수 없는 상황이라면 증명을 사용해야 합니다. 다행히 쿠버네티스 증명 API를 사용하여 이를 생성하고 관리할 수 있습니다. 다음은 새로운 사용자를 기존 클러스터에 추가하는 과정입니다.  

먼저 증명 서명 요청을 생성하여 새로운 증명을 발급받아야 합니다. 다음은 이를 위한 간단한 Go 프로그램입니다.  

```Go
package main

import (
    "crypto/rand"
    "crypto/rsa"
    "crypto/x509"
    "crypto/x509/pkix"
    "encoding/asn1"
    "encoding/pem"
    "os"
)

func main() {
    name := os.Args[1]
    user := os.Args[2]

    key, err := rsa.GenerateKey(rand.Reader, 1024)
    if err != nil {
        panic(err)
    }
    keyDer := x509.MarshalPKCS1PrivateKey(key)
    keyBlock := pem.Block{
        Type: "RSA PRIVATE KEY",
        Bytes: keyDer,
    }
    keyFile, err := os.Create(name + "-key.pem")
    if err != nil {
        panic(err)
    }
    pem.Encode(keyFile, &keyBlock)
    keyFile.Close()

    commonName := user
    // 이러한 업데이트도 원할 수 있다.
    emailAddress := "someone@myco.com"

    org := "My Co, Inc."
    orgUnit := "Widget Framers"
    city := "Seattle"
    state := "WA"
    country := "US"

    subject := pkix.Name{
        CommonName: commonName,
        Country: []string{country},
        Locality: []string{city},
        Organization: []string{org},
        Organization: []string{orgUnit},
        Province: []string{state},
    }

    asn1, err := asn1.Marshal(subject.ToRDNSequence())
    if err != nil {
        panic(err)
    }
    csr := x509.CertificateRequest{
        RawSubject: asn1,
        EmailAddress: []string(emailAddress),
        SignatureAlgorithm: x509.SHA256WithRSA,
    }

    bytes, err := x509.CreateCertificateRequest(rand.Reader, &csr, key)
    if err != nil {
        panic(err)
    }
    csrFile, err := os.Create(name + ".csr")
    if err != nil {
        panic(err)
    }

    pem.Encode(csrFile, &pem.Block{Type: "CERTIFICATE REQUEST", Bytes: bytes})
    csrFile.Close()
}
```

다음과 같이 실행할 수 있습니다.  

```
go run csr-gen.go client <user-name>;
```

client-key.pem과 client.csr 파일이 생성되었습니다. 이제 다음 스크립트를 실행하여 새로운 증명을 생성하고 다운로드합니다.  

```sh
#!/bin/bash

csr_name="my-client-csr"
name="${1:-my-user}"

csr="${2}"

cat <<EOF | kubectl create -f -
apiVersion: certificates.k8s.io/v1beta1
kind: CertificateSigningRequest
metadata:
    name: ${csr_name}
spec:
    groups:
    - system:authenticated
    request: $(cat ${csr} + base64 | tr -d '\n')
    usages:
    - digital signature
    - key encipherment
    - client auth
EOF

echo
echo "Approving signing request."
kubectl certificate approve ${csr_name}

echo
echo "Downloading certificate."
kubectl get csr ${csr_name} -o jsonpath='{.status.certificate}' | base64 --decode > $(basename ${csr} .csr).crt

echo
echo "Cleaning up"
kubectl delete csr ${csr_name}

echo
echo "Add the following to the 'users' list in your kubeconfig file:"
echo "- name: ${name}"
echo "  user:"
echo "      client-certificate: ${PWD}/$(basename ${csr} .csr).crt"
echo "      client-key: ${PWD}/$(basename ${csr} .csr)-key.pem"
echo
echo "Next you may want to add ad role-binding for this user."
```

사용자 활성화를 위해 kubeconfig 파일에 추가할 최종 정보가 출력됩니다. 사용자에게는 접근 권한이 없으므로 네임스페이스에 대한 권한을 부여하려면 쿠버네티스 RBAC를 적용해야 합니다.  

##### 2.3.2. 네임스페이스 생성과 보안  
<br/>

네임스페이스 제공에 앞서 네임스페이스를 생성해야 합니다. 네임스페이스는 kubectl create namespace my-namespace로 생성할 수 있습니다.  

이대 배포할 컴포넌트를 빌드하는 팀의 연락처와 같은 메타데이터를 네임스페이스에 추가하고는 합니다. 일반적으로 애너테이션(annotation) 형태입니다. Jinja 등의 템플릿을 이용해 YAML 파일로 네임스페이스를 먼저 생성하고 애너테이션을 넣을 수 있습니다. 다음은 이와 관련된 간단한 스크립트입니다.  

```sh
ns='my-namespace'
kubectl create namespace ${ns}
kubectl annotate namespace ${ns} annotation_key=annotation_value
```

보안을 위해 특정 사용자에게만 네임스페이스 접근 권한을 부여할 수 있습니다. 네임스페이스 환경 안의 사용자에게 롤바인딩(RoleBinding)을 하는 겁니다. 이를 위해 네임스페이스 안에 롤바인딩 객체를 생성합니다. 롤바인딩은 다음과 같습니다.  

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metatdata:
    name: example
    namespace: my-namespace
roleRef:
    apiGroup: rbac.authorization.k8s.io
    kind: ClusterRole
    name: edit
subjects:
- apiGroup: rbac.authorization.k8s.io
  kind: User
  name: myuser
```

롤바인딩을 생성하려면 kubectl create -f role-binding.yaml을 실행하면 됩니다. 이것은 재사용도 가능합니다. 네임스페이스를 변경하고 바인딩의 네임스페이스를 업데이트하면 됩니다. 사용자가 다른 롤바인딩을 가지고 있지만 않다면 사용자는 클러스터에서 이 네임스페이스에만 접근할 수 있습니다. 전체 클러스터에 읽기 권한을 부여하는 것도 실용적입니다. 이를 통해 개발자는 자신의 작업에 영향을 미치는 다른 사람의 작업을 볼 수 있습니다. 하지만 클러스터의 시크릿 리소스에 접근할 수 있기 때문에 읽기 권한은 부여할 때에는 주의해야 합니다. 일반적으로 개발 클러스터에서는 문제가 없습니다. 모든 사람이 동일한 조직에 속해 있으며 시크릿은 개발에서만 사용되기 때문입니다. 만약 걱정이 된다면 시크릿 읽기 권한을 뺀 세밀한 역할을 생성하면 됩니다.  

특정 네임스페이스의 리소스 사용량을 제한하고 싶다면 리소스쿼터(ResourceQuota)를 이용합니다. 예를 들어 다음 리소스쿼터는 네임스페이스에 속한 파드의 요청과 제한 모두를 10코어와 100GB 메모리로 제한합니다.  

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
    name: limit-comput
    namespace: my-namespace
spec:
    hard:
        requests.cpu: "10"
        requests.memory: 100Gi
        limits.cpu: "10"
        limits.memory: 100Gi
```

##### 2.3.3. 네임스페이스 관리  
<br/>

새로운 사용자 온보딩과 네임스페이스의 생성 방법을 살펴봤습니다. 이제 개발자에게 네임스페이스를 할당하는 방법을 알아봅시다. 보통 두 가지 접근법을 사용합니다. 첫 번째는 온보딩 과정의 일부로 개발자에게 네임스페이스를 부여하는 겁니다. 그러면 온보딩 이후 자연스레 애플리케이션을 개발하고 관리할 수 있는 전용 작업 공간을 가지게 됩니다. 그러나 영구적인 네임스페이스에서 작업한 산출물이 방치될 수 있으므로 개별 리소스를 파악하고 가비지 컬렉션하는 걳이 어려울 수 있습니다. 대안으로는 타임 투 리브(time to live, TTL)로 네임스페이스를 임시로 생성하여 할당하는 방법이 있습니다. 그러면 개발자는 클러스터 내의 리소스를 한시적이라고 생각할 것이며 TTL이 만료되었을 때 네임스페이스 삭제의 자동화를 쉽게 구축할 수 있습니다.  

개발자가 새로운 프로젝트를 시작할 때, 이 모델에서는 프로젝트에 새로운 네임스페이스를 할당하는 도구를 사용합니다. 여기서 네임스페이스 생성 시점에 일반적으로 관리, 회계와 관련된 메타데이터를 넣습니다. 메타데이터는 네임스페이스의 TTL, 네임스페이스에 할당될 개발자, 부여할 리소스(예를 들어 CPU, 메모리), 팀의 정보, 목적 등을 포함합니다. 이 메타데이터는 리소스 사용량을 추적하고 적시에 네임스페이스를 삭제할 수 있도록 보장합니다.  

네임스페이스를 할당하는 도구를 개발하는 것은 어려워 보이지만, 도구화는 상대적으로 간단하게 개발할 수 있습니다. 예를 들어, 네임스페이스를 생성하고 관련 메타데이터를 프롬프트(prompt)로 입력받는 간단한 스크립트를 만들어 새 네임스페이스를 할당할 수 있습니다.  

쿠버네티스와 통합한다면 kubectl 도구를 이용하여 동적으로 새로운 네임스페이스를 생성하고 할당할 수 있는 사용자 리소스 정의(Custom Resource Definition, CRD)를 구현할 수 있습니다. 관심이 있다면, 선언적으로 네임스페이스를 관리하고 RBAC를 사용해보는 좋은 경험이 될 겁니다.  

네임스페이스를 할당해주는 도구를 만들었으니, 이제 네임스페이스의 TTL 만료되었을 때 네임스페이스를 회수하는 도구가 필요합니다. 마찬가지로 네임스페이스를 조사하여 TTL 만료된 것들을 삭제하는 간단한 스크립트를 만들면 됩니다.  

이 스크립트를 컨테이너에 내장하고 ScheduledJob을 이용해 한 시간 간격으로 실행합니다. 이러한 도구를 통합하면 필요할 때마다 독립적인 리소스를 프로젝트에 쉽게 할당할 수 있습니다. 또한 리소스를 적절한 간격으로 회수하여 낭비를 줄이고 오래된 리소스가 새로운 개발에 방해되지 않도록 보장할 수 있습니다.  

##### 2.3.4. 클러스터 단위의 서비스  
<br/>

네임스페이스를 할당하고 관리하는 도구와 더불어 유용한 것으로 클러스터 단위의 서비스가 있으며 이를 개발 클러스터에서 사용하면 좋습니다. 좋은 예는 로그를 집계할 수 있는 서비스로서의 인프라스트럭처(infrastructure as a service, IaaS)입니다. 개발자가 애플리케이션의 동작을 이해하는 가장 쉬운 방법은 무언가를 표준 출력(stdout)에 쓰는 겁니다. kubectl logs를 이용하여 이 로그에 접근할 수 있지만 길이 제한이 있으며 검색이 안 된다는 문제가 있습니다. 그 대신 클라우드 서비스나 일래스틱서치(Elasticsearch) 클러스터와 같은 IaaS 시스템에 로그를 자동으로 적재한다면, 개발자는 필요한 정보를 쉽게 검색할 수 있으며 여러 컨테이너에 존재하는 로깅 정보를 집계할 수 있습니다.  

#### 2.4. 개발자 워크플로 활성화  
<br/>

성공적으로 공유 클러스터를 설정했으니 새로운 애플리케이션 개발자를 온보딩할 수 있습니다. 이제는 애플리케이션 개발을 시작해야 합니다. 온보딩 시점부터 애플리케이션을 처음으로 실행하는 데 걸리는 시간을 이 단계의 핵심 KPI로 설정할 수 있습니다. 이전에 설명한 온보딩 스크립트를 이용해 신속하게 사용자를 인증하고 네임스페이스를 할당할 수 있었습니다. 그렇다면 애플리케이션 시작은 어떻게 해야 할까요? 이 과정을 도와주는 여러 기술이 있지만 (불행하게도) 초기 애플리케이션을 시작하고 실행하기 위해서는 자동화보다 더 많은 것들이 필요합니다.  

#### 2.5. 초기 설정  
<br/>

애플리케이션을 배포할 때 가장 어려운 문제는 모든 의존 관계를 설치하는 것입니다. 특히 최신 마이크로서비스 아키텍처에서 개발을 시작하려면 데이터베이스와 그 외의 마이크로서비스 등 여러 의존 관계를 배포해야 합니다. 애플리케이션 자체를 구축하는 것은 상대적으로 쉽습니다. 하지만 완전한 애플리케이션을 구축하기 위해 모든 의존 관계를 파악하고 배포하는 작업은 불완전하거나 잘못된 문서 때문에 종종 시도와 실패를 반복하게 됩니다.  

이 문제를 해결하려면, 의존 관계를 기술하고 설치하는 규약을 만드는 것이 좋습니다. 이는 자바스크립트의 모든 의존 관계를 설치하는 npm install과 유사합니다. 아마도 나중에는 쿠버네티스 기반의 애플리케이션에 이 서비스를 제공해주는 도구가 만들어질 가능성이 크지만, 그 전까지는 팀의 규약을 따르는 것이 모범 사례입니다.  

규약의 한 가지 예로는 최상위 디렉터리 안의 모든 프로젝트 리포지터리에 setup.sh 스크립트를 생성하는 겁니다. 특정 네임스페이스 안에 모든 의존 관계를 생성하는 것이 setup.sh 스크립트의 역할입니다. 이렇게 하면 애플리케이션의 모든 의존 관계를 올바르게 생성할 수 있습니다. 설치 스크립트의 예는 다음과 같습니다.  

```sh
kubectl create my-service/database-stateful-set.yaml
kubectl create my-service/middle-tier.yaml
kubectl create my-service/configs.yaml
```

다음과 같이 package.json에 아래 내용을 차가하여 이 스크립트를 npm과 통합할 수 있습니다.  

```json
{
    ...
    "scripts": {
        "setup": "./setpu.sh",
        ...
    }
}
```

이 설정을 이용하면 새로운 개발자가 npm run setup을 실행하기만 해도 클러스터의 의존 관계가 설치됩니다. 이렇게 특별한 통합은 당연히 Node.js/npm에만 한정됩니다. 다른 프로그래밍 언어라면 해당 언어의 도구와 통합하는 것이 올바른 방법입니다. 예를 들어 자바의 경우 아파치 메이븐(Apache Maven)의 pom.xml 파일과 통합할 수 있습니다.  

#### 2.6. 반복적 개발  
<br/>

개발자 작업 공간에 필요한 의존 관계를 설치했다면 다음 작업은 애플리케이션을 빠르고 반복적으로 배포하는 것입니다. 이를 위한 첫 번째 과제는 컨테이너 이미지를 구축하고 푸시하는 기술입니다. 여기서는 이미 이러한 기술을 안다고 가정합니다. 아직 잘 알지 못한다면 온라인의 다양한 자료와 책을 통해 배울 수 있습니다.  

컨테이너 이미지를 빌드하고 푸시한 후에는 이를 클러스터에서 실행합니다. 전형적인 롤아웃과 달리 개발 과정에서는 가용성이 중요하지 않습니다. 따라서 새로운 코드를 배포하는 가장 쉬운 방법은 이전 디플로이먼트 객체를 지우고 새로운 이미지를 가리키는 새로운 디플로이먼트를 생성하는 겁니다. 기존 디플로이먼트를 직접 수정할 수도 있지만 이렇게 하면 디플로이먼트 리소스 롤아웃 로직이 동작하게 됩니다. 디플로이먼트가 빠르게 롤아웃되도록 설정할 수 있지만 이는 개발 환경과 운영 환경의 차이를 만들어 위험하고 불안정해질 수 있습니다. 예를 들어 디플로이먼트의 개발 설정을 실수로 운여에 푸시한다고 상상해봅시다. 적절한 테스트와 롤아웃 단계의 지연 없이, 새로운 버전을 운영 환경에 갑작스럽게 배포하게 될 겁니다. 이러한 위험을 피하기 위해서는 디플로이먼트를 삭제하고 다시 생성하는 것이 가장 좋은 방법입니다.  

의존 관계를 설치하는 것과 마찬가지로, 디플로이먼트를 수행하는 스크립트를 만드는 것을 권장합니다. deploy.sh 스크립트의 예는 다음과 같습니다.  

```sh
kubectl delete -f ./my-service/deployment.yaml
perl -pi -e 's/${old_version}/${new_version}/' ./my-service/deployment.yaml
kubectl create -f ./my-service/deployment.yaml
```

이전과 마찬가지로 사용 중인 프로그래밍 언어 도구와 통합한다면 개발자는 npm run deploy를 실행하여 새로운 코드를 클러스터에 간단히 배포할 수 있습니다.  

#### 2.7. 테스트와 디버그 동작  
<br/>

애플리케이션의 개발 버전을 성공적으로 배포했다면, 애플리케이션을 테스트하고 문제가 있다면 디버그해야 합니다. 히자만 여러분과 클러스터 간의 상호작용이 매번 명확하지는 않으므로, 이 또한 쿠버네티스에서의 개발을 어렵게 만드는 요소입니다. kubectl 커맨드라인(command line)은 이를 위한 만능 맥가이버 칼입니다. kubectl logs부터 kubectl exec, kubectl port-forward까지 많은 기능이 있습니다. 그러나 모든 옵션의 사용법을 배우고 이들과 친숙해지려면 상당히 시간이 필요합니다. 게다가 도구는 터미널에서 실행되기 때문에, 다중 윈도로 애플리케이션의 소스 코드와 애플리케이션 실행 화면을 동시에 살펴봐야 하는 어려움이 종종 발생합니다.  

테스트와 디버깅 경험을 개선하기 위해 쿠버네티스 도구가 개발 환경에 점점 통합되고 있습니다. 예를 들어 비주얼 스튜디오 코드(Visual Studio Code, 줄여서 VS 코드)에는 쿠버네티스를 위한 오픈 소스 확장이 있습니다. 이는 VS 코드 마켓플레이스에서 무료로 설치할 수 있습니다. VS 코드는 kubeconfig 파일에 존재하는 클러스터를 자동으로 발견하고 한눈에 클러스터 내용을 볼 수 있는 트리뷰(tree-view) 탐색 창을 제공합니다.  

통합을 통해 클러스터의 상태를 한눈에 볼 수 있을 뿐만 아니라 직관적이고 쉽게 kubectl 도구를 이용할 수 있습니다. 트리뷰에서 쿠버네티스 파드를 우클릭하면 즉시 포트 포워딩을 사용하여 파드의 네트워크 연결을 로컬에 가져올 수 있습니다. 마찬가지로 파드의 로그에 접근할 수 있으며 실행 중인 컨테이너에 터미널로 접속할 수 있습니다.  

이러한 명령과 사용자 인터페이스(user interface, UI) 통합(예를 들어 우클릭은 컨텍스트(context) 메뉴를 보여줌) 그리고 사용자 경험(user experience, UX)과 애플리케이션 코드의 통합은, 쿠버네티스 경험이 거의 없는 개발자도 개발 클러스터에서 빠르게 결과를 낼 수 있도록 도와줍니다.  

물론 VS 코드 확장이 쿠버네티스와 개발 환경 사이의 통합은 아닙니다. 프로그래밍 환경과 스타일(vi, emacs 등)에 따라 설치할 수 있는 다른 것들이 있습니다.  

#### 2.8. 개발 환경 설정 모범 사례  
<br/>

쿠버네티스에 성공적인 워크플로를 설정하는 것은 생산성을 높여 행복의 문을 여는 열쇠입니다. 다음 모범 사례를 따르면 쉽고 빠른 수행을 보장할 수 있습니다.  

+ 개발자 경험을 온보딩, 개발, 테스팅의 세 단계로 나누어 생각해봅시다. 구축한 개발 환경이 이 도믄 단계를 지원하는지 확인해야 합니다.
+ 개발 클러스터를 구축할 때 대규모의 단일 클러스터와 개발자별 클러스터 중 하나를 선택할 수 있습니다. 각각의 장단점이 있지만 일반적으로 대규모의 단일 클러스터가 더 나은 방법입니다.
+ 클러스터에 사용자를 추가할 때, 사용자의 신원을 추가하고 자신의 네임스페이스에만 접근하도록 하세요. 사용할 수 있는 리소스의 크기는 제한해야 합니다.
+ 네임스페이스를 관리할 때, 오랫동안 사용하지 않은 리소스를 어떻게 회수할지 고민하세요. 사용하지 않는 리소스를 삭제하지 않는 것은 개발자의 나쁜 버릇입니다. 정리를 자동화해야 합니다.
+ 모든 사용자를 위해 로그와 모니터링 같은 클러스터 수준의 서비스를 고민하세요. 때로는 헬름 차트와 같은 템플릿을 사용하는 모두를 위해, 데이터베이스와 같은 클러스터 수준의 의존 관계가 필요합니다.  

### 3. 모니터링과 로깅  
<br/>

#### 3.1. 메트릭 vs 로그  
<br/>

+ 메트릭: 특정 기간에 측정한 일련의 숫자
+ 로그: 시스템을 탐색적으로 분석하기 위해 사용  

#### 3.2. 모니터링 기술  
<br/>

블랙박스 모니터링은 애플리케이션 외부에 초점을 둡니다. 일반적으로 CPU, 메모리, 스토리지 등의 시스템 컴포넌트를 모니터링합니다. 이것은 인프라 수준의 모니터링에 유용합니다. 하지만 애플리케이션이 어떻게 동작하는지 파악하거나 현상을 이해하기는 힘듭니다. 클러스터가 정상인지 테스트해보기 위해 수행하는 파드 스케줄링이 블랙박스 모니터링의 예시입니다. 만약 성공한다면 스케줄러와 서비스 디스커버리가 정상이라는 것을 알 수 있으며 이에 따라 클러스터 컴포넌트도 정상이라고 생각할 수 있습니다.  

화이트박스 모니터링은 애플리케이션 상태에 초점을 둡니다. 예를 들면 HTTP요청, 500 오류 수, 요청 레이턴시 등입니다. 이를 통해 시스템 상태가 '왜 이런지를 이해할 수 있습니다. 단순히 '디스크가 가득 찼다'가 아니라 '왜 디스크가 가득 찬 건가요?'라는 질문에도 대답할 수 있습니다.  

#### 3.3. 모니터링 패턴  
<br/>

쿠버네티스와 같은 플랫폼은 더욱 동적이며 일시적입니다. 그래서 이런 환경에 대한 모니터링은 기존과는 다른 시선으로 봐야 합니다. 예를 들어 가상 머신(virtual machine, VM)을 모니터링할 때, VM이 언제나 동작하며 상태가 보존되는지 확인합니다. 하지만 쿠버네티스에서는 파드가 매우 동적이며 라이프사이클(lifecycle)이 짧으므로 쿠버네티스만의 특성을 고려하여 모니터링을 진행해야만 합니다.  

분산 시스템을 모니터링할 때 중요한 모니터링 패턴이 있습니다.  

브랜던 그레그(Brenden Gregg)가 고안한 USE 패턴은 다음에 중점을 둡니다.  

+ Utilization: 사용률
+ Saturation: 포화도
+ Error: 오류율  

이 패턴은 애플리케이션 수준의 모니터링에 사용하기에는 한계가 있어 인프라 모니터링에만 활용됩니다. USE 패턴은 '모든 리소스에 대해 사용률, 포화도, 오류율을 확인'합니다. 이 패턴을 사용하면 시스템의 리소스 제약과 오류율을 빠르게 파악할 수 있습니다. 예를 들어 클러스터 노드의 네트워크 상태를 점검하기 위해 사용률, 포화도, 오류율을 모니터링해서 네트워크 병목이나 네트워크 스택의 오류를 쉽게 알 수 있습니다. USE 패턴 외에도 시스템 모니터링에 활용할 수 있는 도구가 있습니다.  

다른 모니터링 기법으로 톰 윌키(Tom Wilkie)가 개발한 RED가 있습니다. RED 패턴은 다음에 중점을 둡니다.  

+ Requset: 요청
+ Error: 오류율
+ Duration: 소요 시간  

사상은 구글의 네 가지 황금 신호(four golden signals)에서 가져왔습니다.  

+ 레이턴시: 요청을 처리하는 데 걸리는 시간
+ 트래픽: 시스템에 존재하는 요청의 양
+ 오류율: 요청 실패율
+ 포화도: 서비스의 사용률  

예를 들어 쿠버네티스에서 실행 중인 프런트엔드 서비스를 모니터링할 때, 이 기법을 사용해서 다음을 계산할 수 있습니다.  

+ 프런트엔드 서비스는 얼마나 많은 요청을 처리하고 있나요?
+ 사용자는 500 오류를 얼마나 받고 있나요?
+ 요청에 대한 서비스의 사용률이 과도하게 높지는 않나요?  

이처럼 RED 기법은 사용자의 서비스 경험에 더욱 초점을 두고 있습니다.  

USE와 RED는 상호보완적입니다. USE는 인프라 컴포넌트에 초점을 맞추며 RED는 애플리케이션의 최종 UX를 모니터링하는 데 중점을 둡니다.  

#### 3.4. 쿠버네티스 메트릭 개요  
<br/>

컨트롤 플레인 컴포넌트는 API 서버, etcd, 스케줄러, 컨트롤러 관리자로 이루어져 있습니다. 워커 노드는 kubelet, 컨테이너 런타임, kube-proxy, kube-dns, 파드로 이루어져 있습니다. 클러스터와 애플리케이션의 정상인지 확인하려면 모든 컴포넌트를 모니터링해야 합니다.  

##### 3.4.1. cAdvisor  
<br/>

컨테이너 어드바이저 또는 cAdvisor는 오픈 소스 프로젝트로 노드에서 실행 중인 컨테이너의 리소스와 메트릭을 수집합니다. cAdvisor는 쿠버네티스 kubelet에 내장되어 클러스터의 모든 노드에서 실행됩니다. 리눅스(Linux) cgroups(control groups) 트리를 통해 메모리와 CPU 메트릭을 수집합니다. cgroups는 CPU, 디스크 IO, 네트워크 IO 리소스를 고립시킬 수 있는 리눅스커널 기능으로 리눅스 커널에 내장된 statfs를 이용해 디스크 메트릭을 수집합니다. 세부 구현(implementation detail)까지 알 필요는 없지만, 이 메트릭이 노출되는 법과 수집하는 정보의 유형은 이해해야 합니다. cAdvisor는 모든 컨테이너 메트릭의 진실의 원천이기 때문입니다.  

##### 3.4.2. 메트릭 서버  
<br/>

자원이 종료된 힙스터(heapster) 대신에 쿠버네티스 메트릭 서버와 API를 사용합니다. 힙스터는 데이터 싱크 구현 방식에 대한 구조적인 한계를 가집니다. 이 때문에 힙스터 핵심 코드베이스에는 많은 벤더 솔루션이 들어가 있습니다. 이 문제는 리소스와 사용자 정의 메트릭 API를 쿠버네티스의 집계 API로 구현하면서 해결되었습니다. 이제 API를 변경하지 않고도 구현체를 변경할 수 있습니다.  

쿠버네티스 메트릭 서버와 API를 두 가지 측면에서 이해할 수 있습니다.  

첫 번째, 리소스 메트릭 API의 표준 구현체가 메트릭 서버입니다. 메트릭 서버는 CPU와 메모리 같은 리소스 메트릭을 수집합니다. kubelet API로부터 수집하며 메모리에 저장합니다. 이 메트릭은 스케줄러, 수평 파드 오토스케일러(Horizontal Pod Autoscaler, HPA), 수직 파드 오토스케일러(Vertical Pod Autoscaler, VPA)에서 사용됩니다.  

두 번째, 사용자 정의 메트릭 API를 이용해 모니터링 시스템에서 임의의 메트릭을 수집할 수 있습니다. 모니터링 솔루션은 사용자 정의 어댑터를 통해 사용자 정의 메트릭 기반의 HPA를 이용할 수 있습니다. 큐(queue) 크기 등의 메트릭을 가져올 수 있어서 쿠버네티스 외부의 메트릭 기반으로 확장할 수 있게 됩니다.  

이제 메트릭 API의 표준화를 통해 기존의 CPU와 메모리 메트릭뿐만 아니라 다양한 메트릭으로 확장할 수 있게 되었습니다.  

##### 3.4.3. kube-state-metrics  
<br/>

kube-state-metrics는 쿠버네티스에 저장된 오브젝트를 모니터링할 수 있는 쿠버네티스 추가 기능입니다. cAdvisor와 메트릭 서버는 리소스 사용량에 대한 자세한 메토릭을 제공하지만 kube-state-metrics는 클러스터에 배포된 쿠버네티스 오브젝트의 상태를 파악하는 것에 중점을 둔 기능입니다.  

kube-state-metrics가 해결해 줄 질문은 다음과 같습니다.  

+ 파드
    + 클러스터에 몇 개의 파드가 배포되었나요?
    + 대기 중인 파드는 몇 개인가요?
    + 파드 요청을 처리할 수 있을 만큼의 충분한 리소스가 있나요?
+ 디플로이먼트
    + 의도한 상태에서 수행 중인 파드는 몇 개인가요?
    + 몇 개의 레플리카가 가용한가요?
    + 어떤 디플로이먼트가 업데이트되었나요?
+ 노드
    + 워커 노드의 상태는 어떠한가요?
    + 클러스터에서 할당할 수 있는 CPU 코어는 몇 개인가요?
    + 스케줄되지 않는 노드가 있나요?
+ 잡
    + 잡이 언제 시작되었나요?
    + 잡이 언제 완료되었나요?
    + 몇 개의 잡이 실패했나요?  

#### 3.5. 모니터링할 메트릭  
<br/>

어떤 메트릭을 모니터링하나요? 간단한 대답은 '모두'입니다. 하지만 모니터링할 메트릭이 너무 많으면 중요한 신호를 발견하기 어렵습니다. 쿠버네티스에서 모니터링할 때는 다음과 같은 계층적 방식을 취해야 합니다.  

+ 물리적 혹은 가상의 노드
+ 클러스터 컴포넌트
+ 클러스터 추가 기능
+ 최종 사용자 애플리케이션  

이러한 계층적 모니터링 방식으로 신호를 간단하고 정확하게 판단할 수 있습니다. 이 방식을 사용하면 문제에 더 집중할 수 있습니다. 예를 들어 파드가 대기 상태에 진입하면 노드의 리소스 사용량을 먼저 파악하고, 문제가 없으면 클러스터 수준의 컴포넌트를 살펴봅시다.  

+ 노드
    + CPU 사용률
    + 메모리 사용률
    + 네트워크 사용률
    + 디스크 사용률
+ 클러스터 컴포넌트
    + etcd 레이턴시
+ 클러스터 추가 기능
    + 클러스터 오토스케일러
    + 인그레스 컨트롤러
+ 애플리케이션
    + 컨테이너 메모리 사용률과 포화도
    + 컨테이너 CPU 사용률
    + 컨테이너 네트워크 사용률과 오류율
    + 애플리케이션 프레임워크 특정 메트릭  

#### 3.6. 모니터링 도구  
<br/>

+ 프로메테우스  
프로메테우스(Prometheus)는 시스템 모니터링과 알림(alert) 툴킷(toolkit) 오픈 소스로 초창기에는 사운드클라우드(SoundClound)에서 개발했습니다.  

+ InfluxDB  
높은 쓰기와 질의(query)부하를 처리하도록 설계된 시계열 데이터베이스로 TICK(Telegraf, InfluxDB, Chronograf, Kapacitor의 줄임말) 스택의 필수 컴포넌트이고 대량 타임스탬프 데이터와 관련하여 다양하게 활용될 수 있는 스토리지입니다. 예를 들면, 데브옵스(DevOps) 모니터링, 애플리케이션 메트릭, IoT 센서 데이터, 실시간 분석이 있습니다.  

+ 데이터독  
클라우드 규모의 애플리케이션을 위한 모니터링 서비스로 SaaS 기반 데이터 분석 플랫폼을 통해 서버, 데이터베이스, 도구, 서비스를 모니터링할 수 있습니다.  

+ 시스딕  
컨테이너 네이티브 앱을 위한 도커와 쿠버네티스 모니터링을 제공하는 상용 도구로 쿠버네티스와 직접 통합되어 프로메테우스 메트릭을 수집하고 연동하고 질의할 수 있습니다.  

+ GCP 스택 드라이버  
구글 쿠버네티스 엔진(Google Kubernetes Engine, GKE) 클러스터를 모니티링하도록 설계되었습니다. 모니터링과 로깅 서비스를 관리하고, GKE 클러스터에 맞춘 대시보드 인터페이스가 있습니다. 클라우드 애플리케이션의 성능과 업타임, 전반적인 상태를 볼 수 있습니다. 구글 클라우드 플랫폼(Google Cloud Platform, GCP), 아마존 웹 서비스(Amazon Web Service, AWS), 가동 시간 프로브, 애플리케이션 장치로부터 메트릭, 이벤트 메타데이터를 수집합니다.  

+ 컨테이너를 위한 마이크로소프트 애저 모니터  
애저 컨테이너 인스턴스(Azure Container Instance, ACI)나 애저 쿠버네티스 서비스(Azure Kubernetes Service, AKS)가 호스팅하는 쿠버네티스 클러스터에 배포된 것으로, 컨테이너 워크로드의 성능을 모니터링하도록 설계되었습니다. 쿠버네티의 메트릭 API를 통해서 컨트롤러, 노드, 컨테이너의 메모리와 프로세서 메트릭을 수집해서 보여줍니다. 컨테이너 로그도 수집합니다. 쿠버네티스 클러스터에서 모니터링을 활성화하면 메트릭과 로그가 자동으로 리눅스용 로그 애널리틱스의 컨테이너화 버전을 통해 수집됩니다.  

+ AWS 컨테이너 인사이트  
아마존 일랙스틱 컨테이너 서비스(Amazon Elastic Container Service, ECS), 아마존 일래스틱 쿠버네티스 서비스(Amazon Elastic Kubernetes Service, EKS), 아마존 일래스틱 컴퓨트 클라우드(Amazon Elastic Compute Cloud, EC2)에 쿠버네티스 플랫폼을 사용한다면, 아마존 클라우드워치(Amazon CloudWatch) 컨테이너 인사이트를 사용할 수 있습니다. 컨테이너 애플리케이션과 마이크로서비스의 메트릭과 로그를 수집, 집계, 요약할 수 있습니다. 메트릭에는 CPU, 메모리, 디스크, 네트워크와 같은 리소스의 사용량이 있습니다. 또한 컨테이너 재시작 실패와 같은 진단 정보를 제공하므로 문제의 범위를 좁혀 빠르게 해결할 수 있습니다.  

메트릭을 저장하는 방식을 어떻게 구현했는지 살펴보는 것이 중요합니다. 시계열 데이터베이스는 키/값 쌍으로 저장하기 때문에 높은 수준의 메트릭 속성을 보여줄 수 있습니다.  

사용 중인 모니터링 도구를 항상 평가해야 합니다. 새로운 모니터링 도구로 변경하면 운영 적용 방법을 배우기 위한 시간과 비용이 들기 때문입니다. 많은 모니터링 도구가 쿠버네티스와 통합되어 있습니다. 현재 어떤 도구를 가지고 있는지, 그 도구가 요구사항에 부합하는지 평가해야 합니다.  

#### 3.7. 프로메테우스를 사용한 모니터링  
<br/>

프로메테우스는 CNCF가 호스팅하는 오픈 소스 프로젝트입니다. 초창기에는 사운드클라우드가 개발했으며 구글의 내부 모니터링 시스템인 보그몬(Borgmon)에서 많은 개념을 가져왔습니다. 쿠버네티스 레이블 시스템이 동작하는 방시과 유사하게 키 쌍의 다차원 데이터 모델을 구현했습니다.  

메트릭을 수집하기 위해 풀(pull) 모델을 사용합니다. 메트릭 엔드포인트로부터 메트릭을 수집하여 프로메테우스 서버에 메트릭을 저장합니다. 쿠버네티스는 이미 프로메테우스 포맷으로 메트릭을 내보내는 기능을 가지고 있으므로 간단하게 메트릭을 수집할 수 있습니다. 다른 쿠버네티스 에코시스템 프로젝트(NGINX, Traefik, Istio, Linkerd 등)도 이 포맷으로 내보냅니다. 뿐만 아니라 서비스가 내보내는 메토릭을 가져와서 포로메테우스 형태의 메트릭으로 변환할 수도 있습니다.  

클러스터 안이나 외부에 프로메테우스를 설치할 수 있습니다. 별도 '유틸리티 클러스터'에서 모니터링하는 것이 모범 사례입니다. 모니터링 시스템으로 인한 운영 이슈를 방지할 수 있기 때문입니다. 프로메테우스의 고가용성을 보장하고 외부 저장 시스템에 메트릭을 내보낼 수 있는 타노스(Thanos)와 같은 도구도 있습니다.  

+ 프로메테우스 서버  
시스템으로부터 수집된 메트릭을 가져와 저장합니다.  

+ 프로메테우스 오퍼레이터  
프로메테우스 설정을 쿠버네티스 네이티브로 만듭니다. 그리고 프로메테우스와 알림매니저(alertmanager) 클러스터를 관리하고 운영합니다. 사용자는 쿠버네티스 네이티브 리소스 정의를 통해 프로메테우스 리소스를 생성, 제거, 설정할 수 있습니다.  

+ 노드 내보내기  
클러스터 안의 쿠버네티스 노드로부터 수집한 호스트 메트릭을 내보냅니다.  

+ kube-state-metrics  
쿠버네티스 메트릭을 수집합니다.  

+ 알림매니저  
알림을 설정하고 외부 시스템으로 보냅니다.  

+ 그라파나  
프로메테우스의 대시보드 시각화를 제공합니다.  

```sh
helm install --name prom stable/prometheus-operator
```

오퍼레이터를 설치한 후에 다음 파드를 클러스터에 배포합니다.  

```sh
kubectl get pods -n monitoring
```

쿠버네티스 메트릭을 반환하는 질의를 수행해보기 위해 프로메테우스 서버를 살펴봅시다.  

```sh
kubectl port-forward svc/prom-prometheus-operator-prometheus 9090
```

localhost의 9090 포트로 터널을 생성합니다. 이제 웹 브라우저를 열고 프로메테우스 서버 http://127.0.0.1:9090에 접속할 수 있습니다.

이제 프로메테우스를 배포했습니다. 프로메테우스 PromSQL이라는 질의어(query language)를 통해 쿠버네티스 메트릭을 조회해봅시다. PromQL 기본 가이드도 있으니 필요하다면 참고하세요.  

그라파나 파드를 위한 포트포워드 터널을 생성하면 로컬 장비에서 접속할 수 있습니다.  

```sh
kubectl port-forward svc/prom-grafana 3000:3000
```
이제 웹 브라우저로 http://localhost:3000에 접속하여 다음 계정으로 로그인합니다.  

+ 사용자 이름: admin
+ 비밀번호: admin  

그라파나 대시보드 아래에 쿠버네티스/USE 기법/클러스터 대시보드가 있습니다. 이것은 USE 기법의 핵심인 쿠버네티스 클러스터의 사용량과 포화도의 요약 정보를 보여줍니다.  

#### 3.8. 로깅 개요  
<br/>

어떤 로그를 기록해야 하는지에 대한 명확한 정답은 없습니다. 로그 디버그는 필요악이기 때문입니다. 시간이 지남에 따라 환경을 더 잘 이해하게 된다면 로깅 시스템에서 어떤 노이즈를 조정할 수 있는지 알 수 있습니다. 또한 저장된 로그의 크기가 커지는 문제를 해결하려면 보관 기간과 정책을 규정해야 합니다. 최종 UX를 통해 살펴보면 30일에서 45일 사이의 로그를 보관하는 것이 좋습니다. 이 정도면 장기간에 걸쳐 나타나는 문제를 조사하기에 충분하고, 로그를 저장하는 데 필요한 리소스 양도 줄일 수 잇습니다. 규정 때문에 기간을 늘려야만 한다면 저비용 리소스에 로그를 보관하세요.  

쿠버네티스 클러스터에는 여러 컴포넌트 로그가 있습니다. 다음은 메트릭을 수집해야 할 컴포넌트의 목록입니다.  

+ 노드 로그
+ 쿠버네티스 컨트롤 플레인 로그
    + API 서버
    + 컨트롤러 관리자
    + 스케줄러
+ 쿠버네티스 감사(audit) 로그
+ 애플리케이션 컨테이너 로그  

노드 로그를 사용하여 핵심 노드 서비스에서 발생한 이벤트를 수집할 수 있습니다. 예를 들면 워커 노드에서 실행 중인 도커 데몬 로그를 수집할 수 있습니다. 워커 노드에서 컨테이너를 실행하려면 도커 데몬이 정상이어야 합니다. 따라서 도커 데몬 로그를 수집하면 도커 데몬 문제를 진단하는 데 도움이 되며 추후 발생할 수 있는 문제와 관련된 정보도 줍니다. 이외에도 노드에서 로깅할 수 있는 다양한 필수 서비스가 있습니다.  

쿠버네티스 컨트롤 플레인은 여러 컴포넌트로 이루어져 있습니다. 로그를 수집하면 컴포넌트 문제에 대한 통찰을 얻을 수 있습니다. 쿠버네티스 컨트롤 플레인은 정상 클러스터의 핵심입니다. 이를 위해 호스트의 /var/log/kube-APIserver.log, /var/log/kube-scheduler.log, /var/log/kube-controller-manager.log 로그를 집계해야 합니다. 컨트롤러 관리자는 최종 사용자가 정의한 오브젝트를 생성할 의무가 있습니다. 예를 들어 사용자가 로드 밸런서 타입으로 쿠버네티스 서비스를 생성했는데 대기 상태에 있는 경우, 쿠버네티스 이벤트로는 이 문제의 원인을 세부적으로 파악할 수 없을지도 모릅니다. 중앙집중식 시스템에 로그를 집계한다면 더 자세한 내용을 얻을 수 있으며 빠르게 문제를 조사할 수 있습니다.  

쿠버네티스 감사 로그는 보안 모니터링입니다. 시스템 내부에 누가 무엇을 했는지에 대한 통찰력을 주기 때문입니다. 이 로그는 노이즈가 많을 수 있습니다. 그래서 독자의 환경에 맞게 수정해야 합니다. 많은 인스턴스가 존재할 때 활성화하면 로깅 시스템의 리소스 사용량이 치솟을 수 있으므로 쿠버네티스 문서에서 감사 로그 모니터링 가이드를 준수해야 합니다.  

애플리케이션 컨테이너 로그는 실제 애플리케이션이 내보내는 로그에 대한 통찰력을 제공합니다. 이 로그를 다양한 중앙 리포지터리에 전달할 수 있습니다. 가장 권장하는 방법은 모든 애플리케이션 로그를 표준 출력으로 보내는 것입니다. 그러면 통일된 방법으로 애플리케이션 로깅을 할 수 있으며 모니터링 데몬은 도커 데몬으로부터 직접 로그를 수집할 수 있습니다. 다른 방법은 사이드카(sidecar) 패턴을 이용하고, 쿠버네티스 파드 안의 애플리케이션 컨테이너 옆에 로그를 전달하는 컨테이너를 실행하는 방식입니다. 애플리케이션이 파일 시스템에 로그를 남긴다면 이 패턴을 사용하는 게 유용합니다.  

쿠버네티스 감사 로그를 관리하는 많은 옵션과 설정이 있습니다. 감사 로그는 노이즈가 많으며 모든 행동을 로깅하려면 큰 비용이 듭니다. 환경에 적합한 로그 설정을 위해 감사 로깅 문서를 살펴보는 것을 권장합니다.  

#### 3.9. 로깅 도구  
<br/>

메트릭 수집과 마찬가지로 쿠버네티스와 클러스터에서 실행되는 애플리케이션으로부터 로그를 수집할 수 있는 많은 도구가 있습니다. 이런 도구를 이미 사용하고 있다면 도구가 로깅을 구현하는 방법을 알아두면 좋습니다. 로깅 도구는 쿠버네티스 데몬셋(DaemonSet)으로 실행되어야 합니다. 또한 표준 출력으로 로그를 보낼 수 없는 애플리케이션을 위해 사이드카로도 실행될 수 있어야 합니다. 기존의 도구를 사용한다면 이미 많은 운영 지식을 알고 있으니 사용하기에 유용합니다.  

다음은 쿠버네티스와 연동되는 대표적인 도구입니다.  

+ 일랙스틱 스택
+ 데이터독
+ 수모 로직(sumo logic)
+ 시스딕
+ 클라우드 공급자 서비스(GCP 스택드라이버, 애저 모니터, 아마존 클라우드워치)  

로그 중앙집중화를 위한 도구로는 많은 운영 비용을 절감할 수 있는 호스팅 솔루션이 좋습니다. 직접 로깅 솔루션을 호스팅하는 것은 며칠까지는 문제가 없을 수도 있습니다. 하지만 환경이 거대해진다면 유지보수를 위한 시간이 늘어날 겁니다.  

#### 3.10. EFK 스택을 사용한 로깅  
<br/>

EFK 스택을 구현해보는 것이 처음에는 좋습니다. 하지만 어느 순간 스스로 묻게 될 겁니다. '로깅 플랫폼을 관리하는 것이 정말 가치 있는 일인가?' 일반적으로 가치가 없습니다. 직접 로깅 솔루션을 호스팅하는 것은 처음에는 좋지만 시간이 지나면 굉장히 복잡해집니다. 자체 로깅 솔루션 호스팅은 환경의 규모가 커지면서 운영은 어려워집니다. 정답은 없으므로 비즈니스 요건에 따라 직접 호스팅이 필요한지 판단해야 합니다. 많은 EFK 스택 기반의 솔루션 호스팅이 있으므로 직접 호스팅하는 방식에서 다른 방식으로 쉽게 이동할 수 있습니다.  

다음 모니터링 스택을 배포합니다.  

+ 일래스틱 서치: 검색 엔진
+ 플루언트디: 쿠버네티스 환경에서 일래스틱서치로 로그를 전송
+ 키바나: 일래스틱서치에 저장된 로그를 검색, 보기, 상호작용하는 시각화 도구  

메니페스트(manifest)를 쿠버네티스 클러스터에 배포합니다.  

```sh
kubectl create namespace logging
kubectl apply -f https://raw.githubusercontent.com/dstrebel/kbp/master/elasticsearch-operator.yaml -n logging 
```

전송된 로그를 집계하는 일래스틱서치 오퍼레이터를 배포합니다.  

```sh
kubectl apply -f https://raw.githubusercontent.com/dstrebel/kbp/master/efk.yaml -n logging
```

플루언트디와 키바나를 배포합니다. 각각 일래스틱서치로 로그를 전송하고 시각화합니다.  

다음은 클러스터에 배포된 파드입니다.  

모든 파드가 실행 중(Running)이라면 localhost에서 포트 포워딩을 통해 키바나에 접속합니다.  

```sh
export POD_NAME=$(kubectl get pods --namespace logging -l "app=kibana,release=efk" -o jsonpath="{.items[0].metadata.name}")
kubectl port-forward $POD_NAME 5601:5601
```

웹 브라우저에 http://localhost:5601로 접속해 키바나 대시보드를 엽니다.  

쿠버네티스 클러스터로부터 전달된 로그와 상호작용하려면 먼저 인덱스를 생성해야 합니다.  

키바나를 처음으로 시작하면 Management 탭을 열어서 쿠버네티스 로그의 인덱스 패턴을 생성해야 합니다. 시스템이 안내해주는 대로 필수 단계를 진행합니다.  

인덱스를 생성한 이후에 다음과 같이 루씬(Lucene) 질의 문법을 이용해 로그를 검색합니다.  

```
log:(WARN|INFO|ERROR|FATAL)
```

이것은 warn, info, error, fatal을 포함한 모든 로그를 반환합니다.  

#### 3.11. 알림  
<br/>

알림은 양날의 칼입니다. 모니터링할 것과 알림할 것 사이의 균형을 맞춰야 합니다. 너무 많은 알림을 하면 극심한 피로를 줄 수 있으며 중요한 이벤트를 놓칠 수도 있습니다. 예를 들어서 파드가 실패할 때마다 알림할 수 있습니다. 이때 이런 질문을 할 수 있습니다. '파드 실패를 모니터링하지는 않나요?' 글쎄요. 쿠버네티스의 아름다움은 자동으로 컨테이너의 상태를 체크해서 재시작하는 겁니다. 사용자는 분명히 서비스 수준 목표(service level objectives, SLO)에 영향을 미치는 이벤트에만 집중하고 싶을 겁니다. SLO는 가용성, 처리량, 빈도, 응답 시간 등 측정 가능한 특성으로, 서비스의 최종 사용자가 동의하는 것입니다. 사용자의 기대 수준에 맞게 SLO를 설정하며 시스템이 어떻게 동작할지 명확하게 제시해야 합니다. SLO가 없다면 사용자는 서비스에 대한 비현실적인 기대를 가질 수 있습니다. 쿠버네티스와 같은 시스템에서의 알림은 기존에 사용자가 경험한 방식과는 완전히 새로운 접근 방식이 필요합니다. 예를 들어 프런트엔드 서비스의 SLO가 20ms 응답 시간이고 평균보다 더욱 지연이 된다면 이 문제를 알림하고 싶을겁니다.  

어떤 알림이 좋을지, 개입이 필요할지 결정을 내려야 합니다. 일반적인 모니터링에서 높은 CPU나 메모리 사용량, 응답 없는 프로세스에 대해 알림했을 겁니다. 훌륭한 알림처럼 보이지만 누군가가 즉각적인 행동을 취해야 하는지 혹은 대기 중인 엔지니어에게 알려야 할 문제인지 알 수 없습니다. 사람이 즉각 대응해야 하며 애플리케이션의 UX에 영향을 주는 문제라면 대기 중인 엔지니어에게 알림해야 합니다. '저절로 문제가 해결되었습니다'라는 시나리오를 경험한 적인 있다면, 이는 대기 중인 엔지니어에게 알림할 필요가 없었음을 보여주는 좋은 표시입니다.  

즉각 대응할 필요가 없는 알림을 처리하는 한 가지 방법은 문제를 자동으로 해결하는 겁니다. 예를 들어 디스크가 가득 찼다면 디스크의 공간을 확보하기 위해 로그를 자동으로 지우는 겁니다. 또는 앱 디플로이먼트에서 쿠버네티스의 생명성 프로브(liveness probe)를 이용하면 응답이 없는 프로세스를 자동으로 복원하는 데 도움을 얻을 수 있습니다.  

알림을 구축할 때 알림 임계치(alert threshold)를 고려해야 합니다. 임계치가 너무 짧으면 많은 거짓 알람을 받을 수 있습니다. 이를 위해 일반적으로 5분 이하의 임계치를 설정할 것을 권장합니다. 표준 임계치를 알아 두면 많은 다양한 임계치를 편리하게 정할 수 있습니다. 예를 들어 5분, 10분, 30분, 1시간 등의 특정 패턴을 따르는 겁니다.  

알림 알람을 구축할 때 관련 정보를 제공해야 합니다. 예를 들어 '플레이북(playbook)' 링크를 통해 문제 해결에 관한 정보를 줄 수 있습니다. 또한 데이터 센터, 지역, 앱 소유자, 영향을 받는 시스템 등의 정보를 담아야 합니다. 이러한 정보를 이용해 엔지니어는 빠르게 문제에 대한 정의를 내릴 수 있습니다.  

알림을 전달할 알림 채널을 구축해야 합니다. '알림이 발생하면 누구에게 알려야 하지?'라는 생각이 들 때, 단순하게 생각하여 배부 목록이나 팀 이메일로 전달하면 안 됩니다. 큰 그룹으로 알림이 전송되면 수신자는 노이즈로 판단하고 필터링하기 때문입니다. 문제에 대한 책임 있는 사람에게만 알려야 합니다.  

알림이 처음부터 완벽할 수는 없습니다. 절대로 완벽해질 수 없다고 생각하는 것이 오히려 낫습니다. 알림 피로를 주지 않도록 점진적으로 개선하는 것이 중요합니다. 알림 피로는 직원을 탈진시키거나 시스템에 많은 문제를 일으킬 수 있습니다.  

알림에 대한 접근법과 시스템 관리를 더욱 자세히 알고 싶다면 롭 에와스척(Rob Ewaschuk)의 「My Philosophy on Alerting(알림에 대한 나의 철학)」을 읽어보세요(http://bit.ly/Philosophy_on_Alerting). 구글의 사이트 안전성 엔지니어(site reliability engineer, SRE)인 롭의 경험을 기반으로 한 내용입니다.  

#### 3.12. 모니터링, 로깅, 알림 모범 사례  
<br/>

##### 3.12.1. 모니터링 모범 사례  
<br/>

+ 노드와 쿠버네티스의 모든 컴포넌트에 대한 사용률, 포화도, 오류율을 모니터링합니다. 그리고 애플리케이션의 속도, 오류, 시간도 모니터링합니다.
+ 시스템의 예측하기 힘든 상태와 징후를 모니터링할 때는 블랙박스 모니터링을 사용합니다.
+ 시스템과 내부를 조사할 때는 화이트박스 모니터링을 사용합니다.
+ 정확도가 높은 메트릭을 얻으려면 시계열 기반 메트릭을 구현합니다. 애플리케이션 동작에 대한 통찰을 얻을 수 있습니다.
+ 프로메테우스와 같은 모니터링 시스템을 활용합니다. 고차원을 위한 키레이블을 제공하고 문제의 징후에 대한 더 나은 신호를 제공합니다.
+ 평균 메트릭을 사용하여 실제 데이터 기반의 하위 합계와 메트릭을 시각화합니다. 합계 메트릭을 이용하여 특정 메트릭의 분포를 시각화합니다.  

##### 3.12.2. 로깅 모범 사례  
<br/>

+ 전반적인 분석을 위해서는 메트릭 모니터링과 함께 로깅해야 합니다.
+ 30일에서 45일까지만 로그를 저장하세요. 만약 그 이상의 로그가 필요하다면 장기간 보관할 수 있는 저비용 리소스를 사용하세요.
+ 사이드카 패턴에서 로그 전달자를 제한적으로 사용하세요. 너무 많은 리소스를 사용할 겁니다. 데몬셋을 사용하세요.  

##### 3.12.3. 알림 모범 사례  
<br/>

+ 알림 피로를 조심하세요. 사람과 프로세스에 악영향을 끼칠 수 있습니다.
+ 절대로 완벽해질 수 없다는 생각을 가지고 점진적으로 개선하세요.
+ 즉각적인 대응이 필요 없는 일시적인 문제는 알림하지 않고 SLO와 고객에게 영향을 미치는 징후를 알림하세요.  

### 4. 설정, 시크릿, RBAC  
<br/>

컨테이너의 복합적인 특성은 구성 데이터를 도입할 수 있게 합니다. 즉, 런타임 시점에 운영자는 설정 데이터를 컨테이너에 전달할 수 있습니다. 이를 통해 애플리케이션의 기능과 애플리케이션이 실행되는 환경을 분리할 수 있습니다. 컨테이너 런타임 규약을 보면, 설정은 런타임 시점에 환경 변수나 외부 볼륨 마운트를 통해 컨테이너로 전달될 수 있습니다. 애플리케이션을 시작할 대 설정을 효과적으로 변경할 수 있는 겁니다. 따라서 개발자는 환경의 동적인 특성을 염두에 둬야 합니다. 또한 개발자는 환경 변수를 사용할 수 있어야 하고, 애플리케이션 런타임 사용자가 접근 가능한 특정 경로의 설정 데이터를 읽을 수 있어야 합니다.  

시크릿과 같은 민감한 데이터를 네이티브 쿠버네티스 API 객체로 전달하려면 쿠버네티스의 API 접근 보안 방식을 알아야 합니다. RBAC가 대표적입니다. RBAC로 특정 사용자나 그룹의 API 이용에 대한 정교한 권한 구조를 구현할 수 있습니다.  

#### 4.1. 컨피그룹과 시크릿을 통합 설정  
<br/>

쿠버네티스는 컨피그맵과 시크릿 리소스를 이용해 애플리케이션에 설정 정보를 전달합니다. 둘 사이의 중요한 차이는 파드가 수신 정보를 저장하는 방식과 데이터가 etcd에 저장되는 방식입니다.  

##### 4.1.1. 컨피그맵  
<br/>

컨피그맵 API를 이용해 전달받은 설정 정보를 주입할 수 있습니다. 컨피그맵은 유연성이 뛰어나기 때문에 애플리케이션의 다양한 요구사항을 처리할 수 있습니다. 키/값 쌍이나 JSON, XML과 같은 복잡한 대량 데이터, 자산 설정 데이터를 전달할 수 있습니다.  

컨피그맵은 설정 정보를 파드에 전달할 뿐만 아니라 컨트롤러, CRD, 오퍼레이터 등 복잡한 시스템 서비스로도 정보를 제공할 수 있습니다. 앞서 언급했듯이 컨피그맵 API의 대상은 민감하지 않은 문자열 데이터입니다. 만감한 데이터를 전달할 때는 시크릿 API가 더 적합합니다.  

애플리케이션에서 컨피그맵 데이터를 사용하려면 파드에 볼륨을 마운트하거나 환경 변수로 전달하면 됩니다.  

##### 4.1.2. 시크릿  
<br/>

시크릿 역시나 컨피그맵이 제공하는 대부분의 기능을 가지고 있습니다. 중요한 차이는 시크릿의 근본적인 특성에 있습니다. 시크릿 데이터는 보이지 않게 저장되고 처리되어야 합니다. 필요시에는 환경설정을 통해 암호화된 후 저장되어야 합니다. 시크릿 데이터는 베이스64(base64)로 인코딩된 정보로 표현되는데, 암호화되지 않았다는 사실을 이해해야 합니다. 시크릿은 파드에 주입되자마자 일반 텍스트 형태로 보여집니다.  

시크릿 데이터란 베이스64로 인코딩된 데이터에 대해 기본 크기가 1MB로 제한되는 소량의 데이터를 의미합니다. 인코딩 오버헤드를 고려했을 때의 실제 데이터 기준으로는 대략 750KB입니다. 시크릿에는 다음 세 가지 타입이 있습니다.  

+ generic  
일반적인 키/값 쌍입니다. 이것은 파일, 디렉터리로 생성되거나 다음과 같이 --from-literal= 인자를 사용해 문자열 리터럴로 생성됩니다.  

```sh
kubectl create secret generic mysecret --from-literal=key1=$3cr3t1 --from-literal=key2=@3r3t2
```

+ docker-registry  
개인 도거 레지스트리 인증에 필요한 신원입니다. image Pullsecret에서 이 신원을 지정하면 kubectl이 파드템플릿으로 전달합니다.  

```sh
kubectl create secret docker-registry registryKey --docker-server myreg.azurecr.io --docker-username myreg --docker-password $up3r$3cr3tP@ssw0rd --docker-email ignore@dummy.com
```

+ tls  
공개/개인 키 쌍으로 전송 계층 보안(Transport Layer Security, TLS) 시크릿을 생성합니다. cert가 올바른 PEM 포맷이라면, 키 쌍은 시크릿으로 인코딩되어 SSL/TLS를 이용할 파드로 전달됩니다.  

```sh
kubectl create secret tls www-tls --key=./path_to_key/wwwtls.key --cert=./path_to_crt/wwwtls.crt
```

시크릿은 자신을 사용하는 파드가 있는 노드의 tmpfs에만 마운트됩니다. 따라서 파드가 종료될 때 시크릿은 삭제됩니다. 이것은 노드의 디스크에 시크릿이 남는 상황을 방지합니다. 보안상 안전해 보이지만, 기본적으로 시크릿은 etcd 데이터 스토리지에 평범한 텍스트로 저장됩니다. 그러므로 시스템 관리자나 클라우드 서비스 공급자는 etcd 노드 간의 상호 전송 계층 보안(mTLS) 및 etcd에 데이터를 저장할 때 암호화를 하는 등 etcd 환경의 보안에도 신경을 써야합니다. 쿠버네티스 최신 버전에서는 etcd3를 사용하며 네이티브 암호화를 지원합니다. 이를 위해서는 API 서버에 적합한 키 공급자와 키 매체를 수동으로 설정해야 합니다. 쿠버네티스 v1.10부터(v1.12 베타로 승격됨) KMS(key management secret) 공급자를 제공하는데, KMS 공급자는 적절한 키를 보유하기 위해 서드파티 KMS 시스템을 사용함으로써 보안상 더욱 안전한 키 프로세스 제공을 약속합니다.  

#### 4.2. 컨피그맵과 시크릿 API 모범 사례  
<br/>

컨피그맵이나 시크릿을 사용할 때, 대부분의 문제는 객체가 가진 데이터가 업데이트될 때 변경사항이 어떻게 처리되는지를 제대로 알지 못하기 때문에 발생합니다. 처리 규칙을 제대로 이해하고 쉽게 따를 수 있는 몇 가지 기술을 이용한다면 이러한 문제를 잘 헤쳐나갈 수 있습니다.  

+ 새로운 버전의 파드를 다시 배포하지 않고 애플리케이션을 동적으로 변경하려면 컨피그맵과 시크릿을 볼륨으로 마운트합니다. 변경된 파일 데이터를 감지하고 필요에 따라 재설정하려면 애플리케이션을 파일 감시자로 설정합니다. 다음 코드는 컨피그맵과 시크릿을 볼륨으로 마운트하는 디플로이먼트입니다.  

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
    name: nginx-http-config
    namespace: myapp-prod
data:
    config: |
        http {
            server {
                location / {
                    root /data/html;
                }

                location /images/ {
                    root /data;
                }
            }
        }

apiVersion: v1
kind: Secret
metadata:
    name: myapp-api-key
type: Opaque
data:
    myapikey: YWRtd5thSaW4=

apiVersion: apps/v1
kind: Deployment
metadata:
    name: mywebapp
    namespace: myapp-prod
spec:
    containers:
    - name: nginx
      image: nginx
      ports:
      - containerPort: 8080
      volumeMounts:
      - mountPath: /etc/nginx
        name: nginx-config
      - mountPath: /usr/var/nginx/html/keys
        name: api-key
    volumes:
      - name: nginx-config
        configMap:
            name: nginx-http-config
            items:
            - key: config
              path: nginx.conf
      - name: api-key
        secret:
            name: myapp-api-key
            secretname: myapikey
```

volumeMounts를 사용할 때 알아야 할 몇 가지 사항이 있습니다. 첫 번째, 컨피그맵/시크릿이 생성되자마자 파드 명세에 볼륨을 추가해야 합니다. 그런 다음 컨테이너의 파일 시스템에 해당 볼륨을 마운트합니다. 컨피그맵/시크릿의 속성명은 마운트된 디렉터리 안의 새로운 파일명이 되고 각 파일의 내용은 컨피그맵/시크릿에 명시된 값이 됩니다. 두 번째, volumeMounts.subPath 속성을 사용하여 컨피그맵/시크릿을 마운트하면 안 됩니다. 이렇게 하면 컨피그맵/시크릿을 업데이트할 경우 데이터가 볼륨에서 동적으로 업데이트되지 않습니다.  

+ 파드가 배포되기 전, 이를 소비할 파드의 네임스페이스 안에 컨피그맵/시크릿이 존재해야 합니다. 옵션 플래그를 사용하면 컨피그맵/시크릿이 없는 경우 파드가 시작되지 않도록 할 수 있습니다.  

+ 어드미션 컨트롤러(admission controller)를 사용하여 특정 설정 데이터를 확인하고, 특정 설정값이 존재하지 않는 경우 배포를 막을 수 있습니다. 예를 들어 운영 환경에서 모든 자바 워크로드가 특정 JVM 속성을 갖도록 설정해야 하는 경우가 생길 수 있습니다. PodPresets라는 알파 API를 사용하면 별도의 사용자 정의 어드미션 컨트롤러를 작성하지 않고도 애너테이션으로 컨피그맵과 시크릿을 모든 파드에 적용할 수 있습니다.  

+ 일부 애플리케이셔은 JSON이나 YAML 등 단일 설정 파일로 만들어야 합니다. 다음과 같이 | 기호를 이용하여 원시 데이터 블록을 허용할 수 있습니다.  

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
    name: config-file
data:
    config: |
        {
            "ioTDevice": {
                "name": "remoteValve",
                "username": "CC:22:3D:E3:CE:30",
                "port": 51826,
                "pin": "031-45-154"
            }
        }
```

+ 시스템 환경 변수를 사용하여 파드에 컨피그맵 데이터를 주입할 수 있습니다. 여기에는 두 가지 방법이 있습니다. envFrom을 사용한 다음 configMapRef 또는 secretRef를 통해 컨피그맵 안의 모든 키/값 쌍을 환경 변수로 파드에 마운트할 수 있습니다. 또는 configMapKeyRef 또는 secretKeyRef를 사용하여 개별 키를 각각에 해당하는 값으로 할당할 수 있습니다.  

+ configMapKeyRef 또는 secretKeyRef 방법을 사용할 때 실제 키가 존재하지 않는다면 파드를 시작할 수 없습니다.  

+ envFrom을 사용하여 컨피그맵/시크릿의 모든 키/값 쌍을 파드에 적재할 때, 부적합하다고 간주되는 환경 변수값은 건너뛰지만 파드는 시작할 수 있습니다. 부적합한 값에 대해서는 InvalidVariableNames라는 이유와 건너뛴 키에 대한 적절한 메시지를 포함하는 파드 이벤트가 생성됩니다.  

+ 커맨드라인 인자를 컨테이너에 넘겨주는 경우, $(ENV_KEY) 문자열 주입 구문을 사용하여 환경 변수 데이터를 전달하면 됩니다.  

+ 환경 변수 형태로 전달하는 경우, 컨피그맵/시크릿을 업데이트하더라도 파드 안의 값은 업데이트되지 않습니다. 업데이트를 하고싶다면 파드를 삭제하고 레플리카셋 컨트롤러가 새로운 파드를 생성하는 방식으로 파드를 재시작하거나 적절한 애플리케이션 업데이트 전략을 디플로이먼트 명세에 선언하고 디플로이먼트 업데이트를 통해 재시작해야 합니다. 이를 이해하는 것은 매우 중요합니다.  

+ 컨피그맵/시크릿의 모든 변경 사항이 전체 디플로이먼트의 업데이트를 필요로 한다고 가정하는 것이 낫습니다. 이렇게 하면 환경 변수나 볼륨을 사용하더라도 애플리케이션 코드가 새로운 설정 데이터를 받을 수 있습니다. 더 간편한 방법은 CI/CD 파이프라인을 사용해 컨피그맵/시크릿의 name 속성과 디플로이먼트의 참조를 업데이트하는 겁니다. 그러면 일반적인 쿠버네티스 업데이트 전략을 통해 디플로이먼트를 업데이트할 수 잇습니다. 다음의 예제 코드를 살펴봅시다. 헬름을 사용하여 애플리케이션 코드를 쿠버네티스에 배포한다면 디플로이먼트 템플릿 안의 애너테이션을 이용해 컨피그맵/시크릿 안의 데이터가 변경되었을 때, 헬름이 helm upgrade 명령을 이용해 디플로이먼트를 업데이트하게 만들 수 있습니다.  

```yaml
apiVersion: apps/v1
kind: Deployment
[...]
spec:
    template:
        metadata:
            annotations:
                checksum/config: {{ include (print $.Template.BasePath "/configmap.yaml") . | sha256sum }}
[...]
```

##### 4.2.1. 시크릿 관련 모범 사례  
<br/>

+ 초기 시크릿 API에는 요구사항에 맞춰 시크릿 스토리지를 설정할 수 있는 플러그인 아키텍처를 개략적으로 기술했습니다. 해시코프 볼트, 아쿠아(Aqua) 컨테이너 보안 플랫폼, 트위스트락(Twistlock), AWS 시크릿 관리자(AWS Secrets Manager), 구글 클라우드 KMS, 애저 키 볼트와 같은 솔루션을 통해 쿠버네티스보다 더 높은 수준의 암호화 및 감사 기능을 제공하는 외부 스토리지 시스템을 이용할 수 있습니다.  

+ pod.spec에 선언하지 않고 파드가 자동으로 시크릿을 마운트하기 위해 imagePullSecrets을 serviceaccount에 할당합니다. 애플리케이션의 네임스페이스에 기본 서비스 계정을 패치하고 imagePullSecrets을 추가합니다. 이렇게 하면 네임스페이스의 모든 파드에 자동으로 전달합니다.  

+ 배포 파이프라인 과정에서 CI/CD를 사용하여 하드웨어 보안 모듈로 보안 볼트나 암호화된 스토리지에서 시크릿을 가져옵니다. 이것은 직무에 대한 책임을 분리하는 겁니다. 보안 관리 팀은 시크릿을 생성하고 암호화를 담당하고, 개발 팀은 단지 시크릿의 이름만 참조하면 됩니다. 이것은 더욱 동적인 애플리케이션 전달을 위해 권장하는 데브옵스 방식입니다.  

#### 4.3. RBAC  
<br/>

##### 4.3.1. RBAC 기초  
<br/>

###### 4.3.1.1. 대상  
<br/>

첫 번째 컴포넌트는 대상입니다. 실제로 접근을 확인해야 하는 항목입니다. 대상은 일반적으로 사용자, 서비스 계정, 그룹입니다. 이전에 언급한 대로 사용자와 그룹은 권한 모듈을 이용해 쿠버네티스 외부에서 관리합니다. 기본 인증, x.509 클라이언트 증명, 오픈 아이디 연결 시스템을 사용하는 전달 토큰(bearer token)으로 분류할 수 있습니다. 오픈 아이디 연결 시스템에는 애저 액티브 디렉터리, 세일즈포스(Salesforce), 구글이 있습니다.  

쿠버네티스에서의 서비스 계정은 사용자 계정과 다릅니다. 네임스페이스에 종속되며 쿠버네티스 내부에 저장됩니다. 사람이 아닌 프로세스를 나타내기 위해 만들어졌으며 네이티브 쿠버네티스 컨트롤러가 관리합니다.  

###### 4.3.1.2. 규칙  
<br/>

간단히 말해서 API로 특정 객체(리소스) 또는 객체 그룹에 실행할 수 있는 기능 목록입니다. 일반적인 CRUD(생성, 읽기, 갱신, 삭제) 동작 유형과 같지만 추가로 watch, list, exec 기능이 있습니다. 객체는 여러 API 컴포넌트와 연결되며 카테고리로 분류되어 있습니다. 예를 들어 파드 객체는 핵심 API의 일부이며 apiGroup: ""으로 참조할 수 있지만 디플로이먼트는 앱 API 그룹 아래에 존재합니다. 이것이 RBAC 프로세스의 강점이지만 RBAC 제어를 생성할 때 힘들고 혼란스러울 수 있습니다.  

###### 4.3.1.3. 롤  
<br/>

롤은 정의된 규칙을 적용할 범위입니다. 쿠버네티스에는 role과 clusterRole라는 두 가지 롤이 존재합니다. role은 하나의 네임스페이스에 적용되지만 clusterRole은 모든 네임스페이스에 걸쳐 클러스터 전체에 적용됩니다. 다음 예제는 네임스페이스 범위의 롤을 정의한 것입니다.  

```yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    namespace: default
    name: pod-viewer
rules:
- apiGroups: [""] # ""는 핵심 API 그룹을 나타냅니다.
  resources: ["pods"]
  verbs: ["get", "watch", "list"]
```

###### 4.3.1.4 롤바인딩  
<br/>

롤바인딩은 사용자, 그룹과 같은 대상을 특정 롤에 매핑합니다. 롤바인딩에는 두 가지 모드가 있습니다. 네임스페이스에 특정된 roleBinding과 클러스터 전체에 걸친 clusterRoleBinding입니다. 다음은 네임스페이스 범위의 롤바인딩 예입니다.  

```yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    name: noc-helpdesk-view
    namespace: default
subjects:
- kind: User
  name: helpdeskuser@example.com
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: Role # 이것은 Role 또는 ClusterRole 이어야 합니다.
  name: pod-viewer # 이것은 바인딩된 Role 또는 ClusterRole의 이름과 일치해야 합니다.
  apiGroup: rbac.authorization.k8s.io
```

##### 4.3.2. RBAC 모범 사례  
<br/>

+ 쿠버네티스에서 실행되도록 개발된 애플리케이션은 대부분 RBAC나 롤바인딩을 필요로 하지 않습니다. 애플리케이션 코드가 실제로 쿠버네티스 API와 직접 상호작용할 때만 RBAC 설정이 필요합니다.

+ 서비스의 엔드포인트에 따라 설정을 변경하기 위해 쿠버네티스 API에 직접 접근하거나 특정 네임스페이스의 모든 파드를 나열하고자 할 때, 새로운 서비스 계정을 생성해 파드 명세에 명시하는 것이 모범 사례입니다. 그런 다음 목적을 달성하는 데 필요한 최소한의 권한만 가지는 롤을 생성합니다.

+ 오픈 아이디 연결 서비스를 사용해 신원 관리를 합니다. 필요하다면 이중 인증을 합니다. 이를 통해 더 높은 수준의 신원 인증을 허용합니다. 작업을 수행하는 데 필요한 최소한의 권한만 가지는 롤을 사용자 그룹에 매핑합니다.

+ 앞서 언급한 예제와 함께 JIT(just in time) 접근 시스템을 사용해 SRE와 운영자, 작업 수행을 위해 단기간 권한 상향이 필요한 사용자들이 구체적인 작업을 할 수 있도록 해야 합니다. 그 대신에 이러한 사용자는 접속에 대한 강도 높은 감사를 받기 위해 서로 다른 신원을 가져야 하며, 사용자 계정이나 그룹에 바인딩된 롤에 할당된 권한보다 더욱 높은 권한을 가져야 합니다.

+ 쿠버네티스 클러스터에 배포될 CI/CD 도구를 위해 특정 서비스 계정을 사용해야 합니다. 이렇게 하면 클러스터에 객체를 배포하고 삭제한 사람이 누군지 감사할 수 있습니다.

+ 헬름을 사용하여 애플리케이션을 배포하는 경우, 기본 서비스 계정은 kube-system에 배포된 틸러(Tiller)입니다. 해당 네임스페이스에 대한 범위가 지정된 틸러 전용 서비스 계정을 사용하여 각 네임스페이스에 틸러를 배포하는 것이 좋습니다. 사전 단계로 헬름 install, upgrade 명령을 실행하는 CI/CD 도구에서 서비스 계정과 배포 대상 네임스페이스를 사용하여 헬름 클라이언트를 초기화합니다. 서비스 계정 이름은 같아도 상관이 없지만 네임스페이스는 고유한 이름을 가지기를 권장합니다. 현재 헬름 버전은 v3.4.0이며, 틸러가 클러스터에서 사라졌다는 것이 핵심입니다. 서비스 계정과 네임스페이스로 헬름을 초기화하는 예는 다음과 같습니다.  

```sh
kubectl create namespace myapp-prod
kubectl create serviceacount tiller --namespace myapp-prod
cat <<EOF | kubectl apply -f -
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    name: tiller
    namespace: myapp-prod
rules:
- apiGroups: ["", "batch", "extensions", "apps"]
  resources: ["*"]
  verbs: ["*"]
EOF

cat <<EOF | kubectl apply -f -
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
    name: tiller-binding
    namespace: myapp-prod
subjects:
- kind: ServiceAccount
  name: tiller
  namespace: myapp-prod
roleRef:
  kind: Role
  name: tiller
  apiGroup: rbac.authorization.k8s.io
EOF

helm init --service-account=tiller --tiller-namespace=myapp-prod

helm install ./myChart --name myApp --namespace myapp-prod --set global.namespace=myapp-prod
```

일부의 공용 헬름 차트에서는 애플리케이션 컴포넌트를 배포할 대 네임스페이스 값 항목을 선택할 수 없습니다. 이를 위해서는 헬름 차트를 직접 정의하거나 배포할 네임스페이스를 선택할 수 있고 네임스페이스 생성 권한이 있는 틸러 계정을 사용해야 합니다.  

+ 시크릿 API의 watch와 list를 필요로 하는 애플리케이션을 제한합니다. 이렇게 하면 기본적으로 애플리케이션이나 파드를 배포한 사용자가 네임스페이스 안에 존재하는 시크릿을 볼 수 있습니다. 만약 애플리케이션이 특정 시크릿 때문에 시크릿 API에 접근해야 한다면 직접 할당된 시크릿 이외의 것들에 대한 get 사용을 제한해야 합니다.  

### 5. 지속적 통합, 테스트, 배포  
<br/>

#### 5.1. 버전 관리  
<br/>

모든 CI/CD/ 파이프라인은 애플리케이션 실행 이력과 설정 코드 변경을 관리하는 버전 관리로 시작됩니다. 깃은 소스 버전 관리 플랫폼으로서 업계 표준이 되었습니다. 모든 깃 리포지터리에는 운영 코드가 포함된 마스터 브랜치(master branch)가 존재합니다. 기능(feature)과 개발을 위한 브랜치도 별도로 존재하며 나중에 마스터 브랜치로 병합됩니다. 브랜치 전략을 세우는 방법에는 여러 가지가 있는데 조직 구조와 업무 구분에 따라 크게 달라집니다. 쿠버네티스 메니페스트 또는 헬름 차트에는 애플리케이션 코드와 설정 코드가 모두 포함되어 있습니다. 이는 소통과 협업의 데브옵스 원칙을 정립하는 데 도움이 됩니다. 애플리케이션 개발자와 운영 엔지니어가 단일 스토리지에서 공동의 작업을 수행하게 되면, 애플리케이션 운영에 전달하는 팀에 대한 신뢰도가 높아집니다.  

#### 5.2. 지속적 통합  
<br/>

CI는 코드 변경을 버전 관리 리포지터리에 지속적으로 통합하는 과정입니다. 코드를 리포지터리에 커밋할 때는 큰 코드 변경을 가끔식 하는 것보다 작은 코드 변경을 자주 하는 것이 낫습니다. 코드 변경이 커밋될 때마다 빌드가 실행됩니다. 이를 통해 실제로 문제가 발생했을 때 빠른 피드백을 얻을 수 있습니다. 이쯤에서 의문이 들 겁니다. '어째서 애플리케이션을 빌드하는 방법까지 알아야 하나요?' 이는 개발자의 역할이 아닌가요?' 전통적으로는 개발자만의 역할이었지만 최근 기업들이 데브옵스 문화를 점차 수용하면서 운영 팀 역시나 애플리케이션 코드와 소프트웨어 개발 워크플로를 알아가고 있습니다.  

#### 5.3. 테스트  
<br/>

파이프라인에서 테스트를 실행하는 이유는 코드 변경으로 빌드가 실패할 때 빠르게 피드백을 받기 위해서입니다. 사용 중인 언어에 따라 테스트 프레임워크가 결정됩니다. 폭넓은 테스트 집합이 있으면 나쁜 코드가 운영 환경에 배포되는 것을 막을 수 있습니다. 그러려면 테스트가 실패할 때 빌드도 반드시 실패해야 합니다. 또한 컨테이너 이미지가 빌드되어 레지스트리에 푸시되면 안 됩니다.  

또다시 이런 의문이 들지 모릅니다. '테스트를 만드는 것은 개발자의 역할이 아닌가요?' 인프라와 애플리케이션을 자동으로 운영에 배포하려면 모든 코드베이스를 자동으로 테스트하는 방안을 찾아야 합니다. 헬름에는 helm lint라는 도구가 있는데, 차트의 잠재적인 문제를 조사하기 위해 일련의 테스트를 수행합니다. 파이프라인에는 이외에도 실행되어야 할 많은 다른 테스트가 있습니다. 애플리케이션 단위 테스트와 같은 일부는 개발자의 책임이지만 스모크 테스트와 같은 일부는 공동의 협력이 필요합니다. 코드베이스를 테스트하여 운영에 전달하는 것은 팀 단위의 노력이 필요하며 전체적으로 구현해야 합니다.  

#### 5.4. 컨테이너 빌드  
<br/>

이미지를 빌드할 때는 이미지의 크기를 최적화해야 합니다. 이미지가 작을수록 이미지를 가져오고 배포하는 시간이 줄어들며 이미지 보안도 강화됩니다. 이미지 크기를 최적화하는 방법에는 여러 가지가 있지만 그 중 몇몇은 트레이드오프(trade-off)를 가지고 있습니다. 다음 전략은 애플리케이션에서 가능한 한 가장 작은 이미지를 빌드하는 데 도움을 줍니다.  

+ 다중 단계 빌드  
애플리케이션을 실행할 때 불필요한 의존 관계를 제거합니다. 다중 단계 빌드(multistage build)를 이용하면 하나의 도커파일로 애플리케이션 실행에 필요한 정적 바이너리만 포함하는 최종 이미지를 빌드할 수 있습니다.  

+ 배포판이 없는 기본 이미지  
불필요한 바이너리와 셸(shell)은 이미지에서 제거합니다. 이미지의 크기를 눈에 띄게 줄일 수 있으며 보안을 강화할 수 있습니다. 배포판이 없는(distroless) 이미지는 셸이 없으므로 이미지에 디버거를 붙일 수 없다는 단점을 가집니다. 애플리케이션을 디버그할 때는 상당히 고통스럽습니다. 패키지 관리자, 셸, 기타 일반적인 OS 패키지가 없기 때문에 일반적인 OS에서 친숙한 디버그 도구를 사용할 수 없습니다.  

+ 최적화된 기반 이미지  
OS 계층에서 불필요한 것을 제거하고 군살을 뺀 이미지입니다. 예를 들어 알파인 리눅스(Alpine Linux)는 10MB의 기반 이미지와 로컬 개발을 위한 디버거를 제공합니다. 데비안(Debian)과 같은 리눅스 배포판 역시나 최적화된 기반 이미지를 기본으로 제공합니다. 기반 이미지는 개발할 때 필요한 기능을 제공하면서 이미지 크기 최적화와 보안 위험도 낮추기 때문에 훌륭한 선택지입니다.  

이미지를 최적화하는 것은 굉장히 중요하지만 종종 간과되곤 합니다. 승인된 OS만 사용해야 하는 회사의 표준 때문일 수도 있습니다. 하지만 최적화를 통해 컨테이너의 가치를 극대화할 수 있습니다.  

쿠버네티스를 시작하는 회사에서 기존 OS보다 최적화된 데비안 등의 이미지를 사용하여 성공한 사례가 자주 있습니다. 컨테이너 환경에서 운영과 개발에 더욱 능숙해지만 배포판이 없는 이미지에 익술해질 겁니다.  

#### 5.5. 컨테이너 이미지 태그  
<br/>

CI 파이프라인의 또 다른 단계는 도커 이미지를 빌드하여 배포할 이미지 산출물을 만드는 것입니다. 여기서 운영에 배포한 이미지의 버전을 쉽게 분별하기 위한 이미지 태그 전략을 세우는 것이 중요합니다. 가장 중요한 것 중 하나는 latest를 이미지 태그로 사용하지 않는 것입니다. 이것은 버전이 아니므로 롤아웃 이미지의 어떤 코드가 변경된 건지 파악할 수 없습니다. CI 파이프라인에서 빌드된 모든 이미지는 고유한 태그를 가져야 합니다.  

CI 파이프라인에서 이미지를 태그할 때 효과적인 여러 전략이 있습니다. 다음과 같은 전략을 이용하면 코드 변경 및 빌드를 쉽게 파악할 수 있습니다.  

+ BuildID  
CI 빌드를 시작할 때 연관된 BuildID가 생성됩니다. 태그의 일부로 이 값을 쓰면 어떤 빌드에서 이미지가 만들어졌는지 알 수 있습니다.

+ 빌드 시스템-BuildID  
여러 빌드 시스템이 존재한다면 BuildID에 빌드 시스템을 추가합니다.  

+ 깃 해시  
새로운 코드를 커밋하면 깃 해시가 생성됩니다. 이 값을 태그에 사용하면 이미지를 생성한 커밋을 쉽게 알 수 있습니다.  

+ 깃 해시-BuildID  
이미지를 생성한 커밋과 BuildID를 알 수 있습니다. 태그가 길 수 있다는 점에 유의합니다.  

#### 5.6. 지속적 배포  
<br/>

변경 사항이 CI 파이프라인을 성공적으로 통과했다면 CD는 변경 사항을 자동으로 운영에 배포합니다. 컨테이너는 변경 사항을 배포하는 데 큰 이점을 제공합니다. 컨테이너 이미지가 개발, 스테이지, 운영으로 옮겨갈 수 있도록 불변 객체가 되는 겁니다. 예를 들어 일관된 환경을 유지하는 것은 항상 중요한 이슈입니다. 스테이지에서는 잘 동작하던 디플로이먼트가 운영으로 옮겨간 후 배포에 실패하는 경우가 많습니다. 이는 환경 사이의 라이브러리와 컴포넌트 버전 차이, 다시 말해 설정 표류 때문입니다. 쿠버네티스에서는 디플로이먼트 객체의 버전과 배포일관성을 위한 선언적인 방법을 제공합니다.  

명심할 점은 CD에 집중하기 전에 먼저 견고한 CI 파이프라인을 구축해야 한다는 겁니다. 초기에 문제를 발견할 수 있는 견고한 테스트 집합을 갖춰야만 잘못된 코드의 롤아웃을 막을 수 있습니다.  

#### 5.7. 배포 전략  
<br/>

CD 원칙을 알아봤으니 지금부터는 다양한 롤아웃 전략을 살펴봅시다. 쿠버네티스는 애플리케이션의 새로운 버전을 롤아웃하기 위한 여러 가지 전략을 제공합니다. 롤링 업데이트를 제공하는 기본 제공 메커니즘이 있지만 더 나은 고급 전략이 있습니다. 다음 전략을 살펴봅시다.  

쿠버네티스에 내장되어 있는 롤링 업데이트를 사용하면 현재 실행 중인 애플리케이션을 다운타임 없이 업데이트할 수 있습니다. 예를 들어 현재 frontend:v1으로 실행 중인 프런트엔드 앱을 frontend:v2로 업데이트한 경우, 쿠버네티스는 롤링 방식으로 레플리카를 frontend:v2로 업데이트합니다.  

디플로이먼트 객체를 사용하면 한 번에 업데이트할 최대 레플리카 수와 사용할 수 없는 최대 파드 수를 롤아웃 과정에서 설정할 수 있습니다. 다음 메니페스트는 롤링 업데이트 전략을 설정하는 방법의 예입니다.  

```yaml
kind: Deployment
apiVersion: v1
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: frontend
        image: brendanburns/frontend:v1
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1 # 한 번에 업데이트할 최대 레플리카 수
      maxUnavailable: 1 # 롤아웃 중간에 가용하지 않는 최대 레플리카 수
```

롤링 업데이트를 사용하면 클라이언트와의 연결이 끊어질 수 있으므로 주의해야 합니다. 이를 해결하기 위해 준비성 프로브(readiness probe)와 preStop 라이프사이클 훅을 사용할 수 있습니다. 준비성 프로브는 배포된 새로운 버전이 트래픽을 수용할 준비가 되었는지 확인하고 preStop 훅은 현재 배포된 애플리케이션의 커넥션을 드레인(drain)합니다. 이 훅은 컨테이너가 종료되기 전에 동기 방식으로 호출되므로 마지막 종료 신호를 받기 전에 완료되어야만 합니다. 다음 예제는 준비성 프로브와 라이프사이클 훅을 구현한 예입니다.  

```yaml
kind: Deployment
apiVersion: v1
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    spec:
      containers:
      - name: frontend
        image: brendanburns/frontend:v1
        livenessProbe:
          # ...
        readinessProbe:
          httpGet:
            path: /readiness # 프로브 엔드포인트
            port: 8888
        lifecycle:
          preStop:
            exec:
              command: ["/usr/sbin/nginx", "-s", "quit"]
  strategy:
    # ...
```

이 예제에서 preStop 라이프사이클 훅은 정상적으로(gracefully) NGINX를 종료하는 반면 SIGTERM은 빠르게 강제 종료 작업을 수행합니다.  

롤링 업데이트의 또 다른 문제는 롤오버 중간에 두 가지 버전의 애플리케이션이 동시에 실행된다는 겁니다. 따라서 데이터베이스 스키마(schema)는 애플리케이션의 두 가지 버전 모두를 지원해야 합니다. 여기서 스키마에 새로운 앱에 필요한 컬럼(column)을 추가하는 기능 플래그(feature flag) 전략을 사용할 수도 있습니다. 롤링 업데이트가 완료된 이후에는 불필요한 컬럼을 삭제할 수 있습니다.  

또한 우리는 디플로이먼트 메니페스트에 준비성과 생명성 프로브를 정의했습니다. 준비성 프로브는 엔드포인트인 서비스 뒤에 트래픽을 배치하기 전에 애플리케이션이 트래픽을 지원할 준비가 되었는지를 확인합니다. 생명성 프로브는 애플리케이션이 정상적으로 실행 중인지 확인하며 만약 생명성 프로브에 실패할 경우 파드를 재시작합니다. 쿠버네티스는 오류 때문에 파드가 종료되었을 때만 자동으로 재시작합니다. 데드락(deadlock)에 걸려 종료되지 않은 파드도 생명성 프로브를 이용하여 엔드포인트를 점검하고 재시작할 수 있습니다.  

블루/그린 배포를 사용해 예측 가능한 방식으로 애플리케이션을 릴리스할 수 있습니다. 블루/그린 배포는 트래픽이 새로운 환경으로 전환되는 시점을 조절할 수 있으므로 새 버전의 애플리케이션 롤아웃을 효과적으로 통제할 수 있습니다. 대신 기존 환경과 새로운 환경을 동시에 배포할 만큼 충분한 용량이 필요합니다. 이 유형의 배포는 이전 버전으로 쉽게 전환되는 등 많은 장점을 가집니다. 그렇지만 몇 가지 주의할 사항이 있습니다.  

+ 데이터베이스 마이그레이션(migration)이 어려울 수 있습니다. 진행 중인 트랜잭션과 스키마 업데이트의 호환성을 고려해야 하기 때문입니다.
+ 두 환경을 우연히 삭제할 위험이 있습니다.
+ 두 환경을 위한 추가적인 용량이 필요합니다.
+ 하이브리드(hybrid) 배포의 경우 레거시(legacy) 앱에서 발생하는 문제를 해결해야 합니다.  

카나리 배포는 블루/그린 배포와 상당히 유사하지만 신규 배포로 트래픽을 전환하는 것에 대해 다양한 통제권을 제공합니다. 최신 인그레스 구현은 신규 배포로 전달할 트래픽의 비율을 정할 수 있는 기능이 있습니다. 배포 전략 구현에 필요한 여러 기능을 제공하는 이스티오, 링커디, 해시코드의 콘술(Consul)과 같은 서비스 메시(service mesh) 기술을 구현할 수 있습니다.  

카나리 배포에서는 일부 사용자에게만 새로운 기능을 테스트할 수도 있습니다. 예를 들어 새로운 버전의 애플리케이션을 전체 사용자의 10%에게만 우선 롤아웃해서 테스트할 수 있습니다. 적은 수의 사용자에게만 먼저 노출하여, 잘못 배포하거나 기능이 동작하지 않는 위험성을 줄이는 겁니다. 문제가 없다면 더 많은 트래픽 비율을 새로운 애플리케이션으로 이동시킵니다. 특정 리전(region)의 사용자나 특정 프로필을 가진 사용자만을 대상으로 카나리 배포를 할 수 있는 고급 기술도 있습니다. 사용자는 새로운 기능을 테스트하고 있다는 것을 모르기 때문에 이러한 배포를 흔히 A/B 또는 다크 릴리스(dark release)라고 부릅니다.  

카나리 배포에는 블루/그린 배포의 주의 사항을 포함하여 아래와 같은 추가 고려 사항이 있습니다.  

+ 일부 사용자에게만 트래픽을 전환하는 기능
+ 신규 배포 상태와 비교하기 위한 안정된 상태에 대한 확고한 지식
+ 신규 배포가 '좋은' 또는 '나쁜' 상태인지 알 수 있는 메트릭  

카나리 배포는 동시에 여러 버전의 애플리케이션을 실행할 때 어려움을 겪습니다. 예를 들어 데이터베이스 스키마는 두 버전의 애플리케이션을 동시에 지원해야 합니다. 그래서 이 전략을 사용할 때는 의존된 서비스를 다루는 방법과 여러 버전을 실행하는 법을 제대로 파악해야 합니다. 이는 견고한 API 계약을 체결하고 데이터 서비스가 동시에 배포된 여러 버전을 지원하도록 보장하는 것을 포함합니다.  

#### 5.8. 운영에서 테스트  
<br/>

운영 환경에서 테스트를 수행하면 애플리케이션의 탄력성, 확장성, UX에 대한 신뢰를 얻을 수 있습니다. 운영 테스트는 도전적이며 위험하지만 시스템의 안정성을 위해 시도해볼 가치가 있습니다. 시도하기 전에 미리 해결해야 할 중요한 것들이 있습니다. 운영 테스트의 효과를 파악할 수 있는 심층적인 관찰 전략을 마련하는 겁니다. 최종 사용자의 UX에 영향을 미치는 메트릭을 관찰할 수 없다면, 시스템의 복원력을 향상시키기 위한 명확한 지표가 없는 겁니다. 또한 시스템에 주입한 장애로부터의 자동 복구를 위한 높은 수준의 자동화가 필요합니다.  

위험을 줄이면서 시스템을 효과적으로 테스트하려면 많은 도구를 구현해야 합니다. 분산 추적(distributed tracing), 인스투르먼테이션(instrumentation), 카오스 엔지니어링(chaos engineering), 트래픽 새도(traffic shadow)가 필요합니다. 이미 언급한 도구는 아래와 같습니다.  

+ 카나리 배포
+ A/B 테스트
+ 트래픽 전환
+ 기능 플래그  

카오스 엔지니어링은 넷플릭스(Netflix)에서 개발했습니다. 실제 생산 시스템에 실험을 배치해 그 시스템 내의 취약점을 발견하는 겁니다. 통제된 실험 환경에서 시스템의 동작을 관찰하고 파악할 수 있습니다. 다음은 '결전의 날'에 실험을 수행하기 전 구현하고자 하는 내용입니다.  

+ 가설을 세우고 안정된 상태를 파악합니다.
+ 시스템에 영향을 미치는 현업의 다양한 이벤트를 모읍니다.
+ 통제 그룹을 구축하고 안정된 상태와 비교하는 실험을 합니다.
+ 실험을 수행해 가설을 세웁니다.  

시험을 수행할 때 '폭발 반경(blast radius)', 다시 말하자면 생길 수 있는 문제의 범위를 최소화해야 합니다. 또한 실험 수행에는 극심한 노동력이 필요하므로 자동화해야 합니다.  

여기서 '스테이지에서 테스트하면 안 되나요?"라는 의문을 가질 수도 있습니다. 이는 다음과 같은 근본적인 문제를 가집니다.  

+ 리소스 배포의 불일치
+ 운영과의 설정 차이
+ 모의 트래픽과 사용자 행위
+ 현업 워크로드와 유사한 요청을 재현하지 못함
+ 모니터링이 빈약함
+ 배포된 데이터 서비스는 운영과 다른 데이터와 부하를 가짐  

여러 번 강조하지만 운영 모니터링에 대한 확고한 신뢰를 확보해야 합니다. 운영 시스템을 적절하게 관찰하지 못하면 실패할 확률이 높기 때문입니다. 또한 작은 실험부터 시작하여 실험 자체와 그 영향을 파악하면 신뢰를 얻는 데 도움이 됩니다.  

#### 5.9. 파이프라인 구축과 카오스 실험 수행  
<br/>

#### 5.10. CI/CD 모범 사례  
<br/>

CI/CD 파이브라인이 하루 아침에 완벽해질 수는 없습니다. 다음의 모범 사례를 따른다면 파이프라인을 점진적으로 개선해 나갈 수 있습니다.  

+ CI를 이용한 자동화와 빠른 빌드에 집중하세요. 빌드 속도를 최적화한다면 변경된 코드가 실패했을 때 빠른 피드백을 얻을 수 있습니다.

+ 파이프라인에서 안정적으로 테스트하는 데 초점을 두세요. 코드에 문제가 생겼을 때 개발자에게 빠른 피드백을 줄 수 있습니다. 빠른 피드백 반복은 워크플로의 생산성을 향상시킵니다.

+ CI/CD 도구를 결정했다면, 파이프라인을 코드로 정의할 수 있는지 확인하세요. 애플리케이션 코드로 파이프라인 버전 관리를 할 수 있습니다.

+ 이미지 최적화가 되었는지 확인하세요. 이미지 크기를 줄이고, 운영에서 실행할 때 보안상 취약한 지점이 있다면 없애야 합니다. 다단계 도커 빌드를 사용하면 애플리케이션 실행에 불필요한 패키지를 제거할 수 있습니다. 예를 들어 애플리케이션을 빌드할 때는 메이븐이 필요하지만 실제로 이미지를 실행할 때는 필요하지 않습니다.

+ latest 이미지 태그를 사용하지 마세요. BuildID와 깃 커밋을 참조할 수 있는 태그를 사용해야 합니다.

+ CD가 익숙하지 않다면 쿠버네티스 롤링 업그레이드를 사용하세요. 사용하기 쉽고 배포가 편리합니다. CD에 능숙해지고 자신감이 생기면 블루/그린 배포와 카나리 배포 전략을 사용하세요.

+ CD를 이용해 클라이언트 연결과 데이터베이스 스키마가 어떻게 업그레이드되며 애플리케이션에서 어떻게 처리되는지를 꼭 테스트하세요.

+ 운영 테스트는 애플리케이션의 안정성에 도움을 줍니다. 훌륭한 모니터링이 가동 중인지 확인하세요. 또한 소규모로 시작하고 실험의 폭발 반경을 제한하세요.  

### 6. 버전, 릴리스, 롤아웃  
<br/>

#### 6.1. 버전  
<br/>

대부분의 소프트웨어 회사와 개발자는 시멘틱 버전의 유용함에 동의합니다. 특히, 특정 팀에서 개발하는 마이크로서비스가 전체 시스템을 구성하는 다른 마이크로서비스와의 API 호환성에 의존하는 마이크로서비스 아키텍처에서 더욱 유용합니다.  

시멘틱 버전은 기본적으로 세 개의 버전 숫자로 이루어집니다. 메이저 버전, 마이너 버전, 패치 버전이며 보통 1(메이저).2(마이너).3(패치)과 같이 점 표기로 표현합니다.  

#### 6.2. 릴리스  
<br/>

쿠버네티스에는 릴리스 컨트롤러가 없습니다. 릴리스와 관련된 기본 개념이 없는 겁니다. 대신 보통 디플로이먼트 metadata.labels 명세나 pod.spec.template.metadata.label 명세에 릴리스 정보를 넣습니다. 넣는 시점이 중요하며, CD를 사용해 변경 사항을 디플로이먼트에 업데이트하는 방법에 따라 다양한 영향을 미칠 수 있습니다. 헬름이 처음으로 도입되었을 때 주요 개념 중 하나는 클러스터에서 실행 중인 인스턴스를 구별하기 위한 릴리스 개념이었습니다. 물론 릴리스 개념은 헬름 없이도 쉽게 재현할 수 있습니다. 하지만 헬름은 자체적으로 릴리스 내역을 추적하므로 많은 CD 도구가 헬름을 파이프라인에 통합하여 릴리스 서비스를 제공합니다. 다시 한번 강조하지만 클러스터 시스템 전반의 일관성이 핵심입니다.  

어떤 릴리스 이름을 쓸지 약속해야 합니다. 종종 stable이나 canary와 같은 레이블을 씁니다. 이는 서비스 메시와 같은 도구로 정교한 라우팅을 할 때 운영 및 관리가 편리합니다. 다양한 사용자에 대해 수많은 변경을 하는 대규모 조직은 ring-0, ring-1 등으로 표기하는 원형 구조를 채택하기도 합니다.  

쿠버네티스 선언적 모델의 레이블을 살펴봅시다. 레이블은 API의 문법 규칙에 맞는 키/값 쌍으로 자유롭게 표현할 수 있습니다. 핵심은 각 컨트롤러가 레이블을 다루고, 변경하고, 셀렉터가 매치하는 방법입니다. 잡, 디플로이먼트, 레플리카셋, 데몬셋은 레이블을 이용한 셀렉터 기반 파드 연계를 지원합니다. 직접 매핑하거나 집합 기반 표현을 사용할 수 있습니다. 레이블 셀렉터를 생성한 후에는 변경할 수 없다는 점을 이해해야 합니다. 즉 새로운 셀렉터를 추가하고 파드의 레이블을 매치했을 때, 기존 레플리카셋이 업그레이드되는 것이 아니라 새로운 레플리카셋이 만들어집니다.  

#### 6.3. 롤아웃  
<br/>

디플로이먼트 컨트롤러가 등장하기 전에 쿠버네티스 컨트롤러 프로세스가 애플리케이션을 롤아웃하는 유일한 방법은 커맨드라인 인터페이스(command-line interface, CLI)로 특정 replicaController에 대해 kubectl rolling-update 명령을 내리는 것이었습니다. 이것은 매니페스트의 상태가 아니기 때문에 선언적 CD 모델에서 처리하기 어려웠습니다. 시스템이 우연히 롤백되지 않도록 매니페스트가 적적하게 업데이트되었는지, 올바른 버전인지, 더는 필요하지 않다면 아카이브되었는지 등을 신중히 확인해야 했습니다. 디플로이먼트 컨트롤러에는 특정 전략을 이용하여 업데이트를 자동화하는 기능이 있습니다. 시스템은 디플로이먼트의 spec.template의 변경 사항을 기반으로 새로운 선언적 상태를 읽을 수 있습니다. 초기의 쿠버네티스 사용자는 종종 이를 오해하여 디플로이먼트 메타데이터 필드 안에 레이블을 변경하고, 매니페스트를 다시 적용했을 때 업데이트가 일어나지 않으면 당황하고는 했습니다. 디플로이먼트 컨트롤러는 명세의 변경을 판단할 수 있으며 명세에 정의된 전략에 기반하여 디플로이먼트를 업데이트합니다. 배포에는 rollingUpdate와 recreate의 두 가지가 있으며 rollingUpdate가 기본값입니다.  

롤링 업데이트가 지정되면 필요한 레플리카 수만큼 확장하기 위해 새로운 레플리카셋을 생성합니다. 이전의 레플리카셋은 maxUnavailable과 maxSurge 값에 따라 결국엔 완전히 제거됩니다. maxUnavailable은 충분한 수의 새로운 파드가 온라인 상태가 되기 전까지 이전 파드가 제거되지 않도록 막는 값입니다. maxSurge는 특정 수의 이전 파드가 제거되기 전가지 새로운 파드가 생성되지 않도록 막는 값입니다. 디플로이먼트 컨트롤러가 업데이트 이력을 보관하고 있으며 CLI를 이용해 이전 버전으로 롤백할 수 있다는 장점이 있습니다.  

recreate 전략은 서비스 성능의 저하 없이 레플리카셋의 모든 파드를 중단할 수 있는 특정 워크로드에 적합합니다. 이 전략에서 디플로이먼트 컨트롤러는 새로운 설정으로 새로운 레플리카셋을 생성하고 새로운 파드가 온라인으로 전환되기 전에 이전 레플리카셋을 삭제합니다. 큐 기반 시스템 뒤에 서비스를 두는 것이 이러한 유형의 장애를 처리할 수 있는 모범 사례입니다. 새로운 파드가 온라인이 되기를 기다리는 동안 메시지는 큐에서 대기하며, 온라인으로 전환되자마자 메시지 처리가 처리됩니다.  

#### 6.4. 버전, 릴리스, 롤아웃 통합 예제  
<br/>

```yaml
# 웹 디플로이먼트
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gb-web-deploy
  labels:
    apps: guest-book
    appver: 1.6.9
    environment: production
    release: guest-book-stable
    release number: 34e57f01
spec:
  strategy:
    type: rollingUpdate
    rollingUpdate:
      maxUnavailable: 3
      maxSurge: 2
  selector:
    matchLabels:
      app: gb-web
      ver: 1.5.8
      matchExpressions:
        - {key: environment, operator: In, values: [production]}
  template:
    metadata:
      labels:
        app: gb-web
        ver: 1.5.8
        environment: production
    spec:
      containers:
      - name: gb-web-cont
        image: evilgenius/gb-web:v1.5.5
        env:
        - name: GB_DB_HOST
          value: gb-mysql
        - name: GB_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        resources:
          limits:
            memory: "128Mi"
            cpu: "500m"
        ports:
        - containerPort: 80
---
# DB 디플로이먼트
apiVersion: apps/v1
kind: Deployment
metadata:
  name: gb-mysql
  labels:
    app: guest-book
    appver: 1.6.9
    environment: production
    release: guest-book-stable
    release number: 34e57f01
spec:
  selector:
    matchLabels:
      app: gb-db
      tier: backend
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: gb-db
        tier: backend
        ver: 1.5.9
        environment: production
    spec:
      containers:
      - image: mysql:5.6
        name: mysql
        env:
        - name: MYSQL_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        ports:
        - containerPort: 3306
          name: mysql
        valueMounts:
        - name: mysql-persistent-storage
          mountPath: /var/lib/mysql
    volumes:
    - name: mysql-persistent-storage
      persistentVolumeClaim:
        claimName: mysql-pv-claim
---
# DB 백업 잡
apiVersion: batch/v1
kind: Job
metadata:
  name: db-backup
  labels:
    app: guest-book
    appver: 1.6.9
    environment: production
    release: guest-book-stable
    release number: 34e57f01
  annotations:
    "helm.sh/hook": pre-upgrade
    "helm.sh/hook": pre-delete
    "helm.sh/hook": pre-rollback
    "helm.sh/hook-delete-policy": hook-succeeded
spec:
  template:
    metadata:
      labels:
        app: gb-db-backup
        tier: backend
        ver: 1.6.1
        environment: production
    spec:
      containers:
      - name: mysqldump
        image: evillgenius/mysqldump:v1
        env:
        - name: DB_NAME
          value: gbdb1
        - name: GB_DB_HOST
          value: gb-mysql
        - name: GB_DB_PASSWORD
          valueFrom:
            secretKeyRef:
              name: mysql-pass
              key: password
        volumeMounts:
          - mountPath: /mysqldump
            name: mysqldump
    volumes:
      - name: mysqldump
        hostPath:
          path: /home/bck/mysqldump
    restartPolicy: Never
backoffLimit: 3
```

여러 가지 의문이 들겁니다. 디플로이먼트와 이미지에 버전 태그를 다르게 부여하는 방법은 무엇일까요? 하나만 변경되고 나머지가 변경되지 않았을 때는 어떤 일이 생길까요? 이 예제에서 릴리스는 어떤 의미이며 시스템이 변경될 경우 어떤 영향을 끼칠까요? 특정 레이블이 변경되었을 때 디플로이먼트는 언제 업데이트될까요? 버전, 릴리스, 롤아웃에 관한 모범 사례를 살펴본다면 답을 찾을 수 있습니다.  

#### 6.5. 버전, 릴리스, 롤아웃 모범 사례  
<br/>

효과적인 CI/CD와 무중단 배포를 위해서는 버전과 릴리스 관리를 위한 일관성 있는 모범 사례를 알아야 합니다. 데브옵스 팀이 소프트웨어를 매끄럽게 배포하기 위해서는 일관된 매개변수를 정의해야 합니다. 아래는 이와 관련된 모범 사례입니다.  

+ 애플리케이션 전체에 시멘틱 버전을 적용하세요. 컨테이너의 버전과 파드 배포 버전은 다릅니다. 컨테이너와 애플리케이션 자체도 독립적인 라이프사이클을 가집니다. 처음에는 혼란스러울 수 있지만, 무언가가 변경되었을 때 원칙적 계층 방식을 이용해 쉽게 추적할 수 있습니다. 이전 예제에서 컨테이너는 현재 v1.5.5이지만 파드 명세는 1.5.8입니다. 이는 새로운 컨피그맵, 시크릿 추가, 레플리카 값 갱신 등으로 파드 명세가 변경되었다는 의미입니다. 전체 방명록 애플리케이션 자체와 모든 서비스는 1.6.9입니다. 이것은 운영 과정에서 전체 애플리케이션을 구성하는 서비스 전반을 변경했다는 뜻입니다.

+ 디플로이먼트 메타데이터 내의 릴리스와 릴리스 버전/숫자 레이블을 사용해 CI/CD 파이프라인 릴리스를 추적하세요. 릴리스 이름과 숫자는 CI/CD 도구 레코드의 실제 릴리스와 연계되어야 합니다. 이를 통해 클러스터에서 CI/CD 도구 레코드의 실제 릴리스와 연계되어야 합니다. 이를 통해 클러스터에서 CI/CD 과정을 추적할 수 있으며 롤백을 쉽게 식별할 수 있습니다. 이전 예제에서의 릴리스 숫자는 매니페스트를 생성한 CD 파이프라인의 릴리스 아이디에서 가져온 겁니다.  

+ 디플로이먼트 패키지 서비로 헬름을 사용하고 있다면, 헬름 차트와 함께 롤백이 되거나 업그레이드될 서비스를 함께 묶을 수 있도록 각별히 주의하세요. 이를 통해 애플리케이션의 모든 컴포넌트를 쉽게 롤백하여 업그레이드 이전으로 되돌릴 수 있습니다. 핼름은 YAML 설정을 전달하기 전에 템플릿과 모든 헬름 지시자(directive)를 처리하기 때문에, 라이프사이클 훅을 사용해 애플리케이션 템플릿 순서를 지정할 수 있습니다. 운영자는 헬름 라이프사이클 훅을 적절하게 사용하여 업그레이드와 롤백이 올바르게 실행되도록 보장할 수 있습니다. 이전 예제의 Job 명세에서는 헬름 라이프사이클 훅을 사용해 헬름 릴리스를 롤백, 업그레이드, 삭제하기 전에 템플릿이 데이터에비스 백업을 수행했습니다. 또한 성공적으로 실행된 후 Job은 삭제됐습니다. 쿠버네티스의 TTL 컨트롤러의 정식 버전이 출시될 때까지는 Job을 수동으로 삭제해야 합니다.  

+ 조직의 운영 흐름에 맞는 릴리스 명명법(nomencalture)을 합의하세요. stable, canary, alpha만으로도 대부분 충분합니다.  

### 7. 글로벌 애플리케이션 분산과 스테이지  
<br/>

#### 7.1. 이미지 분산  
<br/>

전 세계의 클러스터 애플리케이션을 실행하기 위해서는 전 세계의 리전에서 이미지를 사용할 수 있어야 합니다. 이를 위해서는 이미지 레지스트리가 자동 리전 복제 기능을 가져야 합니다. 클라우드 공급자의 이미지 레지스트리는 이미지를 전 세계에 자동으로 배포하며, 해당 이미지를 풀하려는 클러스터는 가장 가까운 스토리지에서 해결할 수 있습니다. 또한 사용자는 이미지가 필요 없는 곳을 미리 알기 때문에 이미지를 어디에 복제할지 선택할 수 있습니다. 그 예로 마이크로소프트 애저 컨테이너 레지스티리가 있으며 다른 클라우드도 이와 비슷한 서비스를 제공합니다. 리전 복제를 제공하는 클라우드 레지스트리를 사용하면 이미지를 전 세계에 간단하게 분산시킬 수 있습니다. 레지스트리에 이미지를 푸시하고 배포할 리전을 선택하면 레지스트리가 알아서 처리해줍니다.  

클라우드 레지스티리가 없거나 공급자가 자동 리전 복제를 제공하지 않는다면 스스로 문제를 해결해야 합니다. 한 가지 방법은 특정 리전의 단일 레지스트리를 사용하는 겁니다. 이때 고려해야 할 몇 가지 사항이 있습니다. 예를 들어 이미지 풀 레이턴시는 클러스터 구동 시간에 영향을 줍니다. 장애가 발생하면 새로운 서버에서 컨테이너 이미지를 받아야 하기 때문에 장애 대응 시간에 영향을 줍니다.  

단일 레지스트리에서의 또 다른 고민은 단일 장애점(single point of failure)입니다. 레지스트리가 단일 리전이나 데이터 센터에 존재하는 경우, 해당 데이터 센터의 대규모 사고로 인해 레지스트리가 오프라인 상태로 전환될 수 있습니다. 레지스트리가 오프라인으로 전환되면 CI/CD 파이프라인은 멈출 것이며 새로운 코드를 배포할 수 없게 됩니다. 당연히 개발자의 생산성과 애플리케이션 운여에 심각한 영향을 미치게 됩니다. 게다가 새로운 컨테이너는 구동할 때마다 상당한 대역폭을 사용하므로 컨테이너 이미지의 크기가 작더라도 대역폭 사용률의 늘어나면 단일 레지스트리의 비용도 커집니다. 이러한 단점에도 불구하고 소규모 애플리케이션을 소수 리전에 실행할 때는 단일 레지스트리 방식이 적합한 해결책입니다. 설치도 완전한 이미지를 복제하는 것보다 간단합니다.  

클라우드의 리전 복제를 사용할 수 없다면 이미지를 복제하기 위한 방법을 직접 고안해야 합니다. 이를 구현하기 위한 한 가지 방법이 있습니다. us.my-registry.io, eu.my-registry.io 등 이미지 레지스트리에 리전 이름을 사용하는 겁니다. 이러한 접근 방식은 설정과 관리가 편하다는 장점을 가집니다. 각 레지스트리는 완전히 독립적이며 CI/CD 파이프라인 마지막에 있는 모든 레지스트리에 이미지를 간단히 푸시할 수 있습니다.  

각 클러스터가 가장 가까운 리전에서 이미지를 풀하려면 설정을 바꿔야 한다는 단점이 있습니다. 그러나 보통의 애플리케이션이라면 이미 설정 안에 리전마다 차이가 존재할 것이므로 상대적으로 간단히 해결할 수 있으며 이미 적용되어 있을 가능성이 높습니다.  

#### 7.2. 배포 파라미터화  
<br/>

이미지를 모든 곳에 복제했습니다. 이제 글로벌 리전에 배포하기 위해서는 파라미터화해야 합니다. 배포할 리전에 맞춰 애플리케이션 설정이 달라질 겁니다. 예를 들어 리전 복제 레지스트리가 없다면 리전별도 이미지 이름을 바꿔야 합니다. 그리고 리전에 따라 부하가 달라질 수 있으므로 크기(예를 들어 레플리카 수)나 설정도 바꿔야 합니다. 이러한 복잡성을 편리하게 관리하는 것이 글로벌 애플리케이션 성공의 핵심 요소입니다.  

디스크 설정을 관리하는 다양한 방법을 살펴봅시다. 일반적인 방법은 글로벌 리전별로 다른 디렉터리를 쓰는 겁니다. 각 디렉터리에 동일한 설정을 복사하는 것은 간단하지만, 일부 리전은 수정되고 다른 리전은 유지되는 시간을 지나며 공통 설정이 리전별로 달라질 수 있습니다. 따라서 템플릿 기반 접근 방식이 좋은 아이디어입니다. 공통 설정은 단일 템플릿에 보관되어 모든 지역에서 공유되고, 매개변수를 템플릿에 적용하면 지역에 특화된 설정을 생성할 수 있습니다. 이를 위한 대표적인 템플릿 도구로는 헬름이 있습니다.  

#### 7.3. 글로벌 트래픽 로드 밸런스  
<br/>

이제 전 세계에서 애플리케이션을 실행할 수 있습니다. 다음 단계는 트래픽을 애플리케이션으로 전달하는 겁니다. 우선 리전의 근접성을 최대한 이용해 서비스 레이턴시를 낮추려고 시도해봅니다. 하지만 운영 중단이나 기타 서비스 장애가 발생했을 때는 다른 리전에서 트래픽을 처리해야 합니다. 다양한 리전의 디플로이먼트에 대한 트래픽 밸런스를 올바르게 설정하는 것이 시스템의 성능과 안정성을 위한 핵심입니다.  

서비스를 위한 단일 호스트명 myapp.myco.com이 존재한다고 가정해봅시다. 먼저 결정해야할 사항은 DNS 프로토콜을 사용하여 여러 리전 엔드포인트 간에 로드 밸런싱을 할지 여부입니다. DNS를 사용하면 사용자가 myapp.myco.com로 DNS 질의를 할 때 IP 주소가 반환됩니다. 이 주소는 서비스에 접근하려는 사용자의 위치와 현재 가용한 서비스의 위치에 따라 달라집니다.  

#### 7.4. 안정적인 글로벌 롤아웃  
<br/>

애플리케이션을 템플릿화해서 리전별로 적절한 설정을 갖추고 나면 이제 전 세계에 배포해야 합니다. 모든 애플리케이션을 동시에 전 세계로 배포한다면 효율적이며 빠르게 배포를 반복할 수 있습니다. 애자일이긴 하지만 글로벌 장애가 발생할 가능성을 가집니다. 따라서 스테이지 방식을 사용하여 보다 신중하게 롤아웃하는 것이 낫습니다. 글로벌 로드 밸런싱과 결합한다면 중대한 장애에 직면했을 때에도 고가용성을 유지할 수 있습니다.  

글로벌 롤아웃의 일반적인 목표는 소프트웨어를 가능한 한 빠르게 출시하는 동시에, 문제가 발생하여 사용자에게 영향을 미치기 전에 신속하게 감지하는 겁니다. 글로벌 롤아웃을 수행할 때 쯤에는 애플리케이션이 기본적인 기능과 부하 테스트를 올바르게 통과했다고 가정합니다. 이미지가 글로벌 롤아웃 인증을 받기 전에, 애플리케이션이 올바르게 동작할 거라는 가정합니다. 이미지가 글로벌 롤아웃 인증을 받기 전에, 애플리케이션이 올바르게 동작할 거라는 신뢰를 얻을 만큼의 충분한 테스트를 거쳤어야 합니다. 그렇다고 운영에서 애플리케이션이 올바르게 동작할 거라는 의미는 아닙니다. 테스트를 통해 많은 문제를 발견할 수 있지만, 애플리케이션이 운영 트래픽에 롤아웃된 후에야 이전에 없던 문제가 발견되기도 합니다. 운영 트래픽의 특성을 완벽하게 시뮬레이션하기가 어렵기 때문입니다. 예를 들어 영어 입력만으로 테스트를 진행했는데 실제로는 다양한 언어가 입력되는 경우가 생길 수 있습니다. 테스트 데이터가 애플리케이션에 실제로 입력되는 모든 데이터를 포함할 수는 없습니다. 물론 운영에서 새로운 문제가 발견되면 테스트의 범위를 확장해야 합니다. 그럼에도 불구하고 많은 문제가 롤아웃하는 도중에 발견되는 것이 사실입니다.  

리전별 롤아웃은 새로운 문제를 미리 발견할 기회입니다. 각 리전은 운영 리전이기 때문에 이 문제는 다른 리전에서 해결해야 할 잠재적인 장애입니다. 이를 고려한 스테이지를 구성하여 리전별로 롤아웃하는 방법을 준비해야 합니다.  

##### 7.4.1. 사전 롤아웃 검증  
<br/>

특정 버전의 소프트웨어를 전 세계로 롤아웃하기 전에 일종의 모의 테스트 환경에서 소프트웨어를 검증해야 합니다. CD 파이프라인을 올바르게 설치했다면 특정 릴리스 빌드 이전의 모든 코드에 대해 단위 테스트와 일부 통합 테스트가 수행될 겁니다. 그러나 이러한 테스트가 수행되더라도 릴리스 파이프라인을 시작하기 전에 두 종류의 테스트를 해야 합니다. 첫 번째는 완전한 통합 테스트입니다. 실제 트래픽 없이 애플리케이션을 완전히 확장하는 겁니다. 또한 운영 데이터 복제본이나 동일한 크기의 모의 데이터를 실제 운영 데이터 규모로 확장합니다. 현업에서 애플리케이션의 데이터가 500GB인 경우, 사전 운영 테스트에서의 데이터 크기도 이와 비슷해야 합니다(가능하면 동일한 크기가 좋습니다).  

이 부분이 완벽한 통합 테스트를 구성할 때 가장 어려운 부분입니다. 종종 운영 데이터는 실제로 운영에만 존재하기 때문에 모의 데이터를 동일한 규모로 생성하는 것이 쉽지 않습니다. 이러한 복잡성 때문에 애플리케이션 개발 초기 단계부터 준비하여 현실적인 통합 테스트 데이터를 설정하는 것이 모범 사례로 여겨집니다. 데이터셋 크기가 상당히 작은 초기 상황에서 모의 데이터셋 복사본을 설정하는 경우, 통합 테스트 데이터는 운영 데이터와 비슷한 속도로 점차 늘어나게 됩니다. 이미 확장된 운영 데이터를 복제하는 것보다 이 방법이 관리하기에 더 좋습니다.  

애석하게도 많은 사람들이 데이터 복제가 필요하다는 사실을 깨닫지 못한 채, 데이터의 규모가 커져 작업이 복잡해질 때까지 시간을 흘려보냅니다. 이러한 경우 운영 데이터 스토리지 이전 단계에 읽기/쓰기 전환 계층(read/write-deflecting layer)을 배치할 수 있습니다. 당연히 통합 테스트 과정에서 운영 데이터 스토리지에 쓰면 안 됩니다. 운영 데이터 스토리지 앞에 프록시를 구성하여 운영 데이터는 읽지만 쓰기는 별도 테이블에 저장해야 합니다. 이 테이블은 추후에 읽을 수 있습니다.  

통합 테스트의 목표는 환경 구성 방식과 상관없이 모두 같습니다. 일련의 입력과 상호작용이 주어졌을 때 애플리케이션이 예상대로 동작하는지 검증하는 겁니다. 이를 정의하고 실행하는 방식은 다양합니다. 테스트 작업 계획표를 따라 사람이 직접 브라우저를 클릭하면서 사용자와의 상호작용을 시뮬레이션하는 방법도 있습니다(오류가 발생하기 쉬우므로 권장하지 않습니다). 이 과정에서 RESTful API를 탐색하는 테스트가 있지만 API 위에 구축된 웹 UI를 반드시 테스트할 필요는 없습니다. 통합 테스트를 정의하는 방법과 상관없이 목표는 동일합니다. 모든 운영 입력에 대응하여 애플리케이션이 올바르게 동작하는지 자동으로 검증할 수 있는 테스트 집합을 만드는 겁니다. 간단한 애플리케이션의 경우 병합 이전 테스트에서 이러한 검증을 수행할 수 있습니다. 하지만 대규모의 현업 애플리케이션의 경우에는 완전한 통합 환경이 필요합니다.  

통합 테스트를 통해 애플리케이션이 올바르게 동작하는지 검증할 수 있지만 부하 테스트도 추가로 수행해야 합니다. 애플리케이션이 올바르게 동작하는지 검증하는 것과 현업에서 부하를 잘 견딜 수 있는지 증명하는 것은 다른 얘기입니다. 예를 들어 대규모 시스템에서 요청 레이턴시가 20% 증가하면 애플리케이션의 UX에 상당히 나쁜 영향을 미칩니다. 이는 사용자를 당황하게 만들 뿐만 아니라 애플리케이션이 완전히 중단될 수도 있습니다. 운영에서 이러한 성능 하락이 발생하지 않도록 하는 것이 중요합니다.  

통합 테스트와 마찬가지로 적절한 부하 테스트 방법을 고안하는 것은 어렵습니다. 운영 트래픽과 유사하지만 합성과 재현이 가능한 부하를 생성해야 합니다. 가장 쉬운 방법은 실제 운영 시스템에서 트래픽의 로그를 재생하는 겁니다. 애플리케이션이 배포되었을 때 겪는 상황을 그대로 재현할 수 있는 좋은 방법입니다. 그러나 재현이 항상 성공하는 것은 아닙니다. 로그가 오래된 경우, 애플리케이션이나 데이터셋이 변경된 경우, 재현된 과거의 로그 등에서는 신규 트래픽과의 성능 차이가 존재할 수 있습니다. 또한 실제 디펜던시로 과거 트래픽이 전송되는 것이 부적합할 수도 있습니다(예를 들어 데이터가 더는 존재하지 않을 수 있습니다).  

이러한 문제 때문에 많은 시스템, 심지어 중요한 시스템까지도 부하 테스트 없이 개발되고는 합니다. 운영 데이터 모델링과 마찬가지로 초기부터 착수한다면 관리하기 쉽습니다. 애플리케이션이 소수의 의존 관계를 가질 때 부하 테스트를 구축하고 애플리케이션에 맞춰 부하 테스트를 개선하는 것을 반복하는 것이, 대규모 애플리케이션이 된 이후에 부하 테스트를 구축하는 것보다 훨씬 쉽습니다.  

부하 테스트를 고안했다면 다음은 부하 테스트를 관찰할 수 있는 메트릭입니다. 초당 요청 수와 요청 레이턴시는 중요합니다. 사용자가 직접 체감하는 메트릭이기 때문입니다.  

레이턴시는 분포이기 때문에 평균 레이턴시와 이상치 백분위수(percentile) (90, 99백분위수)를 측정해야 합니다. 이상치는 애플리케이션의 '최악'의 UX를 나타냅니다. 평균만 본다면 매우 긴 레이턴시가 묻힐 수 있습니다. 그러나 사용자의 10%가 나쁜 경험을 하고 있다면 이는 제품의 성공 여부에 큰 영향을 미칠 겁니다.  

부하 테스트 과정에서 리소스 사용량(CPU, 메모리, 네트워크, 디스크)을 살펴봐야 합니다. 이 메트릭은 UX에 직접적으로 영향을 주진 않지만, 리소스 사용량의 큰 변화는 사전 운영 테스트에서 인지해야 합니다. 애플리케이션이 갑자기 두 배의 모메리를 사용한다면 부하 테스트를 통과했더라도 원인을 조사해야 합니다. 과도한 리소스 사용 증가는 애플리케이션의 품질과 가용성에 영향을 줍니다. 상황에 따라 릴리스를 운영에 계속해서 매포할 수 있지만, 이때 리소스 사용량이 변경되는 이유를 이해해야 합니다.  

##### 7.4.2. 카나리 리전  
<br/>

애플리케이션이 정상적으로 동작하는 것처럼 보인다면 첫 번째 단계는 카나리 리전이어야 합니다. 카나리 리전은 릴리스 검증하려는 곳으로부터 현업 트래픽을 받는 디플로이먼트입니다. 트래픽을 보내는 곳은 해당 서비스에 의존하는 내부 팀이거나 서비스를 사용하는 외부 고객일 수 있습니다. 카나리 리전의 목적은 이러한 사람들에게 롤아웃이 장애를 일으킬 수 있다는 조기 경고를 주는 겁니다. 통합 테스트와 부하 테스트가 아무리 훌륭하더라도 사용자와 고객에게 치명적인 모든 버그를 발견할 수는 없습니다. 하지만 모든 사람이 실패 확률이 높다는 것을 이해하는 공간에서 서비스를 사용하고 배포한다면 문제를 발견하는 것이 쉬울 겁니다. 이러한 공간이 바로 카나리 리전입니다.  

카나리는 모니터링, 확장, 특성 등에 있어 운영 리전으로 취급되어야 합니다. 하지만 릴리스를 위한 첫 번째 정차역이므로 그만큼 문제가 발생하기 쉽습니다. 사실 이것이 핵심입니다. 일부러 위험도가 낮은 사례에서 카나리 리전을 사용하는 겁니다(예를 들어 내부 사용자 대상). 이를 통해 릴리스될 수 있었던 잘못을 조기에 발견할 수 있습니다.  

카나리의 목표는 릴리스의 초기 피드백을 받는 것이므로, 며칠 동안 릴리스를 카나리 리전에 유지하는 것이 좋습니다. 이를 통해 다른 리전으로 옮기기 전에 폭넓은 고객층이 접근하게 됩니다. 이토록 긴 시간이 필요한 이유는 버그가 확률적이거나 (예를 들어 요청의 1%) 예외적인 사례에서만 발생할 수 있기 때문입니다. 자동 알림이 일어날 만큼 심각하지는 않을 수도 있지만, 고객과의 상호작용 과정에서만 발견되는 비즈니스 로직 문제가 있을 수 있습니다.  

##### 7.4.3. 리전 타입 식별  
<br/>

소프트웨어를 전 세계에 롤아웃할 때는 각 리전의 다양한 특성을 이해하는 것이 중요합니다. 운영 리전에 소프트웨어 롤아웃을 시작할 때 통합 테스트와 카나리 테스트를 수행합니다. 그렇다고 모든 문제를 찾을 수는 없습니다. 다양한 리전에 대해 생각해보세요. 트래픽이 유달리 많은 리전이 있나요? 접근하는 방법이 다른가요? 예를 들어 개발도상국에서는 모바일 웹 브라우저에서 트래픽이 발생할 확률이 높습니다. 그러므로 개발도상국과 근접한 리전에서는 테스트나 카나리 리전보다 더 많은 모바일 트래픽을 가질 수 있습니다.  

또 다른 예는 입력 언어입니다. 비영어권 리전에서는 더 많은 유니코드 문자가 전송될 겁니다. 이로 인해 문자열 처리에서 더 많은 버그가 발견될 수 있습니다. API 주도 서비스를 구축하는 경우, 일부 API를 많이 사용하는 리전이 있을 겁니다. 이는 애플리케이션 내부에 존재하는 차이이며 카나리 트래픽과는 다를 수 있는 예입니다. 이 모든 것들이 운영에서의 사고로 이어질 수 있습니다. 중요하다고 생각하는 다양한 특성을 미리 정리해두세요. 이러한 특성을 파악한다면 글로벌 롤아웃을 계획하는 데 도움이 될 겁니다.  

##### 7.4.4. 글로벌 롤아웃 구축  
<br/>

모든 리전의 특성을 파악했다면 이제 롤아웃 계획을 세웁니다. 당연히 운영에 주는 영향을 최소화하고자 하므로, 가장 먼저 카나리 리전이나 사용자 트래픽이 적은 리전을 시작점으로 고려하는 것이 좋습니다. 이러한 리전은 문제가 발생할 가능성이 매우 적으며 혹시 문제가 발생하더라도 받는 트래픽이 적으니 영향도 작습니다.  

첫 운영 리전으로의 롤아웃을 성공했다면 다음 리전으로 넘어가기 전까지 얼마나 오래 기다릴지 대기 시간을 정해야 합니다. 대기하는 이유는 일부러 릴리스에 지연을 주기 위해서가 아니라, 불에서 연기가 날 때까지 충분한 시간이 필요하기 때문입니다. 이 대기 시간은 일반적으로 롤아웃이 완료되고 모니터링에서 문제의 징후가 보일 때까지 걸리는 시간입니다. 롤아웃에 분명한 문제가 있다면 롤아웃이 완료되자마자 인프라에 문제가 나타날 겁니다. 그렇더라도 시간이 조금은 걸릴 겁니다. 예를 들어 메모리 누수의 경우, 모니터링에서 인지하거나 사용자에게 영향을 미칠 때까지 한 시간 이상이 걸릴 수 있습니다. 연기가 날 때까지 걸리는 시간은 얼마나 오래 기다려야 릴리스가 정상적으로 동작할 확률이 높을지를 나타내는 확률분포입니다. 경험상 문제가 발생하는 평균 시간의 두 배가 걸립니다.  

지난 6개월 동안 문제 발생에 평균 한 시간이 결렸을 경우, 리전 롤아웃 후 두 시간까지도 아무 문제가 없다면 릴리스가 성공할 확률이 높습니다. 애플리케이션의 이력으로부터 풍부한(그리고 의미 있는) 통계를 도출하려면 연기가 날 때까지 걸리는 시간을 더 자세히 측정해야 합니다.  

트래픽이 적은 카나리 리전에 성공적으로 롤아웃했다면 이제 트래픽이 많은 카나리 리전에 롤아웃할 시간입니다. 이 리전은 입력 데이터가 카나리와 유사하지만 더 많은 트래픽을 받습니다. 트래픽이 적은 유사한 카나리 리전으로 성공적으로 롤아웃했으므로 이 시점에서 테스트하는 것은 애플리케이션의 확장성입니다. 이 롤아웃을 안전하게 수행했다면 릴리스의 품질에 대한 강한 확신을 얻을 수 있습니다.  

트래픽이 많은 카나리 리전으로 롤아웃한 다음에는 트래픽 유형이 다른 리전에서도 동일한 패턴을 따라 진행합니다. 예를 들어 아시아 혹은 유럽의 트래픽이 적은 리전으로 롤아웃하는 경우입니다. 이 시점에서 롤아웃 속도를 내고 싶을 수도 있지만 입력이나 부하에서 중요한 차이가 있는 단일 리전부터 롤아웃하는 것이 중요합니다. 다양한 입력 테스트로 충분한 신뢰를 얻었다면 롤아웃이 정상적으로 동작하고 성공적으로 완료될 거라는 강한 확신을 가지고 동시다발적으로 릴리스를 시작하여 속도를 높일 수 있습니다.  

#### 7.5. 문제 발생 시 대처  
<br/>

지금까지 소프트웨어 시스템을 전 세계에 롤아웃하는 데 필요한 과정을 살펴봤습니다. 그리고 문제가 발생할 가능성을 최소화하도록 구성하는 방법도 알아봤습니다. 하지만 실제로 문제가 발생했을 때는 어떻게 해야 할까요? 위기 속에서 받는 급격한 스트레스로 인해 긴급 대응 팀은 가장 간단한 절차조차 기억하고 수행하기 어렵습니다. 게다가 CEO 이하의 모든 직원들이 '장애가 모두 해결되었다'는 신호를 열렬하게 기다릴 거라는 압박감이 더해집니다. 압박감이 가득한 상황에서는 실수할 확률이 높아집니다. 이때 복구 과정에서 특정 단계를 잊어바리는 간단한 실수가 발생한다면 상황을 더욱 악화시킬 수 있습니다.  

이러한 모든 이유로, 롤아웃 과정에서 문제가 발생했을 때 빠르고 차분하고 정확하게 대응하는 능력이 중요합니다. 필요한 모든 것이 모두 완료되었다면 진행 순서대로 정리된 작업 체크리스트와 함께 각 작업의 예상 결과를 만들어야 합니다. 너무 당연해 보이는 것들까지 포함하여 모든 단계를 작성하세요. 추후의 문제 상황에서는 지극히 당연하고 쉬운 단계라 할지라도 잊거나 지나칠 수 있습니다.  

엄청난 스트레스를 받는 상황에서도 정확하게 대응하기 위해서는 먼저 스트레스가 없는 상황에서 연습해야 합니다. 롤아웃의 모든 과정에 대해 같은 연습을 합니다. 이를 위해 문제에 대응하고 롤백을 수행하는 데 필요한 모든 단계를 파악해야 합니다. 이상적인 대응의 첫 단계는 영향을 받는 리전의 사용자 트래픽을 롤아웃하지 않은 리전으로 옮겨 '지혈'하는 겁니다. 그러면 시스템이 정상적으로 동작할 겁니다. 이를 가장 먼저 연습해야 합니다. 트래픽을 성공적으로 이동시킬 수 있나요? 이동시키는 데 얼마의 시간의 걸리나요?  

DNS 기반의 트래픽 로드 밸런서를 사용하여 트래픽을 처음으로 이동시켜보면 컴퓨터가 DNS 항목을 캐시에 보관하는 기간과 다양한 보관 방법에 대해 확실히 알게 될 겁니다. DNS 기반의 트래픽 제한으로 하나의 리전에서 트래픽을 완전히 드레인하려면 꽤 오랜 시간이 걸릴 수 있습니다. 트래픽을 드레인하는 방법에 상관없이 다음을 주목해야 합니다. 무엇이 잘 되었나요? 무엇이 잘 되지 않았나요? 이 데이터를 고려하여 퍼센트 단위로 트래픽 드레인 시간을 설정합니다. 예를 들어 10분 내로 트래픽의 99%를 제거하는 겁니다. 목표를 달성할 때까지 꾸준히 연습합니다. 이를 위해 아키텍처 변경이 필요할 수도 있습니다. 수동으로 명령어를 자르거나 붙여넣지 않도록 자동화도 추가해야 합니다. 필수적인 변화와는 상관없이, 연습을 통해 사고에 더 잘 대응하고 시스템 설계 개선이 필요한 곳을 확인할 수 있습니다.  

시스템에 취할 수 있는 모든 명령에 대해 동일하게 연습해야 합니다. 본격적으로 데이터 복구를 연습해봅시다. 시스템을 이전 버전으로 글로벌 롤백하는 방법을 연습하세요. 수행 시간에 대한 목표를 설정하세요. 실수하는 곳을 기록하고 실수를 줄이기 위해 검증하고 자동화하세요. 사고 대응 목표를 달성하면, 실제 사고가 발생하더라도 올바르게 대처할 수 있다는 자신감을 얻게 됩니다. 하지만 긴급 대응 팀이 훈련하고 배우는 것처럼 모든 사람이 능숙하게 대응할 수 있어야 하며 시스템이 변경되었을 때 대응 방식을 업데이트하고 정기적인 실습을 할 수 있도록 구성해야 합니다.  

#### 7.6. 글로벌 롤아웃 모범 사례  
<br/>

+ 이미지를 전 세계에 분산시키세요. 성공적인 롤아웃을 위해서는 릴리스 비트(바이너리, 이미지 등)가 사용할 위치와 가까이 존재해야 합니다. 이는 네트워크가 지연되거나 불규칙한 상황에 놓일 경우 롤아웃의 안전성을 보장합니다. 지역 분산은 일관성 보장을 위해 릴리스 자동화 파이프라인에 포함되어야 합니다.

+ 최대한 광범위한 통합 및 재현 테스트를 수행하세요. 정상적으로 동작할 것이라는 확고한 믿음이 있는 릴리스만 롤아웃해야 합니다.

+ 카나리 리전에서 릴리스를 시작하세요. 카나리 리전은 대규모 롤아웃을 시작히기 전에 다른 팀이나 대규모 고객이 직접 서비스를 사용하여 검증할 수 있는 사전 운영 환경입니다.

+ 롤아웃하려는 리전의 다양한 특성을 파악하세요. 각각의 차이는 전체 혹은 일부의 중단 원인이 될 수 있습니다. 위험도가 낮은 리전에서부터 롤아웃을 시도해야 합니다.

+ 발생할 수 있는 문제의 대응과 절차(예를 들어 롤백)에 대해 문서화하고 연습하세요. 긴급한 상황에서 무엇을 수행해야 하는지 제대로 기억해야 문제가 악화되는 것을 막을 수 있습니다.  

### 8. 리소스 관리  
<br/>

#### 8.1. 쿠버네티스 스케줄러  
<br/>

쿠버네티스 스케줄러는 컨트롤 플레인에서 호스팅되는 핵심 컴포넌트입니다. 스케줄러는 파드를 클러스터에 어떻게 배치할지를 결정합니다. 클러스터와 사용자가 정의한 제약에 따라 리소스를 최적화합니다. 그리고 놀리 조건과 우선순위 기반의 스코어 알고리즘을 사용합니다.  

##### 8.1.1. 논리 조건  
<br/>

쿠버네티스가 스케줄링할 노드를 결정할 때 사용하는 첫 번째 기능은 논리 조건 함수입니다. 이는 강한 제약을 내포하고 있으므로 참 또는 거짓을 반환합니다. 파드가 4GB 메모리를 요청할 때 특정 노드가 이 요건을 만족하지 못한다고 가정해봅시다. 이 노드는 거짓을 반환하고 파드 스케줄링 후보에서 제거됩니다. 또한 노드가 스케줄링 불가로 설정된 상태라면 마찬가지로 후보에서 제거됩니다.  

스케줄러는 제한성과 복잡성을 기반으로 논리 조건을 확인합니다. 다음과 같은 논리 조건들이 있습니다.  

+ CheckNodeConditionPred, CheckNodeUnschedulablePred, GeneralPred, HostNamePred, PodFitsHostPortsPred, MatchNodeSelectorPred, PodFitsResourcesPred, NoDiskConflictPred, PodToleratesNodeTaintsPred, PodToleratesNodeNoExecuteTaintsPred, CheckNodeLabelPresencePred, CheckServiceAffinityPred, MaxEBSVolumeCountPred, MaxGCEPDVolumeCountPred, MaxCSIVolumeCountPred, MaxAzureDiskVolumeCountPred, MaxCinderVolumeCountPred, CheckVolumeBindingPred, NoVolumeBindingPred, NoVolumeZoneConflictPred, CheckNodeMemoryPressurePred, CheckNodePIDPressurePred, CheckNodeDiskPressurePred, MatchInterPodAffinityPred  

##### 8.1.2. 우선순위  
<br/>

우선 순위에서는 상대적인 값을 기반으로 모든 유효한 노드의 순위를 매깁니다. 우선순위는 다음과 같습니다.  

+ EqualPriority, MostRequestPriority, RequestedToCapacityRatioPriority, SelectorSpreadPriority, ServiceSpreadingPriority, InterPodAffinityPriority, LEastRequestedPriority, BalancedResourceAllocation, NodePreferAvoidPodsPriority, NodeAffinityPriority, TaintTolerationPriority, ImageLocalityPriority, ResourceLimitsPriority  

우선순위 점수를 합산하여 최종 우선순위 점수를 노드에 부여합니다. 예를 들어 파드 하나에 600밀리코어(millicore)가 필요한 경우, 두 개의 노드 중 하나는 900밀리코어이고 하나는 1,800밀리코어라면 후자가 더 높은 우선순위를 가집니다.  

노드가 같은 우선순위를 반환한다면 스케줄러는 selectHost() 함수를 사용하여 라운드 로빈 토너먼트(round-robin tournament)방식으로 노드를 선정합니다.  

#### 8.2. 고급 스케줄링 기술  
<br/>

대부분의 경우, 쿠버네티스는 스스로 파드 스케줄리을 최적화합니다. 쿠버네티스는 넉넉한 리소르르가진 노드에게만 파드를 배치합니다. 또한 균등한 리소스 사용률을 유지하면서 레플리카셋의 파드를 여러 노드에 분산시켜 가용성을 높입니다. 이것으로 충분하지 않다면 쿠버네티스의 리소스 스케줄링 방식을 변경할 수 있습니다. 예를 들어 영역(zone) 실패로 인한 애플리케이션 장애를 막기 위해 가용한 영역 간에 파드를 분산 스케줄링할 수 있습니다. 또는 여러 파드를 단일 호스트에 배치하여 성능을 향상시킬 수도 있습니다.  

##### 8.2.1. 파드어피니티와 안티어피니티  
<br/>

파드어피니티와 안티어피니티로 파드 간의 배치 규칙을 설정할 수 있습니다. 이를 통해 스케줄링 방식을 변경하거나 스케줄러의 배치 결정을 오버라이드(override)할 수 있습니다.  

예를 들어 안티어피니티 규칙으로 레플리카셋의 파드를 여러 데이터 센터 영역에 분산시킬 수 있습니다. 파드에 특정 키 레이블만 설정하면 됩니다. 동일한 노드에 파드를 스케줄링하거나(파드어피니티) 그렇게 되지 않도록 할 수 있습니다(안티어피니티).  

##### 8.2.2. 노드 셀렉터  
<br/>

노드 셀렉터는 특정 노드에 파드를 스케줄링하는 가장 간단한 방식입니다. 키/값 쌍이 있는 레이블 셀렉터를 사용하여 스케줄링을 결정합니다. 예를 들어 그래픽 처리 장치(GPU)와 같은 특수한 하드웨어를 가진 특정 노드에 파드를 스케줄링할 수 있습니다. 그러면 이런 의문이 들겁니다. '노드 테인트도 함께 사용할 수는 없나요?' 가능합니다. 노드 셀렉터는 GPU가 가용한 노드를 요청할 때 상요되지만 노드 테인트는 오직 GPU 워크로드를 위한 용도로 노드를 제한할 때만 사용합니다. 그러므로 둘을 동시에 사용하여 GPU 워크로드 전용 노드를 예약하고 노드 테인트를 사용하여 GPU가 있는 노드를 자동으로 선택할 수 있습니다.  

다음은 노드에 레이블을 생성하고 파드 명세에서 노드 셀렉터를 사용하는 예제입니다.  

```sh
kubectl label node <node_name> disktype=ssd
```

##### 8.2.3. 테인트와 톨러레이션  
<br/>

테인트는 파드가 스케줄링되는 것을 거절하기 위해 노드에 사용됩니다. 안티어피니티와 같지 않냐고요? 맞습니다. 히자만 테인트는 다른 방식과 용도로 쓰입니다. 예를 들면 특정 노드에 특정 성능 요건을 가진 파드만 필요하고 그 외의 다른 파드는 스케줄링하지 않으려는 상황입니다. 테인트는 톨러레이션과 함께 동작합니다. 이는 테인트된 노드를 오버라이드합니다. 두 조합으로 안티어피니티 규칙을 세밀하게 조정할 수 있습니다.  

일반적으로 다음과 같은 사례에서 테인트와 톨러레이션을 사용합니다.  

+ 특수한 하드웨어를 가진 노드
+ 전용 노드 리소스
+ 성능이 낮은 노드 회피  

스케줄링과 실행 중인 컨테이너와 관련해 여러 테인트 타입이 있습니다.  

+ NoSchedule  
톨러레이션이 일치하지 않는 파드가 스케줄링되는 것을 막는 강한 테인트  

+ PreferNoSchedule  
다른 노드에 스케줄링될 수 없는 파드만 스케줄링  

+ NoExecute  
노드에 이미 실행 중인 파드를 축출(evict)  

+  NodeCondition  
특정 조건을 만족시키는 노드를 테인트  

추가로 테인트 기반 축출이라는 강력한 개념이 있습니다. 실행 중인 파드를 축출하는 겁니다. 예를 들어 디스크 드라이브 문제 때문에 노드가 비정상이 되었다면, 테인트 기반 축출을 통해 클러스터의 다른 정상 노드에 파드를 다시 스케줄링할 수 있습니다.  

#### 8.3. 파드 리소스 관리  
<br/>

##### 8.3.1. 리소스 요청  
<br/>

쿠버네티스 리소스 요청에서는 컨테이너가 스케줄링하기 위해 X 크기의 CPU와 메모리를 필요로 한다고 정의합니다.  

클러스터의 가용한 리소스를 보기 위해 kubectl top을 사용합니다.  

```sh
kubectl top nodes
```

##### 8.3.2. 리소스 제한과 파드 서비스 품질  
<br/>

쿠버네티스 리소스 제한으로 파드의 최대 CPU와 메모리 크기를 정의합니다. 이 제한에 도달할 때 리소스마다 다른 일이 벌어집니다. CPU의 경우, 컨테이너는 지정된 제한보다 사용되지 못하게 막힙니다. 메모리가 한계에 도달하면 파드가 재시작됩니다. 파드는 동일한 호스트나 다른 호스트에서 재시작될 수 있습니다.  

컨테이너에 제한을 지정하는 것이 클러스터의 애플리케이션에게 공정하게 리소스를 배분하기 위한 모범 사례입니다.  

파드가 생성되면 다음 QoS중 하나가 할당됩니다.  

+ 보장
+ 폭발
+ 최선의 노력  

CPU와 메모리 모두 요청과 제한이 일치하면 보장 QoS이 할당됩니다. 폭발 QoS는 제한이 요청보다 높게 할당될 때입니다. 컨테이너는 요청을 보장받지만 제한까지만 치솟을 수 있습니다. 요청 또는 한계를 설정하지 않으면 최선의 노력 QoS가 할당됩니다.  

보장 QoS이고 파드에 여러 컨테이너가 존재하는 경우, 컨테이너별로 메모리 요청과 제한을 설정해야 합니다. 또한 CPU 요청과 제한도 설정해야 합니다. 모든 컨테이너에 요청과 제한이 설정되지 않으면 보장 QoS가 할당되지 않습니다.  

##### 8.3.3. PodDisruptionBudget  
<br/>

언젠가는 호스트에서 파드가 축출됩니다. 두 가지 유형의 축출이 있습니다. 자발적 중단과 비자발적 중단입니다. 비자발적 중단은 하드웨어 장애, 네트워크 분할, 커널 패닉(panic), 리소스 부족 등으로 인해 일어납니다. 자발적 추축은 클러스터 유지보수, 클러스터 오토스케일러의 할당 해제, 파드템플릿 업데이트로 인해 발생합니다. 애플리케이션에 미치는 영향을 최소화하려면 PodDisruptionBudget을 설정하여 파드가 축출될 때 애플리케이션의 가동 시간을 보장하세요. PodDisruptionBudget으로 자발적 축출 이벤트 기간에 가용한 최소 파드와 불가용한 최대 파드 정책을 설정할 수 있습니다. 노드 유지보수 때문에 드레인하는 경우를 자발적 축출의 예시로 볼 수 있습니다.  

예를 들어, 주어진 시간 동안 특정 애플리케이션에 속한 파드의 20% 이하가 다운될 수 없도록 지정할 수 있습니다. 또는 항상 가용해야 할 레플리카 수 X를 정책에 명시할 수 있습니다.  

PodDisruptionBudget을 퍼센트로 지정하며 파드 수가 정확히 정해지지 않습니다. 예를 들어 애플리케이션이 7개의 파드를 가지고 있고 maxAvailable을 50%로 지정했다면 해당 파드 수가 3개인지 4개인지 불명확합니다. 이런 경우 쿠버네티스는 가장 가까운 정수로 반올림하므로 maxAvailable은 4개의 파드가 됩니다.  

##### 8.3.4. 네임스페이스를 사용한 리소스 관리  
<br/>

네임스페이스는 쿠버네티스에 배포된 리소를 논리적으로 구분할 수 있습니다. 네임스페이스 별로 리소스쿼터, RBAC, 네트워크 정책을 설정할 수 있습니다. 멀티테넌시(multitenancy) 기능으로 팀이나 애플리케이션에 전용 인프라를 지정하지 않고도 워크로드를 분리할 수 있습니다. 이렇게 하면 논리적인 구분을 유지하는 동시에 클러스터 리소스를 최대로 활용할 수 있습니다.  

예를 들어 팀별로 네임스페이스를 생성하여 CPU, 메모리와 같은 가용한 리소스쿼터를 부여할 수 있습니다.  

네임스페이스를 설정하는 방법을 설계할 때는 특정 애플리케이션에 대한 접근을 제어하는 방법을 고려해야 합니다. 단일 클러스터를 사용하는 여러 팀이 있는 경우 가장 나은 방법은 팀에 네임스페이스를 할당하는 겁니다. 클러스터를 한 팀 전용으로 사용한다면 클러스터에 배포할 서비스별로 네임스페이스를 할당하는 것이 합리적입니다. 만능 해결책은 없습니다. 팀 조직과 역할에 맞춰 설계해야 합니다.  

쿠버네티스 클러스터를 배포한 이후, 다음 네임스페이스를 볼 수 있습니다.  

+ kube-system  
coredns, kube-proxy, metrics-server와 같은 쿠버네티스 내부 컴포넌트는 여기에 배포됩니다.  

+ default  
리소스 객체 안에 네임스페이스를 지정하지 않을 때 사용되는 기본 네임스페이스입니다.  

+ kube-public  
익명이나 인증되지 않은 콘텐츠, 예약된 시스템에 사용됩니다.  

기본 네임스페이스를 사용하면 리소스를 관리할 때 실수하기 쉬우므로 피하는 것이 좋습니다.  

kubectl을 이용해 네임스페이스 관련 작업을 할 때, --namespace 플래그 또는 줄여서 -n을 넣어야 합니다.  

```sh
kubectl create ns team-1
kubectl get pods --namespace team-1
```

특정 네임스페이스 kubectl 컨텍스트를 설정하면, 모든 명령어에 --namespace 플래그를 추가할 필요가 없어 편리합니다. 다음 명령어로 네임스페이스 컨텍스트를 설정할 수 있습니다.  

```sh
kubeclt config set-context my-context --namespace=team-1
```

여러 네임스페이스와 클러스터를 다룰 때 서로 다른 클러스터 컨텍스트를 설정하는 것은 번거롭습니다. 대신 kubedns와 kubectx를 사용하면 다양한 네임스페이스와 컨텍스트를 쉽게 전환할 수 있습니다.  

##### 8.3.5. 리소스쿼터  
<br/>

여러 팀과 애플리케이션이 단일 클러스터를 공유할 때는 네임스페이스에 리소스쿼터를 설정해야 합니다. 리소스쿼터는 단일 네임스페이스가 할당된 리소스 이상을 사용할 수 없도록 클러스터를 논리적인 단위로 분배합니다.  

+ 계산 리소스  
  + request.cpu: CPU 요청의 합은 이 값을 초과할 수 없습니다.
  + limits.cpu: CPU 제한의 합은 이 값을 초과할 수 없습니다.
  + request.memory: 메모리 요청의 합은 이 값을 초과할 수 없습니다.
  + limits.memory: 메모리 제한의 합은 이 값을 초과할 수 없습니다.

+ 스토리지 리소스
  + request.storage: 스토리지 요청의 합은 이 값을 초과할 수 없습니다.
  + persistentvolumeclaims: 네임스페이스에 존재할 수 있는 퍼시스턴트볼륨클레임의 합은 이 값을 초과할 수 없습니다.
  + storageclass.request: 특정 스토리지클래스와 연관된 볼륨클레임은이 값을 초과할 수 없습니다.
  + storageclass.pvc: 네임스페이스에 존재하는 퍼시스턴트볼륨클레임의 합은 이 값을 초과할 수 없습니다.

+ 객체 카운트 쿼터의 예
  + 카운트/pvc
  + 카운트/서비스
  + 카운트/디플로이먼트
  + 카운트/레플리카셋  

위와 같이 쿠버네티스에서 네임스페이스별로 리소스쿼터를 정교하게 분할할 수 있습니다. 이를 통해 멀티테넌트 클러스터에서 리소스 사용을 보다 효율적으로 조절할 수 있습니다.  

실제로 네임스페이스에서 리소스쿼터를 설정하는 법을 살펴봅시다. 다음 YAML 파일을 team-1 네임스페이스에 적용해봅시다.  

```yaml
apiVersion: v1
kind: ResourceQuota
metadata:
  name: mem-cpu-demo
  namespace: team-1
spec:
  hard:
    request.cpu: "1"
    request.memory: 1Gi
    limits.cpu: "2"
    limits.memory: 2Gi
    persistentvolumeclaims: "5"
    request.storage: "10Gi"
```

```sh
kubectl apply quota.yaml -n team-1
```

이 예제에서는 네임스페이스에 CPU, 메모리, 스토리지를 설정합니다.  

리소스쿼터가 디플로이먼트에 어떤 영향을 미치는지 보기 위해 애플리케이션을 배포해봅시다.  

이 예제에서 알 수 있듯이 리소스쿼터를 설정하면 네임스페이스에 설정한 정책에 따라 리소스 배포를 막을 수 있습니다.  

##### 8.3.6. LimitRange  
<br/>

쿠버네티스는 어드미션 컨트롤러를 제공합니다. 이를 통해 명세에 아무런 표시가 없을 때 자동 설정을 할 수 있습니다.  

먼저, 쿼터와 LimitRange를 작업할 네임스페이스를 생성합니다.  

```sh
kubectl create ns team-1
```

LimitRange의 limits 안의 defaultRequest를 네임스페이스에 적용합니다.  

```yaml
apiVersion: v1
kind: LimitRange
metadata:
  name: team-1-limit-range
spec:
  limits:
  - default:
      memory: 512Mi
    defaultRequest:
      memory: 256Mi
    type: Container
```

이를 limitranger.yaml에 저장하고 kubectl apply를 실행합니다.  

```sh
kubectl apply -f limitranger.yaml -n team-1
```

리소스쿼터를 사용할 때는 LimitRange를 사용해야 합니다. 요청과 제한이 명세에 설정되지 않으면 배포가 거절되기 때문입니다.  

##### 8.3.7. 클러스터 확장  
<br/>

클러스터에 배포하기에 앞서 클러스터 안에서 사용할 인스턴스 크기를 결정해야 합니다. 특히 단일 클러스터에 워크로드가 혼재할 때 이것은 과학보다는 기술에 가깝습니다. 먼저 클러스터에서의 좋은 시작점이 무엇일지 파악해야 합니다. CPU와 메모리의 적절한 균형을 목표로 세우는 것도 하나의 방법입니다. 클러스터에 적합한 인스턴스 크기를 결정한 후에는 쿠버네티스의 핵심 기능을 사용하여 확장을 관리할 수 있습니다.  

###### 8.3.7.1. 수동 확장  
<br/>

쿠버네티스에서는 클러스터를 쉽게 확장할 수 있습니다. 특히 Kops나 관리형 쿠버네티스 오퍼링과 같은 도구를 사용하면 더 쉽습니다. 클러스터를 수동을 확장한다는 것은 일반적으로 새로운 노드 수를 결정한다는 뜻입니다. 이를 통해 서비스는 클러스터에 새로운 노드를 추가합니다.  

이 도구로 노드 풀을 생성할 수 있습니다. 즉 이미 실행 중인 클러스터에 새로운 인스턴스 유형의 풀을 추가할 수 있습니다. 이는 단일 클러스터에 워크로드가 혼재되어 있을 때 매우 유용합니다. 예를 들어 어떤 워크로드는 많은 CPU를 사용할 수 있지만, 또 다른 워크로드는 메모리 사용률이 높을 수 있습니다. 노드 풀을 이용하면 단일 클러스터 내에서 여러 인스턴스 유형을 섞을 수 있습니다.  

아마도 모두가 수동이 아닌 자동 확장을 원할 겁니다. 하지만 클러스터 자동 확장을 위해서는 고려할 사항이 많으므로 리소스가 필요할 대 사전 예방적으로 수동 확장하는 것부터 시작하는 편이 낫습니다. 워크로드의 변동성이 심한 경우라면 클러스터 자동 확장이 유용할 수 있습니다.  

###### 8.3.7.2. 클러스터 자동 확장  
<br/>

쿠버네티스는 클러스터의 최소 가용 노드와 최대 가용 노드를 설정할 수 있는 부가 기능인 클러스터 오토스케일러를 제공합니다. 오토스케일러는 대기 상태의 파드가 존재할 때 확장을 결정합니다. 예를 들어 쿠버네티스 스케줄러가 4,000Mib의 메모리 요청을 가진 파드를 스케줄링하려고 할 때 클러스터는 2,000Mib만 가용하다면 파드는 대기 상태가 됩니다. 나중에 새로운 노드가 클러스터에 추가되는 즉시, 대기 중인 파드가 스케줄링됩니다. 클러스터 오토스케일러의 단점은 파드가 대기 상태가 되어야 새로운 노드가 추가된다는 겁니다. 그래서 워크로드가 온라인이 되려면 결국 새로운 노드를 대기해야 합니다. 쿠버네티스 v1.15 이후부터는 클러스터 오토스케일러가 사용자 정의 메트릭 기반의 확장을 지원하지 않습니다.  

클러스터 오토스케일러가 더는 리소스가 필요 없는 클러스터의 크기를 줄일 수도 있습니다. 리소스가 필요 없는 경우, 노드를 드레인하고 파드를 새로운 노드에 다시 스케줄링합니다. 드레인이 수행될 때 애플리케이션에 악영향을 주지 않도록 PodDiruptionBudget을 사용해야 합니다.  

##### 8.3.8. 애플리케이션 확장  
<br/>

쿠버네티스는 클러스터에서 애플리케이션을 확장할 수 있는 여러 방법을 제공합니다. 디플로이먼트 안의 레플리카 수를 수동으로 변경하여 애플리케이션을 확장할 수 있습니다. 레플리카셋이나 복제 컨트롤러를 통해 변경할 수 있지만, 권장하지 않습니다. 수동 확장은 정적인 워크로드 또는 워크로드가 급증하는 시점을 알고 있을 때 유용합니다. 하지만 예기치 않은 급증이 발생하거나 정적이지 않은 워크로드라면 수동 확장이 적합하지 않습니다. 다행히도 쿠버네티스는 자동으로 워크로드를 확장할 수 있는 HPA를 제공합니다.  

먼저 다음 디플로이먼트 매니페스트를 적용하여 디플로이먼트를 수동으로 확장하는 법을 살펴봅시다.  

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: frontend
spec:
  replicas: 3
  template:
    metadata:
      name: frontend
      labels:
        app: frontend
    spec:
      containers:
      - image: nginx:alpine
        name: frontend
        resources:
          request:
            cpu: 100m
```

이 예제는 프런트엔드 서비스를 세 개의 레플리카로 배포합니다. kubectl scale 명령을 이용하면 디플로이먼트의 규모를 확장할 수 있습니다.  

```sh
kubectl sacle deployment frontend --replicas 5
```

이제 프런트엔드 서비스는 다섯 개의 레플리카가 됩니다.  

##### 8.3.9. HPA를 이용한 확장  
<br/>

쿠버네티스 HPA는 CPU, 메모리, 사용자 정의 메트릭 기반으로 디플로이먼트를 확장할 수 있습니다. 디플로이먼트를 감시하여 쿠버네티스 metrics-server로부터 메트릭을 풀합니다. 또한 가용 파드의 최소 수, 최대 수를 설정합니다. 예를 들어 최소 수 3, 최대 수 10으로 HPA 정책을 정의한다면 디플로이먼트가 80% CPU 사용률에 도달했을 때 확장됩니다. 애플리케이션 버그나 이슈 때문에 HPA가 레플리카를 무한정 늘리지 않도록 하려면 최소 수와 최대 수를 설정하는 것이 중요합니다.  

HPA는 메트릭 동기화, 레플리카 확장과 축소를 위한 다음 기본 설정을 가집니다.  

+ horizontal-pod-autoscaler-sync-period: 메트릭 동기화 시간은 기본 30초
+ horizontal-pod-autoscaler-upscale-delay: 두 확장 사이의 레이턴시는 기본 3분
+ horizontal-pod-autoscaler-downscale-delay: 두 축소 사이의 레이턴시는 기본 5분  

상대적인 플래그를 이용하여 기본값을 변경할 수 있지만, 신중해야 합니다. 워크로드의 변동성이 큰 경우 특수한 사례에 맞게 워크로드를 최적화할 수 있도록 연습해야 합니다.  

이전 예제에서 배포한 프런트엔드 애플리케이션을 위한 HPA 정책을 설정해봅시다.  

먼저 포트 80으로 디플로이먼트를 노출합니다.  

```sh
kubectl expose deployment frontend --port 80
```

다음으로 자동 확장 정책을 설정합니다.  

```sh
kubectl autoscale deployment frontend --cpu-percent=50 --min=1 --max=10
```

레플리카 수를 최소 1에서 최대 10까지 확장할 수 있는 정책을 설정합니다. CPU 부하가 50%에 도달하면 확장 작업을 수행합니다.  

몇 분만 기다리면 레플리카가 자동으로 확장됩니다.  

##### 8.3.10. 사용자 정의 메트릭을 사용한 HPA  
<br/>

메트릭 서버 API를 이용하면 사용자 정의 메트릭을 사용하여 애플리케이션을 확장할 수 있습니다. 사용자 정의 메트릭 API와 메트릭 애그리게이터(aggregator)는 서드파티 공급자가 플러그인으로 메트릭을 확장할 수 있도록 하며 HPA는 이러한 외부 메트릭을 기반으로 확장할 수 있습니다. 예를 들어, 기본 CPU나 메모리 메트릭만 사용하는 것이 아니라 외부 스토리지 큐에서 수집한 메트릭을 기반으로도 확장할 수 있습니다. 사용자 정의 메트릭을 사용하여 자동으로 확장하면 애플리케이션에 특화된 메트릭이나 외부 서비스 메트릭으로 확장할 수 있습니다.  

##### 8.3.11. 수직 파드 오토스케일러  
<br/>

VPA는 레플리카를 확장하지 않으므로 HPA와는 다릅니다. 대신 자동으로 요청을 늘립니다. VPA를 사용하면 자동으로 파드 요청을 늘리고 줄이기 때문에 이러한 요청을 수동으로 조절할 필요가 없습니다. 아키텍처로 인해 확장할 수 없는 워크로드의 경우 자동으로 리소스를 확장하는 데 효과적입니다. 예를 들어 MySQL 데이터베이스를 스테이트리스 웹 프런트엔드와 같은 방식으로 확장할 수는 없습니다. 그 대신 MySQL의 마스터 노드가 워크로드에 따라 자동으로 확장되도록 설정할 수 있습니다.  

VPA는 HPA보다 더 복잡하며 다음 세 개의 컴포넌트를 가집니다.  

+ Recommander: 현재와 과거의 리소스 사용량을 모니터하고 컨테이너의 CPU와 메모리 요청의 추천값을 제공합니다.
+ Updater: 파드가 정확한 리소스를 가지고 있는지 확인하고 그렇지 않으면 종료시킵니다. 컨트롤러는 업데이트된 요청으로 컨트롤러를 다시 생성합니다.
+ Admission Plugin: 정확한 리소스 요청을 새로운 파드에 설정합니다.  

v1.15부터는 운영 배포에서 VPA를 권장하지 않습니다.  

#### 8.4. 리소스 관리 모범 사례  
<br/>

+ 파드안티어피니트를 사용해 워크로들 여러 가용 영역으로 분산하여 애플리케이션의 고가용성을 보장합니다.

+ GPU가 활성화된 노드와 같은 특수한 하드웨어를 사용하는 경우, 테인트를 사용해 GPU가 필요한 워크로드만 해당 노드에 스케줄링되도록 합니다.

+ NodeCondition 테인트를 사용하여 노드 실패나 성능 저하를 사전에 방지합니다.

+ 파드 명세에 노드 셀렉터를 적용하여 특수한 하드웨어를 가진 노드에 파드를 스케줄링합니다.

+ 운영으로 이동하기 전에 다양한 노드 크기를 실험하여 비용과 성능의 적절한 조합을 찾습니다.

+ 클러스터에 배포된 모든 파드에 대해 메모리와 CPU 제한을 설정합니다.

+ 여러 팀 또는 여러 애플리케이션이 공정한 리소스를 할당받을 수 있도록 ResourceQuata를 사용합니다.

+ 제한과 요청이 설정되지 않은 파드 명세에 기본 제한과 요청을 설정하기 위해 LimitRange를 구현합니다.

+ 쿠버네티스의 워크로드 프로필을 파악하기 전까지는 수동 클러스터 확장부터 시작합니다. 자동 확장을 사용할 수도 있지만 노드 기동 시간과 클러스터 축소에 대한 추가적인 고민이 필요합니다.

+ 변동성이 있거나 예상치 못한 정점이 있는 워크로드의 경우에는 HPA를 사용합니다.  

### 9. 네트워킹, 네트워크 보안, 서비스 메시  
<br/>

#### 9.1. 쿠버네티스 네트워크 원칙  
<br/>

+ 동일한 파드 내의 컨테이너 간 통신  
동일한 네트워크 공간을 공유합니다. 즉 컨테이너 사이의 localhost 통신이 가능합니다. 따라서 동일한 파드 내의 컨테이너는 다른 포트를 열어야 합니다. 이것은 리눅스 네임스페이스와 도커 네트워크를 통해서 이루어집니다. 파드의 네트워킹을 담당하는 모든 파드에서 일시 중지된(paused) 컨테이너를 사용해 파드 내의 모든 컨테이너가 동일한 로컬 네트워크에 존재하게 됩니다.  

+ 파드 간의 통신  
모든 파드는 네트워크 주소 변환(network address translation, NAT) 없이 통신할 수 있어야 합니다. 즉 수신하는 파드에서 볼 수 있는 송신자의 파드 IP 주소가 실제 IP 주소입니다. 사용하는 네트워크 플러그인에 따라 처리 방식이 달라집니다. 동일한 노드에 존재하는 파드 사이에서는 이 규칙이 적용됩니다. 그리고 동일한 클러스터의 서로 다른 노드에 존재하는 파드 사이에서도 적용됩니다. 또한 NAT 없이 파드와 직접 통신할 수 있는 노드에도 이 규칙이 적용됩니다. 따라서 필요에 따라 호스트 기반의 에이전트와 시스템 데몬이 파드와 통신할 수 있습니다.  

+ 서비스와 파드 간의 통신  
쿠버네티스의 서비스는 견고한 IP 주소와 포트를 나타내며 각 노드는 서비스에 연계된 엔드포인트로 트래픽을 전달합니다. 서비스를 구현하는 기술은 쿠버네티스가 발전하면서 달라지긴 했지만 주로 iptables을 이용하는 방법과 IP 가상 서버를 이용하는 방법이 있습니다. 요즘은 대부분 의사계층 4(pseudo-layer 4) 로드 밸런서 기능을 제공하는 iptables로 구현합니다.  

#### 9.2. 네트워크 플러그인  
<br/>

초창기에 SIG(Special Internet Group)에서는 플러그인이 가능한 아키텍처를 지향한 네트워킹 표준을 제시했습니다. 이 표준을 통해 많은 서드파티 네트워킹 프로젝트가 연동되었으며 쿠버네티스 워크로드에서 향상된 기능을 사용할 수 있게 되었습니다. 가장 기본적인 것은 쿠버네티스가 제공하는 플러그인인 Kubenet입니다. 나머지는 컨테이너 네트워크 인터페이스(Container Network Interface, CNI) 명세를 준수하는 플러그인으로 컨테이너를 위한 범용적인 플러그인 네트워크 솔루션입니다.  

##### 9.2.1. Kubenet  
<br/>

Kubenet은 쿠버네티스에서 바로 사용할 수 있는 가장 기본적인 네트워크 플러그인입니다. 이 플러그인은 파드가 연결될 가상 이더넷(Ethernet) 쌍인 리눅스 브릿지 cbr0을 제공합니다. 파드가 연결되면 클러스터의 노드에 분산되어 있는 사이더(Classless Inter-Domain Routing, CIDR) 범위 내의 IP 주소를 얻게 됩니다. IP 위장(masquerade) 플래그도 있습니다. 파드 CIDR 범위 외부의 IP로 전달되는 트래픽이 위장하려면 이 플래그를 설정해야 합니다. 이것은 파드 간의 통신 규칙을 따르지 않습니다. 파드 CIDR 외부로 전달되는 트래픽은 NAT를 거치기 때문입니다. 패킷이 다른 노드로 전송될 때 일종의 라우팅은 이 트래픽을 올바른 노드로 전달하는 역할을 합니다.  

##### 9.2.2. Kubenet 모범 사례  
<br/>

+ Kubenet으로 가장 단순한 네트워크 스택을 구축할 수 있으며 복잡한 네트워크에서 귀중한 IP 주소를 절약할 수 있습니다. 대표적으로 온프레미스 데이터 센터에 확장된 클라우드 네트워크가 있습니다.

+ 파드 CIDR 범위가 각 클러스터의 파드의 잠재적인 최대 크기를 처리할 만큼 충분히 큰지 확인해야 합니다. kubelet에 설정된 노드당 기본 파드는 110이지만 이는 조정될 수 있습니다.

+ 적합한 노드의 파드로 트래픽이 전송될 수 있도록 경로 규칙을 이해하고 올바른 계획은 세워야 합니다. 클라우드 공급자 환경에서는 일반적으로 자동화되지만 온프레미스 또는 예외적인 사례에서는 자동화와 함께 견고한 네트워크 관리가 필요합니다.

##### 9.2.3. CNI 플러그인  
<br/>

CNI 플러그인 명세에는 몇 가지 기본적인 요구사항이 있습니다. CNI와의 인터페이스, 기본적인 API 동작, 클러스터에서 사용되는 컨테이너 런타임과의 인터페이스입니다. 네트워크 관리 컴포넌트는 CNI에 의해 정의되었지만, 이들은 모두 IP 주소 관리를 포함해야 하며 적어도 네트워크에 컨테이너 추가와 삭제를 허용해야 합니다. 이 명세는 rkt 네트워킹 제안서에서 파생되었습니다.  

코어CNI(coreCNI) 프로젝트는 CNI 플러그인 명세의 기본 요구사항을 만족하는 플러그인을 구현할 수 있는 라이브러리를 제공합니다. 다양한 기능을 수행하는 다른 플러그인을 호출할 수도 있습니다. 이러한 유연성 때문에 컨테이너 네트워크에 사용할 수 있는 수많은 CNI 플러그인 만들어졌습니다. CNI 플러인의 예시로는 클라우드 공급자가 제공하는 마이크로소프트 애저 네이티브 CNI, 아마존 VPC CNI 플러그인부터 전통적인 누아지(Nuage) CNI, 주니퍼 네트워크 콘트레일/텅스텐 패브릭(Juniper Networks Contrail/Tunsten Fabric), VM웨어(VMWare) NSX가 있습니다.  

##### 9.2.4. CNI 플러그인 모범 사례  
<br/>

네트워킹은 쿠버네티스 환경에서 매우 중요한 컴포넌트입니다. 애플리케이션 통신의 신뢰성을 보장하려면 쿠버네티스 내의 가상 컴포넌트와 물리적인 네트워크 환경 간의 상호작용을 신중하게 설계해야 합니다.  

+ 인프라의 전반적인 네트워킹 목표를 달성하는 데 필요한 기능을 평가해야 합니다. 일부 CNI 플러그인은 고가용성, 아중 클라우드 연결성, 쿠버네티스 네트워크 정책 지원 등 다양한 기능을 제공합니다.

+ 공개 클라우드 공급자를 통해 클러스터를 실행 중이라면 클라우드 공급자의 소프트웨어 정의 네트워크(software defined network, SDN)가 CNI 플러그인을 실제로 지원하는지 확인해야 합니다.

+ 네트워크 보안 도구, 네트워크 관찰성, 관리 도구가 CNI 플러그인과 호환되는지 확인하고, 그렇지 않다면 대체할 수 있는 도구를 조사해야 합니다. 쿠버네티스와 같은 대규모 분산 시스템으로 전환할 때는 이러한 필요성이 확대되기 때문에, 관찰성이나 보안 기능에 문제가 없는지 확인해야 합니다. 위브웍스(Weaveworks)의 위브 스코프(Weave Scope), 다이나트레이스(Dyantrace), 시스딕 같은 도구를 쿠버네티스 환경에 추가할 수 있으며 각자의 장점이 있습니다. 애저 AKS, 구글 GCE 또는 아마존 EKS와 같은 클라우드 공급자가 관리하는 서비스에서 실행 중이라면 애저 컨테이너 인사이트, 네트워크 와처(Network Watcher), 스택드라이버, 아마존 클라우드워치와 같은 내장 도구를 사용합니다. 최소한 네트워크 스택과 네 가지 황금 신호에 대한 통찰력을 제공하는 도구를 사용해야 합니다. 네 가지 황금 신호는 레이턴시, 트래픽, 오류, 포화도이며 구글 SRE 팀과 롭 에와스척에 의해 널리 알려졌습니다.

+ SDN 네트워크 공간과 분리된 별도의 오버레이 네트워크를 제공하지 않는 CNI를 사용하는 경우, 노드 IP, 파드 IP, 내부 로드 밸런서와 클러스터 업그레이드, 프로세스 확장에 따른 오버헤드를 감당할 수 있는 적절한 네트워크 주소 공간이 존재하는지 확인해야 합니다.  

#### 9.3. 쿠버네티스의 서비스  
<br/>

파드가 쿠버네티스 클러스터에 배포되면 쿠버네티스 네트워킹의 기본 규칙과 이 규칙을 실현하는 네트워크 플러그인 때문에, 파드는 오직 같은 클러스터 내의 다른 파드와 직접 통신할 수 있습니다. 노드와 동일한 네트워크 공간의 IP를 파드에 부여하는 CNI 플러그인이라면 기술적으로는 파드 IP를 알면 클러스터 외부에서 직접 접근할 수 있습니다. 그러나 쿠버네티스 파드는 수명이 짧기 때문에 이 방법은 파드 서비스에 접근할 때 효율적이지 않습니다. 파드에서 실행 중인 API에 접근하는 기능이나 시스템이 있다고 가정해봅시다. 처음엔 문제가 없을지 몰라도 나중에는 자발적 또는 비자발적 중단으로 인해 파드가 사라질지도 모릅니다. 이때 쿠버네티스는 새로운 이름과 IP 주소로 대체 파드를 생성하므로 이를 찾을 수 있는 메커니즘이 필요합니다. 이것이 바로 서비스 API가 필요한 순간입니다.  

서비스 API는 내구성 있는 IP와 포트를 쿠버네티스 클러스터 내에 할당하고, 서비스의 엔드포인트를 적절한 파드에 자동으로 매핑합니다. 이전에 언급한 iptables 또는 리눅스 노드의 IP 가상 서버를 통해 할당된 서비스 IP와 포트를 엔드포인트 또는 파드의 실제 IP에 매핑하는 겁니다. 이를 관리하는 컨트롤러 kube-proxy 서비스이며 클러스터의 각 노드에서 실행되어 iptables 규칙을 조작합니다. 서비스 객체를 정의할 때 서비스 타입도 정의해야 합니다. 서비스 타입은 엔드포인트가 클러스터 내부에만 노출되는지 혹은 클러스터 외부에도 노출되는지의 여부를 결정합니다.  

##### 9.3.1. ClusterIP 서비스 타입  
<br/>

서비스 타입을 명시하지 않을 때 기본값은 ClusterIP입니다. ClusterIP는 지정된 서비스 CIDR 범위 내에서 IP가 할당됨을 의미합니다. 이 IP는 서비스 객체와 함께 오래 지속되므로 셀렉터 필드를 통해 백엔드 파드에 IP, 포트, 프로토콜을 매핑합니다. 서비스를 선언하면 서비스의 DNS 이름이 만들어집니다. DNS 이름을 통해 클러스터 내에서 서비스 검색을 용이하게 하고, 워크로드는 서비스 이름으로 DNS를 조회해 클러스터 내의 다른 서비스와 통신할 수 있습니다.  

다른 네임스페이스에서 서비스를 찾을 때 사용하는 DNS 패턴은 &lt;service_name&gt;.&lt;namespace_nameA&gt;.svc.cluster.local입니다.  

서비스 정의에 셀렉터가 없다면 엔드포인트 API를 사용해 엔드포인트를 명시적으로 정의할 수 있습니다. 셀렉터 속성을 이용해 셀렉터와 일치하는 파드로부터 엔드포인트를 자동으로 업데이트하는 대신, 특정 엔드포인트인 IP와 포트를 서비스에 추가하는 겁니다. 몇 가지의 유용한 시나리오가 있습니다. 예를 들어 테스트 시점에는 클러스터에 없는 데이터베이스를 사용하고 나중에 쿠버네티스에 배포되는 데이터베이스로 서비스를 변경하는 시나리오입니다. 다른 서비스처럼 kube-proxy에 의해 관리되지 않으므로 헤드리스 서비스라고 불립니다.  

##### 9.3.2. NodePort 서비스 타입  
<br/>

NodePort 서비스 타입은 클러스터의 각 노드의 고수준 포트를 각 노드의 서비스 IP와 포트에 할당합니다. NodePort의 고수준 포트의 범위는 30,000부터 32,767 사이이며 정적으로 할당되거나 서비스 명세 안에 명시할 수 있습니다. NodePort는 일반적으로 자동 로드 밸런스 구성을 제공하지 않는 온프레미스 클러스터 또는 맞춤형 솔루션에서 사용됩니다. 클러스터 외부에서 서비스에 직접 접근하려면 NodeIP:NodePort를 사용하면 됩니다.  

##### 9.3.3. ExternalName 서비스 타입  
<br/>

ExternalName 서비스 타입은 현업에서는 거의 사용되지 않지만, 클러스터 수준의 내구성을 가진 DNS 이름을 외부 DNS 서비스에 전달할 때 유용합니다. 일반적인 예는 mymogodb.documents.azure.com과 같은 클라우드 공급자가 제공하는 고유한 DNS를 가지는 클라우드 공급자의 외부 데이터베이스 서비스입니다.  

기술적으로는 Environment 변수를 사용해 파드 명세에 간단히 추가할 수 있습니다. 그러나 prod-mongodb와 같이 일반적인 이름을 사용하는 것이 유리합니다. 이렇게 하면 실제로 가리키는 데이터베이스를 변경할 때 서비스 명세만 바꾸면 됩니다. 하지만 Environment 변수를 변경하려면 파드를 재시작해야 합니다.  

##### 9.3.4. 로드 밸런서 서비스 타입  
<br/>

로드 밸런서는 특별한 서비스 타입입니다. 클라우드 공급자나 프로그램이 가능한 클라우드 인프라 서비스를 통해 자동화할 수 있습니다. 쿠버네티스 클러스터의 인프라 공급자가 제공하는 로드 밸런싱 메커니즘을 배포할 수 있는 유일한 방법이 LoadBalancer 타입입니다. LoadBalancer 동작 방식은 AWS, 애저, GCE, 오픈스택 등에서 거의 동일합니다. 일반적으로 공개된 로드 밸런스 서비스를 생성하지만 각 클라우드 공급자의 내부 전용 로드 밸런서, AWS 일래스틱 로드 밸런싱 설정 인자 등과 같은 기능을 활성화하는 특별한 애너테이션이 존재합니다. 또한 다음과 같이 서비스 명세에 실제로 사용하는 로드 밸런서 IP와 허용할 소드 범위를 정의할 수 있습니다.  

##### 9.3.5. 인그레스와 인그레스 컨트롤러  
<br/>

기술적으로 쿠버네티스의 서비스 타입은 아니지만, 인그레스 명세에는 쿠버네티스의 워크로드가 트래픽을 수신할 때 중요한 개념이 있습니다. 서비스 API로 정의된 서비스는 기본적인 수준인 계층 3/4 로드 밸런싱을 제공합니다. 실제로 쿠버네티스에 배포된 대부분의 무상태 서비스는 높은 수준의 트래픽 관리가 필요합니다. 일반적으로 애플리케이션 수준의 제어, 특히 HTTP 프로토콜을 관리해야 합니다.  

인그레스 API는 HTTP 수준의 라우터로, 호스트와 경로 기반 규칙으로 특정 백엔드 서비스에게 트래픽을 전달합니다. 예를 들어 www.evilllgenius.com 웹 사이트에 /registration와 /labaccess 두 가지 경로가 있다고 가정해봅시다. 각각 쿠버네티스에 등록된 서비스인 regsvc와 labaccess-svc을 제공합니다. 인그레스 규칙을 정의하여 www.evillgenius.com/registration에 대한 요청은 reg-svc 서비스와 해당 엔드포인트 파드로 전달되고, 마찬가지로 www.evillgenius.com/labaccess에 대한 요청도 labaccess-svc 서비스의 적절한 엔드포인트로 전달되도록 할 수 있습니다. 또한 인그레스 API는 호스트 기반 라우팅을 허용해 단일 인그레스에서 여러 호스트를 지원합니다. 추가 기능으로 포트 443에서 TLS 종료에 대한 인증서 정보를 가진 쿠버네티스 시크릿을 선언할 수 있습니다. 그리고 경로를 지정하지 않은 경우, 표준 404 요류보다 더 나은 UX를 제공하는 기본 백엔드가 있습니다.  

특정 TLS와 기본 백엔드 구성에 대한 세부 사항은 인그레스 컨트롤러가 처리합니다. 인그레스 컨트롤러는 인그레스 API와 분리되어 있으며 운영자는 NGINX, Traefik, HAProxy 등과 같은 인그레스 컨트롤러를 선택해 배포할 수 있습니다. 이름에서 알 수 있듯이 인그레스 컨트롤러는 다른 쿠버네티스 컨트롤러와 마찬가지로 컨트롤러지만 시스템의 일부가 아니며, 동적 구성을 위한 쿠버네티스 인그레스 API와 인터페이스하는 서드파티 컨트롤러입니다. 가장 대중적인 인그레스 컨트롤러는 NGINX이며 쿠버네티스 프로젝트에서 부분적으로 관리됩니다. 이외에도 많은 오픈 소스와 상업용 인그레스 컨트롤러가 있습니다.  

##### 9.3.6. 서비스와 인그레스 컨트롤러 모범 사례  
<br/>

상호 연결된 애플리케이션이 존재하는 복잡한 가상 네트워크 환경을 구축할 때는 신중하게 계획해야 합니다. 애플리케이션의 여러 서비스가 서로 통신하거나 외부와 통신하는 것을 효과적으로 관리하려면 애플리케이션이 변경될 때마다 지속적으로 주의를 기울여야 합니다. 아래는 네트워크를 쉽게 관리할 수 있도록 돕는 모범 사례입니다.  

+ 클러스터 외부에서 접근하는 서비스의 수를 제한해야 합니다. 대부분의 서비스는 ClusterIP로 두고 외부 접근 서비스만 노출하는 것이 이상적입니다.

+ 노출해야 하는 서비스가 HTTP/HTTPS 기반이라면 인그레스 API와 컨트롤러를 사용하여 TLS 종료와 함께 트래픽을 서비스로 라우팅하는 것이 가장 좋습니다. 사용되는 인그레스 컨트롤러 타입에 따라 속도 제한, 헤더 재작성, OAuth 인증, 관찰 가능성 등의 기능을 애플리케이션 자체에 빌드하지 않고도 사용할 수 있습니다.

+ 웹 기반 워크로드의 안전한 수신에 필요한 기능을 가진 인그레스 컨트롤러를 선택해야 합니다. 하나로 표준화하여 전사적으로 사용하세요. 컨트롤러 구현마다 설정 애너테이션이 다르므로 배포 코드가 쿠버네티스 구현 간에 이식되는 것을 방지해야 합니다.

+ 인프라 관리와 인그레스 부하를 클러스터 외부로 옮길 수 있는 기능이 클라우드 서비스 공급자의 인그레스 컨트롤러에 있는지 평가해야 합니다. 당연히 쿠버네티스 API 설정도 가능해야 합니다.

+ API를 외부에 주로 제공한다면 API 기반 워크로드를 세부적으로 조절할 수 있는 Kong 또는 엠배서더(Ambassador)와 같은 인그레스 컨트롤러를 평가합니다. NGINX, Traefik 등도 일부 API 튜닝을 제공하지만 특정 API 프록시 시스템만큼 세밀하지는 않습니다.

+ 인그레스 컨트롤러를 쿠버네티스에 파드 기반 워크로드로 배포할 때 고가용성이 설계되었는지 확인하고 처리량의 성능을 집계해야 합니다. 메트릭을 관찰해 인그레스 규모를 적절하게 확장하고, 워크로드가 확장되는 동안 클라이언트 중단을 막을 수 있는 충분한 대비책이 있어야 합니다.  

#### 9.4. 네트워크 보안 정책  
<br/>

쿠버네티스에 내장된 네트워크폴리시(NetworkPolicy) API를 사용해 워크로드에 정의된 네트워크 수준의 인그레스와 이그레스 접근을 제어할 수 있습니다. 네트워크 정책을 통해 파드 그룹 간 또는 다른 엔드포인트로 통신하는 것을 제어할 수 있습니다. 네트워크폴리시 명세를 자세히 알아보려면 (쿠버네티스 API로 정의되어 있어서 혼란스러울 수는 있지만) 네트워크폴리시 API를 지우언하는 네트워크 플러그인이 필요합니다.  

네트워크 정책은 단순한 YAML 구조이며 데이터 센터 내의 간단한 트래픽 방화벽으로 볼 수 있습니다. 정책 명세에는 podSelector, ingress, egress, policyType 필드가 있습니다. 유일한 필수 필드는 podSelector이며 matchLabels를 가진 쿠버네티스 셀렉터와 동일한 규약을 따릅니다. 여러 네트워크폴리시 정의를 파드에 적용하면 효과가 가중됩니다. 네트워크폴리시 객체는 네임스페이스 수준의 객체이므로 podSelector에 셀렉터가 없으면 네임스페이스의 모든 파드에 정책이 적용됩니다. 인그레스 또는 이그레스 규칙이 정의되어 있으면 파드에 인그레스 또는 이그레스가 허용되는 화이트리스트(whitelist)가 생성됩니다. 즉 셀렉터와 일치하는 파드에 정책이 적용되면 인그레스 또는 이그레스 규칙에 명시되지 않은 모든 트래픽은 차단됩니다. 하지만 셀렉터와 일치하지 않는 파드는 아무런 정책에 속하지 않기 때문에 모든 인그레스와 이그레스가 허용됩니다. 아무 것도 차단하지 않는 새로운 워크로드를 쿠버네티스에 간단히 배포할 수 있습니다.  

ingress와 egress 필드는 기본적으로 소스 또는 대상에 대한 규칙 목록으로 특정 CIDR 범위, podSelector, namespaceSelector입니다. 인그레스 필드를 비워두면 모든 인바운드(inbound)는 차단됩니다. 마찬가지로 이그레스를 비워두면 모든 아웃바운드(outbound)는 차단됩니다. 포트와 프로토콜 목록을 지정하면 허용할 통신 범위를 좁힐 수 있습니다.  

policyTypes 필드에는 정책 객체와 연관된 네트워크 정책 규칙 타입을 지정합니다. 해당 필드가 없으면 ingress와 egress 목록 필드를 참조합니다. 차이점이 있다면 이그레스는 policyTypes에 반드시 이그레스를 명시하고 이그레스 규칙 목록도 존재해야 동작한다는 겁니다. 인그레스는 명시적으로 정의할 필요가 없습니다.  

단일 네임스페이스에 배포된 세 개의 애플리케이션 계층의 프로토타입 예제를 사용해봅시다. 계층 레이블은 tier: "web", tier: "db", tier: "api" 입니다. 다음 네트워크폴리시 매니페스트는 각 계층의 트래픽을 적절히 제한합니다.  

기본 거부(default-deny) 규칙은 아래와 같습니다.  

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: default-deny-all
spec:
  podSelector: {}
  policyTypes:
  - Ingress
```

웹 계층 네트워크 정책은 아래와 같습니다.  

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: webaccess
spec:
  podSelector:
    matchLabels:
      tier: "web"
  policyTypes:
  - Ingress
  ingress:
  - {}
```

API 계층 네트워크 정책은 아래와 같습니다.  

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-api-access
spec:
  podSelector:
    matchLabels:
      tier: "api"
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: "web"
```

데이터베이스 계층 네트워크 정책은 아래와 같습니다.  

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-db-access
spec:
  podSelector:
    matchLabels:
      tier: "db"
  policyTypes:
  - Ingress
  ingress:
  - from:
    - podSelector:
        matchLabels:
          tier: "api"
```

##### 9.4.1. 네트워크 정책 모범 사례  
<br/>

전통적인 네트워크 트래픽 보안 시스템의 영역은 복잡한 네트워킹 규칙을 가진 물리적 하드웨어 장치였습니다. 이제 쿠버네티스 네트워크 정책을 통해 쿠버네티스에서 호스팅되는 애플리케이션의 트래픽을 분류하고 제어하기 위해 애플리케이션 중심적인 방식을 취할 수 있습니다. 어떤 정책 플러그인을 사용하든 다음과 같은 모범 사례를 적용할 수 있습니다.  

+ 천천히 진행하면서 파드로 인그레스되는 트래픽에 집중하세요. 인그레스와 이그레스 규칙을 복잡하게 만들면 네트워크 추적이 어려워집니다. 트래픽이 예상대로 흐르기 시작하면 중요한 워크로드의 흐름을 제어하기 위해 이그레스 규칙을 살펴보세요. 인그레스 명세는 인그레스 규칙 목록이 비어있더라도 많은 옵션을 기본값으로 설정합니다.

+ 사용 중인 네트워크 플러그인에 네트워크폴리시 API에 대한 자체 인터페이스가 있거나 잘 알려진 다른 플러그인을 지원하는지 확인하세요. 플러그인의 예로 캘리코(Calico), 실리움(Cilium), Kube-router, 로마나(Romana), 위브넷(Weave Net)이 있습니다.

+ 네트워크 팀이 '기본 거부' 정책을 자주 사용한다면 보안이 필요한 워크로드를 가진 클러스터의 각 네임스페이스에 다음과 같은 네트워크 정책을 만듭니다. 이렇게 하면 다른 네트워크 정책을 삭제하더라도 실수로 파드가 '노출'되지 않습니다.  

+ 인터넷에서 접근해야 하는 파드가 있는 경우, 레이블을 사용해 수신을 허용하는 네트워크 정책을 명시적으로 적용합니다. 패킷이 들어오는 실제 IP가 인터넷이 아니라 로드 밸런서, 방화벽, 그 외 네트워크 장치의 내부 IP라면 전체 흐름을 알아야 합니다. 예를 들어 allow-internet=true 레이블이 있는 파드로 모든 (외부를 포함한) 트래픽을 허용하려면 다음을 수행합니다.  

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: internet-access
spec:
  podSelector:
    matchLabels:
      allow-internet: "true"
  policyTypes:
  - Ingress
  ingress:
  - {}
```

+ 정책을 간단하게 만들기 위해 애플리케이션 워크로드를 단일 네임스페이스에 배치하세요. 네임스페이스 간 통신이 필요할 때는 흐름을 식별하기 위해 최대한 명시적인 이름과 구체적인 레이블을 사용하세요.

```yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: namespace-foo-2-namespace-bar
  namespace: bar
spec:
  podSelector:
    matchLabels:
      app: bar-app
  policyTypes:
  - Ingress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          networking/namespace: foo
      podSelector:
        matchLabels:
          app: bar-app
```

+ 제한 정책 수가 적은 테스트 네임스페이스를 만들어 정확한 트래픽 패턴을 조사하세요.  

#### 9.5. 서비스 메시  
<br/>

수천 개의 엔드포인트로 로드 밸런싱하는 수백 개의 서비스를 호스팅하는 단일 클러스터를 상상해봅시다. 이들은 서로 통신하고, 외부 리소스에 접근하며, 외부 리소스에서 접근할 수도 있습니다. 시스템 전체에서 입출력되는 엔드포인트의 동적인 특성 때문에 서비스 간의 모든 연결을 관리, 보안, 관찰, 추적하는 것은 상당히 어렵습니다. 서비스 메시의 개념은 쿠버네티스만이 가진 고유한 것이 아닙니다. 이 개념은 서비스가 전용 데이터 플레인과 컨트롤 플레인으로 연결하고 보호할 수 잇는 법을 제어하는 것입니다. 서비스 메시마다 기능이 다르지만 공통적으로 다음 기능들을 제공합니다.  

+ 메시에 분산되는 세밀한 트래픽 조절 정책을 통해 트래픽 로드 밸런싱이 가능합니다.
+ 메시 소속 서비스의 디스커버리가 존재합니다. 서비스에는 클러스터 또는 외부 클러스터의 서비스와 메시 소속의 외부 시스템이 있습니다.
+ 트래픽과 서비스를 관찰할 수 있습니다. 오픈트레이싱(OpenTracing) 표준을 따르는 예거(Jaeger) 또는 집킨(Zipkin)과 같은 추적 시스템을 사용하는 분산 서비스 추적이 있습니다.
+ 상호 인증을 사용한 메시 트래픽 보안이 가능합니다. 파드 대 파드, 데이터 센터 내의 트래픽 보안, 데이터 센터 간의 보안과 제어를 모두 제공하는 인그레스 컨트롤러가 있습니다.
+ 서킷 브레이커, 재시도, 데드라인 등과 같은 패턴을 이용한 복원력, 상태, 장애 방지 기능을 가집니다.  

여기서 핵심은 이러한 모든 기능이 애플리케이션 변경 없이 메시에 속한 애플리케이션에 통합된다는 겁니다. 이러한 놀라운 기능을 어떻게 쉽게 사용할 수 있을까요? 사이드카 프록시를 이용하면 됩니다. 현재 가용한 서비스 메시 대부분은 메시 소속인 각 파드에 데이터 플레인의 일부인 프록시를 주입합니다. 이 프록시를 통해 컨트롤 플레인 컴포넌트는 정책과 보안을 메시 전체에 동기화합니다. 워크로드를 가진 컨테이너는 네트워크 세부 사항을 몰라도 됩니다. 프록시가 대신 분산 네트워크의 복잡성을 처리합니다. 애플리케이션은 단지 localhost를 통해 프록시와 통신합니다. 실제로 컨트롤 플레인과 데이터 플레인은 다른 기술이지만 상호보완적입니다.  

대표적인 서비스 메시는 이스티오입니다. 구글, 리프트(Lyft), IBM의 프로젝트이며 엔보이(Envoy)를 데이터 플레인 프록시로 사용하고 독점적인 컨트롤 플레인 컴포넌트인 믹서(Mixer), 파이롯트(Pilot), 갤리(Galley), 시타델(Citadel)을 사용합니다. 다양한 수준의 기능을 제공하는 링커디 2도 있습니다. 링커디 2는 러스트(Rust)로 구축한 자체적인 데이터 플레인 프록시를 사용합니다. 해시코프는 최근에 많은 쿠버네티스 중심 서비스 메시 기능을 콘술에 추가했습니다. 콘술 사용자는 콘술 자체 프록시와 엔보이 중에서 선택할 수 있으며, 서비스 메시에 대한 상업적 지원도 받을 수 있습니다.  

쿠버네티스에서 서비스 메시 주제는 계속 변하므로 각 메시에 대한 자세한 설명은 의미가 없습니다. 하지만 서비스 메시 인터페이스(service mesh interface, SMI)와 관련해 마이크로소프트, 링커디, 해시코프, 솔로닷아이오(Solo.io), 킨보크(Kinvolk), 위브웍스는 의미 있는 노력을 하고 있습니다. SMI는 서비스 메시 기본 기능에 대한 표준 인터페이스입니다. 이 글을 쓰는 시점에 SMI 명세에는 신원, 전송 수준 암호화, 메시 내의 서비스 간 핵심 메트릭을 수집하는 트래픽 원격 분석, 서비스 간 트래픽을 조절하거나 가중치를 변경하는 트래픽 관리와 같은 트래픽 정책을 다룹니다. 이 프로젝트는 서비스 메시 벤더마다 자신의 제품에 다른 제품과 차별화할 수 있는 부가 기능을 구축함으로써 서비스 메시의 다양성을 추구합니다.  

##### 9.5.1. 서비스 메시 모범 사례  
<br/>

서비스 메시 커뮤니티는 지속적으로 성장하고 있습니다. 또한 점점 더 많은 기업이 요구사항을 정의하면서 서비스 메시 생태계가 크게 변화하고 있습니다. 다음의 모범 사례는 오늘날 서비스 메시가 해결하려는 공통적인 요구사항을 반영한 겁니다.  

+ 서비스 메시가 제공하는 주요 기능의 중요도를 평가하고, 가장 중요한 기능에서 오버헤드가 가장 적은 서비스 메시를 선택합니다. 여기서 오버헤드는 인적 기술 부채와 인프라 자원 부채입니다. 특정 파드 사이의 TLS가 필수라면 플러그인에 TLS가 통합된 CNI가 낫습니다.

+ 다중 클라우드 또는 하이브리드 시나리오같이 여러 시스템 간 메시가 필수 요건일까요? 모든 서비스 메시가 이 기능을 제공하는 것은 아니며, 기능을 제공한다고 하더라도 환경이 불안정해질 수 있는 복잡한 프로세스입니다.

+ 많은 서비스 메시는 오픈 소스 커뮤니티 기반 프로젝트입니다. 환경을 관리할 팀이 서비스 메시에 익숙하지 않다면 상업적 지원이 되는 제품이 더 낫습니다. 이스티오를 기반으로 상업적으로 지원되고 관리되는 서비스 메시를 제공하는 회사도 있습니다. 이스티오는 관리가 복잡하기 때문에 도움이 될 수 있습니다.  

### 10. 파드와 컨테이너 보안  
<br/>

#### 10.1. 파드시큐리티폴리시 API
<br/>

클러스터 수준 리소스인 파드시큐리티폴리스는 파드 명세에서 보안에 민감한 모든 필드를 정의하고 관리할 수 있는 유일한 곳입니다. 파드시큐리티폴리시 리소스를 생성하기 전, 클러스터 관리자와 사용자는 SecurityContext 설정을 워크로드마다 개별적으로 정의해야 합니다. 또는 클러스터에서 맞춤형 어드미션 컨트롤러를 활성화애 파드 보안을 시행해야 합니다.  

간단해 보이나요? 파드시큐리티폴리스는 효과적으로 구현하기 어렵기 때문에 많은 사람들이 꺼버리거나 다른 방법을 사용하는 방식으로 회피합니다. 하지만 파드시큐리티폴리시는 클러스터에서 실행할 수 있는 대상과 권한 수준을 제한하여 공격에 취약한 영역을 좁히는 가장 효과적인 방법입니다. 어렵더라도 파드시큐리티폴리시를 이해하기 위해 시간과 노력을 들일 것을 강력히 권장합니다.  

##### 10.1.1. 파드시큐리티폴리시 활성화  
<br/>

파드시큐리티폴리시 리소스에 정의된 조건을 시행하려면 어드미션 컨트롤러를 활성화시켜야합니다. 이는 요청 처리 과정 중 어드미션 단계에서 정책이 시행된다는 뜻입니다.  

모든 공개 클라우드 공급자와 클러스터 운영 도구에서 파드시큐리티폴리시의 활성화가 널리 제공되지는 않습니다. 사용할 수 있는 경우에는 일반적으로 옵트인(opt-in) 기능으로 제공됩니다.  

파드시큐리티폴리시를 활성화할 때 워크로드가 차단될 수 있으므로 주의해야 합니다. 만반의 준비를 하세요.  

파드시큐리티폴리시를 사용하려면 다음 두 가지를 완료해야 합니다.  

+ 파드시큐리티폴리시 API가 활성화되어 있는지 확인하세요(이 API가 지원되는 쿠버네티스 버전이라면 이미 활성화되어 있어야 합니다). kubectl get psp를 실행하여 활성화 여부를 확인할 수 있습니다. server doesn't have a resource type "PodSecurityPolicies"라는 응답이 아니라면 계속 진행하면 됩니다.

+ api-server flag --enable-admission-plugins를 이용해 파드시큐리티폴리스 어드미션 컨트롤러를 활성화합니다.  

이미 워크로드가 실행 중인 클러스터라면 어드미션 컨트롤러를 활성화하기 전에 필요한 모든 정책, 서비스 계정, 롤, 롤바인딩을 생성해야 합니다.  

--use-service-account-credentials=true 플래그를 kube-controller-manager에 추가할 것을 권장합니다. 이를 통해 kube-controller-manager내의 개별 컨트롤러에서 서비스 계정을 사용할 수 있습니다. kube-system 네임스페이스에서 정책을 보다 세밀하게 제어할 수 있습니다. 다음 명령을 실행하면 플래그가 설정되어 있는지 확인할 수 있습니다.  

PodSecurityPolicies를 정의하지 않으면 암묵적으로 거절된다는 것을 명시해야 합니다. 즉 워크로드와 일치하는 정책이 없으면 파드가 생성되지 않습니다.  

##### 10.1.2. 파드시큐리티폴리시 살펴보기  
<br/>

파드시큐리티폴리시를 이용한 파드 보안을 이해하기 위해 처음부터 끝까지의 작업을 함께 진행해봅시다. 정책을 생성하고 사용하는 과정의 운영 순서를 확실히 파악할 수 있습니다.  

먼저 정책을 변경하거나 생성하지 않고 UX를 테스트해봅시다. 다음은 디플로이먼트 내의 신뢰성 있는 일시 중지 컨테이너를 실행하는 간단한 테스트 워크로드입니다(파일 시스템에 pause-deployment.yaml로 저장합니다).  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pause-deployment
  namespace: default
  labels:
    app: pause
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pause
  template:
    metadata:
      labels:
        app: pause
    spec:
      containers:
      - name: pause
        image: k8s.gcr.io/pause
```

다음 명령을 실행하면 디플러이먼트와 관련 레플리카셋은 있지만 파드는 없다는 것을 확인할 수 있습니다.  

```sh
kubectl get deploy,rs,pods -l app=pause
```

describe 명령을 내리면 이벤트 로그에서 원인을 확인할 수 있습니다.  

```sh
kubectl describe replicaset -l app=pause
```

파드 보안 정책이 정의되지 않았거나 서비스 계정에 파드시큐리티폴리시를 사용하기 위한 접근 권한이 없기 때문입니다. 또한 kube-system 네임스페이스의 모든 시스템 파드가 RUNNING 상태임을 알 수 있습니다. 이는 해당 요청이 이미 어드미션 단계를 통과했다는 뜻입니다. 만약 이 시스템 파드의 재시작 이벤트가 있고 파드시큐리티폴리시 리소스가 정의되어 있지 않으면 테스트 워크로드와 동일한 상태였을 겁니다.  

테스트 워크로드 디플로이먼트를 삭제합니다.  

```sh
kubectl delete deploy -l app=pause
```

이제 파드 보안 정책을 정의하여 이 문제를 해결합시다. 전체 정책 설정 목록은 쿠버네티스 문서에 있습니다. 다음 정책은 쿠버네티스 문서에 제공된 예제를 조금 변형한 겁니다.  

워크로드에 권한을 허용하기 위해 먼저 privileged 정책을 생성합니다. kubectl create -f &lt;filename&gt;을 실행해 다음 리소스를 적용합니다.  

```yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: privileged
spec:
  privileged: true
  allowPrivilegeEscalation: true
  allowCapabilities:
  - '*'
  volumes:
  - '*'
  hostNetwork: true
  hostPorts:
  - min: 0
    max: 65535
  hostIPC: true
  hostPID: true
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'RunAsAny'
  fsGroup:
    rule: 'RunAsAny'
```

다음 정책은 제한된 접근을 정의하며 kube-system 네임스페이스에 존재하는 kube-proxy와 같은 쿠버네티스 클러스터 전반의 서비스 실행을 책임지는 워크로드와는 별도로 많은 워크로드를 처리할 수 있습니다.  

```yaml
apiVersion: policy/v1beta1
kind: PodSecurityPolicy
metadata:
  name: restricted
spec:
  privileged: false
  allowPrivilegeEscalation: false
  requiredDropCapabilities:
  - ALL
  volumes:
  - 'configMap'
  - 'emptyDir'
  - 'projected'
  - 'secret'
  - 'downwardAPI'
  - 'persistentVolumeClaim'
  hostNetwork: false
  hostIPC: false
  hostPID: false
  runAsUser:
    rule: 'RunAsAny'
  seLinux:
    rule: 'RunAsAny'
  supplementalGroups:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  fsGroup:
    rule: 'MustRunAs'
    ranges:
      - min: 1
        max: 65535
  readOnlyRootFilesystem: false
```

다음 명령을 실행하여 정책이 생성되었는지 확인할 수 있습니다.  

```sh
kubectl get psp
```

이제 RBAC를 통해 정의된 정책을 사용할 수 있도록 서비스 계정에 접근 권한을 부여합니다.  

먼저 이전 단계에서 생성한 제한이 있는 파드시큐리티폴리시를 use할 수 있는 ClusterRole을 만듭니다.  

```yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: psp-restricted
rulse:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - restricted
  verbs:
  - use
```

그리고 이전 단계에서 생성한 특권을 가진(privileged) 파드시큐리티폴리시를 use할 수 있는 ClusterRole을 만듭니다.  

```yaml
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: pod-privileged
rulse:
- apiGroups:
  - extensions
  resources:
  - podsecuritypolicies
  resourceNames:
  - privileged
  verbs:
  - use
```

system: serviceaccounts 그룹이 psp-restricted ClusterRole에 접근할 수 있도록 ClusterRoleBinding을 생성합니다. 이 그룹에는 모든 kube-controller-manager 컨트롤러 서비스 계정이 포함됩니다.  

```yaml
kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: psp-restricted
subjects:
- kind: Group
  name: system:serviceaccounts
  namespace: kube-system
roleRef:
  kind: ClusterRole
  name: pod-privileged
  apiGroup: rbac.authorization.k8s.io/v1
```

다시 테스트 워크로드를 생성합니다. 이제 파드가 기동되어 실행되는 것을 알 수 있습니다.  

```sh
kubectl create -f pause-deployment.yaml
kubectl get deploy,rs,pod
```

제한된 정책을 위반하도록 테스트 워크로드 디플로이먼트를 업데이트합니다. privileged=true를 추가하는 겁니다. 그리고 이 매니페스트를 로컬 파일 시스템에 pause-privileged-deployment.yaml로 저장한 다음 kubectl apply -f &lt;filename&gt;로 적용합니다.  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pause-privileged-deployment
  namespace: default
  labels:
    app: pause
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pause
  template:
    metadata:
      labels:
        app: pause
    spec:
      containers:
      - name: pause
        image: k8s.gcr.io/pause
        securityContext:
          privileged: true
```

디플로이먼트와 레플리카셋이 모두 생성된 것을 확인할 수 있습니다. 그러나 파드는 아직 없습니다. 레플리카셋이 이벤트 로그에서 자세한 원인을 볼 수 있습니다.  

```sh
kubectl create -f pause-privileged-deployment.yaml
kubectl get deploy,rs,pods -l app=pause
kubectl describe replicaset -l app=pause
```

위의 예제는 정확한 이유인 Privileged containers are not allowed를 보여줍니다. 테스트 워크로드 디플로이먼트를 삭제합니다.  

```sh
kubectl delete deploy pause-privileged-deployment
```

지금까지는 클러스터 수준의 바인딩만 다루었습니다. 이제 서비스 계정을 사용하여 특권을 가진 정책에 테스트 워크로드 접근을 허용해봅시다.  

먼저 기본 네임스페이스에 serviceaccount를 생성합니다.  

```sh
kubectl create serviceaccount pause-privileged
```

serviceaccount를 허용되는 ClusterRole에 바인딩합니다. 이 매니페스트를 로컬 파일 시스템에 role-pause-privileged-psp-permissive.yaml로 저장한 다음 kubectl apply -f &lt;filename&gt;으로 적용합니다.  

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: pause-privileged-psp-permissive
  namespace: default
roleRef:
  apiGroup: rbac.authorization.k8s.io/v1
  kind: ClusterRole
  name: psp-privileged
subjects:
- kind: ServiceAccount
  name: pause-privileged
  namespace: default
```

마지막으로 pause-privileged 서비스 계정을 사용하도록 테스트 워크로드를 업데이트합니다. 그런 다음 kubectl apply를 사용해 클러스터에 적용합니다.  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: pause-privileged-deployment
  namespace: default
  labels:
    app: pause
spec:
  replicas: 1
  selector:
    matchLabels:
      app: pause
  template:
    metadata:
      labels:
        app: pause
    spec:
      containers:
      - name: pause
        image: k8s.gcr.io/pause
        securityContext:
          privileged: true
        serviceAccountName: pause-privileged
```

이제 파드가 특권을 가진 정책을 사용할 수 있습니다.  

```sh
kubectl create -f pause-privileged-deployment.yaml
```

##### 10.1.3. 파드시큐리티폴리시 해결 과제  
<br/>

+ 합리적인 기본 정책  
파드시큐리티폴리시의 큰 장점은 클러스터 관리자 또는 사용자가 특정 수준의 워크로드 보안을 보장할 수 있다는 것입니다. 현업에서 얼마나 많은 워크로드가 루트로 실행되는지, hostPath 볼륨을 사용하는지, 또는 워크로드를 기동하고 실행할 때 보안상 허점이 있는 정책을 작성해야 하는 위험한 설정은 없는지를 간과하는 경우가 종종 있습니다.  

+ 많은 노동

+ 개발자가 파드시큐리티폴리시에 관심이 있나요?

+ 디버깅의 번거로움  
정책 평가는 어려운 문제입니다. 예를 들어 워크로드와 특정 정책이 일치하는지를 파악하고 싶을 때, 이 단계에서는 이를 도와주는 도구와 로그가 존재하지 않으므로 어렵습니다.  

+ 통제할 수 없는 외부 산출물을 사용하나요?  
도커 허브 또는 다른 공공 리포지터리에서 이미지를 가져오고 있나요? 그 이미지들은 어떤 형태로든 정책을 위반할 수 있고 통제에서 벗어날 수 있습니다. 헬름 차트도 마찬가지입니다. 헬름 차트가 적절한 정책과 함께 제공되고 있나요?  

##### 10.1.4. 파드시큐리티폴리시 모범 사례  
<br/>

파드시큐리티폴리시는 복잡하며 오류가 발생하기 쉽습니다. 클러스터에 파드시큐리티폴리시를 구현하기 전에 다음 모범 사례를 참조하세요.  

+ 모든 것이 RBAC에 달려있습니다. 파드시큐리티폴리시는 RBAC에 의해 결정됩니다. RBAC 정책 설계의 결점은 바로 이 관계에서 비롯됩니다. RBAC와 파드시큐리티폴리시 생성, 유지, 관리의 자동화가 굉장히 중요합니다. 특히 서비스 계정의 접근을 막는 것이 정책의 핵심입니다.

+ 정책의 범위를 파악하세요. 정책을 클러스터의 어디에 적용할지 결정하는 것은 매우 중요합니다. 클러스터 전체나 네임스페이스, 특정 워크로드가 될 수 있습니다. 쿠버네티스 클러스터 동작과 관련한 워크로드는 항상 존재합니다. 이 워크로드는 더 많은 보안 권한이 필요합니다. 그러므로 허용 정책으로 원치 않는 워크로드를 중지할 수 있는 적절한 RBAC가 있어야 합니다.

+ 기존 클러스터에서 파드시큐리티폴리시를 활성화하고 싶은가요? 편리한 오픈 소스 도구를 사용하여 현재 리소스에 기반한 정책을 생성하세요(https://github.com/sysdiglabs/kube-psp-advisor). 훌륭한 출발입니다. 앞으로 더 세련된 정책을 만들 수 있을 겁니다.  

##### 10.1.5. 파드시큐리티폴리시 다음 단계  
<br/>

파드시큐리티폴리시는 클러스터 보안 유지에 도움을 주는 강력한 API지만 큰 비용이 듭니다. 신중한 계획과 실용적인 접근 방식을 통해 파드시큐리티폴리시를 모든 클러스터에 성공적으로 구현할 수 있습니다. 최소한 보안 팀은 만족할 겁니다.  

#### 10.2. 워크로드 격리와 런타임클래스  
<br/>

컨테이너 런타임을 워크로드 격리 경계로 보면 여전히 보안상 안전하지 않다고 간주됩니다. 오늘날 가장 대중적인 런타임들이 과연 가장 안전한 것으로 인식될지는 명확하지 않습니다. 쿠버네티스에 대한 업계의 관심과 성장 모멘텀으로 인해 다양한 격리 수준을 제공하는 여러 컨테이너 런타임이 개발되었습니다. 어떤 런타임들은 친숙하고 신뢰할 수 있는 기술 스택을 기반으로 두기도 하고, 또 다른 런타임들은 문제를 해결하기 위한 완전히 새로운 시도를 합니다. 카타(Kata) 컨테이너, gVisor, 파이어크래커(Firecracker)와 같은 오픈 소스 프로젝트를 사용하면 워크로드를 확실하게 격리할 수 있습니다. 이러한 프로젝트는 중첩된 가상화(VM에서 초경량 VM을 실행) 또는 시스템 호출 필터링과 서비스를 이용합니다.  

다양한 워크로드 격리를 제공하는 컨테이너 런타임의 등장으로 사용자는 동일한 클러스터에서 격리 보장 수준에 맞춰 다양한 런타임을 선택할 수 있습니다. 예를 들어, 동일한 클러스터에서 서로 다른 컨테이너 런타임으로 실행되는 신뢰할 수 있는 워크로드와 신뢰할 수 없는 워크로드가 존재할 수 있습니다.  

런타임클래스는 쿠버네티스에서 컨테이너 런타임 선택을 가능하게 하는 API입니다. 즉 클러스터 관리자가 구성한 컨테이너 런타임 중 하나를 선택하는 겁니다. 쿠버네티스 사용자는 파드 명세의 RuntimeClassName을 사용해 워크로드에 특정 런타임 클래스를 정의할 수 있습니다. 실제로 구현되는 방법은 런타임클래스가 구현할 컨테이너 런타임 인터페이스(Container Runtime Interface, CRI)로 전달되는 RuntimeHandler를 지정하는 겁니다. 그런 다음 노드 라벨링 또는 노드 테인트가 노드 셀렉터 또는 톨러레이션과 함께 사용되어 워크로드가 원하는 런타임클래스를 지원하는 노드에 배치됩니다.  

##### 10.2.1. 런타임클래스 사용  
<br/>

클러스터 관리자가 다른 런타임클래스를 설치한 경우, 파드 명세에 runtimeClassName을 지정하면 사용할 수 있습니다. 예는 다음과 같습니다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: nginx
spec:
  runtimeClassName: firecracker
```

##### 10.2.2. 런타임 구현  
<br/>

다음은 다양한 수준의 보안과 격리를 제공하는 오픈 소스 컨테이너 런타임 구현입니다. 단순 안내 수준이며 자세한 내용은 다루지 않습니다.  

+ CRI containerd: 단순화, 견고성, 이식성을 강조한 컨테이너 런타임을 위한 API 퍼사드(facade)입니다.
+ cri-o: 특수한 목적으로 제작된 경량 오픈 컨테이너 이니셔티브(Open Container Initiative, OCI) 기반의 쿠버네티스 컨테이너 런타임 구현입니다.
+ 파이어크래커: 커널 기반 VM 위에 구축되었으며, 이 가상화 기술로 전통적인 VM의 보안과 격리를 사용하여 가상화되지 않은 환경에서 격리된 가상환경인 microVM을 매우 빠르게 시작할 수 있습니다.
+ gVisor: 새로운 사용자 공간 커널로 컨테이너를 실행하는 OCI 호환 샌드박스 런타임으로, 오버헤드가 적고 안전하며 격리된 컨테이너 런타임을 제공합니다.
+ 카타 컨테이너: 컨테이너와 흡사하게 동작하는 경량 VM을 실행하여 VM과 유사한 보안과 격리를 제공하는 안전한 컨테이너 런타임을 구축하는 커뮤니티입니다.  

##### 10.2.3. 워크로드 격리와 런타임클래스 모범 사례  
<br/>

다음 모범 사례를 통해 일반적인 워크로드 격리와 런타임클래스의 문제를 해결할 수 있습니다.  

+ 런타임클래스를 사용하여 다양한 워크로드 격리 환경을 구현하면 운영 환경이 복잡해집니다. 컨테이너 런타임마다 격리의 근본적인 특성이 다르기 때문에 워크로드 이식이 어려울 수 있습니다. 다양한 런타임에 대한 지원 기능 메트릭스를 이해하는 것은 어려우며 UX 저하를 초래할 수 있습니다. 혼동을 방지하기 위해서는 가급적 단일 런타임을 갖는 개별 클러스터를 보유하는 것이 좋습니다.

+ 워크로드 격리로 멀티테넌시 보안이 유지되는 것은 아닙니다. 안전한 컨테이너 런타임을 구현한다고 해도 쿠버네티스 클러스터와 API까지 동일하게 안전하다는 뜻은 아닙니다. 쿠버네티스의 시작부터 끝까지 전체 영역을 살펴봐야 합니다. 워크로드가 격리되었다고 해서 쿠버네티스 API를 통한 악의적 수정이 불가능한 것은 아닙니다.  

+ 런타임마다 도구가 다릅니다. 디버깅과 내부 검사에 컨테이너 런타임 도구를 사용하는 사례가 있습니다. 만약 런타임이 바뀌면 더는 docker ps로 실행 중인 컨테이너를 볼 수 없으므로 문제 해결에 혼란을 줄 수 있습니다.  

#### 10.3. 파드와 컨테이너 보안 고려 사항  
<br/>

##### 10.3.1. 어드미션 컨트롤러  
<br/>

파드시큐리티폴리시를 깊게 이해하는 것이 부담스럽다면, 완벽하진 않지만 사용 가능한 대안이 있습니다. 어드미션 웹훅으로 SecurityContext 워크로드 설정을 추가하고 DenyExecOnPrivileged와 DenyEscalatingExec와 같은 어드미션 컨트롤러를 사용하면 비슷한 결과를 얻을 수 있습니다.  

##### 10.3.2. 오픈 소스 도구  
<br/>

리눅스 시스템 호출을 감시하고 필터링하거나 버클리 패킷 필터(Burkeley Packet Filter, BPF)를 사용합니다. 이러한 도구 중 하나로는 팔코(Falco)가 있습니다. CNCF 프로젝트인 팔코는 데몬셋으로 간단히 설치할 수 있고 실행 중에 정책을 설정하고 시행할 수 있습니다. 팔코는 단지 하나의 접근법일 뿐입니다. 다양한 도구를 살펴보고 적합한 것을 찾아보기를 권장합니다.  

### 11. 클러스터 정책과 거버넌스  
<br/>

#### 11.1. 정책과 거버넌스의 중요성  
<br/>

의료 서비스나 금융 서비스와 같이 규제가 엄격한 환경에서 운영하거나 단순히 클러스터에서 실행 중인 항목의 제어 수준을 유지하기 위해서는 기업에서 규정한 정책을 구현할 방법이 필요합니다. 정책을 정의한 다음에는 이러한 정책을 구현하고 준수하도록 유지하고 관리하는 방법을 결정해야 합니다. 정책은 규제 준수를 충족하거나 모범 사례를 시행하기 위해 마련될 수 있습니다. 이유가 무엇이든 이러한 정책을 구현할 때는 개발자의 신속성과 자율성을 저해하지 않도록 보장해야 합니다.  

#### 11.2. 다른 정책과의 차이  
<br/>

쿠버네티스의 모든 곳에 정책이 있습니다. 쿠버네티스 리소스 명세에 선언된 모든 것은 해당 정책 정의에 따라 구현되어야 합니다. 네트워크 정책과 파드 보안 정책은 런타임에 구현됩니다. 그렇다면 쿠버네티스 리소스 명세에 실제로 정의된 내용을 관리하는 건 누구일까요? 바로 정책과 거버넌스입니다. 거버넌스 맥락에서의 정책은 런타임에 정책을 구현하는 대신 쿠버네티스 리소스 명세 자체의 필드와 값을 제어하는 것을 의미합니다. 이러한 정책을 따른 쿠버네티스 리소스 명세만 클러스터 상태로 허용되고 커밋됩니다.  

#### 11.3. 클라우드 네이티브 정책 엔진  
<br/>

리소스가 정책을 따르는지의 여부를 판단하려면 다양한 요구사항을 만족시키는 유연한 정책 엔진이 필요합니다. 개방형 정책 에이전트(Open Policy Agent, OPA)는 클라우드 네이티브 생태계에서 대중화되고 있는 유연하고 가벼운 오픈 소스 정책 엔진입니다. OPA를 통해 생태계에 다양한 쿠버네티스 거버넌스 도구가 구현되었습니다. 예를 들면 쿠버네티스 정책과 거버넌스 프로젝트 중 커뮤니티가 형성되고 있는 게이트키퍼(Gatekeeper)가 있습니다. 다양한 정책과 거버넌스 도구가 있지만 정책을 준수하는 쿠버네티스 리소스 명세만 클러스터에 커밋할 수 있음은 모두 동일합니다.  

#### 11.4. 게이트키퍼 소개  
<br/>

게이트키퍼는 클러스터 정책과 거버넌스 오픈 소스로 사용자 정의가 가능한 쿠버네티스 어드미션 웹훅입니다. 게이트키퍼는 OPA 제약 프레임워크를 활용해 CRD 기반 정책을 시행함으로써 정책 작성과 구현을 분리하는 통합된 쿠버네티스 경험을 제공합니다. 정책 템플릿을 제약 템플릿이라고 하며 여러 클러스터에서 공유하고 재사용할 수 있습니다. 게이트키퍼는 리소스 검증과 감사 기능을 제공하며 이식성이 뛰어납니다. 그래서 모든 쿠버네티스 클러스터에서 구현할 수 있으며 이미 사용 중인 OPA의 정책을 게이트키퍼로 이식할 수도 있습니다.  

##### 11.4.1. 정책 예제  
<br/>

복잡하게 생각하지 말고 실제로 해결하려는 문제에만 집중하는 것이 중요합니다. 일반적인 규정 준수 문제를 해결하기 위한 몇 가지 정책을 살펴보겠습니다.  

+ 서비스가 인터넷에 공개적으로 노출되면 안 됩니다.
+ 신뢰할 수 있는 컨테이너 레지스트리의 컨테이너만 허용해야 합니다.
+ 모든 컨테이너에는 리소스 제한이 있어야 합니다.
+ 인그레스 호스트 이름은 겹치지 않아야 합니다.
+ 인그레스는 HTTPS만 사용해야 합니다.  

##### 11.4.2. 게이트키퍼 용어  
<br/>

게이트키퍼는 OPA 용어를 많이 채택했습니다. 게이트키퍼의 동작 방식을 이해하려면 이 용어의 의미를 알아야 합니다. 게이트키퍼는 OPA 제약 프레임워크를 사용합니다.  

###### 11.4.2.1. 제약  
<br/>

제약을 쿠버네티스 리소스 명세의 특정 필드와 값에 적용되는 제한 사항으로 이해하면 쉽습니다. 정책을 길게 풀어 쓴 겁니다. 즉, 제약을 정의하는 것은 무언가를 허용하지 않을 것임을 효과적으로 명시하는 겁니다. 반대로 제약이 없는 경우, 리소스가 암묵적으로 허용됩니다. 이는 원하는 쿠버네티스 리소스 명세 필드와 값을 명시하는 대신에 원하지 않는 것만 거부한다는 뜻입니다. 쿠버네티스 리소스 명세는 항상 바뀌기 때문에 이러한 아키텍처가 매우 적합합니다.  

###### 11.4.2.2. Rego  
<br/>

Rego는 OPA 네이티브 질의어입니다. Rego 질의는 OPA에 저장된 데이터에 대한 조건문입니다. 게이트키퍼는 Rego를 제약 템플릿에 저장합니다.  

###### 11.4.2.3. 제약 템플릿  
<br/>

제약 템플릿은 정책 템플릿으로 볼 수 있습니다. 이식과 재사용이 가능합니다. 제약 템플릿은 타입을 가진 매개변수와 재사용을 위해 파라미터화된 대상 Rego로 이루어집니다.  

##### 11.4.3. 제약 템플릿 정의  
<br/>

제약 템플릿이란 정책을 공유하거나 재사용할 수 있는 정책 템플릿을 만들 수 있는 CRD입니다. 또한 제약 템플릿으로 정책의 매개변수를 검증할 수 있습니다. 이전 예제의 맥락에서 제약 템플릿을 살펴보겠습니다. 다음은 '신뢰할 수 있는 컨테이너 레지스트리의 컨테이너만 허용'하는 정책에 관한 제약 템플릿입니다.  

```yaml
apiVersion: templates.gatekeeper.sh/v1alpha1
kind: ContraintTemplate
metadata:
  name: k8sallowedrepos
spec:
  crd:
    spec:
      names:
        kind: K8sAllowedRepos
        listKind: K8sAllowedReposList
        plural: k8sallowedrepos
        singular: k8sallowedrepos
      validation:
        # '매개변수' 필드를 위한 스키마
        openAPIV3Schema:
          porperties:
            repos:
              type: array
              items:
                type: string
  targets:
    - target: admission.k8s.gatekeeper.sh
      rego: |
        package k8sallowedrepos

        deny[{"msg": msg}] {
          container := input.review.object.spec.containers[_]
          satisfied := [good | repo = input.constraint.spec.parameters.repos[_] ; good = startswith(container.image, repo)]
          not any(satisfied)
          msg := sprintf("container <%v> has an invalid image repo <%v>, allowed repos are %v", [container.name, container.image, imput.constraint.spec.parameters.repos])
        }
```

제약 템플릿은 세 가지 주요 구성 요소로 이루어져 있습니다.  

+ 쿠버네티스 필수 CRD 메타데이터: 이름이 가장 중요합니다.
+ 입력 매개변수의 스키마: 검증 필드로 입력 매개변수와 관련 타입을 정의합니다.  
+ 정책 정의: target 필드로 템플릿화된 Rego를 가집니다. 제약 템플릿을 사용해 템플릿화된 Rego를 재사용할 수 있으므로 일반적인 정책을 공유할 수 있습니다. 규칙이 일치한다면 제약을 위반한 겁니다.  

##### 11.4.4. 제약 정의  
<br/>

이전 제약 템플릿을 사용하려면 제약 리소스를 만들어야 합니다. 제약 리소스의 목적은 앞에서 생성한 제약 템플릿에 필요한 매개변수를 전달하는 겁니다. 다음 예제에서 정의된 리소스의 kind는 K8sAllowedRepos임을 알 수 있으며, 이 리소스는 이전 절에서 정의한 제약 템플릿에 매핑됩니다.  

```yaml
apiVersion: constraints.gatekeeper.sh/v1alpha1
kind: K8sAllowedRepos
metadata:
  name: prod-repo-is-openpolicyagent
spec:
  match:
    kinds:
      - apiGroups: [""]
        kinds: ["Pod"]
    namespaces:
      - "production"
  parameters:
    repos:
      - "openpolicyagent"
```

제약은 두 개의 주요 절로 구성됩니다.  

+ 쿠버네티스 메타데이터: 이 제약은 kind K8sAllowedRepos이며 제약 템플릿의 이름과 일치합니다.
+ 명세: match 필드는 정책이 적용될 범위를 정의합니다. 이 예에서는 운영 네임스페이스의 파드에만 정책을 적용합니다.  

매개변수에 정책의 의도를 정의합니다. 이전 절의 제약 템플릿 스키마의 타입과 일치한다는 점에 유의합니다. 이 경우 openpolicyagent로 시작하는 컨테이너 이미지만 허용합니다.  

제약은 다음과 같은 운영 특성을 가집니다.  

+ 논리적 AND
  + 동일한 필드를 검증하는 여러 정책 중에 하나만 위반해도 전체 요청 거절
+ 빠른 오류 발견을 위한 스키마 검증
+ 셀렉션 기준
  + 레이블 셀렉터를 사용할 수 있음
  + 특정 종류만 제약
  + 특정 네임스페이스만 제약  

##### 11.4.5. 데이터 복제  
<br/>

예를 들어 '인그레스 호스트 이름이 겹치지 않아야 한다면' 현재 리소스와 클러스터의 다른 리소스를 비교해야 합니다. OPA는 이 규칙을 평가하기 위해 다른 모든 인그레스 리소스를 캐시에 넣어야 합니다. 이를 위해 게이트키퍼는 config 리소스를 이용하여 OPA에 캐시되는 데이터를 관리합니다. 또한 config 리소스는 감사 기능에도 사용되는데 이에 대해서는 나중에 살펴봅니다.  

다음 config 리소스 예제는 v1 서비스, 파드, 네임스페이스를 캐시합니다.  

```yaml
apiVersion: config.gatekeeper.sh/v1alpha1
kind: Config
name: config
  namespace: gatekeeper-system
spec:
  sync:
    syncOnly:
    - kind: Service
      version: v1
    - kind: Pod
      version: v1
    - kind: Namespace
      version: v1
```

##### 11.4.6. UX  
<br/>

게이트키퍼는 클러스터 사용자에게 정의된 정책을 위반하는 리소스에 대한 실시간 피드백을 줄 수 있습니다. 이전 절의 예제를 고려해보면 openpolicyagent로 시작하는 리포지터리의 컨테이너만 허용됩니다.  

다음과 같이 지정된 정책을 준수하지 않는 리소스를 만들어봅시다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: opa
  namespace: production
spec:
  containers:
    - name: opa
      image: quay.io/opa:0.9.2
```

제약 템플릿에 정의된 위반 메시지가 표시됩니다.  

```sh
kubectl create -f bad_resources/opa_wrong_repo.yaml
```

#### 11.5. 감사  
<br/>

지금가지 정책을 정의하고 요청 어드미션 프로세스에 적용하는 방법에 대해 논의했습니다. 그렇다면 이미 리소스가 배포된 클러스터에서, 무엇이 정의된 정책을 따르고 있는지 파악하려면 어떻게 해야 할까요? 이것이 감사를 사용하는 이유입니다. 감사를 사용하면 게이트키퍼는 주기적으로 정의된 제약에 대해 리소스를 평가합니다. 이를 통해 정책에 따라 잘못 설정된 리소스를 감지하고 교정할 수 있습니다. 감사 결과는 제약의 상태 필드에 저장되므로 kubectl을 사용하여 쉽게 찾을 수 있습니다. 감사를 사용하려면 감사할 리소스를 복제해야 합니다.  

이전 절에서 정의한 prod-repo-is-openpolicyagent 제약을 살펴봅시다.  

auditTimestamp 필드에 대한 감사가 마지막으로 실행된 시간을 확인할 수 있습니다. 또한 violations 필드에서 제약 조건을 위반하는 모든 리소스를 볼 수 있습니다.  

##### 11.5.1. 게이트키퍼와 친숙해지기  
<br/>

게이트키퍼 리포지터리는 은행의 규정에 알맞는 정책을 작성하는 데 자세한 예제가 되어주는 데모를 제공합니다. 게이트키퍼 동작 방식에 대한 실습을 위해 이 굉장한 데모를 직접 해보는 것을 강력하게 추천합니다.  

##### 11.5.2. 게이트키퍼 다음 단계  
<br/>

게이트키퍼 프로젝트는 지속적으로 성정하고 있으며 다음과 같은 정책과 거버넌스 영역의 문제를 해결하고 있습니다.  

+ 변경(정책 기반 리소스 변경, 레이블 추가)
+ 외부 데이터 소스(정책 검색을 위해 경량 디렉터리 접근 프로토콜 또는 액티브 디렉터리와 통합)
+ 권한(게이트키퍼를 쿠버네티스 인증 모듈로 사용)
+ 드라이 런(dry run, 클러스터에서 정책을 활성화하기 전에 사용자가 정책을 테스트)  

이런 문제에 관심이 있거나 해결하고 싶다는 생각이 드나요? 게이트키퍼 커뮤니티는 프로젝트의 미래를 만들어가는 데 도움이 될 새로운 사용자와 기여자를 찾고 있습니다.  

#### 11.6. 정책과 거버넌스 모범 사례  
<br/>

클러스터에서 정책과 거버넌스를 구현할 때는 다음과 같은 모범 사례를 고려해야 합니다.  

+ 파드의 특정 필드에 정책을 시행하려면 검사하고 시행할 쿠버네티스 리소스 명세를 결정해야 합니다. 예를 들어 디플로이먼트는 레플리카셋을 관리하고, 레플리카셋은 파드를 관리합니다. 세 가지 수준에서 모두 시행할 수 있지만 가장 좋은 선택은 런타임과 가장 근접한 파드입니다. 하지만 이러한 결정이 초래하는 결과가 있습니다. 규정을 따르지 않는 파드를 배포하려고 할 때 사용자에게 친숙한 오류 메시지를 볼 수 없습니다. 사용자가 규정을 따르지 않는 레플리카셋을 만든 것이 아니기 때문입니다. 결국 사용자는 kubectl describe을 실행해 디플로이먼트와 관련된 현재 레플리카셋에서 리소스가 규정을 따르는지 확인해야 합니다. 번거롭게 느껴질 수 있지만, 파드 보안 정책과 같은 다른 쿠버네티스 기능과 일관된 행동입니다.

+ 유형, 네임스페이스, 레이블 셀렉터의 기준에 따라 쿠버네티스 리소스에 제약 조건을 적용할 수 있습니다. 제약을 적용하려는 리소스로만 범위를 최대한 좁혀야 합니다. 이렇게 하면 클러스터의 리소스가 증가하더라도 일관된 정책이 보장됩니다. 그리고 평가할 필요가 없는 리소스는 OPA로 전달되지 않으므로 효율적입니다.

+ 쿠버네티스 시크릿과 같이 민감한 데이터에 정책을 동기화하고 시행하는 것은 권하지 않습니다. OPA가 시크릿을 캐시에 보관한 후에(데이터를 복제하도록 구성되었다면) 리소스가 게이트키퍼에게 전달되는 경우, 이 구간은 잠재적으로 보안 공격을 받을 수 있습니다.

+ 많은 제약 조건이 정의되어 있을 때 하나의 제약 조건이 거부되면 전체 요청이 거부됩니다. 이를 논리적 OR로 만드는 방법은 없습니다.

### 12. 다중 클러스터 관리  
<br/>

#### 12.1. 다중 클러스터의 필요성  
<br/>

쿠버네티스를 도입할 때 이미 두 개 이상의 클러스터가 존재할 수 있으며 스테이징, 사용자 승인 테스트, 개발 등의 클러스터와 운영 클러스터를 분리하려는 단계일 수 있습니다. 쿠버네티스는 네임스페이스를 통한 멀티테넌시 기능을 제공합니다. 네임스페이스는 클러스터를 작은 논리적 단위로 나누는 방법입니다. 네임스페이스에 RBAC, 쿼터, 파드 보안 정책, 네트워크 정책을 정의하여 워크로드를 분리할 수 있습니다. 네임스페이스는 여러 팀과 프로젝트를 분리하는 좋은 방법이지만 다중 클러스터 아키텍처를 구축해야 할 우려가 있습니다. 다음은 단일 클러스터 아키텍처와 다중 클러스터의 사용을 결정할 때 고려해야 할 사항입니다.  

+ 폭발 반경
+ 규정
+ 보안
+ 엄격한 멀티테넌시
+ 리전별 워크로드
+ 특수한 워크로드  

아키텍처를 고민할 때 폭발 반경을 가장 먼저 고려해야 합니다. 이는 다중 클러스터 아키텍처에 대한 설계를 하는 사용자들의 주요 고려사항 중 하나입니다. 마이크로서비스 아키텍처는 서킷 브레이커, 재시도, 벌크헤드(bulkhead), 속도 제한을 이용해 시스템의 손상 범위를 제한합니다. 인프라 계층에도 동일한 설계를 적용한다면 다중 클러스터로 소프트웨어 문제로 인한 계단식 장애(cascading failure)를 막을 수 있습니다. 예를 들어 500개의 애플리케이션을 제공하는 하나의 클러스터가 있을 때 플랫폼 문제가 발생한다면 500개의 애플리케이션 전체에 영향을 미칠 겁니다. 하지만 5개의 클러스터 중 하나에만 플랫폼 계층 문제가 발생했다면 500개의 애플리케이션을 제공하는 애플리케이션의 20%에만 영향을 미칠 겁니다. 이것의 단점은 5개의 클러스터를 관리해야 하며 단일 클러스터보다 통합률이 나쁘다는 겁니다. 댄 우지(Dan Woods)는 운영 쿠버네티스 환경에서 발생하는 계단식 장애에 대한 훌륭한 기사를 썼습니다. 이는 대규모 환경에서 다중 클러스터 아키텍처를 고려해야 할 이유를 보여주는 좋은 예제입니다.  

다중 클러스터 설계가 필요한 또 다른 이유는 결제 카드 산업(payment card industry, PCI), 건강 보험 이식성과 책임성(health insurance portability and accountability, HIPAA)등의 워크로드와 같이 특별한 상황에서의 규정 때문입니다. 쿠버네티스가 멀티테넌트 기능을 제공하고는 있지만, 이러한 워크로드가 범용 워크로드와 분리되어 있다면 더 쉽게 관리할 수 있습니다. 규정 워크로드에는 보안 강화, 비공유 컴포넌트와 관련한 특정 요구사항이나 전용 워크로드 요구사항이 있습니다. 전문화된 방식으로 클러스터를 변경하는 것보다 워크로드를 분리하는 것이 훨씬 간단합니다.  

대규모 쿠버네티스 클러스터의 보안은 관리가 어렵습니다. 쿠버네티스 클러스터에 점점 더 많은 팀을 탐재할수록 각 팀의 다양한 보안 요구사항을 대규모 멀티테넌트 클러스터에서 충족하기가 매우 어려워집니다. RBAC, 네트워크 정책, 파드 보안 정책을 관리하는 것도 큰 규모의 단일 클러스터에서는 어렵습니다. 네트워크 정책을 조금만 변경해도 클러스터의 다른 사용자에게 보안 위험을 일으킬 수 있습니다. 하지만 다중 클러스터를 사용하면 설정 오류로 인한 보안 영향을 제한할 수 있습니다. 대규모 쿠버네티스 클러스터가 요구사항에 적합하다고 판단되면 보안 변경을 위한 매우 훌륭한 운영 프로세스를 갖춰야 하고 RBAC, 네트워크 정책, 파드 보안 정책을 변경할 때 발생하는 폭발 반경을 파악해야 합니다.  

쿠버네티스는 클러스터 내에서 실행되는 모든 워크로드가 동일한 API 경계를 공유하므로 엄격한 멀티테넌시를 제공하지 않습니다. 네임스페이스를 통해 약한 멀티테넌시를 제공할 수는 있지만 적대적인 워크로드로부터 보안을 유지하기에는 부족하비다. 사용자가 많다고 반드시 엄격한 멀티테넌시가 필요한 것은 아닙니다. 사용자는 클러스터 내에서 실행되는 워크로드를 신뢰할 겁니다. 일반적으로 SaaS 기반의 소프트웨어와 신뢰할 수 없는 사용자의 신뢰할 수 없는 워크로드를 호스팅하는 경우, 엄격한 멀티테넌시가 필수적입니다.  

리전 내의 엔드포인트에 전달되는 트래픽을 처리해야 하는 워크로드를 실행할 때, 설계에는 각 리전별로 다중 클러스터가 포함될 겁니다. 전 세계에 분산된 애플리케이션을 사용하기 위해서는 다중 클러스터가 있어야 합니다. 지역적으로 분산해야 하는 워크로드가 있다면 다중 클러스터로 이루어진 페더레이션을 사용하기 좋은 상황입니다.  

고성능 컴퓨팅(high-performance computing, HPC), 머신러닝, 그리드 컴퓨팅과 같은 특수한 워크로드 또한 다중 클러스터 아키텍처를 통해 해결할 수 있습니다. 이러한 타입의 특수한 워크로드에는 특정 타입의 하드웨어가 필요하고, 독특한 성능 프로파일이 있으며, 특수한 사용자가 있습니다. 다중 쿠버네티스 노드 풀을 통해 특수한 하드웨어와 성능 프로파일을 제공할 수 있기 때문에 보편적으로 사용되지는 않습니다. HPC나 머신러닝을 위해 매우 큰 클러스터가 필요한 경우, 이러한 워크로드를 담당할 전용 클러스터를 고려해야 합니다.  

다중 클러스터를 사용하면 '공짜(free)'로 격리할 수 있지만 처음부터 해결해야 할 설계상의 문제가 있습니다.  

#### 12.2. 다중 클러스터 설계 문제  
<br/>

다중 클러스터를 설계할 때 부딪히는 몇 가지 문제가 있습니다. 다음의 문제 때문에 다중 클러스터 설계 과정에서 아키텍처가 지나치게 복잡해질 수 있습니다.  

+ 데이터 복제
+ 서비스 디스커버리
+ 네트워크 라우팅
+ 운영 관리
+ 지속적인 배포  

데이터 복제와 일관성은 지리적 리전과 다중 클러스터에 워크로드를 배포할 때 중요한 요소입니다. 워크로드를 실행할 때 복제 전략을 어디에서 개발하고 실행할 것인지 결정해야 합니다. 대부분의 데이터베이스에는 복제 도구가 내장되어 있지만, 복제 전략을 처리할 수 있도록 애플리케이션을 설계해야 합니다. NoSQL 타입의 데이터베이스 서비스에서는 다중 인스턴스로의 확장을 처리할 수 있어 작업이 어렵지 않지만, 애플리케이션의 경우에는 여러 지리적 리전에 걸친 점진적인 일관성을 보장해야만 합니다. 구글 클라우드 스패너(Spanner)와 마이크로소프트 애저 코스모스DB(CosmosDB)와 같은 클라우드 서비스는 여러 지리적 리전에 걸쳐 존재하는 데이터 처리의 복잡성을 해결하는 데 도움을 주는 데이터베이스 서비스를 구축했습니다.  

각 쿠버네티스 클러스터는 자체 서비스 디스커버리 레지스트리를 배포하지만, 레지스트리는 여러 클러스터 간에 동기화되지 않습니다. 이는 애플리케이션이 서로를 식별하고 검색하는 것을 복잡하게 만듭니다. 콘술과 같은 도구를 이용하면 다중 클러스터 서비스와 쿠버네티스 외부의 서비스까지를 투명하게 동기화할 수 있습니다. 또한 이스티오, 링커디, 실리움(Cilium)과 같은 도구는 다중 클러스터 아키텍처 위에 구축되어 클러스터 간 서비스 검색이 가능하게 합니다.  

쿠버네티스는 플랫 네트워크이고 NAT를 사용하지 않으므로 클러스터 내에서의 네트워킹은 매우 간단합니다. 하지만 클러스터 안팎으로 트래픽을 라우팅하는 것은 더욱 복잡합니다. 클러스터와 인그레스는 1:1 매핑으로 구현되기 때문에 인그레스 리소스는 다중 클러스터 위상을 지원하지 않습니다. 또한 클러스터 간의 이그레스 트래픽과 해당 트래픽을 라우팅하는 방법도 고려해야 합니다. 애플리케이션이 단일 클러스터 내에 존재하는 경우라면 간단하지만, 다중 클러스터를 도입할 때는 다른 클러스터의 애플리케이션에 종속된 서비스에 대한 홉(hop) 레이턴시가 추가됩니다. 밀접하게 결합된 애플리케이션이라면 동일한 클러스터 내에서 서비스를 실행하여 레이턴시와 복잡성을 제거하는 것이 좋습니다.  

다중 클러스터 관리에서 가장 큰 오버헤드는 운영 관리입니다. 한두 개의 클러스터만 관리하고 일관성을 유지하는 대신 환경에서 관리할 클러스터가 많아지는 겁니다. 자동화를 통해 다중 클러스터 운영 부담을 줄여야 합니다. 이때 인프라 배포를 고민하고 추가 기능 관리를 고려해야 합니다. 해시코프의 테라폼(Terraform)과 같은 도구를 사용하면 상태를 여러 클러스터에 일관되게 배포하고 관리할 수 있습니다.  

테라폼과 같은 코드형 인프라 도구를 사용하면 클러스터 배포를 재현할 수 있습니다. 반면에 모니터링, 로깅, 인그레스, 보안 등의 추가 기능을 지속적으로 관리해야 합니다. 보안은 운영 관리를 위해 중요하므로 클러스터 전반에서 보안 정책, RBAC, 네트워크 정책을 유지 및 보수해야 합니다.  

다중 클러스터와 CD를 사용하여 단일 쿠버네티스 API 엔드포인트 대신 다중 쿠버네티스 API 엔드포인트를 처리해야 합니다. 이로 인해 애플리케이션 배포에 문제가 발생할 수 있습니다. 소수의 파이프라인은 쉽게 관리할 수 있지만 파이프라인이 100개일 경우에는 애플리케이션 배포가 매우 어려울 겁니다. 이 문제를 해결하기 위한 다양한 방법을 알아야 합니다.  

#### 12.3. 다중 클러스터 배포 관리  
<br/>

다중 클러스터 배포를 관리할 때의 첫 번째 단계는 테라폼과 같은 코드형 인프라 도구를 사용해 배포를 설정하는 겁니디ㅏ. Kubespray, Kops, 클라우드 공급자의 특정 배포 도구도 좋지만, 가장 중요한 것은 반복적으로 클러스터에 배포할 수 있는 소스 관리 방식의 도구를 사용하는 겁니다.  

여러 클러스터를 훌륭하게 관리하기 위해서는 자동화가 중요합니다. 그렇다고 한번에 모든 것을 자동화하는 것은 불가능합니다. 우선순위를 매겨 클러스터 배포와 운영의 모든 측면부터 자동화해야 합니다.  

쿠버네티스 클러스터 API라는 흥미로운 프로젝트가 개발 중입니다. 클러스터 API는 선언적인 쿠버네티스 스타일의 API로 클러스터를 생성, 구성, 관리하는 프로젝트입니다. 핵심 쿠버네티스 위에 선택적인 추가 기능을 제공합니다. 클러스터 API는 공통 API를 통해 선언된 클러스터 수준의 설정을 제공하므로, 클러스터 도구를 쉽게 자동화하고 구축할 수 있습니다. 프로젝트가 성장해가는 과정을 지켜본다면 도움이 될 겁니다.  

##### 12.3.1. 배포와 관리 패턴  
<br/>

쿠버네티스 오퍼레이터를 소프트웨어로서의 인프라스트럭처를 구현하는 것으로 소개했습니다. 오퍼레이터를 사용하면 애플리케이션 배포와 쿠버네티스 클러스터의 서비스를 추상화할 수 있습니다. 예를 들어 쿠버네티스 클러스터를 모니터링하는 프로메테우스를 표준화한다고 가정합시다. 그러면 각 클러스터와 팀마다 다양한 객체(디플로이먼트, 서비스, 인그레스 등)를 생성하고 관리해야 합니다. 또한 버전, 영속성, 보관 정책, 레플리카와 같은 프로메테우스의 기본 설정도 관리해야 합니다. 쉽게 상상할 수 있듯이, 클러스터와 팀이 많아지면 이러한 솔루션은 관리하기 어렵습니다.  

많은 객체와 설정을 관리하는 대신 prometheus-operator를 설치하는 방법도 있습니다. 그러면 쿠버네티스 API에 Prometheus, ServiceMonitor, PrometheusRule, AlertManager와 같은 새로운 객체 유형이 추가됩니다. 이 객체 유형으로 프로메테우스 배포 관련 모든 세부사항을 명시할 수 있습니다. kubectl 도구를 사용해 다른 쿠버네티스 API 객체를 관리하는 것처럼 이러한 객체도 관리할 수 있습니다.  

오퍼레이터 패턴으로 핵심 운영 작업을 자동화하면 전체 클러스터 관리 능력이 향상됩니다. 2016년에 코어OS(CoreOS) 팀은 오퍼레이터 패턴을 만들어 etcd 오퍼레이터와 prometheus-operator를 구현했습니다. 오퍼레이터 패턴은 다음의 두 가지 개념을 기반으로 만들어졌습니다.  

+ CRD
+ 사용자 정의 컨트롤러  

CRD를 이용해 사용자가 정의한 API를 쿠버네티스 API에 추가할 수 있습니다.  

사용자 정의 컨트롤러는 쿠버네티스 리소스와 컨트롤러의 핵심 개념을 기반으로 합니다. 사용자 정의 컨트롤러를 사용하면 네임스페이스, 디플로이먼트, 파드, CRD와 같은 쿠버네티스 API 객체의 이벤트를 관찰하는 로직을 만들 수 있습니다. 또한 선언적인 방식으로 CRD를 구축할 수 있습니다. 쿠버네티스 디플로이먼트 컨트롤러가 디플로이먼트 객체의 선언적 상태를 항상 유지하기 위해 조정 루프에서 동작하는 방식을 생각해본다면, CRD에도 같은 이점을 가져오게 합니다.  

오퍼레이터 패턴을 활용하여 다중 클러스터에서 운영 자동화를 구축할 수 있습니다. 다음의 일래스틱서치 오퍼레이터를 예로 들어봅시다. 일래스틱서치 오퍼레이터는 다음과 같은 작업을 수행할 수 있습니다.  

+ 마스터, 클라어인트, 데이터 노드의 레플리카
+ 고가용성 배포를 위한 영역
+ 마스터와 데이터 노드의 볼륨 크기
+ 클러스터의 크기 조정
+ 일래스틱서치 클러스터의 백업을 위한 스냅샷(snapshot)  

백업을 위한 스냅샷과 클러스터 크기 조절과 같은 일래스틱서치를 관리하는 데 필요한 많은 작업을 오퍼레이터를 통해 자동화할 수 있습니다. 친숙한 쿠버네티스 객체를 통해 이 모든 것을 관리한다는 것이 장점입니다.  

사용자의 환경에서 prometheus-operator 등의 다양한 오퍼레이터를 사용해 얻을 수 있는 장점과 사용자 정의 오퍼레이터를 구축해 일상적인 운영 업무 부담을 줄일 수 있는 방법을 고민해보세요.  

#### 12.4. 깃옵스를 이용한 클러스터 관리  
<br/>

깃옵스는 위브웍스 직원들에 의해 널리 알려졌습니다. 운영 환경에서 쿠버네티스를 수행한 그들의 경험을 기반으로 깃옵스의 아이디어와 기본 개념이 탄생했습니다. 깃옵스는 소프트웨어 개발 라이프사이클의 개념을 운영에 적용했습니다. 깃옵스를 사용하면 깃 리포지터리가 진실의 원천이 되고 클러스터는 설정된 깃 리포지터리와 동기화됩니다. 예를 들어 쿠버네티스 디플로이먼트 매니페스트를 업데이트하면 해당 설정 변경 사항이 클러스터 상태에 자동으로 반영됩니다.  

깃옵스로 보다 쉽게 다중 클러스터의 일관성을 유지보수할 수 있으며 설정 표류를 방지할 수 있습니다. 깃옵스를 통해 선언적으로 다중 클러스터의 상태를 기술하고 유지할 수 있습니다. 깃옵스는 현업에서 애플리케이션 전달과 운영 모두에 적용할 수 있지만 여기에서는 클러스터 관리와 운영 도구에 중점을 둡니다.  

위브웍스 플럭스(Flux)는 깃옵스를 가능하게 한 최초의 도구입니다. 인투잇(Intuit)의 아르고(Argo) CD와 같이 클라우드 네이티브 생태계에 출시된 많은 도구가 있습니다. 아르고 CD도 깃옵스 방식으로 널리 사용되고 있습니다.  

이제 클러스터에 플러스를 설정하고 리포지터리를 동기화합니다.  

```sh
git clone https://github.com/weaveworks/flux
cd flux
```

디플로이먼트 파일을 수정합니다.  

```sh
vim deploy/flux-deployment.yaml
```

다음 줄을 깃 리포지터리로 수정합니다.  

--git-url=git@github.com:weaveworks/flux-get-started (ex. --giturl=git@github.com:your_repo/kbp)  

이제 클러스터에 플럭스를 배포합니다.  

```sh
kubectl apply -f deploy
```

플럭스가 설치되면 깃 리포지터리에 인증할 수 있는 SSH 키를 생성합니다. 플럭스 커맨드라인 툴을 사용해 SSH 키를 얻습니다. 이 키로 포크한 리포지터리에 접근을 설정할 수 있습니다. 먼저 fluxctl을 설치합니다.  

모든 패캐지에 대해 최신 바이너리를 얻을 수 있습니다.  

```sh
flux identity
```

깃허브를 열고 포크한 리포지터리로 이동한 다음 Setting 내의 Deploy keys로 이동해 Add deploy key를 클릭하고 이름을 지정합니다. Allow write access 체크박스를 선택하고 플럭스 공개 키를 붙여넣은 다음 Add key를 클릭합니다. 배포 키를 관리하는 자세한 방법은 깃허브 설명서를 참조하세요.  

이제 플럭스 로그를 보면 깃허브 리포지터리와 동기화된다는 것을 알 수 있습니다.  

```sh
kubectl -n default logs deployment/flux -f
```

깃허브 리포지터리와 동기화되는 것을 확인한 후 일래스틱서치, 프로메테우스, 레디스, 프런트엔드 파드가 생성된 것을 확인할 수 있습니다.  

```sh
kubectl get pods -w
```

이 예제를 완료하면 깃허브 리포지터리의 상태를 쿠버네티스 클러스터와 쉽게 동기화하는 방법을 알 수 있습니다. 이를 통해 다중 클러스터가 단일 리포지터리와 동기화되며 눈송이 클러스터가 생기는 상황을 피할 수 있으므로 클러스터의 운영 도구들을 쉽게 관리할 수 있습니다.  

#### 12.5. 다중 클러스터 관리 도구  
<br/>

다중 클러스터에서 작업할 때 kubectl를 사용하면 여러 클러스터를 관리하기 위해 여러 개의 컨텍스트를 설정해야 하기 때문에 혼란스러울 수 있습니다. 다중 클러스터를 다룰 때 가장 필요한 도구는 컨텍스트와 네임스페이스를 쉽게 변경할 수 있는 kubectx와 kubens입니다.  

완벽한 다중 클러스터 관리 도구가 필요한 경우, 쿠버네티스 생태계 내에 다중 클러스터를 관리하는 몇 가지 도구가 있습니다. 다음은 가장 많이 사용하는 도구입니다.  

+ 랜처(Rancer)는 UI를 통해 다중 쿠버네티스 클러스터를 중앙집중식으로 관리합니다. 온프레미스, 클라우드, 호스팅되는 쿠버네티스 클러스터를 모니터링, 관리, 백업, 복원할 수 있습니다. 또한 여러 클러스터에 배포된 애플리케이션을 제어하는 도구와 운영 도구를 제공합니다.

+ 퀸(KQueen)은 쿠버네티스 클러스터 프로비저닝(provisioning)을 위한 자체적인 멀티테넌트 서비스 포털을 제공하며 다중 쿠버네티스의 감사, 가시성, 보안에 중점을 둡니다. 퀸은 미란티스(Mirantis) 직원들이 개발한 오픈 소스 프로젝트입니다.

+ 가드너(Gradener)는 쿠버네티스 프리미티브(primitive)를 사용하여 최종 사용자에게 쿠버네티스를 서비스 형태로 제공합니다. 모든 주요 클라우드 업체를 지원하며 SAP에서 개발되었습니다. 가드너는 실제로 서비스형 쿠버네티스를 구축하는 사용자에게 적합한 해결책입니다.  

#### 12.6. 쿠버네티스 페더레이션  
<br/>

쿠버네티스 1.3에서 페더레이션 v1이 등장했으며 페더레이션 v2 이후로는 사용되지 않습니다. 페더레이션 v1의 목적은 여러 클러스터에 애플리케이션을 배포하는 겁니다. 쿠버네티스 API를 이용해 구현되었으며 쿠버네티스 주석에 크게 의존하여 설계에 약간의 문제가 있었습니다. 핵심 쿠버네티스 API와 밀접하게 결합되어 있었으므로 페더레이션 v1은 태생적으로 모놀리식입니다. 설계 당시의 결정이 잘못된 것은 아닙니다. 그때 사용 가능했던 기본 요소를 사용한 겁니다. 이 페더레이션 설계는 쿠버네티스 CRD 도입으로 변경되었습니다.  

페더레이션 v2(현재 KubeFed라고 불리는)는 쿠버네티스 v1.11 이상에서 동작합니다. KubeFed는 CRD와 사용자 정의 컨트롤러 개념을 기반으로 구축되었으며 쿠버네티스에 새로운 API를 추가할 수 있습니다. CRD를 중심으로 구축하면 이전 v1 디플로이먼트 객체에 국한되지 않고 새로운 API 유형을 추가할 수 있습니다.  

KubeFed는 다중 클러스터 관리 목적 외에는 고가용성을 위한 다중 클러스터 배포를 지원합니다. 쿠버네티스 애플리케이션을 다중 클러스터에 전달하기 위해 여러 클러스트러를 단일 관리 엔드포인트로 결합할 수 있습니다. 예를 들어 여러 공개 클라우드 환경에 존재하는 클러스터가 있는 경우, 모든 클러스터 배포를 관리하는 단일 컨트롤 플레인으로 결합하여 애플리케이션의 복원력을 높일 수 있습니다.  

다음과 같은 KubeFed 리소스가 지원됩니다.  

+ 네임스페이스
+ 컨피그맵
+ 시크릿
+ 인그레스
+ 서비스
+ 디플로이먼트
+ 레플리카셋
+ 수평 파드 오토스케일러
+ 데몬셋
+ 잡  

KubeFed에서는 모든 것이 항상 모든 클러스터에 복제되지는 않는다는 것을 이해해야 합니다. 예를 들어 디플로이먼트와 레플리카셋을 사용하여 클러스터에 분산된 레플리카 수를 정의할 수 있습니다. 이것이 디플로이먼트의 기본값이지만 설정을 변경할 수 있습니다. 반면에 네임스페이스를 생성하면 해당 네임스페이스는 각 클러스터에 범위가 지정되고 생성됩니다. 시크릿, 컨피그맵, 데몬셋도 동일한 방식으로 각 클러스터에 복사됩니다. 인그레스 리소스는 앞서 언급한 객체와는 다릅니다. 서비스의 단일 진입점을 가진 전역 다중 클러스터 리소스로 생성되기 때문입니다. KubeFed의 동작 방식에서 알 수 있듯이 KubeFed는 다중 리전, 다중 클라우드, 글로벌 애플리케이션 배포를 지원합니다.  

다음은 페더레이션된 디플로이먼트의 예제입니다.  

```yaml
apiVersion: types.kubefed.io/v1beta1
kind: FederatedDeployment
metadata:
  name: test-deployment
  namespace: test-namespace
spec:
  template:
    metadata:
      labels:
        app: nginx
    spec:
      replicas: 5
      selector:
        matchLabels:
          app: nginx
      template:
        metadata:
          labels:
            app: nginx
      spec:
        containers:
        - image: nginx
          name: nginx
  placement:
    clusters:
    - name: azure
    - name: google
```

이 예제는 5개의 레플리카를 가진 NGINX 파드의 페더레이션된 디플로이먼트를 생성합니다. 그리고 애저 클러스터와 구글 클러스터에 파드를 분산합니다.  

KubeFed는 이미 사용 중이거나 구현할 수 있는 도구를 연동하여 쿠버네티스 고가용성과 다중 클러스터 배포를 성공적으로 수행할 수 있습니다.  

#### 12.7. 다중 클러스터 관리 모범 사례  
<br/>

+ 계단식 장애로 인해 애플리케이션에 큰 피해가 발생하지 않도록 클러스터의 폭발 반경을 제한해야 합니다.
+ PCI, HIPPA, HITRUST와 같은 규제 문제가 있다면 다중 클러스터를 사용하여 이러한 워크로드와 일반 워크로드가 혼합되어 복잡성을 줄여야 합니다.
+ 엄격한 멀티테넌시가 비즈니스 요구사항이라면 워크로드를 전용 클러스터에 배포합니다.
+ 애플리케이션이 여러 리전을 필요로 한다면 글로벌 로드 밸런서를 사용하여 클러스터 사이의 트래픽을 관리합니다.
+ HPC와 같은 특수한 워크로드를 개별 클러스터로 분산하여 워크로드의 특별한 요구사항을 만족시킬 수 있습니다.
+ 여러 리전의 데이터 센터에 워크로드를 분산시켜 배포하는 경우라면 먼저 워크로드에 대한 데이터 복제 전략이 있는지 확인하세요. 여러 리전에 걸친 다중 클러스터를 두는 건 쉬울 수 있지만 여러 리전에 걸쳐 데이터를 복제하는 건 복잡할 수 있습니다. 비동기 및 동기 워크로드를 처리할 수 있는 적절한 전략을 세워야 합니다.
+ prometheus-operator 또는 일래스틱서치 오퍼레이터와 같은 쿠버네티스 오퍼레이터를 활용하여 운영 업무를 자동화합니다.
+ 다중 클러스터 전략을 설계할 때는 클러스터 간 서비스 디스커버리와 네트워킹을 수행하는 방법도 고려해야 합니다. 콘술과 이스티오와 같은 서비스 메시 도구는 클러스터 간 네트워킹을 지원합니다.
+ CD 전략이 리전 또는 다중 클러스터 간의 여러 롤아웃을 처리할 수 있는지 확인하세요.
+ 다중 클러스터 운영 컴포넌트를 관리할 수 있는 깃옵스를 활용하여 모든 클러스터에 일관성을 유지할 수 있는지 조사해야 합니다. 깃옵스가 모든 환경에서 항상 동작하는 것은 아니지만, 최소한 다중 클러스터 환경의 운영 부담을 줄이기 위해서라도 확인해야 합니다.  

### 13. 외부 서비스와 쿠버네티스 통합  
<br/>

#### 13.1. 쿠버네티스로 서비스 가져오기  
<br/>

쿠버네티스와 외부 서비스를 연결하는 가장 일반적인 형태는 쿠버네티스 서비스가 클러스터 외부 서비스를 사용하는 겁니다. 쿠버네티스에서 새로운 애플리케이션을 개발하거나 쿠버네티스를 온프레미스 데이터베이스와 같은 레거시 리소스의 인터페이스로 사용하는 상황입니다. 이 형태는 클라우드 네이티브 서비스를 점진적으로 개발할 때 가장 적합합니다. 데이터베이스 계층에는 굉장히 중요한 데이터가 포함되어 있기 때문에 컨테이너는 물론이고 클라우드로 옮기는 것조차 매우 버겁습니다. 동시에 이러한 데이터베이스 위에 현대적인 계층을 제공 (예를 들어 GraphQL 인터페이스)하면 새로운 세대의 애플리케이션을 구축하는 기반이 될 수 있습니다. 이와 마찬가지로, 미들웨어(middleware)를 빠르게 개발하면서 지속적인 배포를 통해 위험을 최소화하면 민첩성을 크게 향상시킬 수 있으므로 큰 의미를 가집니다. 물론 이를 위해서는 쿠버네티스에서 외부 데이터베이스에 접근할 수 있도록 해야 합니다.  

쿠버네티스에서 외부 서비스에 접근하려면 먼저 네트워킹이 제대로 동작해야 합니다. 데이터베이스의 위치와 쿠버네티스 클러스터의 위치에 따라 네트워킹 세부 동작이 다릅니다. 간단히 설명하자면, 일반적으로 클라우드 기반의 쿠버네티스 공급자는 사용자 제공 가상 네트워크에 클러스터를 배포하며 피어링(peering)을 통해 가상 네트워크를 온프레미스 네트워크와 연결할 수 있습니다.  

쿠버네티스 클러스터의 파드와 온프레미스 리소스 사이의 네트워크 연결을 설정한 다음에는, 외부 서비스가 쿠버네티스 서비스처럼 보이도록 해야 합니다. 쿠버네티스에서 DNS를 통해 서비스를 검색할 수 있으므로, 외부 데이터베이스가 쿠버네티스 네이티브 흉내를 내려면 동일한 DNS에서 데이터베이스를 검색할 수 있어야 합니다.  

##### 13.1.1. 셀렉터가 없는 서비스로 안정적인 IP 주소 사용  
<br/>

첫 번째 방법은 셀렉터가 없는(selector-less) 쿠버네티스 서비스를 사용하는 겁니다. 셀렉터가 없는 쿠버네티스 서비스를 생성하면 서비스와 일치하는 파드가 없으므로 로드 밸런싱이 수행되지 않습니다. 대신 쿠버네티스 클러스터에 추가하려는 외부 리소스의 IP 주소를 갖도록 셀렉터가 없는 서비스를 프로그래밍할 수 있습니다. 이렇게 하면 쿠버네티스 파드가 your-database를 조회할 때 이를 쿠버네티스에 내장된 DNS 서버가 외부 리소스의 IP 주소로 변환합니다. 외부 데이터베이스에 대한 셀렉터가 없는 서비스의 예는 다음과 같습니다.  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-external-database
spec:
  ports:
  - protocol: TCP
    port: 3306
    targetPort: 3306
```

이제 외부 데이터베이스의 IP 주소인 24.1.2.3를 가지도록 엔드포인트를 업데이트합니다.  

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  # 중요! 이 이름은 서비스와 일치해야 합니다.
  name: my-external-database
subsets:
  - address:
      - ip: 24.1.2.3
    ports:
      - port: 3306
```

##### 13.1.2. 안정적인 DNS 이름을 위한 CNAME 기반 서비스  
<br/>

이전 예에서는 쿠버네티스 클러스터와 통합하려는 외부 리소스가 안정적인 IP 주소를 가진다고 가정했습니다. 물리적인 온프레미스 리소스의 경우에는 거의 그렇지만, 네트워크 위상에 따라 다를 수 있습니다. 특히 VM IP 주소가 동적인 클라우드 환경에서는 불안정한 IP 주소를 가질 가능성이 높습니다. 또는 단일 DNS 기반 로드 밸런서 뒤에 여러 개의 레플리카가 존재할 수도 있습니다. 이러한 상황에서 클러스터로 가져오려는 외부 서비스가 불안정한 IP 주소를 가질 수 있습니다. 그래도 보통의 경우, 안정적인 DNS 이름은 가지고 있을 겁니다.  

이때 캐노니컬 네임 레코드(Canonical Name record, 줄여서 CNAME 레코드) 기반 쿠버네티스 서비스를 정의할 수 있습니다. DNS 레코드인 CNAME 레코드는 특정 DNS 주소가 변환될 표준 DNS 이름을 나타냅니다. 예를 들어 bar.com을 포함하는 foo.com CNAME 레코드가 있을 때, foo.com을 조회하면 bar.com을 재귀적인 조회해 올바른 IP 주소를 얻을 수 있습니다. 쿠버네티스 서비스를 이용하면 쿠버네티스 DNS 서버에서 CNAME 레코드를 정의할 수 있습니다. 예를 들어, DNS 이름이 database.myco.com인 외부 데이터베이스가 있을 때 이름이 myco-database인 CNAME 서비스를 생성할 수 있습니다. 이 서비스는 다음과 같습니다.  

```yaml
kind: Service
apiVersion: v1
metadata:
  name: my-external-database
spec:
  type: ExternalName
  externalName: database.myco.com
```

이처럼 정의된 서비스가 있을 때, 파드가 myco-database를 조회하면 재귀적으로 database.myco.com을 얻을 수 있습니다. 물론 이 작업을 수행하려면 외부 리소스의 DNS 이름을 쿠버네티스 DNS에서 해석할 수 있어야 합니다. DNS 이름이 세계적으로 접근 가능(예를 들어 잘 알려진 DNS 서비스 공급자가 제공)한 경우에는 자동으로 동작합니다. 그러나 외부 서비스의 DNS가 회사의 로컬 DNS 서버(예를 들어 내부 트래픽만 서비스하는 DNS 서버)에 존재한다면 쿠버네티스 클러스터는 기본적으로 이 회사 DNS 서버에 대한 질의어를 해석할 수 없습니다.  

대체 DNS 해석기와 클러스터의 DNS 서버가 통신하도록 설정을 변경해야 합니다. DNS 서버의 설정 파일을 사용하여 쿠버네티스 컨피그맵을 업데이트할 수 있습니다. 대부분의 클러스터는 코어DNS(CoreDNS) 서버를 이용합니다. 이 서버는 kube-system 네임스페이스의 coredns라는 이름의 컨피그맵에 Corefile 설정을 넣어 코어DNS 서버를 구성합니다. kube-dns 서버를 계속 사용하는 경우 유사한 방식으로 구성되지만 다른 컨피그맵으로 구성됩니다.  

CNAME 레코드는 안정적인 DNS 이름을 가진 외부 서비스를 클러스터에서 검색이 가능한 이름으로 매핑하는 유용한 방법입니다. 잘 알려진 DNS 주소를 클러스터 로컬 DNS 주소에 다시 매핑하는 것이 직관적이지 않다고 느낄 수 있지만, 조금 더 복잡해지는 대신 모든 서비스의 형태를 일관되게 유지할 수 있습니다. 또한 모든 쿠버테티스 서비스와 마찬가지로 CNAME 서비스가 네임스페이스별로 정의되기 때문에, 쿠버네티스 네임스페이스마다 동일한 서비스 이름(예를 들면 database)을 다른 외부 서비스(예를 들면 canary, production)에 매핑할 수 있습니다.  

##### 13.1.3. 액티브 컨트롤러 방식  
<br/>

제한된 환경에서는 쿠버네티스 내에 외부 서비스를 노출하는 (지금까지 배운) 어떤 방법도 실행할 수 없습니다. 쿠버네티스에 노출하고자 하는 외부 서비스에 대해 안정적인 DNS 주소와 IP 주소가 없기 때문입니다. 이런 상황에서 외부 서비스를 노출하는 것은 훨씬 더 복잡합니다. 하지만 방법이 아예 없는 것은 아닙니다.  

먼저 쿠버네티스 서비스가 어떻게 동작하는지를 이해해야 합니다. 쿠버네티스 서비스는 두 개의 리소스로 구성되어 있습니다. 이미 잘 알고 있는 서비스 리소스와 서비스를 구성하는 IP 주소를 나타내는 엔드포인트 리소스입니다. 정상적인 상황에서 쿠버네티스 컨트롤러 매니저는 서비스 내의 셀렉터를 기반으로 서비스의 엔드포인트에 정보를 채웁니다. 그러나 셀렉터가 없는 서비스를 생성하면 선택한 파드가 없으므로, 앞서 언급된 안정적인 IP 방식과는 달리 서비스의 엔드포인트 리소스가 만들어지지 않습니다. 이때 올바른 서비스의 엔드포인트 리소스를 생성하고 정보를 채우려면 제어 루프가 있어야 합니다. 동적으로 인프라를 조회해 통합하려는 쿠버네티스 외부 서비스에 대한 IP 주소를 얻고, 이 IP 주소로 엔드포인트를 채워야 합니다. 이 작업을 수행한 후에는 쿠버네티스 메커니즘이 DNS 서버와 kube-proxy를 프로그래밍해 트래픽 부하를 외부 서비스에 올바르게 분산시킵니다.  

#### 13.2. 쿠버네티스 서비스 내보내기  
<br/>

이전 절에서 기존 서비스를 쿠버네티스로 가져오는 방법을 배웠습니다. 이와는 반대로 쿠버네티스 서비스를 기존 환경으로 내보내야 할 때도 있습니다. 클라우드 네이티브 인프라에서 개발한 새로운 API에 접근하는 고객 관리용 레거시 애플리케이션이 그 예입니다. 또는 내부 정책이나 규정 요구사항으로 인해 기존 웹방화벽(web application firewal, WAF)과 인터페이스하는 새로운 마이크로서비스 기반 API를 구축할 수도 있습니다. 어떤 이유든 상관없습니다. 쿠버네티스 클러스터의 서비스를 다른 내부 애플리케이션에 노출시키는 것은 애플리케이션의 중요한 설계 요구사항입니다.  

많은 쿠버네티스 환경에서 파드 IP 주소는 클러스터 외부에서 라우팅할 수 없습니다. 이러한 이유로 서비스를 내보내는 것이 어렵습니다. 플란넬(Flannel)과 같은 도구나 기타 네트워킹 공급자를 통해 쿠버네티스 클러스터 내에 라우팅을 설정해 파드 간 또는 파드와 노드 간에 통신할 수 있습니다. 하지만 일반적으로 이 라우팅은 동일한 네트워크 내의 임의의 서버로 확장되지는 않습니다. 또한 클라우드와 온프레미스 연결의 경우, 파드 IP 주소가 항상 VPN 또는 온프레미스 네트워크 피어링 관계를 통해 온프레미스 네트워크로 전달되지 않을 수 있습니다. 결국, 기존 애플리케이션과 쿠버네티스 파드 사이에 라우팅을 설정하는 것이 쿠버네티스 기반 서비스를 내보낼 때의 핵심입니다.  

##### 13.2.1. 내부 로드 밸런서를 사용해 서비스 내보내기  
<br/>

쿠버네티스에서 서비스를 내보내는 가장 쉬운 방법은 내장된 서비스 객체를 사용하는 겁니다. 쿠버네티스에 대한 사전 경험이 있다면 클라우드 기반 로드 밸런서를 연결하여 외부 트래픽을 클러스터의 파드로 가져오는 방법을 이미 알고 있을 겁니다. 그러나 대부분의 클라우드가 내부 로드 밸런서를 제공한다는 사실까지는 깨닫지 못했을 겁니다. 내부 로드 밸런서도 동일하게 가상 IP 주소를 파드에 매핑할 수 있지만, 가상 IP 주소는 내부 IP 주소 공간(예를 들어 10.0.0.0/24)에서 가져오므로 가상 네트워크에서만 라우팅할 수 있습니다. 클라우드 특정 애너테이션을 서비스 로드 밸런서에 추가하여 내부 로드 밸런서를 활성화하세요. 예를 들어 마이크로소프트 애저에서는 service.beta.kubernetes.io/azure-load-balancerinternal: "true" 애너테이션을 추가하면 됩니다. AWS에서는 service.beta.kubernetes.io/aws-load-balancer-internal: 0.0.0.0/0입니다.  

내부 로드 밸런서를 통해 서비스를 내보낼 때, 클러스터 외부의 가상 네트워크에서 볼 수 있는 안정적이고 라우팅 가능한 IP 주소를 받게 됩니다. 해당 IP 주소를 직접 사용하거나 내부 DNS 해석을 설정하여 서비스를 검색할 수 있습니다.  

##### 13.2.2. 노드포트로 서비스 내보내기  
<br/>

아쉽게도 온프레미스 환경에서는 클라우드 기반 내부 로드 밸런서를 사용할 수 없습니다. 이때는 노드포트 기반 서비스를 사용할 수 있습니다. 노드포트 유형의 서비스는 클러스터 내의 모든 노드에 리스너(listener)를 내보냅니다. 리스너는 노드의 IP 주소와 선택한 포트의 트래픽을 정의한 서비스로 전달합니다.  

노드포트 유형의 서비스를 생성하면 쿠버네티스는 자동으로 서비스 파드를 선택합니다. 서비스의 spec.ports[&#42;].nodePort 필드에서 해당 포트를 얻을 수 있습니다. 서비스를 생성할 때 포트를 직접 지정할 수 있지만 클러스터에서 설정된 범위 내에 있어야 합니다. 포트 범위의 기본값은 30000에서 30999입니다.  

서비스가 포트로 노출되면 쿠버네티스 작업이 완료됩니다. 클러스터 외부에 존재하는 기존 애플리케이션에 서비스를 내보내려면 사용자(또는 네트워크 관리자)가 서비스를 검색할 수 있도록 만들어야 합니다. 애플리케이션 구성 방식에 따라 다르지만 ${node}:${port} 쌍 목록을 애플리케이션에 제공할 수도 있습니다. 그러면 애플리케이션은 클라이언트측 로드 밸런싱을 수행합니다. 또는 네트워크 내에 물리적 또는 가상로드 밸런서를 구성하여 가상 IP 주소에서 ${node}:${port} 백엔드 목록으로 트래픽을 보낼 수도 있습니다. 이 구성의 세부 사항은 환경에 따라 달라집니다.  

##### 13.2.3. 외부 서버와 쿠버네티스 통합  
<br/>

이전에 언급한 방식으로 동적 서비스 검색이 되지 않을 경우, 쿠버네티스 서비스를 노출하는 마지막 방법은 외부 애플리케이션을 실행하는 서비스를 쿠버네티스 클러스터의 서비스 디스커버리와 네트워킹 메커니즘에 직접 통합하는 겁니다. 이전 방법보다 굉장히 복잡하므로 꼭 필요할 때만 (드물게) 사용해야 합니다. 일부 관리형 쿠버네티스 환경에서는 불가능할 수도 있습니다.  

네트워킹을 통해 외부 시스템을 클러스터에 통합할 때는 파드 네트워크 라우팅과 DNS 기반 서비스 검색이 모두 올바르게 동작하는지 확인해야 합니다. 가장 간단한 방법은 클러스터에 통합하려는 서버에서 kubelet을 실제로 실행하지만 스케줄링은 비활성화하는 겁니다. kubelet 노드를 클러스터에 결합시키는 방법을 설명하는 책이나 온라인 자료가 많으니 참고하세요. 노드가 결합되는 즉시 kubectl cordon... 명령을 사용해 스케줄링할 수 없는 상태로 표시함으로써 추가 작업이 스케줄링되지 않도록 하세요. 데몬셋이 파드를 노드에 랜딩하는 것까지 막지는 않으므로 KubeProxy와 네트워크 라우팅 관련 파드는 서버에 랜딩되고, 해당 서버에서 실행 중인 모든 애플리케이션에서 쿠버네티스 서비스를 검색할 수 있습니다.  

이 방식은 도커 또는 다른 컨테이너 런타임을 설치해야 하기 때문에 노드에 침투해야 합니다. 이로 인해 많은 환경에서 실행되지 않을 수 있습니다. 복잡하지만 가벼운 무게를 가지는 방법은 서버에 kube-proxy 프로세스를 실행하고 장비의 DNS 서버를 조정하는 겁니다. 파드 라우팅을 올바르게 동작하도록 설정할 수 있다고 가정해봅시다. 이때 kube-proxy를 실행하면, 머신 수준의 네트워킹을 구성하여 쿠버네티스 서비스의 가상 IP 주소가 해당 서비스를 구성하는 파드에 다시 매핑됩니다. 머신의 DNS가 쿠버네티스 클러스터 DNS 서버를 가리키도록 변경하면, 사실상 쿠버네티스 클러스터에 속하지 않는 서버에서도 쿠버네티스를 검색할 수 있습니다.  

이 두 가지 접근 방식은 모두 복잡한 고급 기능이므로 쉽게 봐서는 안 됩니다. 이 수준의 서비스 검색 통합을 고려하고 있다면 클러스터로 연결하려는 서비스를 클러스터 자체로 가져오는 것이 더 쉬운지 조사해야 합니다.  

#### 13.3. 쿠버네티스 간 서비스 공유  
<br/>

이전 절에서는 쿠버네티스 애플리케이션을 외부 서비스에 연결하는 방법과 외부 서비스를 쿠버네티스 애플리케이션에 연결하는 방법에 대해 설명했습니다. 또 다른 중요한 사례는 쿠버네티스 클러스터 간 서비스 연결입니다. 이는 다른 리전의 쿠버네티스 클러스터의 장애를 극복하거나 서로 다른 팀에서 운영하는 서비스를 연결하기 위한 겁니다. 이러한 상호작용을 달성하려면 이전 절에서 설명한 설계를 조합해야 합니다.  

먼저 네트워크 트래픽이 흐를 수 있도록 첫 번째 쿠버네티스 클러스터의 서비스를 내보내야 합니다. 내부 로드 밸런서를 지원하는 클라우드 환경에 있고, 내부 로드 밸런서의 가상 IP 주소를 10.1.10.1로 받았다고 가정합니다. 다음으로 이 가상 IP 주소를 두 번째 쿠버네티스 클러스터에 통합해 서비스 검색을 활성화합니다. 외부 애플리케이션을 쿠버네티스로 가져오는 것(첫번째 절)과 같은 방식으로 이 작업을 수행합니다. 셀렉터가 없는 서비스를 생성하고 해당 IP 주소로 10.1.10.1을 설정합니다. 이 두 단계를 통해 두 개의 쿠버네티스 클러스터에서 서비스 검색 및 서비스 사이의 연결을 통합할 수 있습니다.  

이 단계는 수동 작업이며 소규모의 정적 서비스에서는 적합할 수 있습니다. 그러나 클러스터 간에 보다 긴밀하고 자동적인 서비스 통합을 달성하려면 두 클러스터에서 모두 실행되는 클러스터 데몬을 작성하는 것이 좋습니다. 이 데몬은 myco.com/exported-service와 같은 특정 주석이 있는 첫 번째 클러스터의 서비스를 감시합니다. 이 주석이 있는 모든 서비스를 셀렉터가 없는 서비스를 통해 두 번째 클러스터로 가져옵니다. 마찬가지로 동일한 데몬은 두 번째 클러스터로 내보내졌지만 첫 번째 클러스터에는 더는 존재하지 않는 모든 서비스를 가비지로 수집하고 삭제합니다. 각 지역 클러스터에 이러한 데몬을 설정하는 경우, 모든 클러스터 간에 동적인 동-서(east-west) 연결을 활성화할 수 있습니다.  

#### 13.4. 서드파티 도구  
<br/>

지금까지 쿠버네티스 클러스터 및 일부 외부 리소스에 걸친 서비스를 가져오고, 내보내고, 연결하는 다양한 방법을 설명했습니다. 서비스 메시 기술에 대한 사전 경험이 있다면 이러한 개념이 익숙할 겁니다. 실제로 쿠버네티스, 임의의 애플리케이션, 머신과 서비스를 상호 연결하는 데 사용할 수 있는 다양한 서드파티 도구와 프로젝트가 있습니다. 일반적으로 이러한 도구는 많은 기능을 제공하지만 앞에서 설명한 방법보다 운영상 훨씬 복잡합니다. 그러나 점점 더 많은 네트워킹 상호 연결을 구축해나가기 위해서는 빠르게 반복되고 진화하는 서비스 메시 영역을 알아야만 합니다. 대부분의 서드파티 도구가 오픈 소스 컴포넌트지만, 부가적인 인프라 실행으로 인한 운영 오버헤드를 줄여주는 상업적 지원도 제공합니다.  

#### 13.5. 클러스터와 외부 서비스 연결 모범 사례  
<br/>

+ 클러스터와 온프레미스 간 네트워크 연결을 설정합니다. 네트워킹은 사이트, 클라우드 클러스터 구성에 따라 다를 수 있지만 파드가 온프레미스 서버와 통신할 수 있는지를 먼저 확인해야 합니다.

+ 클러스터 외부의 서비스에 접근하려면 셀렉터가 없는 서비스를 사용하여 통신하려는 시스템(예를 들어 데이터베이스)의 IP 주소를 직접 프로그래밍할 수 있습니다. 고정 IP 주소가 없다면 CNAME 서비스를 사용해 DNS 이름으로 전환할 수 있습니다. DNS 이름이나 고정 서비스조차 없는 경우, 외부 서비스 IP 주소를 주기적으로 쿠버네티스 서비스 엔드포인트와 동기화하는 동적 오퍼레이터를 작성해야 합니다.

+ 쿠버네티스에서 서비스를 내보내려면 내부 로드 밸런서 또는 노드포트 서비스를 사용하세요. 내부 로드 밸런서는 일반적으로 쿠버네티스 서비스 자체에 바인딩될 수 있는 공개 클라우드 환경에서 사용하기가 수월합니다. 내부 로드 밸런서를 사용할 수 없다면 노드포트 서비스로 클러스터의 모든 서버에 서비스를 노출할 수 있습니다.

+ 이 두 가지 방법을 조합해 쿠버네티스 클러스터 사이를 연결할 수 있습니다. 다른 쿠버네티스 클러스터에서 셀렉터가 없는 서비스로 사용되는 쿠버네티스 클러스터 서비스를 외부에 노출할 수 있습니다.  

### 14. 쿠버네티스에서 머신러닝 실행하기  
<br/>

#### 14.1. 머신러닝에서 쿠버네티스의 장점  
<br/>

+ 유비쿼터스  
쿠버네티스는 어디에나 있습니다. 주요 공개 클라우드는 모두 쿠버네티스를 지원하고 있으며 개인 클라우드와 인프라를 위한 배포판도 있습니다. 사용자는 쿠버네티스 플랫폼 생태계의 도구를 이용하여 어디에서나 딥러닝 워크로드를 실행할 수 있습니다.

+ 규모 확장성  
딥러닝 워크플로는 머신러닝 모델을 효율적으로 훈련시키기 위해 많은 양의 컴퓨팅 파워를 사용합니다. 데이터 과학자가 모델 훈련에 필요한 컴퓨팅 규모를 손쉽게 달성하고 세밀하게 조절할 수 있도록, 쿠버네티스에는 자동 확장 기능이 내장되어 있습니다.

+ 기능 확장성  
머신러닝 모델을 효율적으로 학습하려면 일반적으로 특수한 하드웨어가 필요합니다. 쿠버네티스는 클러스터 관리자가 쿠버네티스 소스 코드를 변경하지 않고도 신규 하드웨어 정보를 스케줄러에 빠르고 쉽게 알리도록 합니다. 또한 사용자 정의 리소스와 컨트롤러를 쿠버네티스 API에 완벽하게 통합하여, 하이퍼파라미터 튜닝(hyperparameter tuning)과 같은 특수한 워크플로를 수행할 수 있도록 합니다.

+ 자율성  
데이터 과학자는 필요에 다라 쿠버네티스 자체에 대한 전문 지식 없이도 쿠버네티스를 사용하여 머신러닝 워크플로를 스스로 수행할 수 있습니다.

+ 이식성  
쿠버네티스 API를 기반으로 작업하면 어디에서나 머신러닝 모델을 실행할 수 있습니다. 다른 쿠버네티스 공급자에게 머신러닝 워크로드를 이식할 수도 있습니다.  

#### 14.2. 머신러닝 워크플로  
<br/>

+ 데이터셋 준비  
모델 훈련에 사용되는 데이터셋을 준비하려면 스토리지, 인덱스, 카탈로그, 메타데이터가 필요합니다. 데이터셋의 크기는 수백 메가바이트에서 수백 테라바이트까지 다양합니다. 모델을 훈련시킬 때 이 데이터셋을 전달해야 합니다. 따라서 이 요건을 만족하는 적절한 속성을 가진 스토리지가 있어야 합니다. 일반적으로 대규모 블록과 객체 스토리지가 필요합니다. 그리고 쿠버네티스 네이티브 스토리지 추상화 또는 API를 통해 접근할 수 있어야 합니다.

+ 머신러닝 알고리즘 개발  
데이터 과학자가 머신러닝 알고리즘을 작성, 공유, 협업하는 단계입니다. 오픈 소스 도구인 주피터허브(JupyterHub)는 쿠버네티스의 일반적인 워크로드와 동작이 유사하기 때문에 쿠버네티스에 쉽게 설치할 수 있습니다.

+ 훈련  
데이터셋을 사용하여 모델이 설계한 작업을 수행하는 방법을 학습하는 과정입니다. 이 과정의 결과물은 일반적으로 훈련된 모델 상태의 체크포인트(checkpoint)입니다. 훈련 과정에서는 쿠버네티스의 모든 기능을 활용합니다. 스케줄링, 특수한 하드웨어에 접근, 데이터셋 볼륨 관리, 확장, 네트워킹이 모두 함께 발휘됩니다.  

+ 서빙  
훈련된 모델이 클라이언트의 서비스 요청을 처리하는 단계입니다. 모델은 클라이언트가 제공한 데이터를 기반으로 예측을 합니다. 예를 들어 개와 고양이를 감지하도록 훈련된 이미지 인식 모델이 있을 때, 클라이언트가 개의 그림을 제출한다면 모델은 특성 수준의 정확도로 개와 고양이를 구분할 수 있어야 합니다.  

#### 14.3. 쿠버네티스 클러스터 관리자를 위한 머신러닝  
<br/>

데이터 과학자 팀을 이끄는 클러스터 관리자가 직면하게 될 가장 큰 과제는 용어를 이해하는 겁니다. 시간이 지남에 따라 익숙해져야 하는 수많은 용어들이 있습니다. 이제 머신러닝 워크로드 목적으로 클러스터를 준비할 때 해결해야 할 주요 문제를 살펴봅시다.  

##### 14.3.1. 쿠버네티스에서 모델 훈련  
<br/>

쿠버네티스에서 머신러닝 모델을 훈련할 때는 CPU와 GPU가 필요합니다. 일반적으로 리소스가 많을수록 훈련이 빠르게 완료됩니다. 대부분의 모델 훈련은 리소스가 충분한 단일 서버에서 가능합니다. 많은 클라우드 공급자가 다중 GPU VM을 제공하므로 분산 훈련을 시도하기 전에 VM을 4~8개의 GPU로 수직 확장하는 것이 좋습니다. 데이터 과학자는 모델을 훈련할 때 하이퍼파라미터 튜닝을 합니다. 하이퍼파라미터 튜닝은 모델 훈련에서 최적의 하이퍼파라미터 집합을 찾는 과정입니다. 하이퍼파라미터 훈련이 시작되기 전에 값을 설정하는 파라미터일 뿐입니다. 다양한 하이퍼파라미터 집합으로 동일한 훈련을 반복적으로 수행하는 것이 이 기술에 포함됩니다.  

###### 14.3.1.1. 쿠버네티스에서 모델 훈련 시작  
<br/>

이 예에서는 MNIST 데이터셋을 사용해 이미지 분류 모델을 학습하겠습니다. MNIST 데이터셋은 공개되어 있으며 일반적으로 이미지 분류에 사용됩니다.  

모델을 훈련시키려면 GPU가 필요합니다. 쿠버네티스 클러스터에 GPU가 있는지 확인합니다. 다음의 출력은 쿠버네티스 클러스터에 4개의 GPU가 있다는 것을 의미합니다.  

훈련이 배치 워크로드라면 쿠버네티스의 Job 유형을 사용하여 훈련합니다. 500단계로 훈련을 진행하며 단일 GPU를 사용합니다. 다음 매니페스트의 mnist-demo.yaml 파일을 생성하고 파일 시스템에 저장하세요.  

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  labels:
    app: mnist-demo
  name: mnist-demo
spec:
  template:
    metadata:
      labels:
        app: mnist-demo
    spec:
      containers:
      - name: mnist-demo
        image: lachlanevenson/tf-mnist:gpu
        args: ["--max_steps", "500"]
        imagePullPolicy: IfNotPresent
        resources:
          limits:
            nvidia.com/gpu: 1
      restartPolicy: OnFailure
```

이제 쿠버네티스 클러스터에 잡 리소스를 생성합니다.  

```sh
kubectl create -f mnist-demo.yaml
```

생성된 잡의 상태를 확인합니다.  

```sh
kubectl get jobs
```

파드를 보면 훈련 잡이 실행 중인 것을 확인할 수 있습니다.  

```sh
kubectl get pods
```

파드 로그를 보면 진행 중인 훈련을 볼 수 있습니다.  

```sh
kubectl logs mnist-demo-hv9b2
```

마지막으로 잡 상태를 통해 훈련이 완료된 것을 확인할 수 있습니다.  

```sh
kubectl get jobs
```

훈련 잡을 정리하기 위해 다음 명령을 실행합니다.  

```sh
kubectl delete -f mnist-demo.yaml
```

축하합니다! 쿠버네티스에서 첫 번째 모델 훈련 잡을 완료했습니다.  

##### 14.3.2. 쿠버네티스에서 분산 훈련  
<br/>

분산 훈련은 아직 걸음마 단계이므로 최적화가 어렵습니다. 훈련에 8개의 GPU가 필요한 경우, GPU가 네 개인 두 대보다 8-GPU 서버 하나로 훈련하는 것이 더 빠릅니다. 분산 훈련을 해야 하는 상황은 모델의 크기가 가장 큰 가용 서버에도 맞지 않을 때입니다. 분산 훈련을 반드시 실시해야 한다는 확신이 있다면 아키텍처를 이해하는 것이 중요합니다. 분산 텐서플로 아키텍처로, 모델과 매개변수가 어떻게 분산되었는지 볼 수 있습니다.  

##### 14.3.3. 리소스 제약  
<br/>

머신러닝 워크로드를 수행하려면 클러스터 전반에 걸쳐 매우 구체적인 설정을 해야 합니다. 훈련 단계에서 가장 많은 리소스를 사용합니다. 이전에 언급했듯이, 머신러닝 알고리즘 훈련은 배치 워크로드라는 것을 명심해야 합니다. 특히 시작 시간과 종료 시각을 가집니다. 종료 시간은 모델을 훈련할 때 필요한 리소스를 얼마나 빨리 제공할 수 있는지 달려 있습니다. 확장성이 훈련을 빨리 완료할 수 있는 가장 좋은 방법이지만, 확장성 자체에 대한 병목 현상이 존재합니다.  

##### 14.3.4. 특수한 하드웨어  
<br/>

특수한 하드웨어에서 모델을 훈련하고 서비스하면 당연히 효율적입니다. 특수한 하드웨어의 전형적인 예는 상용 GPU입니다. 쿠버네티스의 장치 플러그인을 통해 GPU에 접근할 수 있습니다. 장치 플러그인은 신규 GPU 리소스를 쿠버네티스 스케줄러에 알리고 스케줄링 대상으로 만듭니다. 장치 플로그인 프레임워크를 통해 벤더는 특정 장치를 구현할 때 쿠버네티스 핵심 코드를 수정할 필요가 없습니다. 장치 플러그인은 각 노드에서 데몬셋으로 실행되며, 특정 리소스를 쿠버네티스 API에 알리는 것을 담당하는 프로세스입니다. NVIDIA GPU에 접근할 수 있는 쿠버네티스용 NVIDIA 장치 플러그인을 살펴보겠습니다. 장치 플러그인을 실행하면 다음과 같은 파드를 만들 수 있으며 쿠버네티스는 해당 리소스를 사용할 수 있는 노드로 스케줄링해줍니다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: gpu-pod
spec:
  containers:
    - name: digits-container
      image: nvidia/digits:6.0
      resources:
        limits:
          nvidia.com/gpu: 2 # 두 개의 GPU를 요구함
```

장치 플러그인은 GPU에만 국한되는 것은 아닙니다. FPGA(field programmable gate array) 또는 인피니밴드(InfiniBand)와 같은 특수한 하드웨어에서도 사용할 수 있습니다.  

###### 14.3.4.1. 특이성 스케줄링  
<br/>

쿠버네티스는 자신이 잘 알지 못하는 리소스에 대한 결정을 내릴 수 없다는 점에 유의하세요. 아마 훈련할 때 GPU를 최대한 쓸 수 없다는 것을 알아챘을 겁니다. 즉 기대하는 수준의 사용률을 달성하지 못하는 겁니다. GPU 코어 수만 나타내고 코어당 실행할 수 있는 스레드(thread) 수는 생략합니다. 또한 GPU 코어가 있는 버스가 무엇인지 노출하지 않으므로 서로 또는 동일한 메모리에 접근해야 하는 잡이 동일한 쿠버네티스 노드에 배치될 수 있습니다. 이러한 모든 고려사항은 향후 장치 플러그인으로 해결 가능한 문제지만, 강력한 신규 GPU 장비를 100% 활용할 수 없는 이유가 궁금할 수 있습니다. 또한 GPU 일부(예를 들어 0.1)를 요청할 수 없다는 점도 언급할 필요가 있습니다. 이는 특정 GPU가 다중 스레드를 지원하더라도 해당 용량만큼 사용할 수 없음을 의미합니다.  

##### 14.3.5. 라이브러리와 드라이버, 커널 모듈  
<br/>

특수한 하드웨어에 접근하려면 일반적으로 특수한 라이브러리와 드라이버. 커널 모듈이 필요합니다. 그리고 이를 컨테이너에서 실행 중인 도구에 사용하려면 컨테이너 런타임에 마운트되어야 합니다. '왜 이것들을 컨테이너 이미지 자체에 추가하지 않나요?' 답은 간단합니다. 도구는 기존 호스트의 버전과 일치해야 하며 해당 시스템에 적합하게 구성되어야 합니다. 각 컨테이너에 호스트 볼륨을 매핑하는 수고를 덜어주는 NVIDIA 도커와 같은 컨테이너 런타임을 사용할 수도 있습니다. 또는 특수한 컨테이너 런타임 대신, 동일한 기능의 어드미션 웹훅을 구축해도 됩니다. 클러스터 보안 프로파일에 영향을 미치는 일부 특수한 하드웨어에 접근하려면 권한이 있는 컨테이너가 필요할 수도 있습니다. 쿠버네티스 장치 플러그인으로 라이브러리와 드라이버, 커널 모듈을 쉽게 설치할 수 있습니다. 장치 플러그인은 스케줄링 가능한 gpu 리소스를 쿠버네티스 스케줄러에 알리기 전에 각 서버에 설치가 완료되었는지 확인합니다.  

##### 14.3.6. 스토리지  
<br/>

스토리지는 머신러닝 워크플로에서 가장 중요합니다. 스토리지는 다음과 같은 머신러닝 워크플로에 직접적인 영향을 미칩니다.  

+ 데이터셋 스토리지 및 훈련 중 워커 노드 간 분산
+ 체크포인트 및 모델 저장  

###### 14.3.6.1. 데이터셋 스토리지와 훈련 중 워커 노드 간 분산  
<br/>

훈련하는 동안 모든 워커 노드는 데이터셋을 가져와야 합니다. 스토리지는 읽기 전용이며 일반적으로 디스크 속도가 빠를수록 좋습니다. 스토리지의 디스크 유형은 데이터셋의 크기에 달려 있습니다. 수백 메가바이트 또는 기가바이트의 데이터셋은 블록 스토리지가 적합하지만, 몇 또는 수백 테라바이트 크기의 데이터셋은 오브젝트 스토리지가 더 적합합니다. 데이터셋을 저장한 디스크의 크기와 위치에 따라 네트워킹 성능이 달라질 수 있습니다.  

###### 14.3.6.2. 체크포인트와 모델 저장  
<br/>

체크포인트는 모델을 훈련할 때 생성되며, 저장된 모델은 서빙에 사용됩니다. 두 경우 모두, 데이터를 저장하려면 각 워커 노드에 스토리지를 연결해야 합니다. 데이터는 일반적으로 단일 디렉터리에 저장되며 각 워커 노드는 특정 체크포인트나 모델 파일을 저장합니다. 대부분의 도구는 체크포인트와 데이터가 한곳에 존재한다고 가정하며 ReadWriteMany를 요구합니다. ReadWriteMany는 많은 노드에서 읽기와 쓰기로 마운트할 수 있다는 뜻입니다. 쿠버네티스 퍼시스턴트볼륨을 사용한다면 가장 적합한 스토리지 플랫폼을 정해야 합니다. 쿠버네티스 문서에 ReadWriteMany을 지원하는 볼륨 플러그인 목록이 있습니다.  

##### 14.3.7. 네트워킹  
<br/>

머신러닝 워크플로의 훈련 단계, 특히 분산 훈련을 실행하는 것은 네트워크에 큰 부하를 줍니다. 텐서플로의 분산 아키텍처를 고려하면 많은 네트워크 트래픽을 발생시키는 두 가지의 개별 단계가 있습니다. 각 파라미터 서버의 변수를 워커 노드로 분배하는 단계와 워커 노드에서 다시 파라미터 서버로 기울기(gradient)를 적용하는 단계입니다. 교환하는 데 걸리는 시간은 모델을 훈련시키는 시간에 직접적인 영향을 줍니다. 따라서 단순히 빠르면 더 좋습니다. 대부분의 공개 클라우드와 서버는 1-Gbps, 10-Gbps, 때로는 40-Gbps 네트워크 인터페이스 카드를 지원하므로 일반적으로 낮은 대역폭에서만 문제가 됩니다. 높은 네트워크 대역폭이 필요하다면 인피니밴드를 사용하면 됩니다.  

일반적으로 원시 네트워크 대역폭이 장애 요소는 아니지만, 애초에 커널에서 데이터를 유선 네트워크로 가져오는 것이 문제가 되는 상황도 있습니다. 원격 직접 메모리 접근을 활용해 워커 노드가 애플리케이션 코드를 수정하지 않고 네트워크 트래픽을 가속화하는 오픈 소스 프로젝트가 있습니다. 원격 직접 메모리 접근을 사용하면 네트워크 내의 서버가 프로세서와 캐시, 운영 체제를 통하지 않고 메인 메모리의 데이터를 전달할 수 있습니다. 컨테이너 네트워크 오버레이의 높은 네트워크 성능을 자랑하는 오픈 소스 프로젝트인 프리플로(Freeflow)도 있습니다.  

##### 14.3.8. 특수한 프로토콜  
<br/>

쿠버네티스에서 머신러닝을 수행할 때 사용되는 특수한 프로토콜이 있습니다. 이 프로토콜은 벤더별로 다릅니다. 하지만 파라미터 서버와 같이 병목 현상이 발생하는 아키텍처를 제거하여 분산 훈련의 확장 문제를 해결하는 것이 공통된 목적입니다. 또한 노드 CPU와 OS의 개입 없이도 여러 노드의 GPU 사이에 정보를 직접 교환할 수 있습니다. 분산 훈련을 보다 효율적으로 확장할 수 있는 방법은 다음과 같습니다.  

+ 메시지 전달 인터페이스는 분산 프로세스 사이에 데이터를 전송하기 위해 표준화된 이식 가능한 API입니다.
+ NVIDIA 군집 통신 라이브러리는 위상 인식 다중 GPU 통신 프리미티브 라이브러리입니다.  

#### 14.4. 데이터 과학자 관심사  
<br/>

다음은 데이터 과학자가 쿠버네티스를 머신러닝 용도로 쉽게 활용할 수 있게 도와주는 대중적인 도구들입니다.  

+ Kubeflox는 쿠버네티스용 머신러닝 툴킷입니다. 쿠버네티스 네이티브로 머신러닝 워크플로를 수행하는 데 필요한 도구를 제공합니다. 주피터 노트북, 파이프라인, 쿠버네티스 네이티브 컨트롤러 등은 데이터 과학자가 쿠버네티스를 머신러닝 플랫폼으로 최대한 활용할 수 있도록 돕습니다.

+ Polyaxon은 다수의 대중적인 라이브러리를 지원하고 모든 쿠버네티스 클러스터에서 실행되는 머신러닝 워크플로 관리 도구입니다. Polyaxon은 상업용 제품과 오픈 소스 제품을 모두 보요하고 있습니다.

+ Pachyderm은 엔터프라이즈급 데이터 과학 플랫폼입니다. 머신러닝 파이프라인을 구축할 수 있는 기능과 데이터셋 준비, 라이프사이클, 버전 관리 등을 위한 다양한 도구를 제공합니다. Pachyderm에는 모든 쿠버네티스 클러스터에 배포할 수 있는 상업용 제품이 있습니다.  

#### 14.5. 쿠버네티스에서의 머신러닝 모범 사례  
<br/>

+ 지능적 스케줄링과 자동 확장을 사용하세요. 머신러닝 워크플로의 대부분 단계는 본질적으로 일괄 처리이므로, 클러스터 오토스케일러의 사용을 권장합니다. GPU 가용 하드웨어는 비싸기 때문에 사용하지 않을 때는 비용을 지불하고 싶지 않을 겁니다. 테인트와 톨러레이션을 사용하거나 시간별 클러스터 오토스케일러를 통해 특정 시간에 배치 작업을 실행하는 것이 좋습니다. 이렇게 하면 클러스터를 머신러닝 워크로드의 요구에 맞게 확장할 수 있습니다. 테인트와 톨러레이션과 관련된 업스트림 규칙은 확장된 리소스를 키로 사용해 노드를 테인트시키는 겁니다. 예를 들어, NVIDIA GPU를 가진 노드는 다음과 같이 테인트합니다. Key: nvidia.com/gpu, Effect: NoSchedule. 이렇게 하면 ExtendedResourceToleration 어드미션 컨트롤러를 사용할 수 있습니다. 이 컨트롤러는 사용자가 수동으로 추가할 필요가 없도록, 테인트에 적절한 톨러레이션을 확장된 리소스를 요청하는 파드에 자동으로 추가합니다.

+ 실제의 모델 훈련은 매우 민감합니다. 한 영역의 속도가 빨라지면 다른 영역에 병목 현상이 발생합니다. 지속적인 관찰과 튜닝에 노력이 듭니다. 일반적으로 GPU가 가장 비용이 많이 드는 리소스이므로 GPU에 병목 현상이 생기는 것이 좋습니다. GPU를 포화 상태로 유지하세요. 병목 현상을 항상 경계하고 GPU, CPU, 네트워크, 스토리지 사용률을 추적하는 모니터링을 구성해야 합니다.

+ 혼합 워크로드 클러스터를 사용하세요. 일상적인 비즈니스 서비스를 실행하는 클러스터 역시나 머신러닝 목적으로 사용될 수 있습니다. 머신러닝 워크로드의 고성능 요구사항을 고려할 때 머신러닝 워크로드만 허용하도록 테인트된 별도의 노드 풀을 사용하는 것을 권장합니다. 이를 통해 머신러닝 노드 풀에서 실행되는 머신러닝 워크로드의 영향으로부터 다른 클러스터를 보호할 수 있습니다. 또한 워크로드 유형에 따라 각각 다른 성능 특성을 가진 여러 GPU 가용 노드 풀도 만들어야 합니다. 머신러닝 노드 풀에서 노드 자동 확장을 활성화하는 것이 좋습니다. 혼합 모드 클러스터는 머신러닝 워크로드가 클러스터에 미치는 성능 영향을 확실히 파악한 후 사용해야 합니다.

+ 분산 훈련을 통해 선형 확장을 달성하세요. 이것은 분산 모델 훈련의 궁극적인 목표입니다. 불행히도 대부분의 라이브러리는 분산될 때 선형으로 확장되지 않습니다. 확장성을 개선하기 위해 많은 연구가 진행 중이지만, 이 문제는 단순히 더 많은 하드웨어를 투입하는 것으로 해결되지 않습니다. 경험상 거의 항상 모델 자체가 병목의 원인이며 모델을 지원하는 인프라가 문제가 아닙니다. 그러나 모델 자체를 지적하기 전에 GPU, CPU, 네트워크, 스토리지의 사용률을 검토해야 합니다. 오픈 소스 도구인 Horovod는 분산 훈련 프레임워크를 개선해 더 나은 모델 확장성을 제공합니다.  

### 15. 고수준 애플리케이션 패턴 구축  
<br/>

쿠버네티스 위에 고수준의 기본 요소를 개발하는 두 가지 방법이 있습니다. 첫 번째는 쿠버네티스를 세부 구현으로 포장하는 겁니다. 이렇게 하면 플랫폼을 사용하는 개발자들은 자신이 쿠버네티스 위에 개발하고 있다는 사실을 인식하지 못합니다. 개발자는 자신을 플랫폼의 사용자로 생각하므로 쿠버네티스는 감춰진 새부 구현이 됩니다.  

두 번째 방법은 쿠버네티스 자체에 내장된 확장 기능을 사용하는 겁니다. 쿠버네티스 서버 API는 매우 유연하여 쿠버네티스 API에 새로운 리소스를 동적으로 추가할 수 있습니다. 이 방식을 통해 사용자는 내장된 쿠버네티스 객체와 새로운 고수준 리소스 모두를 기본 도구로 사용할 수 있습니다. 이 확장 모델은 쿠버네티스의 복잡성을 줄이며 개발자가 사용하기 편리하게 만듭니다. 이는 개발자가 쿠버네티스를 사용하는 핵심 요인입니다.  

두 가지 방법 중에서 어떤 것을 선택해야 할까요? 현재 구축 중인 추상화 계층의 목적에 달려 있습니다. 사용자가 '유리를 깨고' 탈출할 가능성이 없고 사용 편의성이 중요하다면 완전히 격리된 통합 환경을 구축하는 첫 번째 방법이 적합합니다. 머신러닝 파이프라인을 건설하는 것이 좋은 사례입니다. 이 분야는 비교적 이해가 잘 됩니다. 사용자인 데이터 과학자는 쿠버네티스에 익숙하지 않을 겁니다. 데이터 과학자는 분산 시스템이 아닌 자신의 영역에 집중하고 작업을 빠르게 수행하는 것을 핵심 목표로 가집니다. 따라서 쿠버네티스 위에 완전한 추상화를 구축하는 것이 가장 합리적입니다.  

반면에 자바 애플리케이션 배포와 같은 고수준의 개발자 추상화를 구축할 때는 쿠버네티스를 감추는 것보다 확장하는 것이 좋습니다. 그 이유는 두 가지입니다. 첫째, 애플리케이션 개발 영역은 매우 광범위합니다. 특히 애플리케이션과 비즈니스가 시간이 지남에 따라 반복되고 변경되기 때문에 개발자의 모든 요구사항과 사용 사례를 예상하기 어렵습니다. 또 다른 이유는 쿠버네티스 생태계 도구를 계속 사용할 수 있도록 하기 위함입니다. 모니터링, 지속적인 전달 등을 위한 클라우드 네이티브 도구가 수없이 많습니다. 쿠버네티스 API를 대체하는 대신 그대로 확장하면 개발 과정에서 기존 도구와 새로운 도구를 함께 사용할 수 있습니다.  

#### 15.2. 쿠버네티스 확장  
<br/>

##### 15.2.1. 쿠버네티스 클러스터 확장  
<br/>

쿠버네티스 클러스터를 확장하려면 쿠버네티스 리소스의 접점을 이해하는 것이 중요합니다. 이와 관련된 세 가지 기술이 있습니다. 첫 번째는 사이드카입니다. 사이드카 컨테이너는 서비스 메시의 맥락에서 대중화되었습니다. 이들은 애플리케이션 컨테이너와 함께 실행되어, 주 애플리케이션과 분리되어 있으며 종종 별도의 팀에서 유지보수하는 추가 기능을 제공하는 컨테이너입니다. 예를 들어, 서비스 메시에서 사이드카는 컨테이너형 애플리케이션에 투명한 mTLS 인증을 제공할 수 있습니다.  

사이드카를 사용하여 사용자 정의 애플리케이션에 기능을 추가할 수 있습니다.  

사이드카 방식의 궁긍적인 목표는 개발자를 편하게 해주는 것입니다. 하지만 사이드카 사용법을 배워야 한다는 어려움이 있습니다. 다행히도 쿠버네티스를 간편하게 확장해주는 도구인 어드미션 컨트롤러가 있습니다. 어드미션 컨트롤러는 쿠버네티스 API 요청을 클러스터의 백업 스토리지에 저장 (또는 '승인')하기 전에 가로챕니다. 어드미션 컨트롤러를 사용해 API 객체의 유효성을 검사하거나 수정할 수 있습니다. 클러스터에서 생성된 모든 파드에 사이드카를 자동으로 추가할 수 있으므로 개발자는 사이드카는 몰라도 사이드카의 이점을 누릴 수 있습니다. 어드미션 컨트롤러가 쿠버네티스 API와 상호작용하는 방식을 보여줍니다.  

어드미션 컨트롤러의 유용성은 사이드카를 추가하는 것에만 국한되지 않습니다. 개발자가 쿠버네티스에 제출한 객체의 유효성을 검사할 때도 사용할 수 있습니다. 예를 들어 개발자가 쿠버네티스 사용 모범 사례를 따르는 파드와 리소스를 제출할 수 있도록 쿠버네티스 린터(linter)를 구현할 수 있습니다. 개발자가 흔히 저지르는 실수는 애플리케이션의 리소스를 예약하지 않는겁니다. 이러한 상황에서 어드미션 컨트롤러 기반의 린터는 이러한 요청을 가로채서 거부할 수 있습니다. 물론 고급 사용자가 린터 규칙을 적절히 해제할 수 있도록 탈출구(예를 들어 특수 주석)를 마련해야 합니다.  

지금까지는 기존 애플리케이션을 확장하고 개발자가 모범 사례를 따르는지 확인하는 방법에 대해서만 다루었을 뿐, 고수준의 추상화를 추가하는 방법은 아직 다루지 않았습니다. 여기서 CRD를 활용합니다. CRD를 이용하면 기존 쿠버네티스 클러스터에 ReplicatedService 리소스를 추가할 수 있습니다. 개발자가 ReplicatedService의 인스턴스를 만들면 쿠버네티스는 해당 디플로이먼트와 서비스 리소스를 만듭니다. 따라서 ReplicatedService는 공통된 유형을 개발자에게 편리하도록 추상화한 것입니다. CRD는 일반적으로 새로운 리소스 유형을 관리하기 위해 클러스터 자체에 배포되는 제어 루프로 구현됩니다.  

##### 15.2.2. 쿠버네티스 UX 확장  
<br/>

클러스터에 새로운 리소스를 추가하는 것은 새로운 기능을 제공하는 좋은 방법이지만, 활용도를 높이기 위해서는 쿠버네티스 UX를 확장하는 것이 좋습니다. 기본적으로 쿠버네티스 도구는 사용자 정의 리소스와 확장을 인식하지 못합니다. 그래서 사용자 친화적이지 않은 보통의 일반적인 방식으로 처리합니다. 쿠버네티스 커맨드라인을 확장하면 향상된 UX를 제공할 수 있습니다.  

일반적으로 쿠버네티스에 접근할 때는 kubectl 커맨드라인 툴을 사용합니다. 다행히 이 도구는 확장이 가능합니다. kubectl 플러그인은 kubectl-foo와 같은 이름을 가진 바이너리 파일입니다. 여기서 foo는 플러그인의 이름입니다. 커맨드라인에서 kubectl foo ...를 실행하면 플러그인 바이너리가 호출됩니다. kubectl 플러그인을 사용하여 클러스터에 추가한 새로운 리소스를 깊이 이해하는 새로운 UX를 정의할 수 있습니다. kubectl 도구의 친숙함을 살리면서 동시에 적합한 UX를 자유롭게 구현할 수 있습니다. 이 방식은 개발자에게 새로운 도구 활용법을 가르칠 필요가 없으므로 매우 유용합니다. 또한 개발자의 입장에서도 쿠버네티스 지식을 쌓으면서 쿠버네티스 네이티브 개념을 점차적으로 알게 되므로 가치가 있습니다.  

#### 15.3. 플랫폼 구축시 설계 고려 사항  
<br/>

##### 15.3.1. 컨테이너 이미지 내보내기 지원  
<br/>

플랫폼을 구축할 때 사용자가 완전한 컨테이너 이미지 대신 코드, 예를 들어 서비스로서의 함수(function as a service, FaaS) 또는 기본 패키지 (예를 들어 자바의 JAR 파일)를 공급할 수 있도록 단순하게 설계합니다. 이 방식은 사용자가 친숙한 도구와 개발 경험을 그대로 이용할 수 있다는 매력을 가집니다. 플랫폼은 애플리케이션의 컨테이너화를 처리합니다.  

그러나 이 방법은 개발자가 제출한 프로그래밍 환경이 한계에 직면할 때 문제가 발생합니다. 예를 들면 버그를 해결하기 위해 특정 버전의 언어 런타임이 필요할 수 있습니다. 또는 애플리케이션의 자동 컨테이너화 구조를 구성하지 않은 추가 리소스 또는 실행 파일을 패키징해야 할 수도 있습니다.  

어떤 이유든 개발자에게 좋지 않은 상황입니다. 간단한 버그를 수정하거나 새로운 기능을 추가하기 위해 갑자기 애플리케이션을 패키징하는 방법을 배워야 하기 때문입니다.  

그러나 꼭 이러한 방법만 있는 것은 아닙니다. 플랫폼의 프로그래밍 환경을 일반 컨테이너로 내보낼 수 있도록 지원한다면, 플랫폼을 사용하는 개발자가 바닥부터 시작하거나 컨테이너에 대한 모든 것을 알 필요가 없습니다. 현재 애플리케이션을 그대로 재현한 컨테이너 이미지만 있으면 됩니다(예를 들어 기능과 노드 런타임을 포함하는 컨테이너 이미지). 여기서 시작하면 컨테이너 이미지를 그들의 필요에 맞게 조금만 변경하면 됩니다. 이렇게 점진적인 학습을 통해 고수준 플랫폼에서 저수준 인프라로 매끄럽게 나아갈 수 있습니다. 개발자가 힘들게 노력할 필요가 없으므로 플랫폼의 효용성도 향상됩니다.  

##### 15.3.2. 서비스와 서비스 디스커버리를 위한 기존 메커니즘 지원  
<br/>

플랫폼은 진화하면서 다른 시스템과 상호 연결됩니다. 많은 개발자가 플랫폼 안에서 행복하고 생산적이지만, 현업에서의 애플리케이션은 구축한 플랫폼과 저수준의 쿠버네티스 애플리케이션뿐만 아니라 다른 플랫폼으로도 확장됩니다. 큰 규모의 애플리케이션이라면 레거시 데이터베이스나 쿠버네티스에 맞게 구축된 오픈 소스 애플리케이션에 항상 연결됩니다.  

상호 연결을 하려면 서비스와 서비스 디스커버리를 위한 핵심 쿠버네티스 프리미티브를 구축한 플랫폼에서 사용하고 노출하는 것이 매우 중요합니다. 플랫폼 경험을 쌓겠다는 목적으로 밑바닥부터 다시 개발하는 것은 좋은 선택이 아닙니다. 그러면 더 넓은 세상과 상호작용할 수 없는 단결된 플랫폼을 만들 수도 있습니다.  

플랫폼에 정의된 애플리케이션을 쿠버네티스 서비스로 노출하면, 클러스터 내의 모든 애플리케이션은 고수준 플랫폼에서 실행되는지의 여부와 관계없이 애플리케이션을 사용할 수 있습니다. 마찬가지로 서비스 디스커버리를 위해 쿠버네티스 DNS 서버를 사용하면, 고수준 애플리케이션 플랫폼에 정의되어 있지 않더라도 클러스터에서 실행 중인 다른 애플리케이션으로 연결할 수 있습니다. 더 낫고 사용하기 편한 것을 구축하고 싶다는 유혹이 있겠지만, 다양한 플랫폼 사이의 상호 연결성은 충분히 오래되고 복잡한 애플리케이션에 적용하는 일반적인 디자인 패턴입니다. 단절된 플랫폼은 항상 후회를 동반합니다.  

#### 15.4. 애플리케이션 플랫폼 구축 모범 사례  
<br/>

쿠버네티스는 소프트웨어 운여에 필요한 강력한 도구를 제공하지만, 개발자가 애플리케이션을 구축하기 위해 사용하는 도구로 본다면 상당히 부족합니다. 따라서 개발자의 생산성을 높이고 쿠버네티스를 쉽게 사용하기 위해 종종 쿠버네티스 위에 플랫폼을 구축해야 합니다. 이러한 플랫폼을 구축할 때 다음과 같은 모범 사례를 염두에 두세요.  

+ 어드미션 컨트롤러를 사용하여 클러스터에 대한 API 호출을 제한하고 수정해야 합니다. 어드미션 컨트롤러는 쿠버네티스 리소스의 유효성을 검증하고 타당하지 않은 리소스는 거절할 수 있습니다. 어드미션 컨트롤러는 API 리소스를 자동으로 수정해 사용자가 몰라도 되는 새로운 사이드카나 변경 사항을 추가할 수도 있습니다.

+ kubectl 플러그인을 사용하여 기존의 익숙한 커맨드라인 툴에 새로운 도구를 추가하여 쿠버네티스 UX를 확장하세요. 드물지만 별도의 특수한 도구가 더 적합할 수 있습니다.

+ 쿠버네티스 위에 플랫폼을 구축할 때 플랫폼 사용자와 사용자의 요구가 어떻게 발전할 것인지를 신중하게 고민해야 합니다. 간단하고 사용하기 쉽게 만드는 것은 분명 좋은 목표이지만, 사용자가 플랫폼 외부의 모든 것을 다시 구현해야 한다면 결국 좌절하고 실패하게 될 겁니다.  

### 16. 상태와 스테이트풀 애플리케이션 관리  
<br/>

초기 컨테이너 오케스트레이션의 대상 워크로드는 일반적으로 외부 시스템에 상태를 저장하는 스테이트리스 애플리케이션이었습니다. 컨테이너의 수명은 일시적이었으며 상태를 저장하는 백업 스토리지를 일관되게 유지하는 것은 어려웠습니다. 시간이 지남에 따라 상태를 유지하는 컨테이너 기반 워크로드의 필요성이 커지기 시작했습니다. 이에 따라 쿠버네티스는 파드에 스토리지 볼륨 마운트를 허용하도록 발전했습니다. 그리고 쿠버네티스가 직접 관리하는 볼륨은 워크로드와 함께 스토리지를 오케스트레이션하는 데 중요한 구성 요소가 되었습니다.  

만약 컨테이너에 외부 볼륨을 마운트하는 기능이 완벽하다면 쿠버네티스의 대규모 스테이트풀 애플리케이션 사례가 더 많았을 겁니다. 사실상 현업에서 볼륨 마운트 자체는 스테이트풀 애플리케이션의 큰 구조에서 간단한 구성 요소입니다. 노드 장애 이후 상태를 유지해야 하는 애플리케이션은 관계형 데이터베이스 시스템, 분산 키/값 스토리지, 문서 관리 시스템처럼 복잡한 데이터 상태 엔진입니다. 이런 종류의 클러스터 애플리케이션에서는 애플리케이션의 구성원이 서로 통신하는 방법, 구성원을 식별하는 방법, 구성원이 시스템에 나타나거나 사라지는 순서 등 많은 조정을 필요로 합니다.  

오퍼레이터는 쿠버네티스 기본 요소를 제공하며 복잡한 데이터 관리 시스템을 쉽게 운영할 수 있는 비즈니스 또는 애플리케이션 로직을 사용자 정의 컨트롤러로 추가할 수 있습니다.  

#### 16.1. 볼륨과 볼륨 마운트  
<br/>

복잡한 데이터베이스나 처리량이 많은 데이터 큐 서비스에서만 상태 유지가 필요한 건 아닙니다. 종종 컨테이너화된 워크로드로 이동되는 애플리케이션도 특정 디렉터리에서 필요한 데이터를 읽거나 쓸 수 있습니다. 컨피그맵이나 시크릿으로 마운트 된 데이터는 일반적으로 읽기 전용입니다.  

도커, rkt, CRI-O, 싱귤레리티(Singularity)와 같은 주요 컨테이너 런타임은 외부 스토리지 시스템과 연계된 볼륨을 컨테이너에 마운트할 수 있습니다. 가장 단순한 외부 스토리지로는 메모리와 컨테이너 호스트 경로, NFS, Glusterfs, CIFS, Ceph와 같은 외부 파일 시스템이 있습니다. 왜 이것이 필요할까요? 예를 들어 레거시 애플리케이션이 필요한 정보를 로컬 파일 시스템에 기록할 수 있습니다. 방법은 여러 가지입니다. stdout 또는 stderr 사이드카 컨테이너에 로그를 쓰려고 애플리케이션 코드를 업데이트할 수 있습니다. 사이드카 컨테이너는 공유 파드 볼륨을 통해 로그를 외부 소스로 스트리밍합니다. 또는 호스트 기반 로깅 도구를 이용해 호스트 로그와 컨테이너 애플리케이션 로그를 볼륨에서 읽을 수 있습니다. 이 방법은 다음과 같이 쿠버네티스 hostPath 마운트를 이용해 컨테이너에 볼륨을 마운트합니다.  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-webserver
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-webserver
    spec:
      containers:
      - name: nginx-webserver
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
          - name: hostvol
            mountPath: /usr/share/nginx/html
      volumes:
        - name: hostvol
          hostPath:
            path: /home/webcontent
```

##### 16.1.1. 볼륨 모범 사례  
<br/>

+ 데이터를 공유하는 여러 컨테이너가 존재하는 파드의 볼륨 사용을 제한해야 합니다. 예를 들면 어댑터와 앰배서더 유형 패턴입니다. 이러한 공유 패턴은 emptyDir을 사용합니다.

+ 노드 기반 에이전트나 서비스에서 데이터에 접근해야 한다면 hostDir을 사용합니다.

+ 중요한 애플리케이션 로그와 이벤트를 로컬 디스크에 쓰는 서비스를 파악해야 합니다. 가능하다면 stdout이나 stderr로 변경하고, 볼륨 맵 대신 쿠버네티스 환경의 로그 집계 시스템을 통해 로그를 스트리밍합니다.  

#### 16.2. 쿠버네티스 스토리지  
<br/>

지금까지 파드의 컨테이너에 기본 볼륨을 매핑하는 것을 살펴봤습니다. 쿠버네티스가 볼륨 마운트 뒤에 존재하는 스토리지를 관리하는 것이 핵심입니다. 이를 통해 파드가 언제든 재기동될 수 있는 동적인 시나리오가 가능합니다. 또한 파드를 백업하는 스토리지는 파드 위치에 맞게 전환될 수 있습니다. 쿠버네티스는 퍼시스턴트볼륨과 퍼시스턴트볼륨클레임 API를 사용해 파드의 스토리지를 관리합니다.  

##### 16.2.1. 퍼시스턴트볼륨  
<br/>

퍼시스턴트볼륨을 파드에 마운트된 모든 볼륨을 백업하는 디스크로 볼 수 있습니다. 퍼시스턴트볼륨은 클레임 정책을 가지고 있습니다. 클레임 정책은 볼륨을 사용하는 파드의 라이프사이클과 독립된 볼륨의 범위를 정의합니다. 쿠버네티스는 동적 또는 정적으로 정의된 볼륨을 사용할 수 있습니다. 동적으로 볼륨을 생성하려면 쿠버네티스에 정의된 스토리지클래스(storageClass)가 필요합니다. 퍼시스턴트볼륨은 다양한 유형의 클러스터에서 생성될 수 있으며 퍼시스턴트볼륨클레임이 퍼시스턴트볼륨과 일치할 때 파드에 할당됩니다. 볼륨 플러그인은 볼륨을 지원합니다. 쿠버네티스에서 직접 지원되는 수많은 플러그인이 있으며 각각 조정하는 설정 매개변수가 다릅니다.  

```yaml
apiVersion: v1
kind: PersistentVolume
metatdata:
  name: pv001
  labels:
    tier: "silver"
spec:
  capacity:
    storage: 5Gi
  accessModes:
  - ReadWriteMany
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: nfs
  mountOptions:
  - hard
  - nfsvers=4.1
  nfs:
    path: /tmp
    server: 172.17.0.2
```

##### 16.2.2. 퍼시스턴트볼륨클레임  
<br/>

퍼시스턴트볼륨클레임은 파드가 사용할 스토리지에 대한 리소스 요구사항 정의로 쿠버네티스에 전달됩니다. 쿠버네티스는 클레임을 참조해서 클레임 요청과 일치하는 persistentVolume이 있으면 해당 볼륨을 특정 파드에 할당합니다. 반드시 스토리지 요청 크기와 접근 모드를 정의해야 하며 스토리지클래스도 추가로 정의할 수 있습니다. 셀렉터는 특정 기준을 충족하는 퍼시스턴트볼륨을 매치할 때 사용됩니다.  

```yaml
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: my-pvc
spec:
  storageClass: nfs
    accessModes:
    - ReadWriteMany
  resources:
    requests:
      storage: 5Gi
  selector:
    matchLabels:
      tier: "silver"
```

이 클레임은 스토리지클래스 이름과 셀렉터 매치, 크기, 접근 모드가 앞에서 만든 퍼시스턴트볼륨과 모두 같기 때문에 매치됩니다.  

쿠버네티스는 퍼시스턴트볼륨을 클레임과 매치하고 바인딩합니다. 이제 볼륨을 사용하려면 pod.spec에서 다음과 같이 클레임 이름을 참조하면 됩니다.  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-webserver
spec:
  replicas: 3
  selector:
    matchLabels:
      app: nginx-webserver
  template:
    metadata:
      labels:
        app: nginx-webserver
      spec:
        containers:
        - name: nginx-webserver
        image: nginx:alpine
        ports:
        - containerPort: 80
        volumeMounts:
          - name: hostvol
            mountPath: /usr/share/nginx/html
      volumes:
        - name: hostvol
          persistentVolumeClaim:
            className: my-pvc
```

##### 16.2.3. 스토리지클래스  
<br/>

관리자는 퍼시스턴트볼륨을 수동으로 정의하는 대신 스토리지클래스 객체를 생성할 수 있습니다. 스토리지클래스에는 사용할 볼륨 플러그인과 해당 클래스의 모든 퍼시스턴트볼륨이 사용할 특정 마운트 옵션과 매개변수를 정의합니다. 그러면 클레임에 스토리지클래스를 정의할 수 있고, 쿠버네티스는 스토리지클래스 매개변수와 옵션을 기반으로 퍼시스턴트볼륨을 동적으로 만들 수 있습니다.  

```yaml
kind: StorageClass
apiVersion: storage.k8s.io/v1
metadata:
  name: nfs
provisioner: cluster.local/nfs-client-provisioner
parameters:
  archiveOnDelete: True
```

운영자는 DefaultStorageClass 어드미션 플러그인을 사용해 기본 스토리지클래스를 생성할 수 있습니다. API 서버에 이것이 활성화되어 있다면, 명시적으로 스토리지클래스를 정의하지 않는 퍼시스턴트볼륨클레임에 기본 스토리지클래스를 정의할 수 있습니다. 일부 클라우드 공급자는 기본 스토리지클래스를 인스턴스에서 허용하는 가장 저렴한 스토리지에 매핑합니다.  

###### 16.2.3.1. 컨테이너 스토리지 인터페이스와 플렉스볼륨  
<br/>

'제정신이 아닌' 볼륨 플러그인으로 불리는 컨테이너 스토리지 인터페이스(Container Storage Interface, CSI)와 플렉스볼륨(FlexVolume)을 이용해 스토리지 벤더는 사용자 정의 저장 플러그인을 구현할 수 있습니다. 다른 플러그인처럼 쿠버네티스의 코드베이스에 코드가 구현되기를 기다릴 필요가 없는 겁니다.  

CSI와 플렉스볼륨 플러그인은 확장 기능으로 쿠버네티스 클러스터에 배포됩니다. 새로운 기능이 필요하면 스토리지 벤더가 업데이트합니다.  

CSI가 깃허브에서 언급한 목표는 다음과 같습니다.  

> 스토리지 벤더가 플러그인을 한 번만 개발해도 수많은 컨테이너 오케스트레이션 시스템에서 동작할 수 있는 산업 표준 컨테이너 스토리지 인터페이스를 정의합니다.  

플렉스볼륨 인터페이스는 스토리지 공급자가 기능을 추가할 때 사용하는 전통적인 방법입니다. 대신 사용할 클러스터의 모든 노드에 특정 드라이버를 설치해야 합니다. 즉 클러스터의 호스트에 실행 파일을 설치합니다. 이것이 플렉스볼륨을 사용할 때의 최대 단점입니다. 특히 관리형 서비스 공급자에서는 노드에 접근이 어렵고 마스터로는 실제로 접근할 수 없습니다. 하지만 CSI 플러그인은 이 문제를 해결했습니다. 플렉스볼륨과 기본적으로 동일한 기능을 제공하는데, 파드를 클러스터에 배포하는 것만큼 사용하기 편리합니다.  

##### 16.2.4. 쿠버네티스 스토리지 모범 사례  
<br/>

클라우드 네이티브 애플리케이션 설계 원칙은 가능하면 스테이트리스 애플리케이션을 설계하는 겁니다. 그러나 컨테이너 기반 서비스의 규모가 커지면 일반적으로 데이터 스토리지 영속성이 필요합니다. 쿠버네티스 스토리지 관련 모범 사례를 통해 애플리케이션을 설계할 때 필요한 스토리지 구현체를 효과적으로 전달할 수 있습니다.  

+ 가능하면 DefaultStorageClass 어드미션 플러그인을 활성화하고 기본 스토리지클래스를 정의하세요. 퍼시스턴트볼륨이 필요한 애플리케이션의 헬름 차트는 default 스토리지클래스가 설정되므로 애플리케이션을 거의 수정하지 않고도 설치할 수 있습니다.

+ 온프레미스나 클라우드 공급자에서 클러스터의 아키텍처를 설계할 때, 계산과 데이터 계층의 영역과 상호 연결을 고려해야 합니다. 노드와 퍼시스턴트볼륨에 적절한 레이블을 쓰고 어피니티를 사용해 데이터와 워크로드가 최대한 근접해야 합니다. 궁극적으로는 A 영역의 노드에 있는 파드가 B 영역의 노드에 붙어있는 볼륨을 마운트해야 합니다.  

+ 디스크에서 상태를 유지해야 하는 워크로드는 신중하게 고려하세요. 데이터베이스 시스템과 같은 외부 서비스에서 다룰 수 있나요? 만약 클라우드 공급자에서 실행 중이라면 서비스형 몽고DB 또는 MySQL 등의 현재 사용 중인 API와 일치하는 호스팅 API 서비스에서 처리할 수 있나요?

+ 애플리케이션 코드를 스테이트리스로 수정하는 데 얼마나 많은 노력이 드는지 조사해보세요.

+ 쿠버네티스는 워크로드가 스케줄링될 때 볼륨을 추적하고 마운트합니다. 하지만 해당 볼륨에 저장된 데이터의 중복과 백업은 아직 지원하지 않습니다. CSI 명세에는 네이티브 스냅샷 기술을 벤더가 플러그인 할 수 있는 API가 추가되었습니다. 하지만 스토리지 백엔드가 이를 지원해야 합니다.

+ 볼륨이 유지할 데이터의 적절한 라이프사이클을 검증합니다. 기본적으로 회수 정책은 퍼시스턴트볼륨을 동적 프로비저닝하도록 설정되어 있습니다. 따라서 파드가 삭제되면 백업 스토리지 공급자에게서 볼륨을 삭제합니다. 민감한 데이터나 법의학 분석에 사용할 수 있는 데이터는 회수하도록 설정해야 합니다.  

#### 16.3. 스테이트풀 애플리케이션  
<br/>

통념과는 달리, 쿠버네티스는 초기부터 MySQL과 카프카, 카산드라(Cassandra) 등 스테이트풀 애플리케이션을 지원했습니다. 그러나 복잡도가 높았으며 일반적으로 확장성과 내구성에 많은 노력이 드는 소규모 워크로드가 대상이었습니다.  

중요한 차이점을 파악하기 위해서는 레플리카셋이 파드를 스케줄링하고 관리하는 방법이 전통적인 스테이트풀 애플리케이션에 해를 끼칠 수 있다는 것을 이해해야 합니다.  

+ 레플리카셋의 파드는 스케줄링 때 수평 확장되고 무작위로 이름이 할당됩니다.
+ 레플리카셋의 파드는 임의의 방식으로 규모가 축소됩니다.
+ 레플리카셋의 파드는 절대로 이름이나 IP 주소로 직접 호출되지 않고 Service를 통해 호출됩니다.
+ 레플리카셋의 파드는 언제든 재시작하거나 다른 노드로 이동할 수 있습니다.
+ 퍼시스턴트볼륨이 매핑된 레플리카셋의 파드는 오직 클레임을 통해 연결되며 새로운 이름을 가진 신규 파드는 다시 스케줄링될 때 해당 클레임을 넘겨받을 수 있습니다.  

클러스터 데이터 관리 시스템에 대해 피상적으로만 알고 있는 사용자는 이러한 레플리카셋 기반 파드의 특성에서 문제를 바로 알아챌 수 있습니다. 데이터베이스의 현재 쓸 수 있는 복사본이 갑자기 삭제되는 파드를 상상해보세요. 혼란스러울 겁니다.  

쿠버네티스 세계가 낯선 대부분의 사람들은 스테이트풀셋 애플리케이션과 데이터베이스 애플리케이션을 동일하게 생각합니다. 쿠버네티스는 어떤 유형의 애플리케이션을 배포하고 있는지 알지 못한다는 점에서 사실이 아닙니다. 쿠버네티스는 데이터베이스 시스템이 리더를 선출하려는지 구성원 사이에 데이터 복제가 가능한지 등 데이터베이스 시스템에 대해서는 전혀 알지 못합니다. 이것이 스테이트풀셋이 등장한 이유입니다.  

##### 16.3.1. 스테이트풀셋  
<br/>

스테이트풀셋을 통해 신뢰할 수 있는 노드와 파드에서 애플리케이션 시스템을 편리하게 실행할 수 있습니다. 레플리카셋의 전형적인 파드 특성은 스테이트풀셋과 정반대입니다. 스테이트풀셋의 초기 형태인 쿠버네티스 v1.3 명세에서 PetSets은 복잡한 데이터 관리 시스템과 같은 스테이트풀 애플리케이션이 요구하는 중요한 스케줄링과 관리를 위해 도입되었습니다.  

+ 스테이트풀셋의 파드는 수평 확장되며 순차적으로 이름이 할당됩니다. 확장될 때 파드는 서수 이름을 얻고 기본적으로 새 파드는 다음 파드가 추가되기 전에 완전히 온라인 상태가 되어야 합니다(생명성 프로브 또는 준비성 프로브 통과).
+ 스테이트풀셋의 파드는 역순으로 규모가 축소됩니다.
+ 스테이트풀셋의 파드는 헤드리스 서비스 뒤의 이름으로 개별적으로 주소를 지정할 수 있습니다.
+ 볼륨 마운트가 필요한 스테이트풀셋의 파드는 정의된 퍼시스턴트볼륨 템플릿을 사용해야 합니다. 스테이트풀셋의 파드에서 클레임된 볼륨은 스테이트풀셋이 삭제될 때 삭제되지 않습니다.  

스테이트풀셋의 명세는 서비스 선언 및 퍼시스턴트볼륨 템플릿을 제외하고는 디플로이먼트와 매우 유사합니다. 파드가 개별 주소를 가지려면 먼저 헤드리스 서비스를 생성해야 합니다. 헤드리스 서비스는 일반 서비스와 동일하지만 로드 밸런싱은 수행하지 않습니다.  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mongo
  labels:
    name: mongo
spec:
  ports:
  - port: 27017
    targetPort: 27017
  clusterIP: None # 이것은 헤드리스 서비스를 생성합니다.
  selector:
    role: mongo
```

스테이트풀셋의 정의는 몇 가지만 빼고 디플로이먼트와 같습니다.

```yaml
apiVersion: apps/v1beta1
kind: StatefulSet
metadata:
  name: mongo
spec:
  serviceName: "mongo"
  replicas: 3
  template:
    metadata:
      labels:
        role: mongo
        environment: test
    spec:
      terminationGracePeriodSeconds: 10
      containers:
        - name: mongo
          image: mongo:3.4
          command:
            - mongo
            - "--replSet"
            - rs0
            - "--bind_ip"
            - 0.0.0.0
            - "--smallfiles"
            - "--noprealloc"
          ports:
            - containerPort: 27017
          volumeMounts:
            - name: mongo-persistent-storage
              mountPath: /data/db
        - name: mongo-sidecar
          image: cvallance/mongo-k8s-sidecar
          env:
            - name: MONGO_SIDECAR_POD_LABELS
              value: "role=mongo,environment=test"
  volumeClaimTemplates:
  - metadata:
      name: mongo-persistent-storage
      annotations:
        volume.beta.kubernetes.io/storage-class: "fast"
    spec:
      accessModes: [ "ReadWriteOnce" ]
      resources:
        requests:
          storage: 2Gi
```

##### 16.3.2. 오퍼레이터  
<br/>

스테이트풀셋을 통해 복잡한 스테이트풀 데이터 시스템을 쿠버네티스에서 실행할 수 있게 되었습니다. 앞에서 언급했듯이 쿠버네티스는 스테이트풀셋에서 실행되는 워크로드를 실제로 이해하지 못하는 문제가 있습니다. 백업과 장애 극복, 리더 등록, 신규 레플리카 등록, 업그레이드와 같은 복잡한 작업은 정기적으로 수행되며 스테이트풀셋으로 실행할 때 신중하게 고려해야 합니다.  

쿠버네티스 성장 초기에 코어OS 사이트의 SRE는 쿠버네티스를 위한 새로운 종류의 클라우드 네이티브 소프트웨어인 오퍼레이터를 만들었습니다. 원래 의도는 특정 애플리케이션을 실행하는 애플리케이션 도메인 지식을 쿠버네티스를 확장하는 특정 컨트롤러에 담아 캡슐화하는 것이었습니다. 배포와 확장, 업그레이드, 백업, 일반적인 유지보수 작업을 할 수 있도록 카산드라나 카프카에 스테이트풀셋 컨트롤러를 구축한다고 상상해봅시다. 최초로 만들어진 etcd와 프로메테우스 오퍼레이터는 시계열 데이터베이스를 사용해 시간에 따른 메트릭을 보관합니다. 프로메테우스나 etcd 인스턴스의 적절한 생성, 백업, 복원 설정은 오퍼레이터가 처리할 수 있으며 기본적으로 파드나 디플로이먼트처럼 새로운 쿠버네티스 관리 객체입니다.  

최근까지 오퍼레이터는 SRE나 소프트웨어 벤더에서 특정 애플리케이션을 위해 만든 일회성(one-off) 도구였습니다. 2018년 중반에 레드햇(RedHat)은 오퍼레이터 프레임워크를 만들었습니다. 오퍼레이터 프레임워크는 SDK 라이프사이클 관리자와 앞으로 개발될 모듈로 계량, 마켓플레이스, 레지스트리 유형 함수를 포함한 도구입니다. 오퍼레이터는 스테이트풀 애플리케이션뿐 아니라 사용자 정의 컨트롤러 로직을 통한 복잡한 데이터 서비스와 스테이트풀 시스템 적합합니다.  

오퍼레이터는 여전히 쿠버네티스에서 개발 중인 기술입니다. 하지만 쿠버네티스에서 복잡한 분산 시스템을 운영하는 데 필요한 운영 지식을 쿠버네티스에 담으로녀느 데이터 관리 시스템 밴더와 클라우드 공급자, SRE의 기반 기술입니다. 관리되는 오퍼레이터의 최신 목록을 보려면 오퍼레이터허브를 참조하세요.  

##### 16.3.3. 스테이트풀셋과 오퍼레이터 모범 사례  
<br/>

상태를 유지해야 하며 복잡한 구성 작업이 필요한 대규모 분산 애플리케이션의 경우, 쿠버네티스 스테이트풀셋과 오퍼레이터를 사용하면 좋습니다. 오퍼레이터는 계속 발전하고 있으며 큰 규모의 커뮤니티의 지원을 받고 있습니다. 이와 관련된 모범 사례입니다.  

+ 스테이트풀셋을 사용하기로 결정할 때 신중해야 합니다. 보통 스테이트풀 애플리케이션은 오케스트레이터가 아직은 잘 관리할 수 없는 높은 수준의 관리를 필요로 하기 때문입니다.

+ 스테이트풀셋의 헤드리스 서비스는 자동으로 생성되지 않습니다. 파드를 개별 노드처럼 처리하려면 배포 시 주소 할당을 반드시 생성해야 합니다.

+ 애플리케이션이 서수로 이름을 짓거나 신뢰성 있는 확장을 할 때 항상 퍼시스턴트볼륨을 할당해야 하는 것은 아닙니다.

+ 클러스터의 노드가 응답하지 않는 경우 스테이트풀셋의 파드는 자동으로 삭제되지 않습니다. 대신 유예 기간이 지나면 Terminating 또는 Unknown 상태가 됩니다. 이 파드를 정리할 수 있는 유일한 방법은 클러스터에서 노드 객체를 제거하거나, kubelet이 다시 동작해서 파드를 직접 삭제하거나, 오퍼레이터가 강제로 파드를 삭제하는 겁니다. 강제 삭제는 최후의 수단이어야 하며, 삭제된 파드가 있는 노드가 다시 온라인 상태가 되지 않도록 주의해야 합니다. 클러스터에 동일한 이름을 가진 파드가 둘이 될 수 있기 때문입니다. kubectl delete pod nginx-0 --grace-period=0 --force를 실행하면 파드를 강제로 삭제할 수 있습니다.  

+ 파드를 강제로 삭제한 후에도 여전히 Unknown 상태로 유지될 수 있습니다. API 서버에 대한 패치를 통해 항목을 삭제하고 스테이트풀셋 컨트롤러가 삭제된 파드의 새 인스턴스를 생성할 수 있습니다. kubectl patch pod nginx-0 -p '{"metadata":{"finalizers":null}}'.

+ 특정 유형의 리더 선출이나 데이터 복제 확인 프로세스가 있는 복잡한 데이터 시스템을 실행 중이라면, 정상적인 종료 프로세스를 사용해 파드를 삭제하기 전에 preStop hook을 사용하세요. 모든 연결을 종료하고 강제로 리더를 선출하거나 데이터 동기화를 검증해야 합니다.

+ 스테이트풀 데이터가 필요한 애플리케이션이 복잡한 데이터 관리 시스템이라면 애플리케이션의 복잡한 라이프사이클 구성 요소 관리에 도움이 되는 오퍼레이터가 있는지 확인해야 합니다. 애플리케이션이 사내에서 구축되었다면, 애플리케이션에 관리 기능을 추가할 수 있도록 애플리케이션을 오퍼레이터 형태로 패키징하는 것이 가치가 있는지 조사해야 합니다. 코어OS 오퍼레이터 SDK의 예제를 참조하세요.  

### 17. 어드미션 컨트롤과 권한  
<br/>

쿠버네티스 API에 대한 접근 제어는 클러스터 보안의 핵심입니다. 또한 클러스터의 모든 사용자, 워크로드, 구성 요소에 정책과 거버넌스를 부여하는 도구로 사용될 수 있습니다. 다음은 API 요청 흐름입니다.

API 요청 → 인증권한 → 변환어드미션 → 웹훅 어드미션 검토 → 웹훅 어드미션응답 → 스키마검증 → 검증어드미션 → 웹훅 어드미션 검토 → 웹훅 어드미션응답 → etcd  

#### 17.1. 어드미션 컨트롤  
<br/>

아직 존재하지 않는 네임스페이스에 리소스를 정의할 때 네임스페이스가 자동으로 생성되는 원리가 궁금한가요? 혹은 기본 스토리지클래스가 결정되는 방법이 궁금한가요? 이것들은 바로 어드미션 컨트롤러가 하는 일입니다.  

##### 17.1.1. 어드미션 컨트롤러의 정의  
<br/>

어드미션 컨트롤러는 쿠버네티스 API 서버 요청 처리 단계 중 하나이며 인증과 권한의 다음 단계에 존재합니다. 요청 객체를 스토리지에 저장하기 전에 요청을 검증하거나 변형(또는 둘 다)합니다. 변형 어드미션 컨트롤러는 요청 객체를 수정할 수 있지만 검증 어드미션 컨트롤러는 요청 객체를 수정할 수 없습니다.  

##### 17.1.2. 어드미션 컨트롤러의 중요성  
<br/>

어드미션 컨트롤러는 모든 API 서버 요청 처리 과정에 존재하기 때문에 다양한 용도로 사용할 수 있습니다. 다음 세 그룹이 가장 일반적입니다.  

###### 17.1.2.1. 정책과 거버넌스  
<br/>

어드미션 컨트롤러를 통해 비즈니스 요구사항을 만족시키기 위한 정책을 시행할 수 있습니다. 예시는 다음과 같습니다.  

+ dev 네임스페이스 내에서는 내부 클라우드 로드 밸런서만 사용할 수 있습니다.
+ 파드의 모든 컨테이너에는 리소스 제한이 존재해야 합니다.
+ 모든 리소스에 사전 정의된 표준 레이블이나 주석을 추가하여 기존 도구에서 찾을 수 있도록 해야 합니다.
+ 모든 인그레스 리소스는 HTTPS만 사용합니다.  

###### 17.1.2.2. 보안  
<br/>

어드미션 컨트롤러를 사용하여 클러스터 전체 보안 상태를 일관되게 유지할 수 있습니다. 전형적인 예로 파드 명세에서 보안에 민감한 필드를 제어할 수 있는 파드시큐리티폴리시 어드미션 컨트롤러가 있습니다. 예를 들어, 권한 있는 컨테이너나 호스트 파일 시스템의 특정 경로 사용을 거부하는 겁니다. 어드미션 웹훅을 사용하여 보안을 더욱 세분화하거나 사용자가 정의한 보안 규칙을 시행할 수 있습니다.  

###### 17.1.2.3. 리소스 관리  
<br/>

어드미션 컨트롤러를 사용하면 클러스터 사용자에게 모범 사례를 제공하기 위해 다음을 검증할 수 있습니다.  

+ 모든 인그레스의 전체 주소 도메인 네임은 특정 접미사를 가져야 합니다.
+ 인그레스의 전체 주소 도메인 네임이 겹치지 않아야 합니다.
+ 파드의 모든 컨테이너에는 리소스 제한이 존재해야 합니다.  

##### 17.1.3. 어드미션 컨트롤러 유형  
<br/>

어드미션 컨트롤러에는 표준(standard)과 동적(dynamic)이라는 두 가지 등급이 있습니다. 표준 어드미션 컨트롤러는 API 서버와 함께 컴파일되며 각 쿠버네티스 릴리스와 함께 플러그인으로 제공됩니다. 또한 API 서버가 시작될 때 설정해야 합니다. 반면 동적 어드미션 컨트롤러는 런타임 시점에 설정할 수 있으며 핵심 쿠버네티스 코드베이스 외부에서 개발됩니다. 동적 어드미션 컨트롤러는 HTTP 콜백을 통해 어드미션 요청을 받는 어드미션 웹훅이 유일합니다.  

쿠버네티스에는 30개 이상의 어드미션 컨트롤러가 존재합니다. 이 컨트롤러는 쿠버네티스 API 서버에서 다음 플래그를 통해 활성화됩니다.  

--enable-admission-plugins  

표준 어드미션 컨트롤러를 활성화해야 사용할 수 있는 쿠버네티스 기능이 있습니다. 다음은 권장되는 기본 어드미션 컨트롤러입니다.  

--enable-admission-plugins=
NamespaceLifecycle,LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,Priority,ResourceQuota,PodSecurityPolicy  

쿠버네티스 어드미션 컨트롤러 목록과 기능은 쿠버네티스 문서에서 찾을 수 있습니다.  

권장 어드미션 컨트롤러 목록에서 MutatingAdmissionWebhook과 ValidatingAdmissionWebhook을 볼 수 있습니다. 이 표준 어드미션 컨트롤러에는 어드미션 로직이 구현되어 있지 않습니다. 대신 어드미션 요청 객체를 전달하기 위해 클러스터 내부에서 실행되는 웹훅 엔드포인트를 구성하는 데 사용됩니다.  

##### 17.1.4. 어드미션 웹훅 설정  
<br/>

앞에서 언급했듯이 어드미션 웹훅의 주요 장점은 동적으로 구성할 수 있다는 겁니다. 일관성 및 실패 모드와 관련된 트레이드오프와 미치는 영향을 고려하여, 어드미션 웹훅을 효과적으로 구성하는 방법을 아는 것이 중요합니다.  

다음 코드는 ValidatingWebhookConfiguration 리소스 매니페스트입니다. 이 매니페스트는 검증된 어드미션 웹훅을 정의합니다. 주석은 각 필드의 기능에 대해 자세히 설명합니다.  

```yaml
apiVersion: admissionregistration.k8s.io/v1beta1
  kind: ValidatingWebhookConfiguration
  metadata:
    name: ## 리소스 이름
  webhooks:
  - name: ## 어드미션 웹훅 이름으로 어드미션 검토 결과 거절되면 사용자에게 보여집니다.
    clientConfig:
      service:
        namespace: ## 어드미션 웹훅 파드가 만들어질 네임스페이스
        name: ## 어드미션 웹훅에 연결할 때 사용되는 서비스 이름
        path: ## 웹훅 URL
      caBundle: ## PEM으로 인코딩 CA 번들로 웹훅 서버 증명을 검증하는 데 사용
    rules: ## API 서버가 웹훅에 전송해야 하는 리소스와 하위 리소스에 대한 동작을 설명  
    - operations:
      - ## API 서버가 웹훅에게 전송하도록 촉발하는 특정 동작(예를 들어 create, update, delete, connect)
      apiGroups:
      - ""
      apiVersions:
      - "*"
      resources:
      - ## 특정 리소스 이름(예를 들어 디플로이먼트, 서비스, 인그레스)
    failurePolicy: ## 접근 문제나 알 수 없는 오류를 처리하는 방법을 정의합니다. 값을 Ignore 또는 Fail입니다.
```

MutatingWebhookConfiguration 리소스 매니페스트도 마저 살펴보겠습니다. 이 매니페스트는 변형 어드미션 웹훅을 정의합니다. 주석은 각 필드의 기능에 대해 자세히 설명합니다.  

```yaml
apiVersion: admissionregistration.k8s.io/v1beta1
kind: MutatingWebhookConfiguration
metadata:
  name: ## 리소스 이름
webhooks:
- name: ## 어드미션 웹훅 이름으로 어드미션 검토 결과 거절되면 사용자에게 보여집니다.
  clientConfig:
    service:
      namespace: ## 어드미션 웹훅 파드가 만들어질 네임스페이스
      name: ## 어드미션 웹훅에 연결할 때 사용되는 서비스 이름
      path: ## 웹훅 URL
      caBundle: ## PEM으로 인코딩 CA 번들로 웹훅 서버 증명을 검증하는 데 사용
    rules: ## API 서버가 웹훅에 전송해야 하는 리소스와 하위 리소스에 대한 동작을 설명  
    - operations:
      - ## API 서버가 웹훅에게 전송하도록 촉발하는 특정 동작(예를 들어 create, update, delete, connect)
      apiGroups:
      - ""
      apiVersions:
      - "*"
      resources:
      - ## 특정 리소스 이름(예를 들어 디플로이먼트, 서비스, 인그레스)
    failurePolicy: ## 접근 문제나 알 수 없는 오류를 처리하는 방법을 정의합니다. 값을 Ignore 또는 Fail입니다.
```

kind 필드를 제외하고 두 리소스는 동일합니다. 하지만 백엔드에서의 한 가지 차이점은 MutatingWebhookConfiguration은 어드미션 웹훅에서 요청 객체를 수정해서 반환할 수 있지만 ValidatingWebhookConfiguration은 그렇지 않다는 것입니다. 물론 MutatingWebhookConfiguration에서 간단히 검증만 하는 것도 가능합니다. 하지만 보안을 고려해야 하며 최소 권한 규칙(least-privilege rule)을 지켜야 합니다.  

이런 의문이 들 수 있습니다. 'ValidatingWebhookConfiguration 또는 MutatingWebhookConfiguration의 rule 객체 아래 리소스 필드에 ValidatingWebhookConfiguration 또는 MutatingWebhookConfiguration을 정의하면 어떻게 되나요?' 다행히 ValidatingWebhookConfiguration과 MutatingWebhookConfiguration 객체에 대한 어드미션 요청은 ValidatingAdmissionWebhook과 MutatingWebhookConfiguration을 호출하지 않습니다. 실수로 인해 클러스터에 장애가 발생하는 것을 막을 수 있습니다.  

##### 17.1.5. 어드미션 컨트롤 모범 사례  
<br/>

지금까지 어드미션 컨트롤러의 기능에 대해 다뤘습니다. 다음은 어드미션 컨트롤러를 올바르게 활용할 수 있는 모범 사례입니다.  

+ 어드미션 플러그인의 순서는 중요하지 않습니다. 쿠버네티스 초기 버전에서는 어드미션 플러그인 순서에 따라 처리 순서가 달라졌기 때문에 중요했습니다. 현재 지원되는 쿠버네티스에서는 --enable-admission-plugins를 통해 API 서버 플래그로 지정된 어드미션 플러그인 순서가 더는 중요하지 않습니다. 하지만 어드미션 웹훅에서는 순서가 중요하기 때문에 요청 흐름을 이해해야 합니다. 요청 승인 또는 거절은 논리적 AND로 동작하며, 만약 어드미션 웹훅 중 하나가 요청을 거절하면 전체 요청이 거절되고 오류가 사용자에게 재전송됩니다. 또한 변형 어드미션 컨트롤러가 항상 검증 어드미션 컨트롤러 이전에 호출됩니다. 당연히 수정되기 이전에 유효성을 검증하는 것은 소용이 없습니다.

+ 동일한 필드를 변경하면 안 됩니다. 변형 어드미션 웹훅 사이에서는 요청 처리 순서를 정할 수 없기 때문에 구성할 때 문제가 생깁니다. 예기치 못한 결과를 초래할 수 있으니 변형 어드미션 컨트롤러가 동일한 필드를 수정하지 않도록 해야 합니다. 여러 개의 변형 어드미션 웹훅이 존재할 때 일반적으로 검증 어드미션 웹훅으로 변형 끝의 리소스 매니페스트에 문제가 없는지를 점검하는 것이 낫습니다.

+ 장애 허용/장애 차단이 중요합니다. failurePolicy 필드는 변형과 검증 웹훅 설정 리소스에 모두 존재합니다. 이 필드는 어드미션 웹훅에 접근 문제가 있거나 인식할 수 없는 오류가 발생했을 때  API 서버의 처리 방식을 정의합니다. 이 필드를 Ignore 또는 Fail로 설정할 수 있습니다. Ignore이 열리지 않는 것은 기본적으로 요청 처리는 허용함을 의미하지만 Fail은 전체 요청을 거부함을 의미합니다. 당연하게 보이지만 이것이 의미하는 바가 무엇인지를 고민해야 합니다. 중요한 어드미션 웹훅을 무시하면 사용자가 모르는 사이, 비즈니스 관련 정책이 리소스에 적용되지 않는 문제가 생길 수 있습니다. 이 문제를 방지하는 해결책은 API 서버가 지정된 어드미션 웹훅에 도달할 수 없다고 기록할 때 경고를 발생시키는 겁니다. 어드미션 웹훅에 문제가 있을 때 Fail은 모든 요청을 거부하기 때문에 더욱 치명적일 수 있습니다. 이를 방지하기 위해 특정 리소스 요청에만 어드미션 웹훅이 설정되도록 규칙의 적용 범위를 지정할 수 있습니다. 클러스터의 모든 리소스에 적용되는 규칙이 없도록 철칙을 세워야 합니다.  

+ 어드미션 웹훅을 직접 작성할 때, 어드미션 웹훅에서 결정하고 응답하는 시간이 사용자와 시스템 요청에 직접적인 영향을 준다는 것을 기억해야 합니다. 모든 어드미션 웹훅 호출에 30초 타임아웃을 설정하여 이 시간이 지나면 failurePolicy가 적용되도록 합니다. 어드미션 웹훅에서 승인/거절 결정을 내리는 데 단지 몇 초가 걸린다 하더라도, UX에 심각한 영향을 미칠 수 있습니다. 승인/거절 로직을 처리하기 위해 복잡한 로직을 수행하거나 데이터베이스와 같은 외부 시스템에 의존하지 마세요.

+ 어드미션 웹훅의 범위를 정해야 합니다. NamespaceSelector 필드를 이용해 어드미션 웹훅이 동작할 네임스페이스의 범위를 정할 수 있습니다. 웹훅이 동작할 네임스페이스의 범위를 정할 수 있습니다. 이 필드는 기본적으로 비어 있으며 모든 것을 매치합니다. matchLabels 필드를 이용해 네임스페이스 레이블을 매치할 수 있습니다. 네임스페이스별로 옵트인할 수 있으므로 항상 이 필드를 사용하세요.

+ kube-system 네임스페이스는 모든 쿠버네티스 클러스터에 공통으로 사용되는 예약된 네임스페이스입니다. 모든 시스템 수준 서비스가 동작하는 곳이므로 이 네임스페이스에서는 어드미션 웹훅을 실행하지 않는 것이 좋습니다. 간단하게 NamespaceSelector 필드를 사용해 kube-system 네임스페이스와 매치하지 않는 방법이 있습니다. 또한 클러스터 동작에 필요한 모든 시스템 수준 네임스페이스에 대해서도 고려해야 합니다.

+ RBAC를 통해 어드미션 웹훅 설정을 잠가야(lock down)합니다. 이제 어드미션 웹훅 설정의 모든 필드에 대해 알았으므로 클러스터에 대한 접근을 차단하는 간단한 방법을 알았을 겁니다. MutatingWebhookConfiguration과 ValidatingWebhookConfiguration 생성은 클러스터에서 루트 권한이 필요한 작업이므로 RBAC를 사용해 적절히 잠겨 있어야 함은 당연합니다. 그렇지 않으면 클러스터에 장애가 생기거나 애플리케이션 워크로드에 대한 주입 공격이 발생할 수 있습니다.

+ 민감한 데이터를 보내지 마세요. 어드미션 웹훅은 근본적으로 AdmissionRequests를 입력 받아 AdmissionResponse를 출력하는 블랙박스입니다. 어떻게 요청을 저장하고 조작하는지는 사용자에게 보이지 않습니다. 어드미션 웹훅으로 전송하는 요청 페이로드가 무엇인지 고려하는 것이 중요합니다. 쿠버네티스 시크릿 또는 컨피그맵의 경우 민감한 정보를 포함할 수 있습니다. 그러한 정보가 저장되고 공유되는 방법에 대한 강력한 보장이 필요합니다. 어드미션 웹훅과 이러한 리소스를 공유하면 민감한 정보가 유출될 수 있습니다. 따라서 검증과 변형에 필요한 최소한의 리소스 규칙 범위를 정해야 합니다.  

#### 17.2. 권한  
<br/>

우리는 종종 다음 질문에 대한 권한을 생각합니다. '이 사용자가 특정 리소스에 대해 이 동작을 수행할 수 있을까요?' 쿠버네티스에서 각 요청의 권한은 인증 후와 승인 전에 수행됩니다.  

##### 17.2.1. 권한 모듈  
<br/>

권한 모듈은 접근 권한을 부여하거나 거절할 책임이 있습니다. 명시적으로 정의된 정책에 따라 접근 권한을 부여합니다. 그렇지 않으면 모든 요청은 암묵적으로 거부됩니다.  

쿠버네티스는 v1.15부터 다음과 같은 권한 모듈을 제공합니다.  

+ ABAC: 로컬 파일을 통해 권한 정책을 설정할 수 있습니다.
+ RBAC: 쿠버네티스 API를 통해 권한 정책을 설정할 수 있습니다.
+ 웹훅: 원격 REST 엔드포인트를 통해 요청의 권한을 처리할 수 있습니다.
+ 노드: kubelet의 요청에 권한을 부여하는 특수 권한 모듈입니다.  

모듈은 API 서버의 --authorization-mode 플래그를 통해 클러스터 관리자가 구성합니다. 여러 모듈을 차례대로 구성하고 점검할 수 있습니다. 어드미션 컨트롤러와 달리 단일 권한 모듈에서 요청을 승인하면 요청이 진행됩니다. 모든 모듈이 요청을 거부하는 경우에만 요류가 사용자게 반환됩니다.  

###### 17.2.1.1. ABAC  
<br/>

특성 기반 접근 제어(attribute-based access control, ABAC)권한 모듈과 관련된 정책 정의를 살펴보겠습니다. 다음은 사용자 메리에게 kube-system 네임스페이스의 파드에 대한 읽기 전용 접근 권한을 부여하는 것을 보여줍니다.  

```yaml
apiVersion: abac.authorization.kubernetes.io/v1beta1
kind: Policy
spec:
  user: mary
  resource: pods
  readonly: true
  namespace: kube-system
```

메리가 다음과 같은 요청을 한다면 어떻게 될까요? 메리가 demo-app 네임스페이스의 파드에 대한 접근 권한을 갖지 않으므로 이는 거절됩니다.  

```yaml
apiVersion: authorization.k8s.io/v1beta1
kind: SubjectAccessReview
spec:
  resourceAttriubetes:
    verb: get
    resource: pods
    namespace: demo-app
```

이 예제에서 새로운 API 그룹인 authorization.k8s.io가 등장합니다. 이 API 그룹은 API 서버 권한을 외부 서비스에 노출하며 다음 API를 가지므로 디버깅에 도움이 됩니다.  

+ SelfSubjectAccessReview: 현재 사용자에 대한 접근 검토
+ SubjectAccessReview: SubjectAccessReview와 유사하며 모든 사용자에게 적용
+ LocalSubjectAccessReview: SubjectAccessReview와 유사하며 특정 네임스페이스에 한정
+ SelfSubjectRulesReview: 지정된 네임스페이스에서 사용자가 수행할 수 있는 작업 목록을 반환  

정말 멋진 부분은 원하는 대로 리소스를 생성하여 이 API 질의를 할 수 있다는 것입니다. SelfSubjectAccessReview를 사용해 이전 예제를 직접 테스트해봅시다. 출력의 상태 필드를 보면 이 요청이 허용됨을 알 수 있습니다.  

실제로 쿠버네티스의 kubectl 도구로 간편하게 할 수 있습니다. kubectl auth can-i는 이전 예제와 동일한 API를 질의합니다.  

```sh
kubectl auth can-i get pods --namespace demo-app
```

관리자 증명을 사용하여 원하는 다른 사용자의 작업을 확인하기 위해 동일한 명령을 실행할 수 있습니다.  

```sh
kubectl auth can-i get pods --namespace demo-app --as mary
```

###### 17.2.1.2. RBAC  
<br/>

###### 17.2.1.3. 웹훅  
<br/>

클러스터 관리자는 웹훅 권한 모듈을 사용하여 권한 프로세스를 위임할 외부 REST 엔드포인트를 설정할 수 있습니다. 이것은 클러스터에서 실행되며 URL을 통해 접근할 수 있습니다. REST 엔드포인트 설정은 마스터 파일 시스템의 파일에서 찾을 수 있으며, --authorization-webhook-config-file=SOME_FILENAME을 통해 API 서버에 설정됩니다. 설정한 후에는 API 서버가 SubjectAccessReview 객체를 요청 바디에 포함하여 권한 웹훅 애플리케이션에 전송합니다. 권한 웹훅 애플리케이션은 해당 객체를 처리하고 상태 필드에 완료를 담아 반환합니다.  

##### 17.2.2. 권한 모범 사례  
<br/>

클러스터에 구성된 권한 모듈을 변경하기 전에 다음 모범 사례를 참조하세요.  

+ 각 마스터 노드의 파일 시스템에 ABAC 정책을 배치하고 동기화합니다. 따라서 일반적으로 다중 마스터 클러스터에서 ABAC를 사용하지 않을 것을 권장합니다. 설정은 파일에 존재하는 관련 플래그를 따르기 때문에 이는 웹훅 모듈에 대해서도 동일합니다. 게다가 파일 내의 정책을 변경하고 적용하려면 API 서버를 다시 시작해야 합니다. 따라서 단일 마스터 클러스터 내의 컨트롤 플레인이 중단되거나 다중 마스터 클러스터에서 설정이 불일치하게 됩니다. 이러한 세부 사항을 고려할 때 규칙이 쿠버네티스 자체에 설정되고 저장되기 때문에 사용자 권한은 RBAC 모듈만 사용할 것을 권장합니다.

+ 웹훅 모듈은 강력하지만 잠재적으로 매우 위험합니다. 모든 요청이 권한 절차의 대상임을 생각하면 웹훅 서비스에 생기는 문제는 클러스터 장애로 이어집니다. 그러므로 완벽하게 파악되지 않은 외부 권한 모듈을 사용하지 않기를 권장합니다. 또한 웹훅 서비스가 응답이 없거나 가용하지 않은 상황에서 동작하는 클러스터의 실패 모두가 익숙하지 않을 때도 마찬가지입니다.
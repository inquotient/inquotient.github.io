---
title:  활동성, 성능, 테스트
categories:
- Java Concurrency In Practice
feature_text: |
  ## 활동성, 성능, 테스트
feature_image: "https://picsum.photos/2560/600?image=733"
image: "https://picsum.photos/2560/600?image=733"
---
<style>
	thead td { text-align: center; }
	td { border: 1px solid #444444; }
</style>

### 10. 활동성을 최대로 높이기
<br/>
안전성(safety)과 활동성(liveness)의 사이에는 밀고 당기는 힘이 존재하는 경우가 많다. 스레드의 안전성을 확보하기 위해서 락을 사용하곤 하는데, 라깅 우연찮게 일정한 순서로 동작하다 보면 락 순서에 따라 데드락이 발생하기도 한다. 락과 비슷한 관점에서 시스템 자원 사용량을 적절한 수준에서 제한하고자 할 때 스레드 풀이나 세마포어를 사용하기도 하는데 스레드 풀이나 세마포어가 동작하는 구조를 정확하게 이해하지 못하고 있다면 더 이상 자원을 할당받지 못하는 또 다른 형태의 데드락이 발생할 수 있다. 자바 애플리케이션은 데드락 상태에서 회복할 수 없기 때무넹 항상 프로그램의 실행 구조상 데드락이 발생할 가능성이 없는지 먼저 확인해야 한다.  

#### 10.1. 데드락
<br/>
데드락은 예전부터 '식사하는 철학자(dining philosophers)' 문제로 널리 알려져 왔다. 다섯 명의 철학자가 중국 음식점에 저녁 식사를 하러 가서 둥그런 테이블에 앉았다. 테이블에는 다섯 개의 젓가락(다섯 쌍이 아닌 다섯 개)이 개인별 접시 사이에 하나씩 놓여 있다. 철학자는 '먹는' 동작과 '생각하는' 동작을 차례대로 반복한다. 먹는 동안에는 접시 양쪽에 있는 젓가락 두 개를 모아 한 쌍을 만들어야 자신의 접시에 놓인 음식을 먹을 수 있고, 음식을 먹은 이후에는 젓가락을 다시 양쪽에 하나씩 내려 놓고 생각을 시작한다. 이런 상황에서는 젓가락 사용 순서를 관리하는 방법이 필요하다. 예를 들어 각자가 약간의 시간 규칙에 맞춰 음식을 먹도록 할 수도 있다. 즉 음식을 먹고자 하는 철학자가 양쪽의 젓가락을 집으려 할 텐데, 한쪽을 집은 상태에서 다른 한쪽이 이미 사용 중이라면 먼저 잡은 한쪽을 내려 놓고 잠시 기다린다. 그러면 모두들 조금씩 식사를 할 수 있다. 아니면 모든 철학자가 각자 자기 왼쪽에 있는 젓가락을 집은 다음 오른쪽 젓가락을 사용할 수 있을 때까지 기다렸다가 오른쪽 젓가락을 집어서 식사를 한다면, 모든 철학자가 더 이상 먹지 못하는 상황에 다다를 수 있다. 첧학자가 모두가 먹지 못하는 후자의 상황은 음식을 먹는 데 필요한 자원을 모두 다른 곳에서 확보하고 놓지 않기 대문에 모두가 서로 상대방이 자원을 놓기만을 기다리는, 이른바 데드락이 걸린다.  

스레드 하나가 특정 락을 놓지 않고 계속해서 잡고 있으면 그 락을 확보해야 하는 다른 스레드는 락이 풀리기를 영원히 기다리는 수밖에 없다. 스레드 A가 락 L을 확보한 상태에서 두 번재 락 M을 확보한 상태에서 락 L을 확보하려고 대기하고 있다면, 양쪽 스레드 A와 B는 서로가 락을 풀기를 영원히 기다린다. 이런 모습은 데드락의 가장 간단한 경우에 해당하며, 여러 개의 스레드가 사이클을 이루면서 상대방이 확보한 락을 얻으려 대기하는 상태의 데드락도 자주 발생한다. 지향 그래프(directed graph)를 예로 들어보면, 그래프의 노드는 스레드 하나를 의미하고, 그래프의 에지(edge)는 '스레드 B가 확보한 독점 자원을 스레드 A가 가져가려고 대기하는 상태'를 나타낸다. 만약 이런 지향 그래프레엇 사이클이 나타난다면 데드락이 발생하는 것과 동일하다.  

데이터베이스 시스템은 데드락을 검출한 다음 데드락 상황에서 복구하는 기능을 갖추고 있다. 데이터베이스의 트랜잭션(transaction)을 활용하다 보면 어러 개의 락이 필요할 수 있으며, 락은 해당 트랜잭션이 커밋(commit)을 될 때까지 풀리지 않는다. 따라서 그다지 흔하지는 않지만 두 개 이상의 트랜잭션이 데드락 상태에 빠지는 일이 충분히 가능하다. 외부에서의 조치가 없다면 해당 트랜잭션은 영원이 끝나지 않고 대기할 수밖에 없다(서로 다른 트랜잭션에 필요한 락을 확보하고 풀어주지 않는 상태), 하지만 데이터베이스 서버가 이렇게 데드락이 발생한 채로 시스템이 멈추도록 방치하지는 않는다. 데이터베이스 서버가 트랜잭션 간에 데드락이 발생했다는 사실을 확인하고 나면(데드락 확인 작업에는 보통 대기 상태를 나타내는 그래프에서 사이클이 발생하는지를 확인하는 방법을 사용한다), 데드락이 걸린 트랜잭션 가운데 희생양을 하나 선택해 해당 트랜잭션을 강제 종료시킨다. 이렇게 특정 트랜잭션이 강제로 종료되고 나면 남아 있는 다른 트랜잭션은 락을 확보하고 걔속 진행할 수 있다. 만약 트랜잭션을 요청했던 애플리케이션에서 중단된 트랜잭션을 재시도하도록 돼 있었다면, 재시도한 트랜잭션은 데드락이 걸릴 수 있는 상대방 트랜잭션이 모두 끝난 상태에기 때문에 문제 없이 결과를 얻을 수 있게 된다.  

자바 가상 머신(Java Virtual Machine)은 데이터베이스 서버와 같이 데드락 상태를 추적하는 기능은 갖고 있지 않다. 따라서 만약 자바 프로그램에서 데드락이 발생하면 그 순간 게임은 끝이다. 해당 스레드는 프로그램 자체를 강제로 종료하기 전에는 영원히 멈춘 상태로 유지된다. 데드락이 걸린 스레드가 뭘 하는 스레드이냐에 따라 애플리케이션 자체가 완전히 멈춰버릴 수도 있고, 아니면 멈추는 범위가 줄어 일부 모듈만 동작을 멈출 수도 있고, 아니면 전체적인 성능이 떨어지는 영향을 미칠 수도 있다. 데드락이 걸린 상태에서 애플리케이션을 정상적인 상태로 되도릴 수 있는 방법은 애플리케이션을 종료하고 다시 실행하는 것밖에 없고, 다시는 같은 데드락이 발생하지 않기를 바라는 수밖에 없다.  

병렬 프로그램에서 나타나기 쉬운 여러 가지 다른 문제점과 마찬가지로 데드락 역시 처음에는 그 모습을 거의 드러내기 않는다. 프로그램의 일부에 데드락이 발생할 가능성이 있다고 해서 데드락이 실제로 발생하리라는 보장도 없고, 단지 그럴 가능성이 있다는 것뿐이다. 데드락은 상용 서비스를 시작하고 나서 시스템에 부하가 걸리는 경우와 같이 항상 최악의 상황에서 그 모습을 드러내곤 한다.  

##### 10.1.1. 락 순서에 의한 데드락
<br/>
프로그램 내부의 모든 스레드으ㅔ서 필요한 락을 모두 같은 순서로만 사용한다면, 락 순서에 의한 데드락은 발생하지 않는다.  

##### 10.1.2. 동적인 락 순서에 의한 데드락
<br/>
물론 프로그램을 작성하다 보면 데드락을 방지할 수 있을 만큼 락을 사용하는 순서를 충분히 조절하지 못할 수도 있다.  

객체에 순서를 부여할 수 있는 방법 중에 하나는 바로 System.identityHashCode를 사용하는 방법인데, identityHashCode 메소드는 해당 객체의 Object.hashCode 메소드를 호출했을 때의 값을 알려준다.  

거의 발생하지 않는 일이지만 두 개의 객체가 같은 hashCode 값을 갖고 있는 경우에는 또 다른 방법을 사용해 락 확보 순서를 조절해야 하며, 그렇지 않은 경우에는 역시 데드락이 발생할 가능성이 있다. 이와 같은 경우에 락 순서가 일정하지 않을 수 있다는 문제점을 제거하려면 세 번째 타이 브레이킹(tie-breaking) 락을 사용하는 방법이 있다. 이를 테면 계좌에 대한 락을 확보하기 전에 먼저 타이 브레이킹 락을 확보하는데, 타이 브레이킹 락을 확보한다는 것은 두 개의 락을 임의의 순서로 확보하는 위험한 작업을 특정 순간에 하나의 스레드에서만 할 수 있도록 막는다는 의미이다. 따라서 데드락이 발생하는 경우가 생기지 않도록 예방할 수 있다(물론 타이 브레이킹 락을 사용하는 방법 역시 프로그램 전체에 동일하게 적용해야 한다). 그런데 hashCode가 동일한 값을 갖는 경우가 자주 발생한다면 타이 브레이킹 락을 확보하는 부분이 일종의 병목(bottleneck)으로 작용할 가능성도 있다. 하지만 System.identityHashCode 값이 충돌하는 경우는 거의 없다고 봐도 좋기 때문에 타이 브레이킹 방법을 쓰지 않더라도 최소한의 비용으로 최대의 결과를 얻을 수 있다.  

```java
private static final Object tieLock = new Object();

public void transferMoney(final Account fromAcct, final Account toAcct, final DollarAmount amount) throws InsufficientFundsException {
  class Helper {
    public void transfer() throws InsufficientFundsException {
      if (fromAcct.getBalance().compareTo(acmount) < 0)
        throw new InsufficientFundsException();
      else {
        fromAcct.debit(amount);
        toAcct.credit(amount);
      }
    }
  }
  int fromHash = System.identityHashCode(fromAcct);
  int toHash = System.identityHashCode(toAcct);

  if (fromHash < toHash) {
    synchronized (fromAcct) {
      synchronized (toAcct) {
        new Helper().transfer();
      }
    }
  } else if (fromAcct > toHash) {
    synchronized (toAcct) {
      synchronized (fromAcct) {
        new Helper().transfer();
      }
    }
  } else {
    synchronized (tieLock) {
      synchronized (fromAcct) {
        synchronized (toAcct) {
          new Helper().transfer();
        }
      }
    }
  }
}
```

Account 클래스 내부에 계좌 번호와 같이 유일(unique)하면서 불변(immutable)이고 비교도 가능한 값을 키로 갖고 있다면 한결 쉬운 방법으로 락 순서를 지정할 수 있다. Account 객체를 그 내부의 키를 기준으로 정렬한 다음 정렬한 순서대로 락을 확보한다면 타이 브레이킹 방법을 사용하지 않고도 전체 프로그램을 통털어 계좌를 사용할 때 락이 걸리는 순서를 일정하게 유지할 수 있다.  

읿반적으로 락은 아주 짧은 시간 동안만 사용하고 풀어놓은 경우가 대부분이기 때문에, 지금까지 데드락이 얼마나 위험한지에 대해 너무 과장시켜 소개한 것이 아닌가라고 여길 수도 있다. 하지만 실제 상용 시스템에서 데드락은 아주 심각한 문제 가운데 하나이다. 상용 애플리케이션은 하루 24시간만 생각해봐도 락을 확보하고 풀어놓는 과정을 수만 번 아니 수억 번을 처리할 수도 있다. 이 가운데 단 한 건이라도 타이밍이 올바르지 않다면 애플리케이션을 데드락 상태로 몰고 간다. 더군다나 아주 심도 있는 방법으로 부하 테스트(load-testing)을 진행했다 하더라도 발생 가능한 데드락을 모두 찾아낼 수는 없다. 락을 짧은 시간 동안만 사용할수록 락 때문에 문제가 생기는 경우가 줄어들겠지만, 반대로 테스트 과정에서 데드락이 걸리는 문제를 찾아내기가 더 힘들어진다.  

##### 10.1.3. 객체 간의 데드락
<br/>
프로그램에서 여러 개의 락을 확보할 때, 두 개의 락을 여러 메소드에서 확보하는 경우도 많기 때문에 문제점이 항상 눈에 잘 띄는 것은 아니다.  

락을 확보한 상태에서 에일리언 메소드를 호출한다면 가용성에 문제가 생길 수 있다. 에일리언 메소드 내부에서 다른 락을 확보하려고 하거나, 아니면 예상하지 못한 만큼 오랜 시간 동안 계속해서 실행된다면 호출하기 전에 확보했던 락이 필요한 다른 스레드가 계속해서 대기해야 하는 경우도 생길 수 있다.  

##### 10.1.4. 오픈 호출
<br/>
메소드 호출은 그 너머에서 어떤 일이 일어나는지 모르게 막아주는 추상화 방법이다. 하지만 호출한 메소드 내부에서 어떤 일이 일어나는지 알지 못하기 때문에 특정 락을 확보한 상태에서 에일리언 메소드를 호출한다는 건 파급 효과를 분석하기가 굉장히 어렵고, 따라서 위험도가 높은 일이다.  

락을 전혀 확보하지 않은 상태에서 메소드를 호출하는 것을 오픈 호출(open call)이라고 하며, 메소드를 호출하는 부분이 모두 오픈 호출로만 이뤄진 클래스는 락을 확보한 채로 메소드를 호출하는 클래스보다 훨씬 안정적이며 다른 곳에서 불러다 쓰기도 좋다. 데드락을 미연에 방지하고자 오픈 호출을 사용하는 것은 스레드 안전성을 확보하기 위해 캡슐화 기법(encapsulation)을 사용하는 것과 비슷하다고 볼 수 있ㅆ다. 캡슐화 기법을 사용해 작성한 프로그램과 비교할 때 스레드 안전성을 분석하는 일이 훨씬 어려울 수 있다. 이처럼 활동성이 확실한지를 분석하는 경우에도 오픈 호출 기법을 적용한 프로그램이라면 그렇지 않은 프로그램보다 분석 작업이 훨씬 간편하다. 항상 오픈 호출만 사용한다는 점을 염두에 두고 있다면 여러 개의 락을 사용하는 프로그램의 코드 실행 경로를 쉽게 확인할 수 있고 따라서 언제나 일정한 순서로 락을 확보하도록 만들기도 쉽다. 으픈 호출만을 사용해야 한다거나 락을 확보하는 순서를 주의 깊게 정해야 한다는 사실은, 여러 객체가 조합된 코드의 동기화를 맞추는 것보다 동기화가 맞춰진 객체를 조합해 사용하는 것이 훨씬 복잡하다는 것을 의미한다.  

프로그램을 작성할 때 최대한 오픈 호출 방법을 사용하도록 한다. 내부의 모든 부분에서 오픈 호출을 사용하는 프로그램은 락을 확보한 상태로 메소드를 호출하곤 하는 프로그램보다 데드락 문제를 찾아내기 위한 분석 작업을 훨씬 간편하게 해준다.  

synchronized 블록의 구조를 변경해 오픈 호출 방법을 사용하도록 하면 원래 단일 연산으로 실행되던 코드를 여러 개로 쪼개 실행하기 때문에 예상치 못했던 상황에 빠지기도 한다. 일반적으로는 연산의 단일성을 잃는다 해도 별다른 문제가 생기지는 않는다. 아니면 연산의 단일성을 해제함으로써 눈에 띌 만큼 변화가 생길 수는 있지만, 논리적인 기능에는 여전히 아무런 문제가 없을 수 있다.  

연산의 단일성을 잃는다는 것이 일부 상황에서는 큰 문제가 되기도 하며, 따라서 이번에는 연산의 단일성을 확보하는 또 다른 방법을 소개하고자 한다. 연산의 단일성을 확보하는 방법 가운데 하나는 오픈 호출된 이후에 실행될 코드가 한 번에 단 하나씩만 실행되도록 객체의 구조를 정의하는 방법이다. 이를테면 어떤 서비스를 종료한다고 할 때 현재 실행되고 있는 작업이 완료될 때까지 대기하려 할 것이고, 완료된 이후에 서비스에서 사용하던 자원을 모두 반환하는 절차를 밟을 것이다. 만약 서비스 내부에서 확보하게 될 락을 밖에서 미리 확보하고는 작업이 완료될 때까지 대기하는 구조로 돼 있다면 데드락의 위험을 벗어날 수 없다. 하지만 서비스 종료 절차를 시작하기 전에 서비스 내부에서 사용할 락을 풀어준다면 종료 절차가 진행되고 있다는 것을 알아차리지 못한 다른 스레드가 새로운 작업을 시작할 가능성도 있다. 이런 진퇴양난의 상황에 대한 해결책은 먼저 서비스의 상태를 '종료 중'이라고 설정할 동안만 락을 쥐고 있으면서 다른 스레드가 새로운 작업을 시작하거나 아니면 서비스 종료 절차를 시작하지 못하도록 미리 예방해두는 방법이다. 그러고 나면 종료 절차가 모두 끝날 때까지 대기할 것이고, 오픈 호출이 모두 끝나고 나면 서비스의 종료 절차를 모두 끝날 때까지 대기할 것이고, 오픈 호출이 모두 끝나고 나면 서비스의 종료 절차를 진행하는 스레드만이 서비스에 대한 모든 상태를 사용할 수 있도록 정리할 수 있다. 즉 코드 가운데 크리티컬 섹션(critical section)에 다른 스레드가 들어오지 못하도록 하기 위해 락을 사용하는 대신 이와 같이 스레드 간의 약속을 정해 다른 스레드가 작업을 방해하지 않도록 하는 방법이 있다는 점을 알아두자.  

##### 10.1.5. 리소스 데드락
<br/>
스레드가 서로 상대방이 이미 확보하고 앞으로 놓지 않을 락을 기다리느라 서로 데드락에 빠질 수 있는 것처럼, 필요한 자원을 사용하기 위해 대기하는 과정에도 데드락이 발생할 수 있다. 예를 들어 풀에 두 개의 데이터베이스에 대한 연결과 같은 자원을 각각의 풀(pool)로 확보해 놓았다고 가정해보자. 자원 풀은 풀이 비어 있을 때 풀 내부의 자원을 달라고 요청하는 객체가 대기하도록 만들기 위해 일반적으로 세마포어를 사용해 구현하는 경우가 많다. 그런데 특정 작업을 실행하려면 양쪽 데이터베이스에 대한 연결이 모두 필요하고, 양쪽 데이터베이스 연결을 항상 같은 순서로 받아와 사용하지는 않는다고 해보자. 다시 말해, 스레드 A는 데이터베이스 D₁에 대한 연결을 확보한 상태에서 데이터베이스 D₂에 대한 연결을 확보한 상태에서 D₁에 대한 연결을 확보하고자 할 수 있다(풀의 크기가 크면 클수록 이와 같은 문제가 발생할 확률이 줄어들기는 한다. 만약 양쪽 풀에 각각 N개의 연결을 확보하고 있다면, 위와 같이 타이밍이 딱 맞는 상황이 총 N번 발생해야 데드락이 걸린다).  

자원과 관련해 발생할 수 있는 또 다른 데드락 상황은 스레드 부족 데드락(thread-starvation deadlock)이다. 이런 문제에 대한 예는 이미 살펴본 바가 있는데, 단일 스레드로 동작하는 Executor에서 현재 실행 중인 작업이 또 다른 작업을 큐에 쌓고는 그 작업이 끝날 때까지 대기하는 데드락 상황이었다. 이런 경우에는 첫 번째 작업이 영원히 대기할 수밖에 없고, Executor에 등록돼 있던 다른 모든 작업 역시 영원히 대기하게 된다. 다른 작업의 실행 결과를 사용해야만 하는 작업이 있다면 스레드 소모성 데드락의 원인이 되기 쉽다. 크기가 제한된 풀과 다른 작업과 연동돼 동작하는 작업은 잘못 사용하면 이와 같은 문제를 일으킬 수 있다.  

#### 10.2. 데드락 방지 및 원인 추적
<br/>?
한 번에 하나 이상의 락을 사용하지 않는 프로그램은 락의 순서에 의한 데드락이 발생하지 않는다. 물론 그다지 실용적이지 않은 방법일 수 있지만, 가능하다면 한 번에 하나 이상의 락을 사용하지 않도록 프로그램을 만들어 보는 것도 좋다. 작업량을 많이 줄일 수 있기 때문이다. 여러 개의 락을 사용해야만 한다면 락을 사용하는 순서 역시 설계 단계부터 충분히 고려해야 한다. 설계 과정에서 여러 개의 락이 서로 함께 동작하는 부분을 최대한 줄이고, 락의 순서를 지정하는 규칙을 정해 문서로 남기고 그 규칙을 정확하게 다라서 프로그램을 작성해야 한다.  

세세한 수준에서 락을 관리하는 프로그램에서는 두 단계의 전략으로 데드락 발생 가능성이 없는지를 확인해보자. 첫 번째 단계는 여러 개의 락을 확보해야 하는 부분이 어디인지를 찾아내는(우선 너무 많은 곳에서 여러 개의 락을 사용하지 않도록 해보자) 단계이고, 그 다음으로는 이와 같은 부분에 대한 전반적인 분석 작업을 진행해 프로그램 어디에서건 락을 지정된 순서에 맞춰 사용하도록 해야 한다. 가능한 부분에서는 최대한 오픈 호출 방법을 사용하면 이와 같은 분석과 확인 작업이 조금 간편해질 수 있다. 오픈 호출을 사용하지 않는 경우라면 코드 리뷰 과정을 거치거나 자동화된 방법으로 바이트 코드나 소스코드를 분석하는 방법으로 여러 개의 락을 사용하는 부분을 뽑아 낼 수 있다.  

##### 10.2.1. 락의 시간 제한
<br/>
데드락 상태를 검출하고 데드락에서 복구하는 또 다른 방법으로는 synchronized 등의 구문으로 암묵적인 락을 사용하는 대신 Lock 클래스의 메소드 가운데 시간을 제한할 수 있는 tryLock 메소드를 사용하는 방법이 있다. 암묵적인 락은 락을 확보할 때까지 영원히 기다리지만, Lock 클래스 등의 명시적인 락은 일정 시간을 정해두고 그 시간 동안 락을 확보하지 못한다면 tryLock 메소드가 오류를 발생시키도록 할 수 있다. 락을 확보하는 데 걸릴 것이라고 예상되는 시간보다 훨씬 큰 값을 타임아웃으로 정해두고 tryLock을 호출하면, 뭔가 일반적이지 않은 상황이 발생했을 때 제어권을 다시 되돌려 받을 수 있다.  

만약 지정한 시간이 다 되도록 락을 확보하지 못한다 해도, 락을 왜 확보하지 못했는지는 일부러 알려하지 않아도 좋다. 데드락 상황이 발생했을 수도 있고, 특정 스레드의 버그로 인해 락을 확보한 채로 무한 반복에 빠지는 경우도 있을 수 있고, 아니면 단순하게 락을 확보하고 실행되는 작업이 처음 예상했던 것보다 아주 느리게 동작하기 때문일 수도 있다. 어쨌거나 명시적인 락을 사용하면 락을 확보하려고 했지만 실패했다는 사실을 기록해 둘 기회는 갖는 셈이고, 그동안 발생했던 내용을 로그 파일로 남길 수도 있다. 그러고 나면 프로그램 전체를 종료했다가 재시작하는 대신, 프로그램 내부에서 필요한 작업을 재시도하도록 할 수도 있다.  

여러 개의 락을 확보할 때 이와 같이 타임아웃을 지정하는 방법을 적용하면, 프로그램 전체에서 모두 타임아웃을 사용하지 않는다 해도 데드락을 방지하는 데 효과를 볼 수 있다. 락을 확보하려는 시점에서 시간 제한이 걸리면 이미 확보했던 락을 풀어주고 잠시 기다리다가 다시 작업을 시도해 볼 수 있다. 그러면 잠시 기다리는 동안 데드락이 발생할 수 있는 상황이 지나가고, 프로그램은 다시 정상적으로 동작한다(이 방법은 두 개의 락을 한꺼번에 확보해야 하는 경우에 사용할 수 있다. 만약 여러 개의 락을 여러 메소드에 걸쳐 확보하도록 돼 있다면, 미리 확보했던 락을 쉽게 풀어줄 수 없기 때문이다).  

##### 10.2.2. 스레드 덤프를 활용한 데드락 분석
<br/>
물론 데드락을 방지하는 것이 프로그램을 작성할 때 최우선 목표이겠지만, 일단 데드락이 발생했을 때는 JVM이 만들어 내는 스레드 덤프(thread dump)를 활용해 데드락이 발생한 위치를 확인하는 데 도움을 얻을 수 있다. 스레드 덤프에는 실행 중인 모든 스레드의 스택 트레이스(stack trace)가 담겨 있다. 스레드 덤프에는 락과 관련된 정보도 담겨 있는데, 각 스레드마다 어떤 락을 확보하고 있는지, 스택의 어느 부분에서 락을 확보했는지, 그리고 대기 중인 스레드가 어느 락을 확보하려고 대기 중이었는지 등에 대한 정보를 갖고 있다. 이런 정보는 데드락이 발생하지 않았다 해도 프로그램의 오류를 디버그해야 할 때 많은 도움이 된다. 가끔씩 스레드 덤프를 출력해보면 프로그램 내부에서 락이 어떻게 동작하는지를 들여다 볼 수 있다. JVM은 스레드 덤프를 생성하기 전에 락 대기 상태 그래프에서 사이클이 발생했는지, 즉 데드락이 발생한 부분이 있는지 확인한다. 만약 데드락이 있었다고 판단되면 어느 락과 어느 스레드가 데드락에 관여하고 있는지, 프로그램 내부의 어느 부분에서 락 확보 규칙을 깨고 있는지에 대한 정보도 스레드 덤프에 포함시킨다.  

JVM이 스레드 덤프를 생성하도록 하려면 Unix 플랫폼에서는 JVM 프로세스에 SIGQUIT 시그널(kill -3)을 전송하거나 Ctrl-\ 키를 누르면 되고, 윈도우 환경에서는 Ctrl-Break 키를 누른다. 스레드 덤프를 뽑아내는 기능을 내장하고 있는 통합 개발 환경(IDE)도 많다.  

암묵적인 락 대신 명시적으로 Lock 클래스를 사용하고 있을 때, 자바 5.0 버전에서는 해당 Lock에 지정된 정보는 스레드 덤프에 포함시키지 않는다. 다시 말해 자바 5.0에서는 명시적인 Lock에 대한 기록은 스레드 덤프에 포함되지 않는다는 얘기이다. 하지만 자바 6에서는 명시적인 Lock을 사용해도 스레드 덤프에 포함될 뿐 아니라 데드락을 검출할 때 명시적인 락을 포함하는 데드락도 검출해준다. 하지만 락을 어디에서 확보했는지에 대해 출력되는 정보는 암묵적인 락에 대한 내용만큼 정확하지는 않다. 암묵적인 락은 락을 확보하는 시점의 스택 프레임에 연결돼 있지만, 명싲적인 락은 락을 확보한 스레드와 연결돼 있기 때문이다.  

실제로 JVM은 데드락을 추적하는 데 도움이 되는 정보를 주기 위해 많은 내용을 준비하고 있다. 예를 들면 문제점을 일으킨 락이 어느 것인지, 어느 스레드가 관련돼 있는지, 관련된 스레드에서 확보하고 있는 다른 락에는 어떤 것이 있는지, 문제점으로 인해 다른 스레드가 간접적으로라도 불편함을 겪고 있는지에 대한 정보를 제공한다.  

#### 10.3. 그 밖의 활동성 문제점
<br/>
프로그램이 동작하는 데 활동성을 떨어뜨리는 주된 원인은 역시 데드락이지만, 병렬 프로그램을 작성하다 보면 소모(starvation), 놓친 신호, 라이브락(livelock) 등과 같이 다양한 원인을 마주치게 된다.  


##### 10.3.1. 소모
<br/>
소모(starvation)상태는 스레드가 작업을 진행하는 데 곡 필요한 자원을 영영 할당받지 못하는 경우에 발생한다. 소모 상태를 일으키는 가장 흔한 원인은 바로 CPU이다. 자바 애플리케이션에서 소모 상황이 발생하는 원인은 대부분 스레드의 우선 순위(priority)를 적절치 못하게 올리거나 내리는 부분에 있다. 또한 락을 확보한 채로 종료되지 않는 코드(문한 반복문이나 확보할 수 없는 자원을 갖고자 대기하는 등)를 실행할 때, 다른 스레드에서 해당 락을 가져갈 수 없기 때문에 소모 상황이 발생한다.  

자바의 스레드 관련 API에서 제공하는 우선 순위 개념은 단지 스레드 스케줄링과 관련된 약간의 힌트를 제공하는 것뿐이다. 자바의 스레드 API는 총 10단계의 스레드 우선 순위를 지정할 수 있도록 돼 있으며, JVM은 지정된 10단계의 우선 순위를 하위 운영체제의 스케줄링 우선 순위에 적절히 대응시키는 정도로만 사용한다. 이처럼 자바의 스레드 우선 순위를 운영체제의 우선 순위에 대응시키는 기능은 플랫폼마다 다르게 적용되기 때문에 특정 시스템에서는 두 개의 스레드 우선 순위가 같은 값으로 지정될 수도 있고, 다른 운영체제에서는 또 다른 값으로 지정될 수 있다. 일부 운영체제는 10보다 작은 개수의 스레드 우선 순위를 제공하는 경우도 있으며, 이럴 때는 어쩔 수 없이 두 개 이상의 자바 스레드 우선 순위가 같은 값의 운영체제 스레드 우선 순위로 대응되는 수밖에 없다.  

운영체제의 스레드 스케줄러는 자바 언어 명세(Java Language Specification)에서 명시하고 있는 스레드 스케줄링의 공평성(fairness)과 활동성(liveness)을 지원하기 위해 여러 가지 방법을 사용한다. 대부분의 자바 애플리케이션을 보면 애플리케이션 내부에서 동작하는 모든 스레드가 같은 우선 순위로 동작하는데, 바로 우선 순위의 기본 값인 Thread.NORM&#95;PRIORITY이다. 스레드 우선 순위라는 것이 약간 무뎌 보일 뿐더러 우선 순위를 변경했다고 해서 그 효과가 뚜렸하게 나타나지 않는 경우도 많다. 다시 말해 스레드의 우선 순위를 위로 올린다고 해도 아무런 변화가 없거나, 아니면 우선 순위가 높은 스레드만 우선적으로 실행시켜 다른 스레드가 제대로 실행되지 못하게 될 수도 있다. 만약 후자의 모습으로 진행된다면 소모 상화잉 쉽게 발생할 수 있다.  

일반적인 상황에서는 스레드 우선 순위를 변경하지 않고 그대로 사용하는 방법이 가장 현명하다고 할 수 있다. 스레드의 우선 순위를 조절하기 시작하는 순간 프로그램은 실행되는 플랫폼마다 그 실행 모습이 달라질 것이고, 소모 상화잉 발생할 위험도 떠안아야 한다. 약간 이해할 수 없는 위치에서 Thread.sleep 메소드나 Thread.yield 메소드를 호출해 우선 순위가 낮은 스레드에게 실행할 기회를 주려는 부분이 있는지를 찾아보면, 우선 순위를 원상 복귀시키거나 기타 다른 응답성 문제를 해소해야 할 프로그램인지를 쉽게 가려낼 수 있다. Thread.yield와 Thread.sleep(0) 메소드의 정확한 의미는 자바 언어 명세에도 별로 나타나 있지 않다. JVM은 yield나 sleep(0) 메소드를 구현할 때 더미 명령을 실행하거나 아니면 단순하게 스케줄링에 도움을 주는 힌트로 사용하기도 한다. 특히 Unix 시스템에서 sleep(0)이라는 기능을 갖고 있어야 할 필요도 없다. 단지 현재 실행 중인 스레드를 운영체제 작업 큐의 맨 뒤에 쌓아 두고, 동일한 우선 순위를 갖는 다른 스레드가 먼저 동작할 수 있도록 한다. 실제로 몇몇 JVM은 yield 메소드의 기능을 이와 같은 방법으로 처리하도록 돼 있다.  

스레드 우선 순위를 변경하고 싶다 해도 꾹 참아라. 우선 순위를 변경하고 나면 플랫폼에 종속적인 부분이 많아지며, 따라서 활동성 문제를 일으키기 쉽다. 대부분의 병렬 애플리케이션은 모든 스레드의 우선 순위에 기본값을 사용하고 있다.  

##### 10.3.2. 형편 없는 응답성
<br/>
소모 상황보다 약간 나은 경우는 바로 응답성이 떨어지는 상황이다. 응답성이 떨어지는 경우는 백그라운드 스레드를 사용하는 GUI 애플리케이션에서 굉장히 일상적이다. 백그라운드 작업이 CPU를 많이 활용한다면 이벤트 스레드와 서로 CPU를 차지하겠다고 다투는 통에 사용자 화면의 응답성이 떨어질 수도 있다. 만약 스레드의 우선 순위를 조절해야 하는 부분이 있다면, 바로 지금과 같이 백그라운드에서 실행되고 있는 기능이 CPU를 많이 사용해 응답성을 저해하는 부분이 해당된다. 특정 스레드에서 동작하는 기능이 백그라운드로 동작하는 게 효율적이라면, 해당 백그라운드 스레드의 우선 순위를 낮춰 화면 응답성을 훨씬 높여 줄 수 있다.  

애플리케이션의 응답성이 떨어진다면 락을 제대로 관리하지 못하는 것이 원인일 수 있다. 특정 스레드가 대량의 데이터를 넘겨 받아 처리하느라 필요 이상으로 긴 시간 동안 락을 확보하고 있다면 넘겨준 대량의 데이터를 사용해야 하는 다른 스레드는 데이터를 받아올 때가지 상당히 긴 시간동안 대기해야 한다.  

##### 10.3.3. 라이브락
<br/>
라이브락(livelock)도 일종의 활동성 문제 가운데 하나인데, 대기 중인 상태가 아니었다해도 특정 작업의 결과를 받아와야 다음 단계로 넘어갈 수 있는 작업이 실패할 수밖에 없는 기능을 계속해서 재시도하는 경우에 쉽게 찾아 볼 수 있다. 라이브락은 메시지를 제대로 전송하지 못했을 때 해당 전송 트랜잭션을 롤백(roll back)하고 실패한 메시지를 큐의 맨 뒤에 쌓아두는 트랜잭션 메시지 전송 애플리케이션에서 자주 나타난다. 만약 메시지를 처리하는 핸들러에서 특정 타입의 메시지를 제대로 처리하지 못하고 실패한 메시지를 처리하는 핸들러에서 특정 타입의 메시지를 제대로 처리하지 못하고 실패한 것으로 처리하는 버그가 있다면, 특정 메시지를 큐에서 뽑아 핸들러에 넘겼을 때 핸드러는 같은 결과를 내면서 계속해서 호출될 것이다(이런 모습으로 동작하기 때문에 독약 메시지(position message) 문제라고 부르기도 한다). 메시지를 처리하는 스레드가 대기 상태에 들어가지는 않았지만, 다음 작업으로 진행하지도 못하는 상태에 빠진 것이다. 이런 형태의 라이브락은 에러를 너무 완벽하게 처리하고자 회복 불가능한 오류를 회복 가능하다고 판단해 계속해서 재시도하는 과정에 나타난다.  

라이브락은 여러 스레드가 함께 동작하는 환경에서 각 스레드가 다른 스레드의 응답에 따라 각자의 상태를 계속해서 변경하느라 실제 작업은 전혀 진전시키지 못하는 경우에 발생하기도 한다. 이런 문제점은 너무나 친절한 두 사람이 길을 걷다 마주쳤을 때, 한 명이 옆으로 비켜나면 그와 동시에 상대방도 옆으로 비켜나고, 다시 옆으로 비켜서면 상대방도 같은 방향으로 비켜서느라 서로 앞으로 가지 못하는 상황과 비슷하다.  

이런 형태의 라이브락을 해결하려면 작업을 재시도하는 부분에 약간의 규칙적이지 않는 구조를 넣어두면 된다. 예를 들어 이더넷 네트웍으로 연결돼 있는 두 개의 컴퓨터에서 하나의 랜 선을 통해 동시에 서로 신호를 보내고자 한다면, 양쪽에서 보낸 패킷이 서로 충돌한다. 양쪽 컴퓨터에서 충돌이 일어났음을 검출하고 나면, 양쪽 모두 같은 신호를 다시 보내고자 재시도한다. 그런데 양쪽 컴퓨터가 충돌을 감지한 이후 정확하게 1초 후에 똑같이 재시도한다면 패킷이 다시 충돌할 수밖에 없고, 또 1초 후에 재시도하면 역시 충돌할 수밖에 없다. 이런 문제를 해결하기 위해 재시도할 때까지 잠시 기다리는 시간을 서로 임의로 지정하게 한다(실제 이더넷 프로토콜은 충돌이 반복적으로 일어났을 때 지수 함수를 활용해 대기 시간을 산출한다. 따라서 패킷이 넘쳐나는 일도 막을 수 있고, 충돌이 발생했던 컴퓨터 간에 또 다시 충돌이 일어날 위험을 줄이고 있다). 임의의 시간 동안 기다리다가 재시도하는 방법은 이더넷뿐 아니라 일반적인 병렬 프로그램에서도 라이브락을 방지하고자 할 때 사용할 수 있는 훌륭한 해결 방법이다.  

### 요약
<br/>
활동성과 관련된 문제는 심각한 경우가 많은데, 활동성 문제를 해결하려면 일반적으로 애플리케이션을 종료하는 것 외에는 별다른 방법이 없다는 데 심각성의 원인이 있다. 가장 흔한 형태의 활동성 문제는 바로 락 순서에 의한 데드락이다. 락 순서에 의한 데드락을 방지하려면 애플리케이션을 설계하는 단계부터 여러 개의 락을 사용하는 부분에 대해 충분히 고려해야 한다. 애플리케이션 내부의 스레드에서 두 개 이상의 락을 한꺼번에 사용해야 하는 부분이 있다면, 항상 일정한 순서를 두고 여러 개의 락을 확보해야만 한다. 이런 문제에 대한 가장 효과적인 해결 방법은 항상 오픈 호출 방법을 사용해 메소드를 호출하는 것이다. 오픈 호출을 사용하면 한 번에 여러 개의 락을 사용하는 경우를 엄청나게 줄일 수 있고, 따라서 여러 개의 락을 사용하는 부분이 어디인지 쉽게 찾아낼 수 있다.  

### 11. 성능, 확장성
<br/>
스레드를 사용하는 가장 큰 목적은 바로 성능을 높이고자 하는 것이다. 물론 스레드를 사용해 발생하는 그 모든 복잡한 문제를 참아내도록 하는 유일한 목적이라고 평가하는 사람도 있겠다. 스레드를 사둉하면 시스템의 자원을 훨씬 효율적으로 활용할 수 있고, 애플리케이션으로 하여금 시스템이 갖고 있는 능력을 최대한 사용하게 할 수 있다. 그와 동시에 기존 작업이 실행되고 있는 동안에라도 새로 등록된 작업을 즉시 실행할 수 있는 준비를 갖추고 있기 때문에 애플리케이션의 응답 속도를 향상시킬 수 있다.  

병렬 프로그램의 성능을 분석하고, 모니터링하고, 그 결과로 성능을 향상시킬 수 있는 방법에 대해 알아본다. 애플리케이션의 성능을 높이는 일이 간단하지만은 않은 것이 성능을 높이는 방법은 대부분 애플리케이션의 내부 구조를 복잡하게 만들어야 하는 경우가 많고, 따라서 안전성과 활동성에 문제가 생길 가능성도 적지 않다. 최악의 경우에는 성능을 높이기 위해 적용한 프로그래밍 기법 때문에 프로그램의 다른 부분에서 역효과를 가져오거나 성능상에 문제를 일으킬 수도 있다. 일반적인 경우에는 물론 성능이 높은 것이 좋고 성능을 높이고 아면 만족스러운 경우가 많지만, 그렇다 해도 성능 때문에 안전성을 해칠 수는 없다. 일단 프로그램이 정상적으로 동작하도록 만들어 놓고 난 다음, 프로그램이 빠르게 동작하도록 만드는 편이 낫다고 본다. 더군다나 예상했던 성능 기준이 있었다면, 그 기준에 미치지 못할 경우에만 성능 문제를 살펴보는 것으로는 충분하다. 병렬 애플리케이션을 설계하는 동안에는 성는을 최대한으로 끌어올리는 일이 큰 부분을 차지하지 않을 때가 많다.  

#### 11.1. 성능에 대해
<br/>
성능을 높인다는 것은 더 적은 자원을 사용하면서 더 많은 일을 하도록 한다는 말이다. 여기에서 '자원'이라는 단어에는 여러 가지 뜻이 있을 수 있는데, 처리해야 할 작업이 있을 때 CPU, 메모리, 네트웍 속도, 디스크 속도, 데이터베이스 처리 속도, 디스크 용량 등의 자원 가운데 어느 것이 될지 모르지만 항상 모자라는 부분이 발생할 것이다. 어떤 작업을 실행할 때 충분하지 못한 특정 자원 때문에 성능이 떨어지는 현상이 나타난다면, 작업의 성능이 해당 자원에 좌우된다고 한다. CPU에 좌우될 수도 있고, 데이터베이스 속도에 좌우될 수도 있다.  

어쨌거나 목적은 전체적인 성능을 모두 높이는 것일테지만, 여러 개의 스레드를 사용하려 한다면 항상 단일 스레드를 사용할 때보다 성능상의 비용을 지불해야만 한다. 스레드 간의 작업 내용을 조율하는 데 필요한 오버헤드(락 걸기, 신호 보내기, 메모리 동기화하기 등)도 이런 비용이라고 볼 수 있고, 컨텍스트 스위칭이 자주 발생한다는 점, 스레드를 생성하거나 제거하는 일이 빈번하다는 점, 여러 스레드를 효율적으로 스케줄링해야 한다는 등의 부분도 모두 비용이라고 할 수 있다. 이와 같은 비용을 지불한다해도 스레드를 효율적으로 잘 적용하면 성능이나 응답성이 높아지고 처리 용량도 커지는 등의 여러 장점을 얻을 수 있다. 반대로 잘못 설계된 병렬 애플리케이션은 순차적으로 작업을 처리하는 프로그램보다 오히려 느리게 동작하는 경우도 간혹 생긴다.  

더 나은 성능을 목표로 해서 프로그램이 병렬로 동작하도록 만들 때는 두 가지 부분을 우선적으로 생각해야 한다. 먼저 프로그램이 확보할 수 있는 모든 자원을 최대한 활용해야 하고, 남는 자원이 생길 때마다 그 자원 역시 최대한 활용할 수 있도록 해야한다. 프로그램의 성능을 모니터링하는 관점에서 얘기해보자면, 앞에서 언급한 부분은 바로 CPU가 최대한 바쁘게 동작해야 한다는 것과 동일하게 생각할 수 있다(물론 쓸데없는 일을 하느라 CPU가 불이 나게 실행되는 것은 의미가 없고, 반드시 꼭 필요한 일을 하느라 바쁜 상황을 말한다). 프로그램의 실행 속도가 계산 부분의 속도에 집중돼 있다면 하드웨어적으로 CPU를 더 꽂아 전체적인 성능을 높일 수도 있다. 반대로 프로그램을 실행하는데 CPU에 여유가 있다면, CPU를 더 꽂는다 해도 프로그램의 성능에는 별 도움이 되지 않을 게 분명하다. 프로그램에서 스레드를 활용하면 작업을 잘게 나눠 시스템에 꽂힌 CPU가 충분히 열심히 동작해야 할 만큼의 작업을 싱행시켜 노는 CPU가 없을 만큼 작업 실행 성능을 높일 수 있다.  

##### 11.1.1. 성능 대 확장성
<br/>
애플리케이션의 성능은 여러 가지 측면에서 자료를 수집해 측정할 수 있는데, 이를테면 서비스 시간, 대기 시간, 처리량, 효율성, 확장성, 용량 등의 수치를 뽑아 낼 수 있다. 이런 수치 가운데 일부(서비스 시간, 대기 시간 등)는 특정 작업을 처리하는 속도가 "얼마나 빠르냐"를 말해주고, 또 다른 수치(용량, 처리량)는 동일한 자원을 갖고 있을 때 "얼마나 많은" 양의 일을 할 수 있는지 알려준다.  

확장성(scalability)은 CPU, 메모리, 디스크, I/O 처리 장치 등의 추가적인 장비를 사용해 처리량이나 용량을 얼마나 쉽게 키울 수 있는지를 말한다.  

병렬 프로그램 환경에서 확장성을 충분히 가질 수 있도록 애플리케이션을 설계하고 튜닝하는 방법은 기존에 해오던 일반적인 성능 최적화 방법과 다른 부분이 많다. 성능을 높이기 위해 튜닝 작업을 하는 경우에 그 목적은 어쨌건 동일한 일을 더 적은 노력으로 하고자 하는 것이다. 예를 들어 이미 계산했던 결과를 캐싱해서 실행 속도를 높이거나, O(n²)의 시간이 걸리는 알고리즘을 O(n log n) 시간에 처리할 수 있는 알고리즘으로 바꾸는 드의 작업이 바로 성능 튜닝을 의미한다. 확장성을 목표로 튜닝을 한다면 처리해야 할 작업을 병렬화해 시스템의 가용 자원을 더 많이 끌어다 사용하면서 더 많은 일을 처리할 수 있도록 하는 방법을 많이 사용하게 된다.  

이처럼 성능이라는 단어에 포함된 '얼마나 빠르게'와 '얼마나 많이'라는 두 가지의 의미는 완전히 다른 뜻을 가지며, 심지어 어떤 경우에는 서로 화합할 수 없는 상황도 발생한다. 더 높은 확장성을 확보하거나 하드웨어의 자원을 더 많이 활용하도록 하다 보면, 앞서 큰 작업 하나를 작은 여러 개의 작업으로 분할해 처리하는 것처럼 개별 작업을 처리하는 데 필요한 작업의 양을 늘리는 결과를 얻을 때가 많다. 우습게도 단일 스레드 애플리케이션에서 사용하던 성능 개선 방안은 대부분 확장서의 측면에서 효과적이지 않다.  

흔히 많이 사용하는 3-티어 모델(3개의 티어는 각각 프리젠테이션 티어, 비즈니스 로직 티어, 스토리지 티어로 나뉘고, 각 티어는 서로 다른 시스테미 관리한다)을 보면 시스템의 확장성을 높이도록 변경하려 할 때 성능의 측면에서 얼마나 많은 손해를 보는 경우가 많은지 쉽게 알 수 있다. 프리젠테이션, 비즈니스 로직, 스토리지의 세 가지 부분이 하나로 통합돼 있는 단일 애플리케이션을 다중 티어 애플리케이션과 비교해 보면, 다중 티어 애플리케이션이 웬만큼 잘 만들어졌다 하더라도 별다른 튜닝을 하지 않은 단일 애플리케이션의 성능이 훨씬 나을 가능성이 많다. 그도 그럴 것이, 단일 구조의 애플리케이션은 서로 다른 티어 간에 작업을 주고받는 도중에 발생하는 네트웍 시간 지연 현상도 없을 것이고, 연산 작업을 서로 다른 추상적인 계층을 통과시켜가며 처리하는 데 드는 부하(메시지를 큐에 쌓아두고 처리해야 하거나, 작업 순서를 조율하는 데 드는 추가 비용, 작업과 함께 데이터가 물리적으로 복사되는 데 필요한 비용 등)가 적기 때문에 당연한 결과라고 볼 수 있다.  

하지만 단일 구조의 애플리케이션이 처리할 수 있었던 최대 부하를 넘어서는 작업량을 감당해야 하는 순간이 오면, 문제는 심각해진다. 처리 용량을 단시간에 급격하게 증가시키는 일이 절대적으로 어렵기 때문이다. 그래서 할 수 없이 더 많은 부하에 견딜 수 있도록 시스템 자원을 계속 투입하면서도 서비스 시간이 훨씬 길어지거나 단위 작업당 필요한 하드웨어 자원의 양이 크게 늘어나는 일을 감수해야 한다.  

그러다 보니 서버 애플리케이션을 만들 때는 성능의 여러 가지 측면 가운데 '얼마나 빠르게'라는 측면보다 '얼마나 많이'라는 측면, 즉 확장성과 처리량과 용량이라는 세 개의 측면을 훨씬 중요하게 생각하는 경우가 많다(사용자와의 직접적인 상호 작용이 일어나는 애플리케이션이라면 대기 시간이라는 값이 훨씬 중요해진다. 대기 시간을 줄여야 사용자가 진행 상태를 보면서 실제로 무슨 일이 일어나는지 궁금해하며 무작정 기다리는 시간이 줄어 들기 때문이다).  

##### 11.1.2. 성능 트레이드 오프 측정
<br/>
공학적인 모든 선택의 순간에는 항상 트레이드 오프(trade-off)가 존재하기 마련이다. 강 위에 다리를 건설할 때 좀더 두꺼운 강판을 사용하면 다리가 수용할 수 있는 용량이 늘러나고 안전성도 높아지겠지만 건설 비용 역시 크게 증가할 수밖에 없다. 소프트웨어 공학이라는 관점에서 보면 다리 건설 문제와 같이 자금과 실생활에서의 위험 요소 사이에서 트레이드 오프가 발생할 일은 별로 없지만, 트레이드 오프에서 어떤 부분을 선택해얗 할지를 결정하는 데 필요한 정보가 그다지 충분하지 않은 경우가 많다. 예를 들어 '퀵소트' 알고리즘은 대량의 자료를 정렬할 때는 효율이 높지만, 자료의 양이 많지 않을 때에는 훨씬 기초적인 '버블 소트' 알고리즘이 더 효율적이기도 하다. 프로그램을 작성하는 도중에 효율적인 정렬 기능을 구현해야 할 필요가 있다면, 먼저 정렬할 대상 데이터의 규모가 어느 정도인지 먼저 알아낼 필요가 있고, 평균적인 처리 시간을 중점적으로 최적화할지, 최악의 경웨 중점을 둬야 할지, 아니면 예측성에 중점을 둬야 하는지에 대한 결정을 내릴 수 있도록 추가적인 자료를 뽑아내는 것도 좋다. 하지만 일반적으로는 정렬 기능을 라이브러리로 구현하는 입장에서 알 수 있는 정보는 굉장히 제한적이다. 그러다 보니, 요구 사항을 충분히 받지 못해 정확하게 구현하지 못한 상태에서 효과가 없다고 판단하는 경우가 많기 때문에 어쩔 수 없이 최적화 기법을 잘 적용하지 못하는 원인이 된다.  

최적화 깁버을 너무 이른 시점에 적용하지 말아야 한다. 일단 제대로 동작하게 만들고 난 다음에 빠르게 동작하도록 최적화해야 하며, 예상한 것보다 심각헤게 성능이 떨어지는 경우에만 최적화 기법을 적욯하는 것으로도 충분하다.  

공학적인 결정을 내려야 하는 시점에는 어떤 효과를 얻고자 할 때 다른 비용을 지출해야만 할 수 있고(예를 들어 서비스 시간을 단축시키기 위해 메모리를 늘려야 하는 경우), 또 어떤 경우에는 안전성을 확보하기 위해 비용을 지불해야 할 수도 있다. 여기서 말하는 안전성이 앞서 소개했던 다리 건설의 예에서와 같이 인간 생활에서의 설제적인 위험을 뜻하지는 않는다. 성능을 최적화하는 다수의 경우에 코드의 가독성과 유지보수의 용이함을 비용으로 지불한다. 즉, 좀더 '최적화'되거나 동작하는 모습이 덜 분명한 코드일수록 이해하기가 어렵고 유지보수하기도 어렵다. 일부 최적화 기법을 사용하다 보면, 캡슐화된 구조를 깨야만 하는 것처럼 훌륭한 객체 지향적인 설계 원칙에서 벗어나야만 하는 경우도 있다. 속도가 빠른 알고리즘은 복잡한 경우가 많은데, 그러다 보니 오류가 발생할 위험도 한 층 높아진다(여기에서 설명하는 비용이나 위험을 정확하게 이해하지 못하고 있다면, 다음 내용으로 계속 진행할 만큼 충분히 생각을 해보지 않았을 수 있다).  

성능을 높이기 위한 대부분의 결정 사항에는 다양한 변수가 관여하곤 하고 처한 상황에 따라 결정 사항이 크게 달라진다. 특정 방법이 다른 방법보다 "빠르다"고 말하기 전에 다음과 같은 질문에 대답해 볼 필요가 있다.  

+ '빠르다'단어가 무엇을 의미하는가?
+ 어떤 조건을 갖춰야 이 방법이 실제로 '뻐르게' 동작할 것인가? 부하가 적을 때? 아니면 부하가 걸릴 때? 데이터가 많을 때? 이런 질문에 대한 대답에 명확한 수치를 보여줄 수 있는가?
+ 위의 조건에 해당하는 경우가 얼마나 많이 발생하는가? 이 질문에 대한 대답에 명확한 수치를 보여줄 수 있는가?
+ 조건이 달라지는 다른 상황에서도 같은 코드를 사용할 수 있는가?
+ 이 방법으로 성능을 개선하고자 할 때, 숨겨진 비용, 즉 개발 비용이나 유지 보수 비용이 증가하는 부분이 어느 정도인지? 과연 그런 비용을 감수하면서까지 성능 개선 작업을 해야 하는가?  

이와 같은 판단 기준은 성능과 관련된 설계와 개발에 대한 결정 사항이라면 어디에든 적용해 볼 수 있지만, 병렬 프로그래밍의 입장에서도 한 번 생각해보자. 최적화를 하는 데 과연 위와 같이 보수적인 방법을 취해야 할까? 병렬 프로그래밍에서 발생하는 오류의 가장 큰 원인이 바로 성능을 높이려는 여러 가지 기법에서 비롯된다고 봐도 무리가 아니다. 일단 단순한 동기화 방법이 "너무 느리다"고 전제하고는, 직접적인 동기화 구문을 덜 사용할 수 있게 해주고 아주 훌륭한 모습을 갖추긴 했지만 위험성을 많이 내포하고 있는 여러 가지 방법이 공개돼 있으며, 이런 방법이 동기화 구문과 관련된 여러 가지 규칙을 사용하지 않아도 된다는 핑곗거리로 자주 소개되곤 한다. 병렬 프로그램에서 발생하는 버그는 추적하고 발견해 수정하기가 어렵다고 정평이 나 있지만, 버그의 원인이 될 가능성이 조금이라도 있는 위험도 높은 코드는 매우 주의 깊게 살펴봐야 한다.  

성능을 높이기 위해 안전성을 떨어뜨리는 것은 최악의 상황이며, 결국 안전성과 성능 둘 다를 놓치는 결과를 얻을 뿐이다. 트깋 병렬 프로그램의 관점에서, 성능 문제가 어디에 존재하며 어떤 방법을 사용해야 성능을 높잉 수 있다거나 확정성을 높ㅇ일 수 있다고 다수의 개발자가 알고 있는 직관적인 지식 가운데 상당수가 올바르지 않다고 봐야 한다. 따라서 성능을 튜닝하는 모든 과정에서 항상 성능 목표에 대한 명확한 요구 사항이 있어야 하며, 그래야 어느 부분을 튜닝하고 어느 시점에서 튜닝을 그만 둬야 하는지 판단할 수 있다. 또한 매우 실제적인 환경에서 실제와 같은 사용자 부하의 특성을 동일하게 나타낼 수 있는 성능 측정 도구가 있어야 한다. 그리고 성능 튜닝 작업을 한 다음에는 반드시 원하는 목표치를 달성했는지 다시 한 번 측정 값을 뽑아내야 한다. 여러 가지 최적화 기법을 적용할 때는 안전성이 떨어지고 유지보수의 어려움이 불가피하게 나타나기 마련이다. 따라서 특별히 성능을 높이기를 원하지 않는다면, 이와 같은 안전성이나 유지보수 문제에 대한 비용을 일부러 지불해 가면서 성능을 높일 필요가 없다. 더군다나 비용을 지불해도 원하는 결과를 충분히 얻을 수 없다면, 더더욱 비용을 지불할 필요가 없다.  

추측하지 말고, 실제로 측정해보라.  

시장에 나온 성능 측정용 제품을 보면 소프트웨어의 성능을 굉장히 세밀하게 측정해 주고, 성능을 떨어뜨리는 병목이 어디에 있는지 눈으로 직접 볼 수 있게 해준다. 하지만 프로그램이 어떻게 동작하는지를 알기 위해 비싼 돈을 들여야만 하는 것은 아니다. 예를 들어  perfbar라는 무료 애플리케이션을 사용하면 CPU가 얼마나 바쁘게 동작하는지를 쉽게 보여준다. 따라서 CPU가 열심히 움직이도록 만드는 게 성능 튜닝의 큰 목표라고 한다면, perfbar 역시 성능을 더 높여야 하는지 도는 튜닝의 결과가 얼마나 효과적인지에 대한 결과를 알려줄 수 있는 좋은 방법이다.  

#### 11.2. 암달의 법칙
<br/>
일부 작업은 자원을 많이 투입하면 더 빨리 처리할 수 있다. 예를 들어, 곡식을 추구할 때 작업 인력이 더 많으면 추수 작업을 더 빨리 끝낼 수 있다. 어떤 작업은 기본적으로 순차적으로 처리해야만 한다. 예를 들어, 곡식이 자리는 과정은 작업 인력을 더 투입한다고 해서 빠르게 할 수 있는 일이 아니다. 프로그램을 작성할 때 스레드를 사용하려는 주된 이유가 멀티프로세서의 성능을 최대한 활용하려는 것이라면, 프로그램에서 처리하는 내용이 병렬화를 할 수 있는 일인지 확실히 해둬야 하고, 작업을 병렬화했을 때 그 가능성을 최대한 활용할 수 있어야 한다.  

대부분의 병렬 프로그램에는 병렬화할 수 있는 작업과 순차적으로 처리해야 하는 작업이 뒤섞인 단위 작업의 덩어리를 갖고 있다. 암달의 법칙(Amdahl's law)을 사용하면 병렬 작업과 순차 작업의 비율에 따라 하드웨어 자원을 추가로 투입했을 때 이론적으로 속도가 얼마나 빨라질지에 대한 예측 값을 얻을 수 있다. 암달의 법칙에 따르면, 순차적으로 실행돼야 하는 작업의 비율을 F라고 하고 하드웨어에 꽂혀 있는 프로세서의 개수를 N이라고 할 때, 다음의 수식에 해당하는 정도까지 속도를 증가시킬 수 있다.  

속도 증가량 ≤ 1/(F+(1-F)/N)  

N이 무한대까지 증가할수록 속도 증가량은 최고 1/F까지 증가한다. 1/F라는 속도 증가량은 순차적으로 실행돼야 하는 부분이 전체 작업의 50%를 차지한다고 할 때 프로세서를 아무리 많이 꽂는다 해도 겨우 두 배 빨라진다는 결과이다. 그리고 순차적으로 실행해야 하는 부분이 전체의 10%에 해당한다면 최고 10배까지 속도를 증가시킬 수 있다고 예측할 수 있다. 암달의 법칙을 활용하면 작업을 순차적으로 처리하는 부분이 많아질 때 느려지는 정도가 얼마만큼인지를 수치화할 수 있다. 하드웨어에 CPU가 10개 꽂혀 있을 때, 10%의 순차 작업을 갖고 있는 프로그램은 최고 5.3배 만큼의 속도를 증가(CPU 활용도는 5.3배/10개=0.53, 즉 53%)시킬 수 있다. 같은 상황에서 CPU를 100개를 꽂는다면 최대 9.2배까지 속도가 증가(CPU 활용도는 9.2배/100개=0.092, 즉 9.2%)할 것이라고 예상할 수 있다. 그러다 보니, 속도를 최대 10배까지 증가시키려면 CPU 활용도가 너무나 비효율적으로 떨어질 수밖에 없다.  

암달의 법칙에 따르면 프로세서의 개수가 증가하면 할 수록, 순차적으로 실행해야 하는 부분이 아주 조금이라도 늘어나면 프로세서 개수에 비해 얻을 수 있는 속도 증가량이 크게 떨어진다.  

하지만 멀티프로세서 시스템에서 애플리케이션을 실행할 때 속도가 얼마만큼 빨라질 것인지에 대한 예측을 해보려면, 애플리케이션 내부에서 순차적으로 처리해야 하는 작업이 얼마나 되는지를 먼저 확인해야 한다.  

다음 코드에서 doWork 메소드를 N개의 스레드가 동시에 실행한다고 해보자. doWork 메소드는 공유돼 있는 작업 큐에 쌓인 작업을 가져와서 처리하게 돼 있다. 그리고 각 작업은 다른 스레드나 다른 작업과 아무런 관련 없이 독립적으로 동작한다고 가정하자. 큐에 작업을 어떻게 샇아 둘 것인가에 대한 고민도 일단은 접어둔다면, 프로세서를 더 장착할 때마다 애플리케이션이 얼마나 높아질까? 척 들여다 보면 애플리케이션의 모든 작업을 완벽하게 병렬화했다고 생각할 수 있고, 따라서 프로세서를 더 꽂을수록 더많은 작업을 병렬로 실행할 수 있다. 하지만 내부를 잘 살펴보면 순차적으로 처리해야만 하는 부분이 있다. 바로 작업 큐에서 작업을 하나씩 뽑아 내는 부분이다. 작업 큐는 모든 작업 스레드에서 사용할 수 있게 공유돼 있으므로, 여러 스레드가 동시다발적으로 큐를 사용하려 할 때 안전성을 잃지 않도록 적당한 양의 동기화 작업이 선행돼야 한다. 예를 들어 큐의 상태를 안정적으로 유지하고자 락을 사용했다면, 특정 스레드가 큐에서 작업을 하나 뽑아 내는 그 시점에, 역시 큐에서 작업을 가져가고자 하는 다른 모든 스레드는 큐를 독점적으로 사용할 수 있을 때까지 대기해야만 한다. 따라서 작업 큐와 관련된 부분에서는 프로그램이 순차적으로 처리될 수밖에 없다.  

단일 작업 하나가 실행되는 시간에는 Runnable을 실행하는 데 드는 시간뿐만 아니라 공유돼 있는 작업 큐에서 작업을 뽑아내는 데 필요한 시간도 포함돼 있다. 작업 큐로 LinkedBlockingQueue를 사용하고 있다면, 큐에서 작업을 뽑아낼 때 대기하는 시간이 동기화된 LinkedList를 사용할 때보다 훨씬 적게 든다. LinkedBlockingQueue는 동기화된 LinkedList보다 훨씬 확장성이 좋은 알고리즘을 사용하고 있기 때문이다. 하지만 어찌됐건 간에 데이터를 한 군데에 공유해두고 사용하는 모든 부분은 항상 순차적으로 처리해야만 한다.  

더군다나 이 예제에는 흔히 사용하는 또다른 순차적인 처리 부분이 빠져 있는데, 바로 단위 작업의 처리 결과를 취합하는 부분이다. 단위 작업을 실행하고 나면 항상 뭔가 그 결과가 있게 마련이고, 만약 의미 있는 결과를 내놓지 않는 작업이라면 아예 프로그램에서 제거하는 게 맞을 수도 있다. 자바의 Runnable 인터페이스는 기본적으로 작업을 실행한 결과를 처리하는 방법에 대해서는 아무것도 언급하지 않고 있다. 따라서 Runnable 작업은 항상 실행 결과를 로그 파일에 적어두거나, 특정 데이터 구조에 실행 결과를 쌓아두도록 돼 있다. 그렇다면 로그 파일이나 결과를 저장하는 기타 데이터 구조 모두 여러 작업이 무작위 순서로 만들어내는 결과를 받아들일 수 있어야 하기 때문에 역시 순차적으로 처리해야만 하는 부분이라고 볼 수 있다. 만약 실행 결과를 공유하지 않고 각 작업이 스스로 보관하고 있는다고 해도, 모든 작업이 끝난 이후에 작업마다 쌓여 있는 결과를 취합하는 과정이 역시 순차적으로 처리해야만 하는 부분이 되겠다.  

```java
public class WorkerThread extends Thread {
  private final BlockingQueue<Runnable> queue;

  public WorkerThread(BlockingQueue<Runnable> queue) {
    this.queue = queue;
  }

  public void run() {
    while (true) {
      try {
        Runnable task = queue.take();
        task.run();
      } catch (InterruptedException e) {
        break; /* 스레드를 종료시킨다 */
      }
    }
  }
}
```

모든 병렬 프로그램에는 항상 순차적으로 실행돼야만 하는 부분이 존재한다. 만약 그런 부분이 없다고 생각한다면, 프로그램 코드를 다시 한 번 들여다보라.  

##### 11.2.1. 예제: 프레임웍 내부에 감춰져 있는 순차적 실행 구조
<br/>
애플리케이션의 내부 구조에 순차적으로 처리해야 하는 구조가 어떻게 숨겨져 있는지를 알아보려면, 스레드 개수를 증가시킬 때마다 성능이 얼마나 빨라지는지를 기록해두고, 성능상의 차이점을 기반으로 순차적으로 처리하는 부분이 얼마만큼인지 추측해볼 수 있다. 공유돼 있는 Queue가 있을 때 여러 개의 스레드가 값을 하나씩 뽑아낸 다음 뭔가 작업을 실행하는 일을 계속해서 반복하는 간단한 애플리케이션의 실행 속도 측정 결과를 볼 수 있다. 작업을 처리하는 단계에는 단순하게 스레드 내부에서만 동작하는 연산 과정이 진행된다. 큐가 비었다는 사실을 스레드가 알게 되면, 해당 스레드는 큐에 일정 개수의 작업을 추가해서 작업을 가져가려는 다른 스레드가 계속해서 실행할 수 있도록 했다. 물론 여러 스레드가 작업을 가져가지 위해 공유된 큐에 동시에 접근하는 부분에는 순차적인 처리 부분이 들어가지만, 작업 간에 공유하는 데이터는 없기 때문에 실제로 작업을 실행하는 부분은 완벽하게 병렬화가 가능하다.  

다음 그림에 나타나 있는 여러 곡선을 보면 스레드 안전성이 보장된 두 가지의 Queue 구현 클래스의 성능을 한눈에 볼 수 있다. 하나는 synchronizedList 메소드로 동기화된 LinkedList 클래스이고, 다른 하나는 ConcurrentLinkedQueue 클래스이다. 실행 테스트는 8개의 CPU가 장착되고 솔라리스 운영체제가 설치돼 있는 Sparc V88o 서버에서 이뤄졌다. 각 실행 단위가 동일한 양의 '작업'을 처리해야 한다고 할 때, 단순히 적절한 큐 구현 클래스를 사용하는 것만으로도 확장성을 크게 높일 수 있다는 사실을 알 수 있다.  

<img src="/assets/images/Java_Concurrency_In_Practice/11-2.jpg" width="100%" height="100%"/>
<br/>

ConcurrentLinkedQueue 클래스의 처리량은 계속해서 증가하다가 프로세서의 개수에 해당하는 수치에 다다르면 더 이상 증가하지 않고 일정하게 유지되는 경향을 보인다. 반대로 동기화된 LinkedList 클래스의 성능은 스레드가 3개 정도까지는 증가하다가 그 이후에는 동기화 관련 부하가 늘어나서 성능이 떨어진다. 스레드 개수가 4개나 5개만 돼도 스레드 간에 큐에 들어 있는 락을 차지하려는 경쟁이 치열해지면서 컨텍스트 스위칭을 하느라 성능에 큰 영향을 주게 된다.  

여기에서 처리량의 차이점이 발생하는 원인을 살펴보면 바로 두 개의 큐 구현 클래스가 작업을 순차적으로 처리하는 정도의 차이점에 원인이 있음을 알 수 있다. 동기화된 LinkedList 클래스는 전체 큐의 상태를 하나의 락으로 동기화하고 있으며, 따라서 offset나 remove 메소드를 호출하는 동안 전체 큐가 모두 락에 걸린다. 반면 ConcurrentLinkedQueue 클래스는 정교한 큐 알고리즘, 즉 개별 링크 포인터마다 단일 연산으로 업데이트하는 방법을 사용해 대기 상태에 들어가는 경우를 최소화한다. 따라서 동기화된 LinkedList에서는 추가 작업과 삭제 작업이 모두 순차적으로 처리돼야 하지만, ConcurrentLinkedQueue에서는 개별 포인터에 대한 업데이트 연산만 순차적으로 처리하면 된다.  

##### 11.2.2. 정성적인 암달의 법칙 적용 방법
<br/>
암달의 법칙을 사용하면 프로그램 내부에서 순차적으로 처리돼야만 하는 부분의 비율을 알고 있을 때, 하드웨어를 추가함에 따라 얼마만큼 처리 속도가 증가할 것인지를 수치화해서 예측할 수 있다. 물론 프로그램 내부에서 순차적으로 처리돼야 하는 부분의 비율을 정확하게 알아내는 일이 쉽지는 않지만, 그 비율을 알지 못한다 해도 경우에 따라 암달의 벙칙을 유용하게 사용할 수 있다.  

사람의 생각은 항상 환경에서 큰 영향을 받게 마련인데, 대부분의 경우 멀티프로세서 시스템이라 하면 두 개나 네 개의 프로세서가 달린 경우를 떠올리고, 자금에 여유가 있다면 잘해야 열 몇 개의 프로세서를 장착하는 정도밖에 생각하지 못한다. 하지만 멀티코어 CPU가 대중적으로 많이 보급되면서, 이제는 수백 개에서 수천 개의 프로세서를 장착한 시스템을 어렵지 않게 생각하게 됐다. 이런 환경에서 4개의 프로세서가 장착된 하드웨어에서 확장성이 충분하다고 생각됐던 알고리즘이 훨씬 규모가 큰 시스템을 대상으로 본다면 지금까지 알지 못했던 확장성에서의 병목을 맞닥뜨리게 될 가능성도 높다.  

수백 개 도는 수천 개의 프로세서가 동작하는 상황까지 가정한 상태에서 프로그램의 알고리즘을 평가한다면, 어느 시점쯤에서 확장성의 한계가 나타날 것인지를 예측해 볼 수 있다. 락의 적용 범위를 줄이는 이와 같은 방법을 암달의 법칙이라는 측면에서 바라보면, 락을 두 개로 분할하는 정도로는 다수의 프로세서를 충분히 활용하기 어렵다는 결론을 얻을 수 있다. 하지만 락 스트라이핑 방법을 사용할 때는 프로세서의 수가 늘어남에 따라 분할 개수를 같이 증가시킬 수 있기 때문에 확장성을 얻을 수 있는 훨씬 믿을만한 방법이라고 할 수 있다(물론 성능 최적화 문제는 원하는 성능 요구 사항의 수준에 따라 적용하는 것만으로도 충분하다. 예를 들어 락을 두 개로 분리하는 락 분할 방법만 사용해도 원하는 성능을 충분히 발휘할 수 있다).  

#### 11.3. 스레드와 비용
<br/>
단일 스레드 프로그램은 스케줄링 문제가 발생하지도 않거니와 동기화 문제나 그에 다른 부하도 발생하지 않는다. 게다가 내부 자료의 일관성을 유지하기 위해 락으로 동기화할 필요도 없다. 실행 스케줄링과 스레드 간의 조율을 하다 보면 성능에 부정적인 비용이 발생한다. 따라서 스레드를 사용하는 경우, 병렬로 실행함으로써 얻을 수 있는 이득이 병렬로 실행하느라 드는 비용을 넘어서야 성능을 향상시킬 수 있다.  

##### 11.3.1. 컨텍스트 스위칭
<br/>
다른 스레드 없이 메인 스레드 하나만 스케줄링한다고 하면, 메인 스레드는 항상 실행될 것이다. 반대로 CPU 개수보다 실행 중인 스레드의 개수가 많다고 하면, 운영체제가 특정 스레드의 실행 스케줄을 선점하고 다른 스레드가 실행될 수 있도록 스케줄을 잡는다. 이처럼 하나의 스레드가 실행되다가 다른 스레드가 실행되는 순간 컨텍스트 스위칭(context switching)이 일어난다. 컨텍스트 스위칭이 일어나는 상세한 구조를 보면, 먼저 현재 실행 중인 실행 상태를 보관해두고, 다음 번에 실행되거로 스케줄된 다른 스레드의 실행 상태를 다시 읽어들인다.  

컨텍스트 스위칭은 단숨에 공짜로 일어나는 일이 아니다. 스레드 스케줄리을 하려면 운영체제와 JVM 내부의 공용 자료 구조를 다뤄야 한다는 문제가 있다. 운영체제와 JVM 역시 프로그램 스레드가 사용하는 것과 같은 CPU를 함께 사용하고 있다. 따라서 운영체제나 JVM이 CPU를 많이 사용하면 할수록 실제 프로그램 스레드가 사용할 수 있는 CPU의 양은 줄어든다. 물론 컨텍스트 스위칭에 운영체제와 JVM이 사용하는 CPU 부분만 관련된 건 아니다. 컨텍스트가 변경되면서 다른 스레드를 실행하려면 해당 스레드가 사용하던 데이터가 프로세서의 캐시 메모리에 들어 있지 않을 확률도 높다. 그러면 캐시에서 찾지 못한 내용을 다른 저장소에서 찾아와야 하기 때문에 원래 예정된 것보다 느리게 실행되는 셈이다. 이런 경우에 대비하고자 대부분의 스레드 스케줄러는 실행 대기 중인 스레드가 밀려 있다고 해도, 현재 실행 중인 스레드에게 최소한의 실행 시간을 보장해주는 정책을 취하고 있다. 그러면 컨텍스트 스위칭에 들어가는 시간과 비용을 나누는 효과를 볼 수 있고, 그 결과 인터럽트 받지 않고 실행할 수 있는 최소한의 시간을 보장받기 때문에 전체적인 성능이 향상되는 효과를 볼 수 있다(물론 응답 속도에는 어느 정도 손해를 감수해야 한다).  

스레드가 실행하다가 락을 확보하기 위해 대기하기 시작하면, 일반적으로 JVM은 해당 스레드를 일시적으로 정지시키고 다른 스레드가 실행되도록 한다. 특정 스레드가 빈번하게 대기 상태에 들어간다고 하면 스레드별로 할당된 최소 실행 시간조차 사용하지 못한 경우도 있다. 대기 상태에 들어가는 연산을 많이 사용하는 프로그램(블로킹 I/O를 사용하거나, 락 대기 시간이 길거나, 상태 변수의 값을 기다리는 등)은 CPU를 주로 활용하는 프로그램보다 컨텍스트 스위칭 횟수가 훨씬 많아지고, 따라서 스케줄링 부하가 늘어나면서 전체적인 처리량이 줄어든다(넌블로킹 알고리즘을 사용하면 컨텍스트 스위칭에 소모되는 부하를 줄일 수 있다).  

컨텍스트 스위칭에 필요한 부하와 비용은 플랫폼마다 다르지만, 대략 살펴본 바에 따르면 최근 사용되는 프로세서상에서 5,000~10,000 클럭 사이클 또는 수 마이크로 초 동안의 시간을 소모한다고 알려져 있다.  

유닉스 시스템의 vmstat 명령이나 윈도우 시스템의 perfmon 유틸리티를 사용하면 컨텍스트 스위칭이 일어난 횟수를 확인할 수 있으며, 커널 수준에서 얼마만큼의 시간을 소모했는지 알아볼 수 있다. 커널 활용도가 10%가 넘는 높은 값을 갖고 있다면 스케줄링에 부하가 걸린다는 의미이며, 아마도 애플리케이션 내부에서 I/O 작업이나 락 관련 동기화 부분 때문에 대기 상태에 들어가는 부분이 원인일 가능성이 높다.  

##### 11.3.2. 메모리 동기화
<br/>
동기화에 필요한 비용은 여러 곳에서 발생하기 마련이다. synchronized와 volatile 키워드를 사용해 얻을 수 있는 가시성을 통해 메모리 배리어(memory barrier)라는 특별한 명렬어를 사용할 수 있다. 메모리 배리어는 캐시를 플러시하거나 무효화하고, 하드웨어와 관련된 쓰기 버퍼를 플러시하고, 싫랭 파이프라인을 늦출 수도 있다. 메모리 배리어를 사용하면 컴파일러가 제공하는 여러 가지 최적화 기법을 제대로 사용할 수 없게 돼 간접적인 성능 문제를 가져올 수 있다. 메모리 배리어를 사용하면 명령어 재배치를 대부분 할 수 없게 되기 때문이다.  

동기화가 성능에 미치는 영향을 파악하려면 동기화 작업이 경쟁적인지, 비경쟁적인지 확인해야 한다. synchronized 키워드가 동작하는 방법은 비경쟁적인 경우 (volatile은 항상 비경쟁적이다)에 최적화돼 있기 때문에 '빠른 경로 fast-path'의 비경쟁적인 동기화 방법은 대부분의 시스템에서 20-250클럭 사이클을 사용한다고 알려져 있다. 물론 클럭 사이클릉 ㄹ전혀 사용하지 않는 것은 아니지만, 전반적인 애플리케이션 성능의 측면에서 봤을 때 비경쟁적이면서 꼭 필요한 동기화 방법은 성능에 그다지 큰 영향이 없다고 할 수 있겠다. 더군다나 비경쟁적인 동기화 방법을 피하느라 다른 방법을 사용하다 보면, 애플리케이션의 안전성을 크게 해치는 결과를 얻을 수 있으며, 나중에 찾기 어려운 심각한 동기화 관련 버그를 찾느라 크게 고생할 가능성도 있다.  

최근에 사용하는 JVM은 대부분 다른 스레드와 경쟁할 가능성이 없다고 판단되는 부분에 락이 걸려 있다면 최적화 과정에서 해당 락을 사용하지 않도록 방지하는 기능을 제공하기도 한다. 예를 들어 락을 거는 객체가 특정 스레드 내부에 한정돼 있다면, 해당 락을 다른 스레드에서 사용하며 경쟁 조건에 들어갈 수 없기 때문에 JVM은 자동으로 해당 락은 무시하고 넘어간다.  

훨씬 정교하게 만들어진 JVM의 경우에는 유출 분석(escape analysis)을 통해 로컬 변수가 외부로 공개된 적이 있는지 없는지, 다시 말해 해당 변수가 스레드 내부에서만 사용되는지를 판단하기도 한다. 예를 들어 다음 예제에서 getStoogeNames 메소드를 보면 List 형의 값을 가리키는 변수는 메소드 내부에 선언된 stooges뿐이다. 그리고 물론 메소드 내부에서 선언된 변수는 항상 스레드 내부에 종속돼 있다. 좀 허술한 JVM에서 다음 예제를 실행시키면 Vector 객체에 add하는 부분과 toString을 호출하는 부분을 더해 총 4번 락을 잡았다 놓았다 반복하게 된다. 반면 정교한 고급 컴파일러와 JVM은 stooges 변수가 메소드 외부에 유출된 적이 없다는 것을 판단하고 락을 4번이나 잡았다 놓는 과정 없이 빠르게 실행시킨다. 이런 류의 최적화 방법은 흔히 락 생략(lock elision)이라고 부르며, IBM의 JVM에서 사용하는 방법으로, 자바 7 버전의 핫스팟 VM에서도 지원할 예정이다.  

유출 분석을 사용하지 않는 경우라면, 락 확장(lock coarsening), 즉 연달아 붙어 있는 여러 개의 synchronized 블록을 하나의 락으로 묶는 방법을 사용하기도 한다. 락 확장 기능을 갖고 있는 JVM에서 다음 예제의 getStoogeNames 메소드를 실행한다면 add 메소드를 3번 호출하는 부분과 toString을 호출하는 부분을 하나로 묶어 락을 한 번만 확보하고 해제한다. 물론 락 확장 기능을 사용하는 JVM은 락을 확보하고 해제하는 데 걸리는 시간과 synchronized 블록 내부의 작업에 걸리는 시간을 살펴보고, 락을 확장하는 거싱 효율적이라고 판단되는 경우에만 확장하기도 한다. 굉장히 잘 만들어진 컴파일러라면 getStoogeNames 메소드가 항상 같은 값을 리턴한다는 사실을 파악할 수 있다. 그러면 처음 한 번 실행된 이후에는 항상 처음 실행했던 결과를 리턴하도록 메소드이 내용을 새로 컴파일 해두기도 한다. 락 확장 방법을 사용하면 동기화 관련 부하를 줄이는 데 도움을 줄 뿐만 아니라 최적화 모듈이 좀더 큰 단위의 블록을 대상으로 추가적인 최적화 작업을 진행할 기회가 생기기도 한다.  

```java
public String getStoogeNames() {
  List<String> stooges = new Vector<String>();
  stooges.add("Moe");
  stooges.add("Larry");
  stooges.add("Curly");
  return stooges.toString();
}
```

경쟁 조건에 들어가지 않는 동기화 블록에 대해서는 그다지 걱정하지 않아도 좋다. 동기화 블록의 기본적인 구조가 상당히 빠르게 동작할 뿐만 아니라 JVM 수준에서 동기화와 관련한 추가적인 최적화 작업을 진행하기 때문에 동기화 관련 부하를 줄이거나 아예 없애주기도 한다. 대신 경쟁 조건이 발생하는 동기화 블록을 어떻게 최적화할지에 대해서 고민하자.  

특정 스레드에서 진행되는 동기화 작업으로 인해 다른 스레드의 성능이 영향을 받을 수 있다. 동기화 작업은 공유돼 있는 메모리로 통하는 버스에 많은 트래픽을 유발하기 때문이다. 공유 메모리로 통하는 버스는 제한적인 대역폭을 갖고 있으며, 여러 개의 프로세서가 공유한다. 특정 스레드가 동기화 작업을 진행하느라 공유 메모리로 통하는 버스의 대역폭을 꽉 잡고 있다면, 동기화 작업을 진행해야 할 다른 스레드는 성능이 떨어질 수밖에 없다. 이런 관점에서 보자면 넌블로킹 알고리즘을 사용한다고 해서 아무런 단점 없이 원할하게 사용할 수 있다는 의견에 반대되는 면이다. 다시 말해 경쟁이 많이 벌어지는 상황에서는 락을 중점적으로 사용하는 알고리즘보다 넌블로킹 알고리즘이 동기화관련 메모리 버스의 트래픽을 많이 점유할 수 있다는 것이다.  

##### 11.3.3. 블로킹
<br/>
경쟁하지 않는 상태에서의 동기화 작업은 전적으로 JVM 내부에서 처리할 수 있다. 하지만 경쟁 조건이 발생할 때에는 동기화 작업에 운영체제가 관여해야 할 수 있는데, 운영체제가 관여하는 부분은 모두 일정량의 자원을 소모한다. 락을 놓고 경쟁하고 있다면, 락을 확보하지 못한 스레드는 항상 대기 상태에 들어가야 한다. JVM은 스레드를 대기 상태에 둘 때 두 가지 방법을 사용할 수 있는데, 첫 번째 방법은 스핀 대기(spin waiting), 즉 락을 확보할 때까지 계속해서 재시도하는 방법이고, 두 번째 방법은 운영체제가 제공하는 기능을 사용해 스레드를 실제 대기 상태로 두는 방법이다. 두 개의 방법 가운데 어느 쪽이 효율적이냐 하는 문제의 답은 컨텍스트 스위칭에 필요한 자원의 양과, 락을 확보할 때까지 걸리는 시간에 크게 좌우된다. 대기 시간을 놓고 보면, 대기 시간이 짧은 경우에는 스핀 대기 방법이 효과적이고, 대기 시간이 긴 경우네는 운영체제의 기능을 호출하는 편이 효율적이라고 한다. 일부 JVM은 이전에 실행되던 패턴을 분석한 결과를 놓고 두 가지 방법 가운데 좀더 효과적인 방법을 선택해 사용하기도 하지만, 대부분의 경우에는 운영체제의 기능을 호출하는 방법을 사용한다.  

락을 확보하지 못했거나 I/O 관련 작업을 사용 중이라거나 기타 여러 가지 조건에 걸려 스레드가 대기 상태에 들어갈 때는 두 번의 컨텍스트 스위칭 작업이 일어나며, 이 과정에는 운영체제와 각종 캐시 등의 모듈이 연결돼 있다. 첫 번째 컨텍스트 스위칭은 실행하도록 할당된 시간 이전에 대기 상태에 들어가느라 발생하는 것이고, 두 번째는 락이나 기타 필요한 조건이 충족됐을 때 다시 실행 상태로 돌아오는 컨텍스트 스위칭이다(락을 확보하고자 경쟁하다가 대기 상태에 들어갈 때, 락을 확보하고 있는 스레드에게도 부하가 생긴다. 필요한 작업을 마치고 락을 해제할 때 운영체제에게 대기 상태에 들어간 스레드를 동작시키라고 요청해야 하기 때문이다).  

#### 11.4. 락 경쟁 줄이기
<br/>
작업을 순차적으로 처리하면 확장성(scalability)을 놓치고, 작업을 병렬로 처리하면 컨텍스트 스위칭에서 성능(performance)에 악영향을 준다. 그런데 락을 놓고 경재하는 상황이 벌어지면 순차적으로 처리함과 동시에 컨텍스트 스위칭도 많이 일어나므로 확장성과 성능을 동시에 떨어뜨리는 원인이 된다. 따라서 락 경쟁을 줄이면 줄일수록 확장성과 성능을 함께 높일 수 있다.  

락으로 사용 제한이 결려 있는 독점적인 자원을 사용하려는 모든 스레드는 해당 자원을 한 번에 하나의 스레드만 사용할 수 있기 때문에 순차적으로 처리될 수밖에 없다. 물론 락을 사용해야 하는 분명한 이유가 있기는 하다. 즉 공유된 데이터가 망가지지 않게 보호해준다는 분명한 목적이 있지만, 그에 따르는 대가 역시 분명하다. 락을 확보하고자 지속적으로 경쟁하는 상황에서는 확장성에 문제가 생긴다.  

병렬 애플리케이션에서 확장서에 가장 큰 위협이 되는 존재는 바로 특정 자원을 독점적으로 사용하도록 제한하는 락이다.  

락을 두고 발생하는 경쟁 상황에는 크게 두 가지의 원인을 생각해 볼 수 있겠다. 즉 락을 얼마나 빈번하게 확보하려고 하는지, 그리고 한 번 확보하고 나면 해제할 때까지 얼마나 오래 사용하는지가 중요한 요인이다. 큐잉 이론을 보면 안정적인 시스템의 사용자 수는 사용자가 접속하는 평균 비율과 사용자가 접속을 끊을 때까지의 평균 사용 시간을 곱한 값과 같다는 리틀의 법칙(Little's Law, Little. 1961)이 았다. 그리고 리틀의 법칙에 따르면 이런 현상은 당연하다. 이 두 가지 요인을 곱한 값이 충분히 작은 값이라면, 락을 두고 경재하는 상황 때문에 확장성에 심각한 문제가 생기지는 않을 것이다. 반대로 락을 필요로 하는 굉장히 많은 수의 스레드가 경쟁을 하고 있다면 락을 확보하지 못한 다수의 스레드가 계속 대기 상태에 머물러야 하며, 특히 심각한 경우에는 작업할 내용이 쌓여 있음에도 불구하고 CPU는 실제로 놀고 있을 가능성도 있다.  

락 경쟁 조건을 줄일 수 있는 몇 가지 방법이 있다.  

+ 락을 확보한 채로 유지되는 시간을 최대한 줄여라.
+ 락을 확보하고자 요청하는 횟수를 최대한 줄여라.
+ 독점적인 락 대신 병렬성을 크게 높여주는 여러 가지 조율 방법을 사용하라.  

##### 11.4.1. 락 구역 좁히기
<br/>
락 경쟁이 발생할 가능성을 줄이는 효과적인 방법 가운데 하나는 바로 락을 유지하는 시간을 줄이는 방법이다. 락이 곡 피룡하지 않은 코드를 synchronized 블록 밖으로 뽑아내어 락이 영향을 미치는 구역을 좁히면 락을 유지하는 시간을 줄일 수 있다. 특히 I/O 작업과 같이 대기 시간이 발생할 수 있는 코드는 최대한 synchronized 블록 밖으로 끄집어내자.  

서로 확보하고자 난리가 난 락을 특정 스레드가 오래 잡고 있는 경우에 시스템의 확장성이 얼마나 떨어지는지는 쉽게 확인할 수 있다. 만약 락을 확보하는 시간이 2밀리초이고 모든 스레드가 해당 락을 사용해야 한다면, CPU가 아무리 많이 꽂혀 있다하다라도 1초에 최대 500건 미만의 작업만 처리할 수 있다. 그런데 확보하고 유지하는 시간을 2밀리초에서 1밀리초로 단축시키면 락 때문에 막혀 있던 처리량을 최대 1초에 1000건까지도 넘볼 수 있다. 물론 여기에서 계산한 처리량은 락 경쟁이 심해지면서 더욱 커지는 컨텍스트 스위칭 부하에 대해서는 고려하지 않은 값이라는 점을 알아두자.  

synchronized 블록을 줄이면 줄일수록 애플리케이션의 확장성을 늘일 수 있다고는 하지만, 그렇다고 해서 단일 연산으로 실행돼야 할 명령어까지 synchronized 블록 밖으로 빼내거나 해서는 안 된다. 또한 synchronized 블록에서 동기화를 맞추는 데도 자원이 필요하기 때문에 하나의 synchronized 블록을 두개 이상으로 쪼개는 일(물론 쪼개도 올바로 동작한다는 가정하에)도 어느 한도를 넘어서면 성능의 측면에서 오히려 악영향을 미칠 수 있다. JVM에서 락 확장(lock coarsening) 기법을 사둉한다면, 개발자가 일부러 쪼갠 synchronized 블록이 결국은 하나로 합쳐진 것처럼 동작할 가능성도 있다. 물론 가장 최적의 설정은 항상 플랫폼마다 다를 수 있다는 점을 알아두자. 대신 일반적으로 볼 때 대기 상태에 들어 갈 수 있는 연산뿐만 아니라 '아주 작은' 연산까지 최대한 synchronized 블록 밖으로 빼내는 정도로 충분하다.  

##### 11.4.2. 락 정밀도 높이기
<br/>
락을 점유하고 있는 시간을 최대한 줄이고, 따라서 락을 확보하기 위해 경쟁하는 시간을 줄일 수 있는 또 다른 방법으로는 바로 스레드에서 해당 락을 덜 사용하도록 변경하는 방법이 있다. 이런 방법에는 락 분할(splitting)과 락 스트라이핑(striping) 방법이 있는데, 두 가지 모두 하나의 락을 여러 개의 상태 변수를 한번에 묶어두지 않고, 서로 다른 락을 사용해 여러 개의 독립적인 상태 변수를 각자 묶어두는 방법이다. 두 가지 기법을 활용하면 락으로 묶이는 프로그램의 범위를 조밀하게 나누는 효과가 있으며, 따라서 결국 애플리케이션의 확장성이 높아지는 결과를 기대할 수 있다. 하지만 반대로 락의 개수가 많아질수록 데드락이 발생할 위험도 높아진다는 점을 주의해야 한다.  

간단하게 머릿속에서 실험을 해보자. 예를 들어 애플리케이션에서 각 객체마다 서로 다른 락을 사용하는 대신 전체를 대상으로 단 하나의 락만을 사용한다고 가정하면 어떤 일이 일어날지 생각해보자. 그렇다면 synchronized 블록으로 둘러싸인 모든 코드가 전부 순차적으로 실행될 것이라고 쉽게 예상할 수 있다. 애플리케이션 내부의 여러 스레드가 하나뿐인 락을 확보하기 위해 서로 엄청나게 경쟁을 할 것이고, 두 개 이상의 스레드가 동시에 락을 확보하려는 경우가 많아질 것이다. 반대로 다수의 락을 사용해 각 객체별로 필요한 만큼만 락을 확보하도록 하면 스레드 간의 경쟁을 크게 줄일 수 있다. 따라서 경쟁이 줄어들면 락을 확보하고자 대기하는 경우 역시 줄어들기 때문에 애플리케이션의 확장성이 늘어날 것이라고 예상할 수 있다.  

락이 두 개 이상의 독립적인 상태 변수를 한번에 묶어서 동기화하고 있다면 해당하는 코드 블록을 상태 변수에 맞춰 두 개 이상의 락으로 동기화하도록 분할해 확장성을 높일 수 있다. 락을 확보하고자 대기하는 경우를 줄일 수 있기 때문이다.  

락을 하나에서 둘로 분할하는 방법은 경쟁 조건이 아주 심하지는 않지만 그래도 어느 정도 경쟁이 발생하고 있는 경우에 가장 큰 효과를 볼 수 있다. 반대로 경쟁 상황이 거의 발생하지 않는 경우에는 락을 분할한다고 해서 큰 효과를 보지는 못하지만, 부하가 걸리면서 경쟁이 발생하기 시작했을 때 성능이 떨어지는 시점을 늦출 수도 있다. 어느 정도의 경쟁이 발생하는 상황에서 락을 두 개 이상으로 분할하면 대부분의 동기화 블록에서 락 경쟁이 일어나지 않도록 할 수 있으며, 따라서 처리량과 확정성의 측면에서 큰 이득을 얻을 수 있다.  

##### 11.4.3. 락 스트라이핑
<br/>
만약 경쟁 조건이 굉장히 심한 락을 두 개로 분할하고 나면, 결국 경쟁이 심한 락이 두 개가 생기는 모양새가 될 수 있다. 물론 두 개의 스레드가 동시에 실행될 수 있으니 확장성이 약간 늘어난다고 볼 수 있지만 여러 개의 CPU를 사용하는 시스템에서 병렬성(concurrency)을 크게 높여주지는 못한다.  

락 분할 방법은 때에 따라 독립적인 객체를 여러 가지 크기의 단위로 묶어내고, 묶인 블록을 단위로 락을 나누는 방법을 사용할 수도 있는데, 이런 방법을 락 스트라이핑(lock striping)이라고 한다. 예를 들어 ConcurrentHashMap 클래스가 구현된 소스코드를 보면 16개의 락을 배열로 마련해두고 16개의 락 각자가 전체 해시 범위의 1/16에 대한 락을 담당한다. 따라서 N번째 해시 값은 락 배열에서 N mod 16의 값에 해당하는 락으로 동기화된다. ConcurrentHashMap에서 사용하는 해시 함수가 적당한 수준 이상으로 맵에 들어 있는 항목을 분산시켜 준다는 가정하에 락 경쟁이 발생할 확률을 1/16으로 낮추는 효과가 있다. 결국 ConcurrentHashMap은 최대 16개의 스레드에서 경쟁 없이 동시에 맵에 들어 있는 데이터를 사용할 수 있도록 구현돼 있는 셈이다(CPU의 개수가 많은 하드웨어를 사용하는 경우 병렬성을 높이기 위해 락을 개수를 더 놀려볼 수도 있다, 하지만 적절한 수치 이상의 많은 경쟁 조건이 발생하다고 확인된 경우에만 기본값인 16보다 더 큰 값을 사용하도록 한다).  

락 스트라이핑을 사용하다 보면 여러 개의 락을 사용하도록 쪼개놓은 컬렉션 전체를 한꺼번에 독점적으로 사용해야 할 필요가 있을 수 있는데, 이런 경우네는 단일 락을 사용할 때보다 동기화시키기가 어렵고 자원도 많이 소모한다는 단점이 있다. 대부분의 작업을 처리할 때는 쪼개진 락 하나만 확보하는 것으로도 충분하지만, ConcurrentHashMap 클래스에서 해시 공간의 크기를 늘리고 해시 함수를 새롭게 적용하는 작업과 같이 간혹 전체 컬렉션을 독점적으로 사용해야 하는 경우가 생긴다. 이런 경우네는 보통 쪼개진 락을 전부 확보한 이후에 처리하도록 구현한다. 암묵적인 락을 한꺼번에 확보하려면 재귀 호출 방법을 사용할 수밖에 없다.  

##### 11.4.4. 핫 필드 최소화
<br/>
락 분할 방법과 락 스트라이핑은 여러 개의 스레드가 각자 방해받지 않으면서 독립적인 데이터(또는 같은 데이터 구조 내부에서 서로 다른 부분)를 사용할 수 있도록 해주기 때문에 애플리케이션의 확장성을 높여준다. 애플리케이션의 내부를 살펴봤을 때 락으로 동기화시킨 데이터에 대한 경쟁보다 락 자체에 대한 경쟁이 더 심한 상태인 경우에 락 분할 방법으로 확장성에 이득을 얻을 수 있다. 하나의 락으로 두 개의 독립적인 변수 X와 Y를 동기화하고 있고, 스레드 A는 변수 X를 사용하려고 하고 스레드 B는 변수 Y를 사용하려고 한다고 해보자. 그러면 스레드 A와 스레드 B는 서로 독립적인 데이터를 사용하기 때문에 데이터를 두고 경쟁하지는 않지만, 하나의 락으로 동기화돼 있기 때문에 락을 확보하기 위해 경쟁하게 된다.  

모든 연산에 꼭 필요한 변수가 있다면 락을 정밀도(granularity)를 세밀하게 쪼개는 방법을 적용할 수 없다. 이 부분은 성능과 확장성이 서로 공존하기 어렵게 만드는 또다른 요인이라고 볼 수 있겠다. 예를 들어 자주 계산하고 사용하는 값을 캐시에 저장해두도록 최적화한다면 확장성을 떨어뜨릴 수밖에 없는 '핫 필드(hot fields)'가 나타난다.  

HashMap 클래스를 구현한다고 하면 맵 내부에 들어 있는 항목의 개수를 세는 size 메소드를 어떻게 구현할 것인지 선택해야 한다. 가장 간단한 방법은 size 메소드를 호출할 때마다 항목의 수를 매번 새로 계산하는 방법이다. 약간 더 최적화된 방법 가운데 흔히 사용하는 방법은 항목의 개수 카운터를 두고 항목이 추가되거나 제거될 때마다 카운트를 증가시키거나 감소시키는 방법이다. 이 방법을 사용하면 항목을 추가하거나 삭제할 때 카운트를 정확한 값으로 맞추느라 약간의 시간이 더 필요하겠지만 size 메소드의 실행 시간을 O(n)에서 O(1)으로 크게 줄일 수 있다.  

항목의 개수를 따로 관리하는 방법으로 최적화해 size 메소드나 isEmpty 메소드 등의 처리 속도를 높이면 단일 스레드 애플리케이션이나 완전히 동기화된 애플리케이션에서는 문제없이 잘 동작한다. 하지만 맵 내부의 항목을 변경하는 모든 기능을 호출할 때 공유된 변수인 개수 카운터의 값을 변경해야 하기 때문에 멀티스레드 애플리케이션에서는 확장성을 높이는 일이 굉장히 어려워진다. 해시 맵을 관리하는 부분에 락 분할 방법을 사용한다 해도 카운터 변수에 접근하는 부분을 동기화해야 하므로 전체 맵을 놓고 독점적으로 락을 걸어야만 하는 상황이 생긴다. 결국 성능의 측면에서 최적화라고 생각했던 기법, 즉 맵 내부의 항목을 캐시해주는 방법이 확장성의 발목을 잡는 셈이다. 이와 같이 모든 연산을 수행할 때마다 한 번씩 사용해야 하는 카운터 변수와 같은 부분을 '핫 필드'라고 부른다.  

JDK에 포함된 ConcurrentHashMap 클래스는 전체 카운트를 하나의 변수에 두지 않고, 락으로 분배된 각 부분마다 카운터 변수를 따로 두고 관리하면서 size 메소드를 호출하면 각 카운터 변수의 합을 알려주는 방법을 사용한다. 즉 ConcurrentHashMap은 모든 항목의 개수를 하나씩 세는 대신 각 락이 담당하는 부분마다 카운터를 두고 있으며, 해당 부분은 락으로 이미 동기화돼 있기 때문에 추가적인 락을 사용할 필요가 없다. 다른 내용 변경 변경 기능보다 size 메소드가 훨씬 빈번하게 호출된다면 한 번 size를 호출했을 때 그 결과 값을 volatile 변수에 캐시해둘 수 있겠다. 컬렉션의 내용이 변경될 때마다 캐시된 값을 -1로 변경하면 그 이후에 size 메소드를 호출할 때 캐시된 개수가 -1임을 확인하고 개수를 다시 계산한다.  

##### 11.4.5. 독점적인 락을 최소화하는 다른 방법
<br/>
락 경쟁 때문에 발생하는 문제점을 줄일 수 있는 또 다른 방법은 바로 좀더 높은 병렬성으로 공유된 변수를 관리하는 방법을 도입해 독점적인 락을 사용하는 부분을 줄이는 것이다. 예를 들어 병렬 컬렉션 클래스를 사용하거나 읽기-쓰기(read-write) 락을 사용하거나 분변(immutable) 객체를 사용하고 단일 연산 변수를 사용하는 등의 방법이 여기에 해당된다.  

ReadWriteLock 클래스를 사용하면 여러 개의 reader가 있고 하나의 writer가 있는 상황으로 문제를 압축할 수 있다. 다시 말해 여러 개의 스레드에서 공유된 변수의 내용을 읽어가려고 하고 대신 값을 변경하지는 못한다. 그리고 값을 변경할 수 있는 단 하나의 스레드는 값을 쓸 때 락을 독점적으로 확보한다. ReadWriteLock은 읽기 연산이 대부분을 차지하는 데이터 구조에 적용하기가 알맞으며, 전체적으로 독점적인 락을 사용하는 경우보다 병렬성 측면에서 확장성을 크게 높여준다. 만약 읽기 전용의 데이터 구조라면 불변 클래스의 형태를 유지하는 것만으로도 동기화 코드를 완전히 제거해 버릴 수 있다.  

단일 연산 변수(atomic variable)를 사용하면 통계 값을 위한 카운터 변수나 일련번호 생성 모듈, 링크로 구성된 데이터 구조에서 첫 번째 항목을 가리키는 링크와 같은 '핫 필드'가 존재할 때 핫 필드의 값을 손쉽게 변경할 수 있게 해준다. 단일 연산 클래스는 순자나 객체에 대한 참조 등을 대상으로 굉장히 정밀도가 높은(따라서 확장성에 무리를 주지 않는)단일 연산 기능을 제공하며, 그 내부적으로는 CPU 프로세서에게 제공하는 저수준의 병렬처리 기능(이를테면 비교 후 치환 기법 등)을 활용하고 있다. 작성 중인 클래스 내부에 다른 변수와의 분변조건에 관여하지 않는 핫 필드가 몇 개 정도 있다면 해당하는 핫 필드를 단일 연산 변수로 변경하는 것만으로도 확장성에 이득을 볼 수 있다(클래스 내부에서 사용하는 핫 필드의 개수를 중이면 애플리케이션의 확장성을 더 높일 수 있다. 단일 연산 변수를 사용한다고 해도 핫 필드와 관련해 소모되는 자원을 줄여줄 뿐 자원 소모를 아예 없애지는 못하기 때문이다).  

##### 11.4.6. CPU 활용도 모니터링
<br/>
애플리케이션의 확장성을 테스트할 때 그 목적은 대부분 CPU를 최대한 활용하는 데 있다. 유닉스 환경의 vmstat이나 mpstat과 같은 유틸리티, 또는 윈도우 환경의 perfmon과 같은 유틸리티를 사용하면 CPU가 얼마나 열심히 일하는지를 확인할 수 있다.  

만약 두 개 이상의 CPU가 장착된 시스템에서 일부 CPU만 열심히 일하고 나머지는 놀고 있다면, 가장 먼저 해야 할 일은 프로그램의 병렬성을 높이는 방법을 찾아 적용하는 일이다. 특정 CPU만 열심히 일하는 경우는 상당 부분의 연산 작업이 특정 스레드에서만 일어난다는 것을 뜻하며, 따라서 CPU가 여러 개 장착된 하드웨어를 애플리케시녕에서 충분히 활용하지 못한다고 볼 수 있다.  

CPU를 충분히 활용하지 못하고 있다면, 일반적인 몇 가지 원인을 생각해 볼 수 있다.  

+ 부하가 부족하다  
가장 기본적으로 볼 때 테스트하는 프로그램이 CPU 사용량을 측정할 만큼 충분한 부하 상황을 만들지 않은 것일 수 있다. 그러면 부하를 점점 늘려가면서 CPU 사용률이나 응답 시간이나 서비스 시간 등의 항목이 어떻게 증가하는지를 측정해 볼 수 있겠다. 애플리케이션이 허덕거릴 만큼의 부하를 만들어 내는 일이 쉽지 않을 수도 있다. 즉 테스트를 당하고 있는 서버가 문제가 아니라 클라이언트 시스템 쪽이 문제일 수 있다.  

+ I/O 제약  
iostat이나 perfmon 등의 유틸리티를 사용하면 애플리케이션의 성능 가운데 디스크 관련 부분이 얼마나 되는지를 살펴볼 수 있다. 그리고 네트웍의 트래픽 수준을 모니터링하면 대역폭을 얼마나 사용하고 있는지도 쉽게 파악할 수 있다.  

+ 외부 제약 사항  
애플리케이션에서 외부 데이터베이스 또는 웹 서비스등을 사용하고 있다면 성능으이 발목을 잡는 병목이 외부에 있을 가능성도 높다. 이와 같은 외부적인 부분은 프로파일러(profiler)를 활용하거나 데이터베이스 모니터링 도구를 사용하면 외부 작업을 처리하는 데 얼마만큼의 시간이 소모되는지 확인이 가능하다.  

+ 락 경쟁  
각종 프로파일링 도구를 활용하면 애플리케이션 내부에서 락 경쟁 조건이 얼마나 발생하는지 알아볼 수 있으며, 어느 락이 가장 빈번하게 경쟁의 목표가 되는지도 알 수 있다. 프로파일러를 사용하지 않는다 해도 랜덤 샘플링(random sampling), 즉 특정 시점에 스레드 상태를 덤프(dump)해서 락을 확보하기 위해 대기 중이라면 스레드 덤프를 뽑아 봤을 때 해당 스레드 부분에 "waiting to lock monitor ..."와 같은 메시지가 표시된다. 경쟁의 대상이 되는 경우가 적은 락일수록 스레드 덤프로 봤을 때 눈에 잘 띄지 않는다. 반대로 경쟁의 대상이 되는 락은 최소한 하나 이상의 스레드가 해당 락을 확보하기 위해 대기하고 있을 것이기 때문에 스레드 덤프에서 쉽게 확인할 수 있다.  

애플리케이션이 CPU를 적절한 수준 이상으로 충분히 사용하고 있다고 생각되면, 위에 소개했던 여러 가지 모니터링 방법을 사용해서 CPU를 추가했을 때 얼마나 이득을 볼 수 있을 것인지 예측해 볼 수 있다. 스레드를 총 4개만 활용하는 애플리케이션이 있다고 하면 4개의 CPU를 사용하는 시스템에서 하드웨어를 충분히 활용할 수 있겠지만, 만약 8개의 CPU를 갖고 있는 시스템이라면 동시에 더 많은 스레드를 돌릴 수 있지만 애플리케이션은 4개의 스레드만 사용하기 대문에 하드웨어를 제대로 활요하지 못한다(물론 애플케이션의 설정, 예를 들어 스레드 풀의 크기를 더 크게 지정해 작업을 분산시키는 방법도 좋다). vmstat으로 보는 화면의 한쪽 컬럼에는 실행 상태에 놓여 있지만 CPU가 모자라 실행하지 못하는 스레드이 수가 표시된다. 만약 CPU 사용량이 지속적으로 높게 유지되면서 남는 CPU가 나타나기를 기다리는 스레드가 많아진다면 CPU를 더 장착하는 것으로 성능을 높일 수 있다.  

##### 11.4.7. 객체 풀링은 하지 말자
<br/>
초기 버전의 JVM에서는 객체를 새로 메모리에 할당하는 작업과 가비지 컬렉션 작업이 상당히 느린 편이었지만, 그 이후에는 성능이 크게 개선됏다. 다 그런 것이겠지만 스레드 동기화, 그래픽 관련 모듈, JVM 시동 시간, 리플렉션 등의 기능도 모두 실험적인 기능으로 초기에 발표되던 시점에는 다들 그랬을 것이다. 실제로 최근에 자바 프로그램에서 메모리를 할당하는 작업이 C 언의 malloc 함수보다 빨라졌다. 게다가 핫스팟 1.4.x 버전과 5.0 버전에서 new Object 명령을 수행하는 데 필요한 CPU 인스트럭션은 겨우 10개 정도에 불과하다.  

예전에 객체 관련 할당과 제거 작업이 느렸을 때는 객체를 더 이상ㅇ 사용하지 않는다 해도 가비지 컬렉터에 넘기는 대신 재사용할 수 있게 보관해두고, 곡 필요한 경우에만 새로운 객체를 생성하는 객체 풀(object pool)을 많이 활용했었다. 이런 방법을 사용해 가비지 컬렉션에 소모되는 시간을 줄일 수 있다고는 하지만, 그렇다 해도 단일 스레드 애플리케이션에서 아주 무겁고 큰 객체를 제괴하고는 일반적으로 성능에 좋지 않은 영향을 미치는 것으로 알려져 있다. 객체 풀은 단순하게 CPU의 시간을 소모하는 것 뿐만 아니라 여러 가지 다른 문제점을 안고 있는데, 예를 들어 객체 풀의 크기를 제대로 정하는 일도 간단한 일이 아니다. 풀의 크기를 너무 작게 잡으면 객체 풀을 사용하는 의미를 잃게 되고, 반대로 풀의 크기가 너무 크면 당장 쓰지 않을 객체를 확보하고 있는 바람에 가비지 컬렉터가 필요한 메모리를 확보하는데 어려움을 겪을 수 있다. 또한 객체를 재사용할 때 객체를 새로 생성한 것과 같은 초기 상태로 제대로 돌려놓지 못해 이상한 버그가 발생할 위험도 있다. 또한 특정 스레드가 객체를 풀에 반환한 이후에도 계속해서 반환한 스레드를 사용하느라 오류가 발생할 위험도 적지 않다. 더군다나 세대 단위로 동작하는 가비지 컬렉터의 경우 올드(old) 세대에서 젋은(young) 세대로 이어지는 참조가 많이 발생해 가비지 컬렉션 작업에 부하가 걸릴 수도 있다.  게다가 크기가 작거나 중간 크기인 객체를 풀로 관리하는 일은 오히려 상당한 자원을 소모하는 것으로 알려져 있다.  

병렬 애플리케이션에서는 객체 풀링을 사용했을 때 훨씬 많은 비용을 지불해야 할 수도 있다. 스레드 내부에서 필요로 하는 객체를 새로 생성할 때에는 힙 데이터 구조를 사용할 때 동기화해야 하는 부분을 건너뛸 수 있도록 스레드 내부의 할당 블록을 사용하기 때문에 스레드 간에 조율해야 할 일이 겅의 없다고 볼 수 있다. 그런데 이와 같은 스레드에서 공통의 객체 풀 하나를 놓고 객체를 재사용한다면 풀에 들어 잇는 객체를 사용하고자 할 때마다 모종의 동기화 방법을 사용해야 하며, 따라서 락을 확보하기 위해 스레드가 대기 상태에 들어가야 할 가능성이 생긴다. 그런데 스레드가 락 경쟁에 밀려 대기 상태에 들어가 기다리는 작업 흐름은 메모리에 객체를 할당하는 일보다 훨씬 자원을 많이 소모하는 일이기 때문에 발생하는 락 경쟁 상황은 애플리케이션의 확장성에 지대한 영향을 미치는 병목이 될 수 있다(경쟁 조건이 거의 발생하지 않는 동기화 기법이라 해도 메모리에 객체를 할당하는 것보다는 더 많은 자원을 소모한다). 객체 풀 역시 성능을 최적화할 수 있는 방법 가운데 하나라고 생각하기도 하지만, 반대로 확장성에는 심각한 문제를 일으킬 수 있다. 객체 풀은 그것만의 적절한 용도가 있으며, 성능을 최적화하는 데 사용하기에는 그다지 적절한 방법이 아니다. J2ME(Java 2 Micro Edition)이나 RTSJ(Real-Time Specification for Java)와 같이 매우 제한된 환경에서는 메모리 관리의 측면이나 응답 속도를 관리하는 측면에서 객체 풀이 꼭 필요하기도 하다.  

#### 11.5. 예제: Map 객체의 성능 분석
<br/>
단일 스레드 환경에서 ConcurrentHashMap은 동기화된 HashMap보다 약간 성능이 빠르다. 하지만 병렬 처리 환경에서는 ConcurrentHashMap의 성능이 빛을 발한다. ConcurrentHashMap의 구현 내용을 살펴보면 가장 많이 사용하는 기능이 현재 맵 내부에 갖고 있는 값을 찾아내 가져가는 연산이라고 가정하고 있으며, 따라서 ConcurrentHashMap은 여러 개의 스레드에서 get 메소드를 연달아 호출하는 경우에 가장 빠른 속도를 낸다.  

동기화된 HashMap 클래스가 속도가 떨어지는 가장 큰 이유는 물론 맵 전체가 하나의 락으로 동기화돼 있다는 점이고, 따라서 한 번에 단 하나의 스레드만이 맵을 사용할 수 있다. 또한 ConcurrentHashMap은 대부분의 읽기 연산에는 락을 걸지 않고 있으며 쓰기 연산과 일부 읽기 연산에는 락 스트라이핑을 활용하고 있다. 이런 기법에 힘입어 대부분의 경우 대기 상태에 들어가지 않고도 다수의 스레드가 동시에 ConcurrentHashMap의 기능을 사용할 수 있다.  

다음 그림을 보면 여러 가지 종류의 Map, 즉 ConcurrentHashMap, ConcurrentSkipListMap, synchronizedMap으로 처리한 HashMap과 TreeMap과 같이 구현 방법을 놓고 봤을 때 각자가 얼마나 확장성을 갖고 있는지 나타나 있다. ConcurrentHashMap과 ConcurrentSkipListMap은 애초에 설계할 때부타 멀티스레드 환경에서 사용하는 것을 목표로 만들어졌고, synchronizedMap을 활용해 동기화시킨 HashMap과 TreeMap은 아주 단순하게 강제로 동기화를 맞춘 것이라고 볼 수 있다. 성능을 측정하는 각 경우마다 N개의 스레드가 임의의 키를 선택한 다음 그 키에 해당하는 값을 맵에서 읽어는 단순한 코드를 반복적으로 실행한다. 만약 키에 해당하는 값을 맵에 들어 있지 않은 경우에는 p=0.6의 확률로 임의의 값을 맵에 추가하게돼 있다. 그리고 키에 대한 값이 존재하면 p=0.02의 확률로 해당하는 값을 맵에서 제거한다. 테스트 프로그램은 8개 CPU가 장착된 V88o 장비에서 릴리즈되기 전의 자바 6 버전으로 실행했다. 또한 그래프에 표시된 값은 1개 스레드로 ConcurrentHashMap을 테스트할 때의 값으로 정규화돼 있다는 점을 참고하자(자바 5.0에서의 같은 테스트를 해보면 synchronizedMap으로 동기화된 맵과 원래 멀티스레드에 대응하도록 만들어진 컬렉션 간의 속도 차이가 훨씬 크다).  

ConcurrentHashMap과 ConcurrentSkipListMap에 대한 결과를 보면 스레드 수가 늘어남에 따라 성능이 잘 따라와 준다는 사실을 알 수 있다. 스레드 개수가 늘어남에 따라 처리량도 함께 늘어나고 있다, 다음 그림에 표시된 스레드 개수가 그다지 크지 않아 보일 수도 있지만, 여기에 사용했던 테스트 프로그램은 반복문 내부가 맵의 기능을 활용하는 코드로만 가득 채워져 있기 때문에 다른 작업을 많이 수행하는 일반적인 애플리케이션의 코드보다 락 경쟁이 훨씬 많이 발생하는 구조로 돼 있다는 구조로 돼 있다는 점을 알아두자.  

보다시피 synchronizedMap으로 동기화된 맵이 보여주는 성능 수치는 그라딪 훌륭하지 않다. 단일 스레드로 동작할 때에는 ConcurrentHashMap과 대등한 속도를 보여주지만, 경쟁 조건이 발생하지 않는 상황에서 경쟁이 발생하는 상황으로 넘어가면 성능이 급격하게 저하하는 것을 볼 수 있다. 이런 성능 저하는 락 경쟁을 제대로 막지 못하는 경우에 흔히 발생한다. 경쟁이 많이 발생하지 않는 상황에서는 연산하는 데 필요한 시간이 실제 작업에 필요한 시간과 크게 차이나지 않으며, 스레드가 추가될수록 성능도 함께 증가한다. 하지만 한 번 경쟁이 발생하기 시작하면 연산에 필요한 시간의 대부분이 컨텍스트 스위칭과 스케줄링에 필요한 대기 시간으로 소모되며, 스레드를 추가한다 해도 성능을 거의 끌어오리지 못한다.  

<img src="/assets/images/Java_Concurrency_In_Practice/11-3.jpg" width="100%" height="100%"/>
<br/>

#### 11.6. 컨텍스트 스위치 부하 줄이기
<br/>
다양한 연산이 대기 상태에 들어갈 수밖에 없는 특성을 작고 있다. 이렇게 실행과 대기의 두 가지 상태를 옮겨 다니는 것을 컨텍스트 스위치라고 한다. 서버 애플리케이션에서 대기 상태에 들어가기 쉬운 경우는 예를 들어 요청을 처리하는 가운데 출력할 로그 메시지를 생성하는 작업을 들 수 있다. 컨텍스트 스위치 횟수를 줄이면 서버의 처리량에 어떤 변화가 있는지를 확인하기 위해 두 가지의 로그 출력 방법을 적용했을 때의 처리 속도를 비교해 보도록 하자.  

대부분의 로그 출력 프레임웍은 println 문장을 적당히 감싸고 있을 뿐이다. 만약 로그로 출력하고자 하는 내용이 있다면 필요한 곳마다 여기저기에서 println 문장을 호출하면 된다. 또 다른 방법은 로그 출력 작업이 로그 출력만을 전담으로 하는 백그라운드 스레드에 의해서 진행되며, 실제로 로그 메시지를 출력하고자 했던 스레드는 실제로 로그를 출력하지는 않는다. 개발자의 입장에서는 두 가지 방법에 별 다른 차이점이 없어 보일 수도 있다. 하지만 두 가지 방법을 봤을 때 성능에 차이가 있을 수 있는데, 출력되는 로그 메시지의 양이나 로그 메시지를 몇 개의 스레드에서 출력하는지, 아니면 컨텍스트 스위치를 하는 데 얼마만큼의 자원이 필요한지 등에 의해 차이가 발생한다.  

I/O 작업을 다른 스레드에 넘기는 방법으로 동작하는 로그 출력기를 구현하면 성능을 높여줄 수 있지만, 실제 적용했을 때 발생할 수 있는 복잡하고 다양한 문제점에 대한 준비를 설계 단계에서 미리 해둬야 한다. 예를 들어 인터럽트가 걸린 경우(로그 메시지를 출력하느라 대기 상태에 들어가 있을 때 인터럽트가 걸리면 어떻게 되는지?), 로그를 확실하게 출력해주는지(일단 출력할 내용으로 쌓여 있는 메시지는 애플리케이션이 종료되기 전에 모두 출력할 수 있는지?), 로그 출력 속도는 빠른지(출력할 수 있는 양보다 더 많은 양의 메시지가 쌓인다면 어떻게 할 것인지?), 서비스의 라이프 사이클 문제(로그 서비스를 어떻게 종료할 것인지? 로그 서비스의 실행 상태를 로그 출력 코드에 어떻게 알려줄 수 있을 것인지?) 등도 충분히 고려해야 한다.  

어떤 추가 작업이 있든지 간에 로그 출력 기능에 걸리는 시간은 항상 I/O 스트림 클래스와 관련된 모든 작업 시간을 포함한다. 즉 I/O 연산이 대기 상태에 들어가면 해당 스레드가 대기 중인 시간까지 전체 작업 시간에 포함된다. 그러면 운영체제는 I/O 작업이 끝날 때까지, 또는 그보다 조금 더 긴 시간까지 해당 스레드를 대기 상태에 집어 넣는다. I/O 작업이 마무리되고 나면, 다른 스레드가 아직 동작 중일 수 있으며 해당 스레드가 할당받은 정도의 시간까지는 계속 실행될 것이고, 어떤 스레드는 스케줄링 큐에 먼저 들어가서 대기 중일 수도 있다. 그러면 서비스 시간이 조금 더 늘어나는 셈이다. 아니면 다수의 스레드가 동시다발적으로 로그 메시지를 출력하고자 한다면 메시지를 출력하는 출력 스트림 객체에 대한 락을 두고 경쟁이 발생할 수 있다. 그러면 블로킹(blocking) I/O의 경우와 같이 스레드가 락을 확보하기 위해 대기 상태에 들어가면서 컨텍스트 스위치가 발생하는 결과가 나타난다. 정리해보면, 로그 메시지를 그 즉시 출력하는 방법은 I/O 연산과 스트림에 대한 락에 직접적으로 연결돼 있으며, 따라서 컨텍스트 스위치가 빈번하게 발생할 가능성이 높고 서비스 시간은 점점 늘어난다.  

요청에 대한 서비스 시간이 늘어나는 일은 여러 가지 측면에서 좋지 않은 상태이다. 가장 먼저 서비스 시간은 서비스의 품질과 직접적으로 연관돼 있다. 서비스 시간이 길어진다는 얘기는 바로 누군가가 서비스의 결과를 얻기 위해 오랫동안 기다린다는 말과 같다. 더욱 심각한 문제는 서비스 시간이 락 경쟁을 심화시킨다는 점이다. 락을 오랫동안 확보하고 있을수록 락에 대한 경쟁이 발생할 가능성이 높아지기 때문에 락을 확보하고 있는 시간은 최대한 줄여야 한다. 특정 스레드가 락을 확보한 상태에서 I/O 연산이 끝날 때까지 대기 상태에 들어가 있다면, 실행 중인 다른 스레드가 이미 누군가가 확보하고 있는 락을 필요로 할 가능성이 높다. 락을 놓고 경쟁하고 있다는 말은 컨텍스트 스위치가 많이 일어나고 있다는 말과 같으므로, 병렬 처리 시스템은 대부분의 락을 대상으로 경쟁이 발생하지 않는 경웨 훨씬 성능을 보여준다. 만약 컨텍스트 스위치가 자주 일어나는 방법으로 프로그램을 작성하는 버릇이 있다면 전반적인 성능이 저하될 수밖에 없다.  

요청을 처리하는 스레드의 외부로 I/O 작업을 뽑아내는 방법은 요청을 처리하는 평균 시간을 중려주는 좋은 방법이다. log 메소드를 호출하는 스레드는 더 이상 출력 스트림에 대한 락을 확보할 필요도 없고 I/O 작업이 완료될 때까지 대기하지 않아도 된다. 단지 출력할 로그 메시지를 큐에 쌓아두는 즉시 리턴돼 본연의 작업을 계속해서 진행할 수 있다. 반대로 메시지 큐를 사용하기 위한 경쟁이 발생할 가능성이 높긴 하지만, 메시지를 큐에 쌓는 put 연산이 실제로 출력 스트림에 메시지를 출력하는 연산(시스템 콜이 필요할 수도 있다)보다 훨씬 가벼운 연산임은 분명하다. 따라서 실제 상황에서 로그를 출력하고자 할 때 스레드가 대기 상태에 들어갈 일은 거의 없다고 봐도 좋고, 로그 메시지를 출력하느라 컨텍스트 스위치가 발생할 확률도 줄일 수 있다. 이런 작업을 통해서 I/O 작업과 락 경쟁이 발생할 수 있는 복잡하면서 불확실한 코드를 깔끔하게 쭉 뻗은 코드로 변경할 수 있겠다.  

따지고 보면 작업이 실제로 처리되는 위치를 옮기고 있을 뿐이고, 사용자가 직접 그 속도를 느끼기 어려운 위치로 I/O 작업을 이동시켰을 뿐이다. 로그 관련 I/O 작업을 모두 단 하나의 스레드에서 처리하도록 넘기고 있기 때문에 로그 출력 스트림을 공유하지 않아도 되고, 따라서 대기 상태에 들어갈 수 있는 원인을 미연에 방지하고 있다. 이런 구조를 갖춰두면 스케줄링, 컨텍스트 스위칭, 락 관리와 같은 각 부분에서 사용하는 자원의 양을 크게 줄일 수 있기 때문에 전반적인 성능을 높일 수 있다.  

로그를 출력하고자 요청하는 다수의 스레드에서 발생할 I/O 연산을 단 하나의 스레드에서 처리하도록 한구데로 몰아두는 일은, 불을 끄고자 할 때 서로 양동이를 들고 뛰어다니는 경우와 줄을 맞춰 서서 양동이를 넘겨주는 방법(bucket-bridge)으로 불을 끄는 방법의 차이로 비유해 볼 수 있겠다. "수십 명의 사람이 양동이를 들고 바쁘게 뛰어다니는" 방법을 생각해보면 양동이에 물을 받는 부분과 물을 뿌리고자 하는 화재 발생지점 양쪽에서 사람들 사이에 경쟁이 발생할 수밖에 없다(결국 화재 발생 지점에 뿌려지는 물의 양이 줄어든다). 게다가 각 사람이 여러 종류의 작업(양동이에 물 받고, 달려가서, 물을 뿌리고, 다시 달리는)을 모두 처리해야 하기 때문에 비효율적이기도 하다. 하지만 줄서서 양동이를 넘겨주는 방법을 사용하면 물을 받는 부분에서 화재 발생 지점까지 양동이를 일정한 속도로 넘겨줄 수 있으므로 물을 옮기는 데 에너지를 더 적게 소모하고, 작업자 각각이 자신이 맡은 일만 집중적으로 할 수 있다. 사람이 일을 하는 경우에도 각종 인터럽트가 발생하면 효율이 떨어지는 것과 동일하게, 스레드의 입장에서는 대기상태에 들어가거나 컨텍스트 스위치가 일어나는 일이 원래 작업을 처리하는 데 상당한 방해가 된다.  

### 요약
<br/>
멀티스레드를 사용하는 큰 이유 중의 하나가 바로 다중 CPU 하드웨어를 충분히 활용하고자 하는 것이다. 병렬 처리 애플리케이션의 성능에 대해 논의하면서 실제적인 서비스 시간 보다는 애플리케이션의 데이터 처리량이나 확장성을 좀더 집중적으로 살펴봤다. 암달의 법칙에 따르면 애플리케이션의 확장성은 반드시 순차적으로 실행돼야만 하는 코드가 전체에서 얼마만큼의 비율을 차지하냐에 달렸다고 한다. 자바 프로그램의 내부에서 순차적으로 처리해야만 하는 가장 주요한 부분은 바로 독점적인 락을 사용하는 부분이기 때문에, 락으로 동기화하는 범위를 세분화해 정밀도를 높이거나 락을 확보하는 시간을 최소한으로 줄이는 기법을 사용해 락을 최소한만 사용해야 한다. 그리고 독적적인 락 대신 독점적이지 않은 방법을 사용하거나 대기 상태에 들어가지 않는 방법을 사용하는 것도 중요하다.  

### 12. 병렬 프로그램 테스트
<br/>
병렬 프로그램 역시 순차적으로 처리하는 프로그램과 유사한 디자인 패턴을 가져다 쓰는 경우가 많다. 병렬 프로그램은 단지 순차적인 프로그램에 비해 곳곳에 작동 내용을 확인하기 어려운 부분을 포함하고 있다는 차이점이 있다. 따라서 순차적인 프로그램에 비해 각 부분 간의 상호 작용이 훨씬 복잡하며, 미리 예상하고 분석해야 할 가능한 오류 상황도 훨씬 많다.  

프로그램 자체도 그렇지만 테스팅 방법 역시 순차적인 프로그램을 테스트하던 방법을 대부분 그대로 가져와 사용한다. 순차적인 프로그램의 정확성과 성능을 측정하던 방법을 병렬 프로그램에도 그대로 볼 수 있지만, 테스트 결과로 얻을 수 있는 값의 범위가 순차적인 프로그램보다 훨씬 다양하다는 특징이 있다. 병렬 프로그램을 테스트하는 프로그램을 작성할 때 처음 부딪히는 부분은 바로 순차적인 프로그램보다 문제 상황이 발생할 확률이 훨씬 적다는 데 있다. 결국 발생 확률이 훨신 떨어지는 결과를 제대로 확인해야 하기 대문에 테스트 프로그램에서 대상 애플리케이션을 훨신 강하게 밀어붙여야 하고, 순차적인 프로그램보다 긴 시간동안 테스트하는 일이 많다.  

병렬 프로그램을 테스트한 결과는 전통적으로 사용해왔던 문제 상황인 안전성(safety)과 활동성(liveness)의 문제로 귀결된다. 이미 1장에서 '안 좋은 일이 발생하지 않는 상황'을 안전성이라고 하고, '결국 좋은 일이 발생하는 상황'을 활동성이라고 정의한 바 있다.  

클래스가 동작하는 형태가 설계했던 모습 그대로 움직이는지를 확인하는 안전성 테스트는 대부분 변수의 값이 정확한지를 확인하는 것부터 시작한다. 이를테면 갖고 있는 항목의 개수를 독립 변수에 캐시하고 변경 사항이 발생할 때마다 업데이트하도록 만들어진 연결 리스트(linked list)를 구현하고 있다면, 갖고 있는 항목의 개수와 캐시된 변수의 값이 일치하는지를 확인하는 부분이 가장 기본적인 안전성 테스트라고 볼 수 있다. 단일 스레드 환경에서는 리스트에 들어 있는 항목이 테스트 도중에 변경될 수 없기 때문에 굉장히 쉽게 테스트할 수 있다. 하지만 다수의 스레드가 동작하는 병렬 처리 환경에서는 항목의 개수를 세는 작업과 세어진 개수가 캐시된 변수의 값과 일치하는지를 확인하는 두 가지 작업을 단일 연산으로 수행하지 않는 한 올바르지 않은 결과가 속출할 것이다. 이런 테스트를 병렬 환경에서 올바르게 진행하려면 대상 리스트를 독점적으로 사용할 수 있도록 준비해야 한다. 예를 들어 구현하고 있는 리스트 클래스에서 현재 항목의 목록에 대한 스냅샷(snapshot)을 뽑아주는 기능을 구현하거나, 테스트 프로그램이 값을 제대로 비교하거나 테스트 코드를 안전하게 실행할 수 있도록 '테스트 포인트(test point)'를 마련하는 방법도 있다.  

지금까지 이 책에서는 올바르게 구현되지 않은 클래스에서 '불행히도 딱 맞아 떨어지는' 오류 상황을 표현하기 위해 타이밍 다이어그램(timing diagram)을 사용하곤 했다. 테스트 프로그램은 물론 발생 가능한 상황을 최대한 넓게 지나다니면서 불행하게 타이밍이 딱 맞는 상황을 놓치지 않아야 한다. 반대로 안 좋은 타이밍을 만들어 내기 위해 준비한 테스트 프로그램이 오히려 실제로 발생할 수 있는 상황을 표현하지 못해 오류을 남겨두는 경우도 생길 수 있다. 오류를 확인하기 위해 디버깅 관련 코드를 추가하면 사라져 버리는 류의 버그를 우스개소리로 하이젠버그(Heisenbugs)라고 하기도 한다. (양자 물리학에서 불확실성 이론을 확립한 독일의 물리학자(하이젠베르그(Heisenberg)의 이름에서 따온 것이다)

#### 12.1. 정확성 테스트
<br/>
병렬 프로그램을 테스트하기 위한 테스트 프로그램을 작성할 때는 순차적인 프로그램을 테스트하는 경우와 똑같은 분석 작업으로 시작한다. 올바른 값을 정확하게 알고 있는 변수가 어떤 것이 있는지, 그 변수가 최종적으로 어떤 값을 가져야 하는지 등의 내용을 확인해야 한다. 만약 설계가 충분히 이뤄진 경우에는 이와 같은 변수에 대한 사항이 설계 문서에 포함돼 있을 수 있다. 테스트 프로그램을 작성하는 나머지 시간은 전부 설계 과정에서 놓친 기능 명세를 찾아가는 과정이다.  

정확성 테스트에 대해 확실하게 이해할 수 있는 예제로 크기가 제한된 버퍼 클래스에 대한 테스트 케이스를 구현해 보자. 먼저 다음 예제를 보면 테스트할 대상이 될 BoundedBuffer 클래스의 소스 코드가 나타나 있다. BoundedBuffer 클래스는 Semaphore를 사용해 크기를 제한하고 제한된 크기를 초과한 경웨 대기 상태에 들어가도록 하고 있다.  

```java
@ThreadSafe
public class BoundedBuffer<E> {
  private final Semaphore availableItems, availableSpaces;
  @GuardedBy("this") private final E[] itmes;
  @GuardedBy("this") private int putPosition = 0, takePosition = 0;

  public BoundedBuffer(int capacity) {
    availableItems = new Semaphore(0);
    availableSpaces = new Semaphore(capacity);
    itmes = (E[]) new Object[capacity];
  }

  public boolean isEmpty() {
    return availableItems.availablePermits() == 0;
  }

  public boolean isFull() {
    return availableSpaces.availablePermits() == 0;
  }

  public void put(E x) throws InterruptedException {
    availableSpaces.acquire();
    doInsert();
    availableItems.release();
  }

  public E take() throws InterruptedException {
    availableItems.acquire();
    E item = doExtract();
    availableSpaces.release();
    return item;
  }

  private synchronized void doInsert(E x) {
    int i = putPosition;
    itmes[i] = x;
    putPosition = (++i == itmes.length) ? 0 : i;
  }

  private synchronized E doExtract() {
    int i = takePosition;
    E x = itmes[i];
    items[i] = null;
    takePosition = (++i == items.length)? 0 : i;
    return x;
  }
}
```

BoundBuffer 클래스의 내부를 보면 배열을 기반으로 하는 큐의 형태로 구현돼 있고, 대기 상태에 들어갈 수 있는 put 메소드와 take 메소드를 갖고 있다. put과 take 메소드는 개수가 지정된 세마포어를 사용해 동기화를 맞추고 있다, availableItems라는 세마포어를 보면 현재 버퍼 내부에서 뽑아낼 수 있는 항목의 개수를 담고 있으며, 물론 버퍼를 생성한 최초 시점에는 0이라는 값을 갖는다(최초에는 버퍼가 비어 있을테니 당연하다). 이와 비슷하게 availableSpaces는 버퍼에 추가할 수 있는 항목의 개수가 몇 개인지를 담고 있고, 그 값은 버퍼가 생성되는 최초 시점에 버퍼의 크기에 맞춰져 있다.  

take 메소드는 먼저 availableItems 세마포어에서 가져갈 항목이 있는지에 대한 확인을 받아야 한다. 만약 버퍼에 항목이 하나 이상 들어 있었다면 즉시 확인에 성공하고, 버퍼가 비어 있었다면 버퍼에 항목이 추가될 때까지 대기 상태에 들어간다. availableItems에서 확인을 받고 나면 버퍼의 다음 항목을 뽑아내고 availableSpaces의 값이 늘어나게 된다. 개수가 지정된 세마포어를 사용할 때 실제로 '허가'를 받는 것은 아니다. acquire 메소드를 호출해 리턴되면 해당 세마포어에 '허가'를 받은 셈이고, release 메소드를 호출해 리턴되면 '허가'를 반납한 셈이다. put 메소드는 take 메소드와는 반대로 구성돼 있으며, 결국 put 메소드나 take 메소드를 호출한 이후 리턴된 이후에는 항상 양쪽 세마포어가 갖고 있는 값의 합이 항상 버퍼의 크기와 일치한다(실제로 크기가 제한된 버퍼를 구현해 사용하고자 한다면 ArrayBlockingQueue 클래스나 LinkedBlockingQueue 클래스를 사용하는 게 올바른 방법이다. 하지만 여기에서 버퍼를 구현하면서 이런 기법을 사용한 것은 추가와 삭제 작업을 다른 데이터 구조를 사용해 구현할 수도 있다는 점을 보여주고자 함이다).  

##### 12.1.1. 가장 기본적인 단위 테스트
<br/>
BoundedBuffer 클래스를 테스트하기 위한 가장 기본적인 단위 테스트(unit test) 클래스는 순차적인 개념으로 생각했을 때의 방법과 별로 다르지 않다. BoundedBuffer 인스턴스를 하나 생성하고, 메소드를 이것저것 호출해보고, 최종적인 상태와 변수의 값 등을 확인해 보는 방법이다. 예를 들어 BoundedBuffer 인스턴스를 생성한 직후에는 자신이 데이터를 하나도 갖고 있지 않으며 가득 차지도 않았다는 점을 표현해야만 한다. 이보다 약간 더 복잡한 예를 들어본다면, 용량이 N인 BoundedBuffer 클래스에 N개의 항목을 추가(대기 상태에 들어가는 일 없이 추가할 수 있어야 한다)하고, 버퍼 클래스 스스로가 용량이 가득 찼다고 표현해야만 한다. 이와 같은 내용을 테스트하는 데 테스트 케이스가 다음 예제에 나타나 있다,  

```java
class BoundedBufferTest extends TestCase {
  void testIsEmptyWhenConstructed() {
    BoundedBuffer<Integer> bb = new BoundedBuffer<Integer>(10);
    assertTrue(bb.isEmpty());
    assertFalse(bb.isFull());
  }

  void testIsFullAfterPuts() throws InterruptedException {
    BoundedBuffer<Integer> bb = new BoundedBuffer<Integer>(10);
    for (int i = 0; i < 10; i++)
      bb.put(i);
    assertTrue(bb.isFull());
    assertFalse(bb.isEmpty());
  }
}
```

본 테스트 메소드는 전적으로 순차적으로 동작하는 상황을 테스트한다. 이와 같이 순차적으로 동작하는 테스트 케이스를 작성해두면 데이터를 놓고 경쟁이 발생하는 상황을 테스트하기 이전에 테스트 케이스에서 발생한 오류가 멀티스레드 환경에서 발생하는 오류가 아니라는 점을 확인할 수 있다는 점에서 유용한 면이 있다.  

##### 12.1.2. 블로킹 메소드 테스트
<br/>
병렬로 동작하는 상황을 테스트하고자 한다면 스레드를 두 개 이상 실행시켜야 하는 경우가 대부분이다. 그런데 테스트를 도와주는 프레임웍은 대부분 병렬 처리 환경에 적절히 대응하지 못하는 경우가 많다. 예를 들어 스레드를 생성하는 기능이나 실행된 스레드가 의도하지 않은 방법으로 종료되는 일이 있는지를 모니터링하는 등의 기능을 제공하지 않는다는 말이다. 만약 테스트 케이스 내부에서 생성한 도우미 스레드가 오류 상태를 확인했다 해도 프레임웍 입장에서는 스레드가 발견한 오류가 정확하게 어떤 테스트와 연관돼 있는지조차 제대로 알아내기가 어렵다. 다라서 따로 실행되고 있는 스레드에서 성공과 실패 여부를 파악하는 경우에, 파악된 성공 또는 실패 여부를 다시 원래 테스트 케이스의 메소드에 알려줄 수 있는 방법이 마련돼 있어야 테스트 결과를 단위 테스트 프레임웍에서 제대로 리포팅할 수 있다.  

java.util.concurrent 패키지에 대한 표준 부합 테스트를 진행할 때 실패 건이 발생하는 경우 어떤 테스트에서 실패했는지를 정확하게 파악하는 일이 굉장히 중요했다. 따라서 JSR 166 전문가 그룹(expert group)에서는 테스트 케이스에서 실패 상황이 발생했을 때 해당 건을 모아 뒀다가 tearDown 메소드에서 모든 오류 상황을 표시하는 기능을 구현한 기반 클래스를 하나 구현했다. 단 이 기반 클래스를 사용할 때에는 모든 테스트를 진행할 때 항상 특정 테스트에서 생성된 스레드는 해당 테스트가 종료되기 직전에 모두 종료돼야 한다는 조건을 만족해야 한다. 물론 JSR166TestCase 클래스의 소스코드를 전부 들여다 볼 필요는 없다. 중요한 점은 테스트가 오류 없이 정상적으로 끝났는지, 아니면 오류가 발생했을 때 오류를 제대로 찾아내 수정할 수 있도록 오류 관련 정보를 충분히 출력해줘야 한다는 점이다.  

만약 특정 메소드가 어떤 상황에서는 반드시 대기 상태에 들어가야 한다고 하면, 해당 기능에 대한 테스트는 테스트를 담당했던 스레드가 더 이상 실행하지 않고 멈춰야만 테스트가 성공이라고 볼 수 있다. 대기 상태에 들어가는 메소드를 테스트하는 것은 반드시 예외 상황이 발생해야 하는 메소드를 테스트하는 것과 비슷하다. 만약 대상 메소드가 리턴돼 버린다면 테스트는 실패한 것이다.  

대기 상태에 들어가는 메소드를 테스트할 때에는 여러 가지 복잡한 사항이 있다. 대상 메소드를 호출해서 제대로 대기 상태에 들어갔다고 하면, 어떤 방법으로건 대기 상태를 풀어서 대기 상태에 들어갔었음을 확인해야 한다. 이런 테스트를 할 수 있는 가장 확실한 방법은 바로 인터럽트를 거는 방법이다. 예를 들어 대기 상태에 들어가야 하는 메소드를 호출할 때는 따로 스레드를 실행시켜 호출하고, 해당 스레드가 대기 상태에 들어갈 때까지 기다리고 있다가 대기 상태에 들어가면 인터럽트를 걸고, 원하는 연산을 제대로 처리했는지 확인하는 순서로 진행한다. 물론 이와 같이 인터럽트를 활용해 테스트하려면 대기 상태에 들어갈 대상 메소드가 인터럽트에 적절하게 대응하도록, 다시 말해 인터럽트가 걸리는 즉시 리턴되거나 InterruptedException을 던지는 등의 행동을 하도록 만들어져 있어야 한다.  

'스레드가 대기 상태에 들어갈 때까지 기다리는' 방법은 일단 말로 표현하기는 쉽지만, 실제로는 그다지 간단하지 않다. 대기 상태에 들어가기 전에 배치된 프로그램 코드가 실행되는 데 얼마만큼의 시간이 걸릴 것인지를 예측하고 있어야 하고, 그보다 오래 기다려 보는 수밖에 없다. 만약 기다리도록 지정한 시간이 예상보다 짧아서 테스트가 제대로 이뤄지지 않는다면 기다리는 시간을 언제든지 늘릴 수 있도록 테스트 프로그램을 준비해둬야 한다.  

다음 예제의 코드를 보면 대기 상태에 들어가는 메소드를 테스트하는 방법의 예를 볼 수 있다. 다음 예제의 코드를 보면 먼저 비어 있는 버퍼의 take 메소드를 호출하는 taker 스레드를 생성하도록 돼 있다. taker 스레드가 호출한 take 메소드가 리턴된다면 taker 스레드는 오류가 발생했다는 사실을 기록해둔다. 테스트 프로그램을 실행하면 먼저 taker 스레드를 실행하고 적당량 이상 오래 기다려 보고, 그 다음에는 taker 스레드에 인터럽트를 건다. 만약 taker 스레드가 take 메소드에서 정상적으로 대기 상태에 들어가 있었다면 인터럽트가 걸렸을 때 InterruptedException을 띄울 것이고, InterruptedException을 받은 catch 구문에서는 예외가 발생한 상황이 정상이라고 판단하고 스레드를 그대로 종료시킨다. 그러면 taker 스레드를 실행시켰던 테스트 프로그램은 taker 스레드가 종료될 때까지 join 메소드로 기다리고, Thread.isAlive 메소드를 사용해 join 메소드가 정상적으로 종료됐는지를 확인한다. 다시 말해, taker 스레드가 정상적으로 인터럽트에 응답했다면 join 메소드가 즉시 종료돼야 맞다.  

일반적인 join 메소드 대신 타임아웃을 지정하는 join 메소드를 사용하면 take 메소드가 예상치 못한 상황에 걸려 응답하지 않은 경우에도 테슽 프로그램을 제대로 종료시킬 수 있다. 여기에 소개된 테스트 메소드만으로는 take 메소드의 여러 가지 특성을 테스트할 수 있다. 즉 아무것도 없을 때 take 메소드를 호출하면 대기 상태에 들어가야 한다는 것뿐만 아니라 대기 중에 인터럽트가 걸리면 InterruptedException을 발생시켜야 한다는 기능도 테스트할 수 있다. 이처럼 join 메소드를 사용해 정상적으로 종료되는지를 확인하는 작업은 Runnable 인터페이스를 구현하는 대신 Thread 클래스를 직접 상속받아 사용하는 편이 더 나은 몇 안 되는 방법 가운데 하나이다. 또한 이와 같은 방법을 사용하면 테스트 프로그램에서 버퍼에 항목을 직접 추가하면서 항목이 추가되는 시점에 대기 상태에 들어가 있던 taker 스레드가 정상적으로 대기 상태에서 빠져나오는지도 확인할 수 있다.  

Thread.getState 메소드를 사용해 스레드가 특정 조건에 맞춰 대기 상태에 들어가 있는지를 확인하고자 하는 마음이 없지 않겠지만, Thread.getState 메소드를 사용하는 방법은 그다지 믿을만하지 못하다. 스레드가 대기 상태에 들어갈 때 JVM의 구현 방법에 따라 스핀 대기(spin waiting) 기법을 활용할 수도 있으므로, 특정 스레드가 대기 상태에 들어갔다고 해서 항상 스레드가 WAITING 또는 TIMED&#95;WAITING 상태에 놓여 있다고 볼 수 없기 때문이다. Object.wait 메소드나 Condition.awaint 메소드에서 정상적이지는 않지만 예정보다 일찍 리턴될 수가 있는데, 원래 대기하게 된 원인 조건이 아직 해소되지 않았음에도 불구하고 스레드의 상태가 WAITING 또는 TIMED&#95;WAITING에서 일시적으로 RUNNABLE 상태로 전환될 가능성도 있다. 이런 세부적인 구현사의 문제를 차지하고라도 대상 스레드가 대기 상태에 들어간 이후에 대기 상태에 안정적으로 자리잡기까지 약간이 시간이 걸릴 수도 있다. 따라서 병렬성(concurrency)을 제어하는 용도로 Thread.getState 메소드를 사용하지는 말아야 하며, 일반적으로 테스트 프로그램에서도 그다지 유용하지 않다. Thread.getState 메소드는 디버깅 정보를 얻어내는 둉도로 사용하는 일이 거의 전부이다.  

```java
void testTakeBlocksWhenEmpty() {
  final BoundedBuffer<Integer> bb = new BoundedBuffer<Integer>(10);
  Thread taker = new Thread() {
    public void run() {
      try {
        int unused = bb.take();
        fail(); // 여기에 들어오면 오류!
      } catch (InterruptedException success) { }
    }
  };
  try {
    taker.start();
    Thread.sleep(LOCKUP_DETECT_TIMEOUT);
    taker.interrupt();
    taker.join(LOCKUP_DETECT_TIMEOUT);
    assertFalse(taker.isAlive());
  } catch (Exception unexpected) {
    fail();
  }
}
```

##### 12.1.3. 안전성 테스트
<br/>
소개하는 테스트 루틴은 크기가 제한된 버퍼의 여러 가지 속성을 테스트한다. 하지만 공유된 데이터를 서로 사용하고자 경쟁하는 데서 발생하는 오류는 제대로 테스트하지 못한다. 병렬 처리 화경에서 동작하는 클래스의 기능을 동시 다발적으로 호출할 때 발생하는 문제를 제대로 테스트하려면, put 메소드나 take 메소드를 호출하는 여러 개의 스레드를 충분한 시간 동안 동작시킨 다음 테스트 대상 클래스의 상태가 올바른지, 잘못된 값이 들어 있지는 않은지 확인해야 한다.  

병렬 실행 환경에서 발생하는 오류를 확인하는 프로그램을 작성하다 보면 닭이 먼저냐 달걀이 먼저냐하는 문제에 걸리게 된다. 즉 테스트 프로그램 자체가 병렬 프로그램이 돼야 하기 때문이다. 심지어는 올바른 테스트 프로그램을 작성하는 일이 테스트 대상 클래스 자체를 구현하는 일보다 훨씬 어려운 경우도 있다.  

안전성을 테스트하는 프로그램을 효과적으로 작성하려면 뭔가 문제가 발생했을 때 잘못 사용되는 속성을 '높은 확률로' 찾아내는 작업을 해야 함과 동시에 오류를 확인하는 코드가 테스트 대상의 별렬성을 인위적으로 제한해서는 안 된다는 점을 고려해야 한다. 테스트하는 대상 속성의 값을 확인할 때 추가적인 동기화 작업을 하지 않아도 된다면 가장 좋은 상태라고 볼 수 있다.  

(BoundedBuffer 클래스와 같이) 프로듀서-컨슈머 디자인 패턴을 사용해 동작하는 클래스에 가장 적합한 방법은 바로 큐나 버퍼에 추가된 항목을 모두 그대로 뽑아 낼 수 있는지 확인하고, 그 외에는 아무런 일도 하지 않는지 확인하는 방법이다. 이런 방법을 아무런 생각 없이 구현하려면 테스트 대상과 함께 똑같은 내용을 담을 제2의 리스트를 마련해 두고, 큐나 버퍼에 항목이 추가될 때 제2의 리스트에도 같은 항목을 추가한다. 그리고 큐나 버퍼에서 항목을 제거할 때 제2의 리스트에서도 항목을 제거하고, 큐나 버퍼에서 항목을 모두 제거했을 때 제2의 리스트도 역시 깔끔하게 비어 있는지를 확인할 수 있겠다. 하지만 이런 방법은 제2의 리스트에 항목을 추가하고 제거하는 과정에 스레드 동기화 작업이 필요하기 때문에 테스트 스레드의 스케줄링 부분이 약간 꼬여버릴 가능성이 있다.  

또 다른 방법을 보면 큐에 들어가고 나오는 항목의 체크섬(checksum)을 구한 다음 순서를 유지하는 체크섬의 형태로 관리하고, 쌓인 체크섬을 비교해 확인하는 방법이 있다. 만약 체크섬을 비교해 양쪽이 동일하다면 테스트를 통과한다. 이 방법은 버퍼에 집어 넣을 항목을 생성하는 프로듀서가 하나만 동작하고 하나의 컨슈머가 버퍼의 내용을 가져다 사용하는 구조에서 가장 효과가 큰 테스트 방법이다. 올바른 항목을 뽑아내는지 테스트하는 것과 더불어 올바른 순서로 항목을 가져올 수 있는지도 테스트할 수 있기 때문이다.  

이 방법을 다수의 프로듀서와 다수의 컨슈머가 연결돼 있는 구조에서 테스트하는 프로그램까지 확장시켜 적용하려면 항목이 추가되는 순서에 상관없는 체크섬 방법을 사용해야 하며, 결국 마지막에 체크섬을 모두 합해 볼 수 있어야 한다. 그렇지 않으면 체크섬을 계산하는 부분을 동기화하느라 확장성 측면에서 병목이 나타날 수 있고, 그러다 보면 테스트에 걸린 시간을 제대로 측정할 수 없게 된다(더가히 연산이나 XOR 연산과 같이 교환 법칙을 만족하는 연산 방법이라면 체크섬 용도로 활용할 수 있겠다).  

작성한 테스트 프로그램이 실제로 원하는 내용을 테스트하는지 확인하려면 사용하고 있는 체크섬 연산을 컴파일러가 예측할 수 없는 연산인지도 확인해야 한다. 예를 들어 테스트용 데이터로 일련번호를 사용하면 일단 결과가 항상 동일할 것이고, 컴파일러가 최적화를 충분히 할 수 있는 능력이 있다면 결과를 미리 계산해 버릴 수도 있다.  

너무 똑똑한 컴파일러 때문에 발생하는 문제를 해결하려면 테스트에 사용할 데이터를 일련번호 대신 임의의 숫자를 생성해 사용해야 한다. 하지만 너무 허술한 난수발생기(RNG, random number generator)를 사용하면 이 또한 테스트 결과가 잘못 나올 수도 있다. 허술한 난수 발생기는 현재 시간과 클래스 간에 종속성이 있는 난수를 생성하는 경우가 있는데, 대부분의 난수 발생기가 스레드 안전성을 확보한 상태이고 추가적인 동기화 작업이 필요하기 때문이다. 대부분의 속도 측정 벤치마크 프로그램을 보면 결국 난수 발생기가 얼마나 심각한 병목 현상을 야기하는지를 테스트하는 경우가 많으며, 개발자나 사용자 모두 이런 일이 벌어진다는 사실을 잘 알지 못하는 일이 많다. 각 테스트 스레드마다 독립적인 난수 발생기 인스턴스를 사용하도록 하면 스레드 안전성 때문에 동기화하느라 성능의 병목을 야기하는 경우를 막을 수 있다.  

범용 난수 발생기를 사용하는 대신 아주 간단한 난수 발생기를 사용하는 것도 좋은 방법이다. 이런 테스트 프로그램을 만드는 데 아주 품질 좋은 난수 발생기를 사용해야만 할 필요는 없기 때문이다. 그저 테스트 프로그램을 실행할 때마다 어느 정도 적절한 임의성(randomness)만 확보하면 된다. 다음 예제의 xorShift 메소드는 산 값에 중급의 품질을 제공하는 난수 발생기다. 클래스 인스턴스의 hashCode 값과 nanoTime 값을 사용해 xorShift 메소드를 사용하면 거의 예측할 수 없을 뿐더러 실행할 때마다 새로운 난수를 생성할 수 있다.  

```java
static int xorShift(int y) {
  y ^= (y << 6);
  y ^= (y >>> 21);
  y ^= (y << 7);
  return y;
}
```

다음 예제들에서의 PutTakeTest 클래스는 항목을 생성해서 큐에 쌓는 프로듀서 스레드를 N개 생성해 실행시키고, 큐에 쌓인 항목을 뽑아내는 N개의 컨슈머 스레드 역시 생성해 실행한다. 각 스레드는 큐에 항목을 추가하거나 제거할 때마다 각 스레드마다 나눠져 있는 각자의 체크섬을 업데이트하고, 각자의 체크섬은 테스트가 끝나는 시점에 하나로 합해 결과가 올바른지 테스트한다. 이렇게 각 스레드마다 체크섬을 따로 운영하면 따로 동기화할 필요도 없고 따라서 경쟁이 발생하지 않으므로 실제로 원하는 테스트에만 집중할 수 있다.  

플랫폼마다 다르지만 스레드를 생성하고 실행하는 일이 상당히 부하가 걸리는 작업일 가능성도 있다. 스레드가 처리할 작업이 굉장히 짧은 시간이면 충분한 작업일 때, 이와 같은 스레드를 반복문 내부에서 차례로 생성해 실행시킨다면 결국 최악의 경우에는 각 스레드가 병렬로 실행되는 대신 순차적으로 실행될 가능성도 있다. 아주 최악의 상화이 아니라 해도 처음 실행되는 스레드가 다른 스레드보다 먼저 실행되다는 사실을 놓고 보면 여러 개의 스레드가 함께 실행되는 시간이 예상보다 훨씬 적어진다. 예를 들어 최초에 실행된 스레드는 한동안 혼자만 실행될 것이고, 그 다음 스레드가 살행되면 일부분만 두 개의 스레드가 함께 실행되며, 마지막 스레드까지 모두 실행된 이후에야 모든 스레드가 병렬로 동작하게 된다(이런 모양은 테스트 가 끝나가는 과정에서도 나타난다. 먼저 시작한 스레드가 먼저 종료돼 버릴 것이기 때문이다).  

이와 같은 문제를 해결하는 방법은 바로 CountDownLatch를 사용해 모든 스레드가 준비될 때까지 대기하고, 또다른 CountDownLatch를 사용해 모든 스레드가 완료될 때까지 대기하는 방법이었다. CyclicBarrier를 사용해도 이와 같은 효과를 낼 수 있는데, 전체 작업 스레드의 개수에 1을 더한 크기로 초기화해두고 작업 스레드와 테스트 프로그램이 시작하는 시점에 모두 동시에 시작할 수 있도록 대기하고, 끝나는 시점에서도 한꺼번에 끝내도록 대기하는 방법이다. 이런 류의 방법을 사용하면 모든 스레드가 생성돼 실제 작업을 시작할 준비가 끝나기 전에는 누구도 작업을 시작하지 않는다. PutTakeTest 역시 이와 같은 방법을 사용해 작업 스레드가 한꺼번에 시작하고 한꺼번에 종료하도록 하고 있으며, 여러 개의 스레드가 병렬로 처리되는 상황을 훨씬 잘 묘사할 수 있다. 그렇다 해도 그 내부에서 스케줄러가 작업 스레드를 순차적으로 실행시키지 않는다는 보장은 어디에도 없다. 하지만 실행 시간을 충분히 길게 잡으면 스케줄러의 구현 방법에 다라 테스트 결과가 예상치 못하게 꼬여 나오는 경우를 줄일 수 있다.  

PutTakeTest에 적용돼 있는 마지막 테크닉은 테스트가 끝났을을 알리느라 스레드 간에 통신 기능을 구현하는 대신, 테스트를 시작할 때 이미 종료 조건을 결정지어두는 방법이다. test 메소드가 시작되면 동일한 숫자의 프로듀서와 컨슈머가 생성된다. 각 프로듀서는 항목을 추가하고, 각 컨슈머는 항목을 뽑아내기 때문에 전체적으로 추가된 항목의 수와 제거된 항목의 수는 일치한다.  

PutTaskTest와 같은 유형의 테스트 프로그램은 테스트 대상의 안전성을 확인하기에 좋다. 예를 들어 세마포어를 제어하는 버퍼를 구현할 때 범하기 쉬운 오류 중의 하나는 바로 항목을 추가하거나 뽑아내는 작업이 (synchronized 구문이나 ReetrantLock과 같은) 상호 배타적인 상태에서 이뤄져야 한다는 점을 잊은 채 동기화를 빼먹고 구현하는 부분이다. 만약 doInsert 메소드와 doExtract 메소드의 동기화 구분을 빼먹은 버전의 BoundedBuffer를 대상으로 PutTakeTest 테스트 프로그램을 돌려보면 얼마지나지 않아 바로 오류를 찾아낼 수 있다. 따라서 수십 개의 스레드를 사용하도록 설정하고, 각 스레드마다 수백 만개의 put 또는 take 연산을 실행하도록 하며, 버퍼의 크기도 다양하게 지정해보고, 다양한 플랫폼에서 PutTakeTest 프로그램으로 테스트를 거친다면 put과 take 메소드에 관한 한 부족함이 없는 완벽한 테스트 결과를 얻을 수 있을 것이다.  

테스트 프로그램은 스레드가 교차 실행되는 경우의 수를 최대한 많이 확보할 수 있도록 CPU가 여러 개 장착된 시스템에서 돌려보는 게 좋다. 그렇다고 CPU가 수십 개 달렸다고 해서 서너 개의 CPU가 장착된 시스템에 비해 테스트 효율이 좋아진다고 보기는 어렵다. 절묘한 타이밍에 공유된 데이터를 사용하다 나타나는 오류를 찾으려면 CPU가 많이 있는 것보다 스레드를 더 많이 돌리는 편이 낫다. 스레드가 많아지면 실행 중인 스레드와 대기 상태에 들어간 스레드가 서로 교차하면서 스레드 간의 상호 작용이 발생하는 경우의 수가 많아지기 때문이다.  

```java
public class PutTakeTest {
  private static final ExecutorService pool = Executors.newCacheThreadPool();
  private final AtomicInteger putSum = new AtomicInteger(0);
  private final AtomicInteger takeSum = new AtomicInteger(0);
  private final CyclicBarrier barrier;
  private final BoundedBuffer<Integer> bb;
  private final int nTraials, nParis;

  public static void main(String[] args) {
    new PutTakeTest(10, 10, 100000).test(); // 예제 인자 값
    pool.shutdown();
  }

  PutTakeTest(int capacity, int npairs, int ntrials) {
    this.bb = new BoundedBuffer<Integer>(capacity);
    this.nTrials = ntrials;
    this.nPairs = npairs;
    this.barrier = new CyclicBarrier(npairs * 2 + 1);
  }

  void test() {
    try {
      for (int i = 0; i < nPairs; i++) {
        pool.execute(new Producer());
        pool.execute(new Consumer());
      }
      barrier.await(); // 모든 스레드가 준비될 때까지 대기
      barrier.await(); // 모든 스레드의 작업이 끝날 따까지 대기
      assertEquals(putSum.get(), takeSum.get());
    } catch (Exception e) {
      throw new RuntimeException(e);
    }
  }

  class Producer implements Runnable {
    public void run() {
      try {
        int seed = (this.hashCode() ^ (int)System.nanoTime());
        int sum = 0;
        barrier.await();
        for (int i = nTrials; i > 0; --i) {
          bb.put(seed);
          sum += seed;
          seed = xorShift(seed);
        }
        putSum.getAndAdd(sum);
        barrier.await();
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }
  }

  class Consumer implements Runnable {
    public void run() {
      try {
        barrier.await();
        int sum = 0;
        for (int i = nTrials; i > 0; --i) {
          sum += bb.take();
        }
        takeSum.getAndAdd(sum);
        barrier.await();
      } catch (Exception e) {
        throw new RuntimeException(e);
      }
    }
  }
}
```

미리 지정된 개수만큼의 연산을 실행하고 테스트틀 마치는 프로그램은 테스트 도중에 테스트 대상 클래스의 버그로 인해 예외가 발생하는 등의 상황에 맞닥뜨리면 테스트 프로그램이 종료되지 않고 계속해서 실행될 가능성이 있다. 이런 위험을 방지할 수 있는 가장 간편한 방법은 테스트 프로그램이 동작하는 시간에 제한을 두고 제한된 시간이 넘어가도 프로그램이 종료되지 않으면 테스트를 중단하도록 하는 방법이다. 얼마나 오래 기다려야 하는지는 주로 경험적인 시간으로 정하는 수밖에 없고, 만약 제한 시간을 넘기는 문제가 발생한다면 실제로 프로그램에 오류가 있는 것인지 아니면 좀 더 오래 기다려어야 하는지를 분산해서 확인해야 한다(이런 문제는 병렬 프로그램을 테스트할 때만 발생하는 문제는 아니다. 순차적으로 실행되는 프로그램이라 해도 무한 반복에 빠진 경우와 실행 시간이 오래 걸리는 경우를 구분해야 한다).  

##### 12.1.4. 자원 관리 테스트
<br/>
지금가지 테스트하고자 했던 목적은 모두 대상 클래스가 미리 정의된 스펙에 명시된 기능을 올바르게 수행할 수 있는지를 테스트하고자 함이었다. 말하자면 해야 할 일을 하는지 확인하는 테스트였다. 테스트 프로그램으로 테스트하고자 하는 두 번째 측면이 있는데, 바로 하지 말아야 할 일을 실제로 하지 않는지 테스트하는 일이다. 예를 들어 자원을 유출하는 등의 일을 해서는 안 될 것이다. 다른 객체를 사용하거나 관리하는 모든 객체는 더 이상 필요하지 않은 객체에 대한 참조를 필요 이상으로 긴 시간동안 갖고 있어서는 안 된다. 이처럼 데이터를 갖고 있는 객체의 참조를 해제하지 않고 유출되면 가비지 컬렉터가 메모리(또는 스레드, 파일 핸들, 네트웍 소켓, 데이터베이스 연결, 기타 여러 가지 제한적인 자원)를 확보할 수 없다. 따라서 결국에는 자원이 모자라게 되고 프로그램에서는 오류가 발생한다.  

자원을 관리하는 문제는 BoundedBuffer와 같은 클래스에서 더욱 큰 문제이다. 버퍼의 크기를 제한하는 이유는 오로지 프로듀서가 컨슈머보다 빨리 동작해서 자원이 고갈되는 상황을 방지하고자 하는 것뿐이기 때문이다. 서버의 크기를 제한해두면 너무 활발하게 동작하는 프로듀서의 호라동을 필요한 만큼 멈추도록 할 수 있으며, 그 결과 메모리와 기타 자원을 계속해서 소모하지 않도록 막을 수 있다.  

메모리를 원하지 않음에도 불구하고 계속해서 잡고 있는 경우가 있는지 확인하려면 애플리케이션이 사용하는 메모리의 상황을 들여다 볼 수 있는 힙 조사(heap inspection)용 도구를 사용해볼 만하다. 사용으로 판매하고 있거나 오픈소스로 공개돼 있는 여러 가지 도구를 사용하면 애플리케이션의 힙 사용 젇도를 상세하게 확인할 수 있다. 다음 예제의 testLeak 메소드는 힙 조사 도구가 코드를 넣을 수 있는 위치를 준비하고 있다. 해당하는 위치에는 힙 조사 도구가 생성한 코드가 들어가는데, 힙 조사 도구가 추가한 코드는 가비지 컬렉션을 강제로 실행하고 힙 사용량과 기타 메모리 사용 현황을 불러오는 기능을 담당한다. 기술적으로 보면 가비지 컬렉션을 강제로 하게 할 수는 없다. System.gc 메소드는 그저 JVM에게 가비지 컬렉션을 지금 하는 게  어떠냐 하고 부탁을 할 뿐이다. 특히나 핫스팟 VM은 -XX: +DisableExplicitGC 옵션을 사용해 System.gc 메소드에 반응하지 않도록 설정할 수도 있다.  

testLeak 메소드는 크기가 제한된 버퍼에 상당한 메모리를 차지하는 객체를 여러 개 추가하고, 추가된 객체를 제거한다. 그러면 버퍼에는 아무런 내용이 없기 때문에 2번 자리에서 측정한 메모리 사용량이 1번 위치에서 측정한 메모리 사용량과 비교할 때 거의 차이가 없어야 한다. 그런데 예를 들어 doExtract 메소드에서 뽑혀 나간 객체를 담고 있던 부분의 참조를 null로 세팅(items[i]=null)하지 않았다면 양쪽 지점에서 측정한 메모리 사용량이 분명히 다를 것이다(이런 부분은 참조 변수에 직접 null을 설정해야 하는 몇 안 되는 경우에 해당된다. 대부분의 경우에는 참조에 null을 설정하는 일이 도움이 되지 않을 뿐더러 오히려 피해가 생길 수 있다).  

```java
class Big { double[] data = new double(100000); }

void testLeak() throws InterruptedException {
  BoundedBuffer<Big> bb = new BoundedBuffer<Big>(CAPACITY);
  int heapSize1 = /* 힙 스냅샷 */;
  for (int i = 0; i < CAPACITY; i++)
    bb.put(new Big());
  for (int i = 0; i < CAPACITY; i++)
    bb.take();
  int heapSize2 = /* 힙 스냅샷 */;
  assertTrue(Math.abs(heapSize1-heapSize2) < THRESHOLD);
}
```

##### 12.1.5. 콜백 사용
<br/>
클라이언트가 제공하는 코드에 콜백 구조를 적용하면 테스트 케이스를 구현하는 데 도움이 된다. 콜백 함수는 객체를 사용하는 동안 중요한 시점마다 그 내부의 값을 확인시켜주는 좋은 기회로 사용할 수 있다. ThreadPoolExecutor 클래스가 작업을 담당하는 Runnable과 스레드를 생성하는 ThreadFactory의 여러 콜백 함수를 호출하는 예를 들면 알기 쉽다.  

스레드 풀이 제대로 동작하는지 테스트하려면 실행 정책에 맞게 여러 측면에서 적절한 수치를 뽑아낼 수 있는지를 테스트하면 된다. 예를 들어 스레드를 생성해야 할 시점이라면 스레드가 생성돼야 하고, 스레드 생성 시점이 아니라면 스레드가 생성돼서는 안 된다. 원하는 기능 모두를 완벽하게 테스트할 수 있는 프로그램을 작성하려면 상당한 양의 노력을 들여야 할 수 있다. 하지만 테스트하고자 하는 기능 가운데 대부분은 그 테스트 프로그램을 상대적으로 간단하게 작성할 수 있는 경우가 많다.  

먼저 ThreadPoolExecutor 클래스에서 테스트용으로 작성한 TestingThreadFactory를 사용해 스레드를 생성하도록 해보자. 다음 예제는 TestingThreadFactory 클래스는 생성된 스레드의 개수를 세는 기능을 갖고 있고, 실제 테스트 케이스에서는 TestingThreadFactory가 알고 있는 스레드의 개수가 올바른지를 확인해 볼 수 있다. 이에 더해 기능이 추가된 Thread 객체를 생성하도록 TestingThreadFactory를 좀더 변경하면 생성된 스레드가 언제 종료되는지를 추적할 수 있다. 그러면 테스트 케이스는 없어져야 할 스레드가 적절한 시점에 올바르게 사라지는지도 확인할 수 있다.  

```java
class TestingThreadFactory implements ThreadFactory {
  public final AtomicInteger numCreated = new AtomicInteger();
  private final ThreadFactory factory = Executors.defaultThreadFactory();

  public Thread newThread(Runnable r) {
    numCreated.incrementAndGet();
    return factory.newThread(r);
  }
}
```

예를 들어 코어 풀 크기(core pool size)가 최대 풀 크기(maximum pool size)보다 작게 설정돼 있다면 실행할 대상이 늘어날 때마다 스레드의 개수가 함께 들어나야 한다. 스레드 풀에 오래 실행될 작업을 많이 추가해두면, 다음 예제에서 보다시피 스레드 개수가 올바르게 늘어나는지 등의 수치를 확인하기에 충분할 만큼 어느 정도 시간을 벌어주는 역할 을 한다.  

```java
public void testPoolExpansion() throws InterruptedException {
  int MAX_SIZE = 10;
  TestingThreadFactory threadFactory = new TestingThreadFactory();
  ExecutorService exec = Executors.newFixedThreadPool(MAX_SIZE, threadFactory);

  for (int i = 0; i < 10 * MAX_SIZE; i++)
    exec.execute(new Runnable() {
      public void run() {
        try {
          Thread.sleep(Long.MAX_VALUE);
        } catch (InterruptedException e) {
          Thread.currentThread().interrupt();
        }
      }
    });
  for (int i = 0; i < 20 && threadFactory.numCreated.get() < MAX_SIZE; i++)
    Thread.sleep(100);
  assertEquals(threadFactory.numCreated.get(), MAX_SIZE);
  exec.shutdownNow();
}
```

##### 12.1.6. 스레드 교차 실행량 확대
<br/>
병렬 프로그램에서 나타나는 오류는 대부분 발생 확률이 상당히 낮은 경우가 많다. 따라서 병렬 프로그램의 오류를 찾아내는 테스트 과정은 수치와의 싸움이긴 하지만, 그래도 확률을 높여 좀더 많은 기회를 만들어 낼 방법이 없는 것은 아니다. 아미 몇 개의 CPU 프로세서가 장착된 하드웨어에서 CPU의 개수보다 많은 수의 스레드로 동작하는 프로그램이 단일 CPU 하드웨어나 CPU의 개수가 많은 수의 스레드로 동작하는 프로그램이 단일 CPU 하드웨어나 CPU의 개수가 많은 하드웨어에서 동작하는 프로그램보다 교차 실행(interleaving)되는 양이 훨씬 많다는 점을 언급했었다. 이와 비슷하게 CPU 프로세서의 개수, 운영체제, 프로세서 아키텍처 등을 다양하게 변경하면서 테스트해보면 특정 시스템에서만 발생하는 오류를 찾아낼 수 있다.  

스레드의 교차 실행 젇도를 크게 높이고 그와 동시에 테스트할 대상 공간을 크게 확대시킬 수 있는 트릭이 있는데, 바로 공유된 자원을 사용하는 부분에서 Thread, yield 메소드를 호출해 컨텍스트 스위치가 많이 발생하도록 유도할 수 있다(JVM 표준에 따르면 Thread.yield 메소드를 구현할 때 아무런 동작이 없는 no-op 인스트럭션으로 구현할 수도 있도록 돼 있기 때문에 이 방법은 플랫폼별로 그 양상이 다르게 나타날 수 있다. 그 대신 0보다 크지만 아주 짧은 시간동안 실행을 멈추도록 Thread.sleep 메소드를 호출하는 방법을 사용하면 전체적인 실행 속도가 약간 떨어질 수 있지만 컨텍스트 스위칭을 일으키는 효과는 훨씬 명확하게 나타날 수 있다). 다음 예제에 나타난 코드를 보면 한쪽 계좌에서 일정 금액을 다른 계좌로 이체하는데, 값을 변경하는 두 번의 연산 가운데 "양쪽 계좌 잔액의 합은 항상 0이다"라는 명제가 일치하지 않는 시점이 존재한다. 작업 도중에 Thread.yield 메소드를 호출해주면 공유된 데이터를 사용할 때 적절한 동기화 방법을 사용하지 않은 경우 특정한 타이밍에 발생할 수 있는 버그가 실제로 노출되는 가능성을 높일 수 있다. Thread.yield 메소드를 호출하는 코드와 같이 테스트할 때는 사용하다가 상용으로 사용할 때는 해당 코드를 제거해야 하는 경우에는 관점 지향 프로그래밍(AOP, Aspect Oriented Programming) 기법을 활용해 간편하게 처리할 수 있다.  

```java
public synchronized void transferCredits(Account from, Account to, int amount) {
  from.setBalance(from.getBalance() - amount);
  if (random.nextInt(1000) > THRESHOLD)
    Thread.yield();
    to.setBalance(to.getBalance() + amount);
}
```

#### 12.2. 성능 테스트
<br/>
성능 테스트 프로그램은 대부분 기능 테스트의 확장된 버전인 경우가 많다. 물론 오류가 있는 코드의 성능을 테스트하는 우스운 상황을 미연에 방지하려면 성능 테스트 프로그램에 최소한의 기본적인 기능 테스트 코드를 추가해 두는 것도 좋은 방법이다.  

성능 테스트와 기능 테스트 프로그램 간에는 중복되는 부분이 있을 수밖에 없기는 하지만 양쪽의 목표는 확연하게 서로 다르다. 성능 테스트는 특정한 사용 환경 시나리오를 정해두고, 해당 시나리오를 통과하는 데 얼마만큼의 시간이 걸리는지를 측정하고자 하는 데 목적이 있다. 여기에서 의미가 있는 사용 환경 시나리오를 찾아내는 일이 그다지 쉬운 일은 아니다. 가장 이상적인 시나리오라면 테스트하고자 하는 대상 클래스가 실제 애플리케이션에서 사용되는 환경을 최대한 동일하게 반영해야 한다.  

일부 상화엥서는 테스트해야 할 시나리오가 명확하게 눈에 보이기도 한다. 크기가 제한된 버퍼는 거의 모든 경웨 프로듀서-컨슈머 패턴에 사용된다. 따라서 프로듀서가 생성한 데이터가 컨슈머에게 얼마나 빠르게 넘어가는지를 테스트하면 된다. 이런 경우라면 기존의 PutTakeTest 클래스를 좀더 확장시켜 바로 성능 테스트로 활용할 수 있다.  

성능 테스트의 두 번째 목적은 바로 성능과 관련된 스레드의 개수, 버퍼의 크기 등과 같은 각종 수치를 뽑아내고자 함이다. 이와 같은 수치는 플랫폼의 특성(예를 들어 CPU 프로세서의 종류, CPU 프로세서의 스탭 레벨(stepping level), CPU의 개수, 메모리 용량 등)에 따라 민감하게 바뀔 수도 있으므로 동적으로 수치를 알아내 적용할 수 있다면 가장 좋겠지만, 일반적으로는 적절한 값을 사용하면 대부분의 플랫폼에서 거의 비슷한 결과를 얻을 수 있다고 널리 알려져 있다.  

##### 12.2.1. PutTakeTest에 시간 측정 부분 추가
<br/>
지금부터 PutTakeTest에 추가할 가장 중요한 기능은 바로 실행하는 데 걸린 시간을 측정하는 기능이다. 단일 연산을 실행한 이후 해당 연산에 대한 시간을 구하기보다는, 단일 연산을 굉장히 많이 실행시켜 전체 실행 시간을 구한 다음 실행했던 연산의 개수로 나눠 단일 연산을 실행하는 데 걸린 평균 시간을 찾는 방법이 더 정확하다. 작업 스레드가 시작하고 종료하는 부분에 이미 CyclicBarrier를 적용해뒀기 때문에, 다음 예제과 같이 배리어(barrier)가 적용되는 부분에서 시작 시간과 종료 시간을 측정할 수 있도록 기존 클래스를 확장할 수 있다.  

배리어에서 시간을 측정하는 기능을 갖고 있는 배리어 액션(barrier action)을 사용하도록 하려면 CyclicBarrier를 초기화하는 부분에 다음과 같이 원하는 배리어 액션을 지정한다.  

```java
this.timer = new BarrierTimer();
this.barrier = new CyclicBarrier(nParis * 2 + 1, timer);
```

```java
public class BarrierTimer implements Runnable {
  private boolean started;
  private long startTime, endTime;

  public synchronized void run() {
    long t = System.nanoTime();
    if (!started) {
      started = true;
      startTime = t;
    } else
      endTime = t;
  }

  public synchronized void clear() {
    started = false;
  }

  public synchronized long getTime() {
    return endTime - startTime;
  }
}
```

배리어 기반의 타이머를 사용하도록 변경한 test 메소드는 다음 예제에서 볼 수 있다.  

TimePutTakeTest를 실행해보면 몇 가지 결과를 얻을 수 있다. 먼저 여러 가지 설정을 사용했을 때 프로듀서와 컨슈머 간에 데이터를 얼마나 빠르게 넘겨줄 수 있느냐 하는 수치를 얻을 수 있다. 그리고 스레드이 개수가 많아질 때 크기가 제한된 버퍼가 얼마나 확장성을 받쳐주는지 알아볼 수 있다. 또한 버퍼의 크기를 얼마로 제한해야 최고의 성능을 내는지도 알아볼 수 있다. 이런 궁금증을 풀어내려면 여러 가지 인자에 다양한 값을 설정하면서 테스트 프로그램을 실행해봐야 하는데, 다음 예제와 같은 테스트 실행 프로그램을 사용하면 편리하다.  

다음 그림을 보면 CPU가 4개 장착된 하드웨어에서 버퍼의 크기를 1, 10, 100, 1000으로 변경하면서 실행한 결과가 그래프로 나타나 있다. 일단 버퍼 크기를 1로 지정한 경우에는 성능이 크게 떨어진다는 사실을 쉽게 알 수 있다. 버퍼 크기가 1인 경우 각 스레드가 대기 상태에 들어가고 나오면서 아주 적은 양의 작업밖에 할 수 없기 때문에 성능이 떨어지는 것은 당연하다. 여기에서 버퍼의 크기만 늘려주면 성능은 빠르게 증가하지만, 10을 넘는 크기를 지정하면 버퍼의 크기에 비해 성능이 향상되는 정도가 떨어지는 것을 볼 수 있다.  

스레드의 개수를 크게 늘린다 해도 성능이 별로 떨어지지 않는다는 수치를 보고나면 약간 혼동스러운 결과라고 생각할 수도 있겠다. 그 원인은 테스트 프로그램의 결과만으로는 이해하기가 어렵고, 테스트 프로그램이 실행되는 동안 perfbar 등의 유틸리티를 사용해 CPU의 성능을 보다 보면 쉽게 이해할 수 있다. 스레드가 많이 실행되고 있다 하더라도, 실제 작업을 하는 양은 그다지 많지 않고 대신 스레드가 대기 상태에 들어갔다 나왔다하는 동기화를 맞추느라 CPU 용량의 대부분을 사용하기 때문이다. 그러다보니 더 많은 스레드를 사용해 동일한 작업을 처리하도록 해도 성능에는 별 악영향이 없다는 섣부른 판단을 내리는 경우도 많다.  

하지만 이와 같은 결과를 놓고 크기가 제한된 버퍼를 사용하는 프로듀서-컨슈머 패턴의 구조라면 언제든지 스레드를 추가해도 좋다는 방향으로 해석하기 전에 조심해야 한다. 여기서 사용했던 테스트 프로그램은 실제 애플리케이션을 시뮬레이션하기에는 너무나 인공적으로 만들어졌기 때문이다. 프로듀서는 큐에 샇을 항목을 생성할 때 거의 아무런 작업 없이 계속해서 객체만 생성한다. 프로듀서는 큐에 쌓을 항목을 생성할 때 거의 아무런 작업 없이 계속해서 객체만 생성한다. 컨슈머 역시 큐에서 가져온 항목을 사용한다는 말이 무색할 정도로 아무 작업을 하지 않는다. 프로듀서-컨슈머 패턴으로 움직이는 실제 애플리케이션을 생각한다면 항목을 생성하고 사용하는 과정에서 무시할 수 없을 만큼 상당한 양의 작업이 이뤄질 것이다. 그럼 테스트 프로그램에서 봤던 여유가 줄어들 것이며 스레드를 너무 많이 추가했다는 여파를 눈으로 직접 확인할 수 있게 된다. TimedPutTakeTest의 주 목적은 프로듀서-컨슈머 패턴의 프로그램에서 프로듀서와 컨슈머 간에 값을 넘겨줄 때 얼마만큼의 성능을 낼 수 있는지, 병목이 있다면 어디에 있는지를 알아내는 정도에 그친다는 사실을 알아두자.  

```java
public void test() {
  try {
    timer.clear();
    for (int i = 0; i < nPairs; i++) {
      pool.execute(new Producer());
      pool.execute(new Consumer());
    }
    barrier.await();
    barrier.await();
    long nsPerItem = timer.getTime() / (nPairs * (long)nTrials);
    System.out.println("Throughput: " + nsPerItem + " ns/item");
    assertEquals(putSum.get(), takeSum.get());
  } catch (Exception e) {
    throw new RuntimeException(e);
  }
}
```

```java
public static void main(String[] args) throws Exception {
  int tpt = 100000; // 스레드별 실행 횟수
  for (int cap = 1; cap <= 1000; cap *= 10) {
    System.out.println("Capacity: " + cap);
    for (int pairs = 1; pairs <= 128; pairs *= 2) {
      TimedPutTakeTest t = new TimedPutTakeTest(cap, pairs, tpt);
      System.out.print("Pairs: " + pairs + "\t");
      t.test();
      System.out.print("\t");
      Thread.slepp(1000);
      t.test();
      System.out.println();
      Thread.sleep(1000);
    }
  }
  pool.shutdown();
}
```

<img src="/assets/images/Java_Concurrency_In_Practice/12-1.jpg" width="100%" height="100%"/>
<br/>

##### 12.2.2. 다양한 알고리즘 비교
<br/>
BoundedBuffer 클래스가 꽤나 탄탄하고 성능도 괜찮게 구현돼 있음은 분명하지만, ArrayBlockingQueue나 LinkedBlockingQueue 등의 클래스에 비해서는 성능이 떨어진다(성능이 떨어지다보니 JDK 라이브러리에 포함되지 못한 것이 아니겠는가?). java.util.concurrent 패키지에 포함돼 있는 클래스의 알고리즘은 주의 깊게 선택하고 튜닝돼 있다. 튜닝 과정에는 물론 여기에서 소개한 것과 같은 테스트 프로그램이 일부 활용됐을 것이며, 상식적으로 생각할 수 있는 최고의 성능을 낼 수 있으면서 다양한 종류의 기능을 제공한다. 병렬 프로그램에 익숙한 전문가라면 물론 일부 기능을 제외하면서 원하는 기능의 속도를 높으도록 새로운 클래스를 구현할 수도 있을 것이다. BoundedBuffer 클래스의 속도가 떨어지는 가장 큰 이유는 바로 put과 take 연산 양쪽에서 모두 스레드 경쟁을 유발할 수 있는 연산, 예를 들어 세마포어를 확보하거나 락을 확보하고 세마포어를 다시 해제하는 등의 연산을 사용하기 때문이다. 고성능 클래스가 사용하는 알고리즘을 보면 스레드 간의 경쟁을 유발할 수 있는 부분이 훨씬 적다.  

다음 그림을 보자. TimePutTakeTest 테스트 프로그램을 약간 변형시키고, 듀얼 하이퍼스레드 CPU가 장착된 하드웨어에서 버퍼 크기가 256인 클래스 3개를 비교 실행한 결과이다. 이 테스트 결과를 보면 LinkedBlockingQueue 클래스가 ArrayBlockingQueue보다 확장성이 약간 더 좋다고 보이는데, 언뜻 생각하기에는 약간 이상한 결과라고 의심이 될 수도 있다. 연결 큐(linked queue)는 새로운 항목을 추가할 때마다 버퍼 항목을 메모리에 새로 할당받아야 하며, 따라서 배열 기반의 큐보다 더 많은 일을 해야 하기 때문이다. 그런데 객체를 할당하는 부하가 더 크고 가비지 컬렉션에도 부하가 더 걸린다고 해도, 잘 튜닝된 연결 리스트 알고리즘을 사용하면 큐의 처음과 끝 부분에 서로 다른 스레드가 동시에 접근해 사용할 수 있다. 따라서 연결 리스트 기반의 큐는 put과 take 연산에 대해서 배열 기반의 큐보다 병렬 처리 환경에서 훨신 안정적으로 동작한다. 그리고 메모리 할당 작업은 일반적으로 스레드 내부에 한정돼 있기 때문에 메모리를 할당한다 해도 스레드 간의 경쟁을 줄일 수 있는 알고리즘의 확장성이 더 높을 수밖에 없다(전통적인 성능 튜닝 방법과 관련해 알고 있던 상식이 확장성의 측면에서는 오히려 성능이 떨어지는 결과를 가져올 수 있는 사례이다).  

<img src="/assets/images/Java_Concurrency_In_Practice/12-2.jpg" width="100%" height="100%"/>
<br/>

##### 12.2.3. 응답성 측정
<br/>
지금까지 병렬 프로그램의 성능을 측정할 때 가장 중요한 항목인 처리량을 측정하는 방법에 대해서 살펴봤다. 하지만 일부 상황에서는 단일 작업을 처리하는 데 얼마만큼의 시간이 걸리는지를 측정하는 일이 더 중요한 경우도 있다. 그리고 단일 작업 처리 시간을 측정할 때는 보통 측정 값의 분산(variance)을 중요한 수치로 생각한다. 간호 평균 처리 시간은 길지만 처리 시간의 분산이 작은 값을 유지하는 일이 더 중요할 수 있기 때문이다. 즉 '예측성' 역시 중요한 성능 지표 가운데 하나임을 알아야 한다. 처리 시간에 대한 분산을 구해보면 "100밀리초 안에 작업을 끝내는 비율이 몇 % 정도인가?" 와 같은 서비스 품질(quality of service)에 대한 수치를 결과로 제시할 수 있다.  

서비스 시간에 대한 분산을 시각적으로 표현할 수 있는 가장 효과적인 방법은 바로 작업을 처리하는 데 걸린 시간을 히스토그램으로 그려보는 방법이다. 분산은 사실 평균을 구하는 것보다 아주 조금 더 복잡한 난이도를 갖고 있다. 다시 말해 작업 처리 시간을 모두 더할 뿐만 아니라 각 처리 시간을 목록으로 관리하고 있어야 한다. 그런데 개별 작업을 처리하는 속도가 아주 빠르다면 통계 값에 오류가 생기기 쉽다(예를 들어 컴퓨터가 갖고 있는 시간 측정의 최소 단위와 비슷한 시간 안에 작업을 처리할 수 있다면 작업 처리 시간을 제대로 측정할 수가 없다). 이런 오류를 방지할 수 있도록 put과 take 등의 연산을 일정 개수로 묶어 일괄 처리하고, 일괄 처리하는 데 걸린 시간을 하나의 작업 시간으로 묶어서 생각하도록 하자.  

<img src="/assets/images/Java_Concurrency_In_Practice/12-3.jpg" width="100%" height="100%"/>
<br/>

위의 그림을 보면 버퍼의 크기를 1000으로 지정하고 256개의 병렬 작업이 각각 1000개의 항목을 버퍼에 넣는데, 한쪽은 공정한 세마포어를 사용(회색 부분)하고 다른쪽은 불공정 세마포어를 사용(흰색 부분)해 테스트한 결과가 나타나 있다. 불공정한 방법을 사용한 테스트 결과에는 작업 처리 시간이 최소 104밀리초에서 최대 8,714밀리초까지 걸렸다. 보다시피 최소 시간과 최대 시간의 차이가 80배가 넘는다. 최소 시간과 최대 시간의 차이를 줄이러면 동기화 코드에 공정성을 높이면 된다. BoundedBuffer의 경우 세마포어를 생성할 때 공정한 모드로 초기화시켜 공정성을 높일 수 있다. 위의 그림에서 보다시피 이렇게 동기화 부분에 공정성을 높이고 나면 처리 시간의 분산 값을 엄청나게 줄여주는 효과(이제 최소 38,194밀리초에서 최대 38,207밀리초 사이에 처리된다)를 볼 수 있지만 처리 속도가 크게 떨어진다는 역효과를 가져온다(좀더 일반적인 종류의 테스트를 더 오랜 시간 동안 실행시켜 그 결과를 받아보면 아마도 성능 저하가 훨씬 크게 나타날 것이다).  

만약 버퍼 크기를 굉장히 작게 잡고 사용한다면 매번 연산마다 모두 컨텍스트 스위칭이 발생하고, 따라서 컨텍스트 스위칭 부하가 엄청나게 늘어나서 결국 불공정 동기화 방법을 사용한다 해도 실행 속도가 크게 느려진다는 사실을 살펴본 바 있다. 공정하기 때문에 속도가 느려지는 상황은 스레드가 대기 상태에 들어가기 때문이라고 생각할 수 있다. 따라서 이번 테스트에서 버퍼 크기를 1로 지정하고 다시 실행해보면 불공정한 세마포어를 사용해도 공정한 세마포어를 사용한 경우와 거의 비슷한 속도로 느려진다는 결과를 얻는다. 다음 그림을 보면 이 경우에 공정함의 문제가 평균 실행 시간을 크게 늦추거나 실행 시간의 분산을 훨신 낮게 바꿔주지는 못한다는 사실이 나타나 있다.  

결국 스레드가 아주 빡빡한 동기화 요구사항 때문에 계속해서 대기 상태에 들어가는 상황이 아니라면 불공정한 세마포어를 사용해 처리 속도를 크게 높일 수 있고, 반대로 공장한 세마포어를 사용해 처리 시간의 분산을 낮출 수 있다. 공정성 문제로 속도가 빨라지거나 분산 값이 줄어드는 정도가 굉장히 심한 편이기 때문에 세마포어를 사용할 때는 항상 어느 방법을 사용할 것인지 결정해야만 한다.  

<img src="/assets/images/Java_Concurrency_In_Practice/12-4.jpg" width="100%" height="100%"/>
<br/>

#### 12.3. 성능 측정의 함정 피하기
<br/>
이론적으로는 성능 테스트 프로그램을 작성하는 일은 그다지 어렵지 않다. 일반적인 사용 시나리오를 알아보고, 알아낸 사용 시나리오를 여러 차례 실행시키고, 실행하는 데 걸린 시간을 측정하면 된다. 하지만 실제로 테스트 프로그램을 작성할 때는 성능을 올바로 나타내지 못하는 잘못된 수치를 뽑아내는 잘못된 코딩 방법으로 프로그램을 작성하지 않도록 주의해야 한다.  

##### 12.3.1. 가비지 컬렉션
<br/>
가비지 컬렉션이 언제 실행될 것인지는 미리 알고 있을 수가 없으며, 따라서 시간을 측정하는 테스트 프로그램이 동작하는 동안 가비지 컬렉션 작업이 진행될 가능성도 높다. 테스트 프로그램이 총 N번의 작업을 실행하는데 N번의 작업을 실행하는 동안은 가비지 컬렉션이 진행되지 않았다 해도 N+1번째에 가비지 컬렉션이 진행될 수도 있다. 따라서 테스트 실행 횟루를 살짝 변경하기만 해도 테스트당 실행 시간은 엉터리 값으로 크게 바뀔 수 있다.  

가비지 컬렉션 때문에 테스트 결과가 올바르지 않게 나오는 경우를 막을 수 있는 두 가지 방법을 생각해보자. 먼저 테스트가 진행되는 동안 가비지 컬렉션 작업이 실행되지 않도록 하는 방법이 있을 수 있겠고, 아니면 테스트가 진행되는 동안 가비지 컬렉션이 여러 번 실행된다는 사실을 명확히 하고 테스트 결과에 객체 생성 부분이나 가비지 컬렉션 부분을 적절하게 반영하도록 하는 방법이 있겠다. 일반적으로는 후자의 방법이 많이 사용되는데, 테스트 프로그램을 훨씬 긴 시간동안 실행할 수 있으며 실제 상황에서 나타나는 성능을 좀더 가깝게 반영하기 때문이다.  

프로듀서-컨슈머 패턴으로 구성된 대부분의 애플리케이션은 상당한 양의 객체를 메모리에 할당하고 가비지 컬렉션 부하도 큰 편이다. 프로듀서는 계속해서 큐에 쌓을 항목을 생성해내고, 컨슈머는 큐에서 뽑아낸 항목을 사용하는 구조이기에 어쩔 수 없다. 따라서 BoundedBuffer 클래스를 대상으로 테스트 프로그램을 적당히 오랜 시간동안 동작시키면 일정 횟수 이상 가비지 컬렉션이 동작할 것이며, 실제 적용할 때와 유사한 성능 결과를 얻을 수 있겠다.  

<img src="/assets/images/Java_Concurrency_In_Practice/12-5.jpg" width="100%" height="100%"/>
<br/>

##### 12.3.2. 동적 컴파일
<br/>
자바 언어와 같이 동적으로 컴파일하면서 실행되는 언어로 작성된 프로그램은 C나 C++와 같이 정적으로 컴파일된 상태에서 실행되는 언어로 만들어진 프로그램보다 그 성능을 측정하는 테스트 프로그램을 작성하기도 어렵거니와 결과를 해석하기도 여러운 면이 있다. 핫스팟(HotSpot) JVM이나 기타 최근 사용되는 JVM은 바이트코드(byte code) 인터프리트(interprete) 방식과 동적 컴파일(dynamic compilation) 방법을 혼용해 사용한다. 예를 들어 클래스의 바이트코드를 처음 읽어들인 이후에는 인터프리터를 통해 바이트코드를 실행한다. 그리고 일정 시점이 지난 이후 메소드가 특정 횟수 이상 자주 실행된다는 판단이 들면 동적 컴파일러가 해당 메소드를 기계어 코드로 컴파일한다. 컴파일이 완료되면 그 이후에는 인터프리트하는 대신 컴파일된 코드를 직접 실행시킨다.  

그런데 컴파일 작업이 언제 실행되는지는 알 수 없다. 실행 시간을 측정하는 테스트 프로그램은 대상 클래스의 코드가 모두 컴파일된 이후에 실행돼야 마땅하다. 대부분의 애플리케이션은 실제 사용할 때 필요한 거의 모든 메소드가 컴파일된 상태에서 실행된다고 봐야 하는데, 이런 상황에서 인터프리트되는 코드의 실행 속도는 측정할 가치가 거의 없기 때문이다. 하지만 테스트 프로그램이 시간을 측정하는 도중에 컴파일러가 동적으로 메소드 코드를 컴파일하도록 놔둔다면 두 가지 측면에서 테스트 결과에 오류가 생길 가능성이 있다. 먼저 컴파일하는 과정에서 CPU를 상당 부분 소모할 것이 분명하며, 또한 인터프리트되는 코드와 컴파일된 코드, 컴파일하는 시간을 모두 테스트 결과에 포함시키면 일관성이 부족한 결과 값을 얻을 수밖에 없다. 위의 그림을 보면 동적 컴파일과 관련한 여러 가지 요소가 테스트 결과 값을 어떻게 뒤섞여 놓는지 알 수 있다. 위의 그림에 나타난 세 가지 항목은 각각의 조건에서 동일한 횟수의 테스트 모듈을 실행하는 과정을 보여준다. A는 컴파일하지 않고 계속해서 인터프리터로 실행하는 모습을, B는 인터프리터로 실행하다 중간에 컴파일해 실행되는 모습을, C는 B보다 먼저 컴파일을 진행하고 실행되는 모습을 의미한다. 보다시피 컴파일 작업이 언제 실행되는지가 전체 실행 시간에 큰 영향을 미치고, 그에 따라 단일 연산에 소모되는 시간 역시 영향을 미친다. JVM에 따라 애플리케이션 스레드에서 컴파일하기도 하고, 백그라운드 스레드에서 컴파일하기도 한다. 어느 방법이긴 성능 테스트에 서로 다른 양상으로 영향을 준다.  

컴파일된 프로그램 코드는 때에 따라 디컴파일(인터프리트하는 코드로 복원)하고 다시 재컴파일하는 과정을 거치는 경우도 있다. 예를 들어 이전 컴파일 과정에서 가정했던 사항이 변경됐거나, 아니면 실제로 실행해보면서 얻은 성능 평가 결과를 놓고 다른 최적화 방법을 적용해 다시 컴파일하도록 하기도 한다.  

컴파일된 코드와 컴파일되지 않은 코드 때문에 성능 측정치가 올바르지 않게 나타나는 상황을 예방하는 가장 간단한 방법은 테스트 프로그램을 긴 시간(최소한 몇 분 이상)동안 실행시켜 컴파일될 부분은 모두 컴파일되고, 추가로 컴파일하거나 인터프리터로 실행되는 코드를 최소화하는 방법이다. 또 다른 방법으로는 시간을 측정하지 않는 '워밍업'하는 테스트를 한 번 미리 실행시켜 필요한 코드를 모두 컴파일시키고, 그 이후에 시간을 측정하는 실제 테스트 프로그램을 실행시켜 성능 측정치를 뽑아내는 방법도 있다. 핫스팟 JVM을 사용하는 경우라면 -XX:+PrintCompilation 옵션을 사용해 동작 컴파일 작업이 실행될 때 메시지를 출력시킬 수 있다. 이렇게 메시지를 출력시켜보면 컴파일이 모두 끝나고 성능을 측정하기 좋은 시간이 언제쯤인지 추정해볼 수 있다.  

동일한 테스트 프로그램을 하나의 JVM에서 여러 번 실행해보면 그 가운데 적당한 테스트 결과를 골라낼 수 있다. 초기에 실행했던 결과는 워밍업 과정이라고 보고 제외하고, 그 이후의 측정 결과를 봤을 때 측정 값의 변동이 크다면 똑같은 테스트를 실행하는 데 걸리는 시간이 왜 일정하게 유지되지 않는지에 대한 원인을 찾아봐야 할 것이다.  

JVM은 일상적인 내부 작업을 처리하기 위해 여러 개의 백그라운드 스레드를 사용한다. 서로 관련되지 않으면서 한번에 CPU를 중점적으로 사용하는 기능을 테스트하고자 한다면 테스트를 여러 번 실행하는 사이마다 약간의 쉬는 시간을 두어 JVM이 일상 작업을 처리할 수 있도록 배려하는 게 좋다. 그래댜 시간을 측정하는 테스트가 진행될 때 꼭 해야 하는 JVM 내부 작업을 처리하느라 CPU를 소모하고 그로 인해 테스트 실행 시간 결과 값에 오류가 발생하는 일이 줄어든다(반대로 동일한 테스트를 여러 번 실행하는 것과 같이 서로 관련 있는 여러 건의 작업을 테스트할 때 JVM이 일상적인 작업을 할 수 있도록 시간을 배려하도록 하면 실제 상황보다 훨씬 나고간적인 결과를 얻을 가능성이 있다).  

##### 12.3.3. 비현실적인 코드 경로 샘플링
<br/>
런타임 컴파일러는 컴파일할 코드에 대한 최적화 정보를 얻기 위해 실행 과정에서 여러 가지 성능 값을 추출한다. JVM은 더 나은 코드를 생성할 수 있도록 프로그램 실행에 관련된 특정 정보를 사용하기도 한다. 예를 들어 특정 프로그램에서 사용하는 메소드 M을 컴파일했을 때의 결과 코드와 다른 프로그램에서 사용하는 동일한 메소드를 컴파일한 결과가 다를 수 있다. 특히 어떤 경우에는 JVM이 코드를 컴파일할 때 일시적으로만 효과를 발휘할 수 있는 몇 가지 가정을 설정하고 그에 따라 컴파일하기도 한다. 그리고 만약 설정했던 가정이 어느 시점 이후에 올바르지 않은 가정이라고 판단되면 컴파일한 코드를 무효로 하고 새로 컴파일하기도 한다. 예를 들어 JVM은 단형 호출 전환(monomorphic call transformation)이라는 방법을 사용하기도 하는데, 현재 읽어들인 모든 클래스 가운데 특정 메소드를 상속받아 오버라이드하는 클래스가 없다면 가상 메소드 호출 부분을 직접 메소드 호출로 변경해 동작시킨다. 그런데 나중에 추가로 읽어들인 클래스 가운데 해당 메소드를 상속받아 오버라이드하는 클래스가 있다면 이전에 컴파일돼 있던 내용을 무효화한다.  

따라서 특정 애플리케이션에서 사용하는 시나리오 패턴만을 묘사해 테스트하는 것보다는 그와 유사한 다른 시나리오 패턴도 한데 묶어서 테스트하는 일도 중요한 부분이다. 이렇게 테스트하지 않았다면, 예를 들어 완전히 단일 스레드에서 동작했어야 할 테스트 프로그램에 동적 컴파일러가 일반적인 서버 애플리케이션처럼 최소한의 병렬성을 필요로 하는 상황에 맞게 특별한 최적화 기법을 사용해 코드를 컴파일해 문제가 발생할 수도 있다. 그래서 단일 스레드 프로그램의 성능을 테스트하고자 할 때도 단일 스레드 프로그램의 성능뿐만 아니라 멀티스레드 애플리케이션의 성능도 함께 테스트하는 것이 좋다(TimedPutTakeTest에서는 테스트할 때 필요한 최소 스레드 개수가 2개이기 때문에 이런 문제가 발생하지 않는다).  

##### 12.3.4. 비현실적인 경쟁 수준
<br/>
병렬 애플리케이션은 두 종류의 작업을 번갈아가며 실행하는 구조로 동작한다. 여러 스레드가 공유하는 큐에서 다음 처리할 작업을 뽑아내는 것과 같이 공유된 데이터에 접근하는 종류의 작업이 있고, 큐에서 가져온 작업을 실행하는 것과 같이 스레드 내부의 데이터만을 갖고 실행되는 작업이 있다(물론 큐에서 가져온 작업을 처리할 때 공유된 데이터를 사용하지 않아야 한다). 전체 작업을 두 종류의 작업으로 구분해 봤을 때 각각 얼마만큼의 비율을 차지하는지에 따라 경쟁의 수준이 달라지고 성능과 확장성 측면에서 굉장히 다른 결과를 내놓게 된다.  

이를테면 N개의 스레드가 서로 공유하는 작업 큐에서 작업을 가져다 실행한다고 하고, 각 작업은 CPU 중심의 작업이며 오랜 시간 동안 실행된다고 하면(그리고 공유된 데이터는 별로 사용하지 않는다면) 스레드 간의 경쟁이 거의 발생하지 않을 것이다. 실행 성능은 CPU의 처리 속도에 굉장히 의존하게 된다. 반대로 개별 작업이 아주 짧은 시간 안에 빠르게 실행된다면 작업 큐에서 서로 작업을 가져가려고 경쟁이 많이 발생할 것이며 전체적인 실행 성능은 동기화 방법에 따라 좌지우지된다.  

병렬 테스트 프로그램에서 실제 상황과 유사한 결과를 얻으려면 직접적으로 알고자 하는 부분, 즉 병렬 처리 작업을 조율하는 동기화 부분의 성능과 함께 스레드 내부에서 실행되는 작업의 형탱도 실제 애플리케이션과 비슷한 특성으 띠고 있어야 한다. 실제 애플리케이션의 작업 스레드가 처리하는 개별 작업이 테스트 프로그램의 가상 개별 작업과 다른 특성을 갖고 있다면 성능상의 병목이 어느 지점인지를 파악할 때 전혀 엉뚱한 지점을 지목하게 될 수도 있다. synchronizedMap 메솓로 생성한 락 동기화 기반의 Map을 놓고 봤을 때 락을 확보하려는 부분에서 스레드 간의 경쟁이 많이 발생하느냐 별로 발생하지 않느냐의 차이가 성능 측정치에 지대한 영향을 미친다. 어찌됐건 애플리케이션의 작업 구조상 공유된 데이터에 접근해 사용하는 부분보다 스레드 내부 작업의 양이 상대적으로 많다고 하면 스레드 경쟁 정도가 크게 떨어지고, 경쟁이 적어지니 전반적으로 괜찮은 성능을 낼 수 있을 것이다.  

이런 관점에서 보면 TimedPutTakeTest 테스트 프로그램에서 사용했던 모델은 일부 애플리케이션의 구조를 묘사하기에는 그다지 훌륭하지 못하다고 볼 수 있다. 스레드 내부에서 별다른 작업을 하지 않기 때문에 성능 측정치는 스레드 간의 경쟁 정도에 좌우되며, 프로듀서와 컨슈머 간에 큐를 사용해 데이터를 주고받는 애플리케이션 모두가 이와 같이 스레드 내부의 작업이 적다고 볼 수는 없기 때문이다.  

##### 12.3.5. 의미 없는 코드 제거
<br/>
(어던 프로그래밍 언어를 사용하는 간에) 최적화 컴파일러는 의미 없는 코드(dead code)(실행 결과에 영향을 주지 않는 코드)를 제거하는 데 뛰어난 능력을 갖고 있으며, 따라서 훌륭한 성능 측정 프로그램을 작성하는 일이 그다지 쉬운 일은 아니다. 일반적으로 성능 측정을 하는 동안에는 실제적인 계산 작업을 거의 하지 않기 때문에 최적화 컴파일러 입장에서는 1차 제거 대상이 될 수 있다. 대부분의 경우에는 최적화 컴파일러가 의미 없는 코드를 자동으로 제거해주면 그보다 더 좋은 일이 있겠냐마는, 성능 측정 프로그램을 실행하는 경우에는 최적화된 이후 예상했던 것보다 훨씬 적은 코드만이 실행될 수 있기 때문에 큰 문제가 되기도 한다. 운이 좋다면 최적화 컴파일러가 실행 코드 대부분을 제거해버리고 너무나 빠르게 실행된다는 성능 측정 결과를 내놓을 수도 있는데, 이런 경우에는 결과 값을 보고 값이 이상하다는 걸 한눈에 알 수 있을테니 그나마 다행이다. 그렇지 않다면 어느 정도의 코드만 제거되고 빠르게 실행된다는 결과를 내놓고, 테스터는 뭔가 다른 그럴싸한 이유를 붙여 성능이 잘 나온다고 판단하는 오류를 범할지도 모른다.  

의미 없는 코드 제거 기능은 정적으로 컴파일하는 언어로 성능을 측정하는 경우에도 비슷한 문제점을 발생시킬 수 있다. 하지만 컴파일 과정을 미리 진행하기 때문에 생성된 기계어 코드를 들여다보면 컴파일러가 최적화 과정에서 코드를 얼마만큼 제거해 버렸는지를 정확하게 파악할 수 있다. 반대로 동적인 컴파일 방법을 사용하는 언어의 경우에는 이와 같이 컴파일된 기계어 코드를 살펴보기 어려워 이런 정보를 얻기가 어렵다.  

여러 가지 성능 테스트를 실행해보면 핫스팟 JVM의 클라이언트 모드(-client)보다 서버 모드(-server)로 실행했을 때의 결과가 훨씬 좋다. 서버 모드의 동적 컴파일러가 클라이언트 모드의 컴파일러보다 더 호율적인 코드를 생성할 수 있다는 것 뿐만 아니라 의미 없는 코드를 최적화하는 능력도 더 낫기 때문이다. 다만 성능을 측정할 때 잘 동작해 의미 없는 코드를 대부분 최적화하곤 했지만, 실제 애플리케이션을 실행할 때는 그다지 최적화하지 못할 수도 있다. 그래도 CPU가 여러 개 장착된 시스템에서는 상용 서버시스를 실행하거나 성능 측정 프로그램을 실행하는 경우 모두 -client 대신 -server 옵션을 지정하는 게 좋다. 그저 의미 없는 코드로 제거돼 버리지 않고 제대로 실행돼 올바른 결과를 낼 수 있도록 테스트 프로그램을 주의 깊게 작성하기만 하면 된다.  

훌륭한 성능 측정 프로그램을 작성하려면 최적화 컴파일러가 의미 없는 코드를 제거하는 과정에 성능 측정상 필요한 부분까지 제거하지 않도록 약간의 편법을 써야 할 필요가 있다. 그러려면 프로그램 코드가 만들어내는 모든 결과 값을 프로그램 어디에선가 사용하도록 해야 한다. 물론 그 때문에 추가적으로 동기화를 해야 하거나 더 많은 자원을 소모하도록 하지는 않는 것이 좋다.  

예를 들어 PutTakeTest에서 큐에 추가하거나 큐에서 제거하는 모든 항목마다 체크섬 값을 계산하고, 나중에 모든 스레드의 체크섬을 합산해 올바르게 동작했는지 확인하는 부분이 있었다. 이처럼 올바르게 동작하는지 여부를 확인하기 위해 사용했던 체크섬 값도 결국 실제로 사용하는 부분은 없기 때문에 최적화 컴파일러에 의해 의미 없는 코드로 제거될 가능성이 있는 부분이다. 원래 이 체크섬 부분은 버퍼 알고리즘이 정상적으로 동작하는지를 확인하기 위한 것이었지만, 콘솔에 출력하는 기능을 넣어서 실제로 사용하는 값이라는 사실을 최적화 컴파일러에게 알려주자. 그렇다 해도 I/O 기능을 호출하면 성능 측정 값에 영향을 줄 수 있기 때문에 성능 테스트를 실행하는 동안에, 특히 시간을 측정하는 동안에는 I/O 기능을 사용하지 않는 편이 좋다.  

I/O를 사용해야 하지만 성능에 영향을 줄 수 있으니 사용하기도 곤란할 때는 다음과 같은 방법을 써보자. 아래 코드와 같이 hashCode 메소드로 현재 클래스의 해시 값을 가져오고, 그 해시 값과 임의의 숫자, 예를 들어 System.nanoTime과 같은 값을 비교하도록 한다. 그러면 두 값이 거의 일치할 일이 없을 것이며, 비교문 안에 I/O를 사용하는 출력문을 적어두면 성능 측정에 영향을 줄 만한 작업은 하지 않으면서 최적화 컴파일러가 의미 없는 코드로 판단해 제거해 버리는 일을 방지할 수 있다.  

```java
if (foo.x.hashCode() == System.nanoTime())
  System.out.println(" ");
```

보다시피 위의 코드에서 사용했던 비교 구문이 참일 가능성은 거의 없다. 만약 비교 구문이 참이 된다 해도 그저 콘솔에 의미 없는 공백 문자 하나를 출력할 뿐이다(print 메소드는 println 메소드를 호출할 때까지 메모리에 버퍼링하도록 돼 있으므로 hashCode 값과 System.nanoTime 값이 일치하는 경우가 발생한다 해도 실제로 공백 문자열이 콘솔에 즉시 출력되는 일은 없다).  

프로그램 내부에서 계산했던 모든 값을 어떤 방법으로건 사용해야 할 뿐만 아니라 그 사용처를 추측할 수 없어야 한다. 괜찮은 최적화 컴파일러가 동작하고 있을 때 만약 결과 값을 예측할 수 있었다면 최적화 컴파일러가 매번 계산 과정을 실행하는 대신 미리 계산된 값을 사용하기도 한다. 이번에 PutTakeTest 프로그램을 작성할 때는 이런 가능성을 고려하고 작업했지만 정적인 입력 값을 갖고 동작하는 테스트 프로그램은 항상 최적화 컴파일러가 미리 계산된 값을 사용할 수 있다는 사실을 주의해야 한다.  

#### 12.4. 보조적인 테스트 방법
<br/>
훌륭한 테스트 프로그램을 작성하면 테스트 프로그램을 통해 "모든 버그를 찾아낼 수 있다"고 믿고 싶겠지만, 그런 프로그램은 현실에 존재하지 않는다. NASA에서는 어떤 상용 업체가 흔히 할 수 있는 것보다 훨씬 많은 엔지니어링 자원을 개발보다 테스트 부분에 더 많이 투자하고 있다고 하지만(개발자 1명당 20여 명의 테스트 인력이 있다고 한다), 그래도 NASA에서 만든 프로그램 역시 버그가 있다. 프로그램이 조금만 복잡해진다면 아무리 테스트를 많이 한다고 해도 오류를 모두 잡아내는 일이 불가능하다.  

테스팅의 목적은 '오류를 찾는 일'이 아니라 대상 프로그램이 처음 작성할 때 설계했던 대로 동작한다는, 즉 '신뢰성을 높이는 작업'이라고 봐야 한다. 결국 모든 버그를 찾아내는 일이 불가능하기 때문에, 품질보증(QA, quality assurance) 전략으로는 항상 가능한 테스트 자원 내에서 최대한의 신뢰성을 끌어낼 수 있도록 방향을 잡아야 한다. 순차적으로 실행되는 프로그램에 비해 병렬 프로그램은 오류가 발생할 가능성이 더 높기 때문에 병렬 프로그램에서 순차적 프로그램과 비슷한 수준의 신뢰도를 확보하려면 훨씬 많이 테스트해야 한다. 지금까지는 단위 테스트와 성능 테스트를 효과적으로 작성할 수 있는 방법을 중점적으로 살펴봤다. 테스팅 작업은 병렬 프로그램이 제대로 동작한다는 신뢰도를 높이는 과정에서 아주 중요한 역할을 담당하고는 있지만, 사용할 수 있는 여러 가지 QA 방법 가운데 하나로 인식해야 한다.  

QA 방법론에도 여러 가지가 있는데, 각 방법을 적절하게 활용하면 오류의 유형에 따라 좀더 효율적으로 오류를 발견할 수 있다. 코드 리뷰(code review)나 정적 분석(static analysis)과 같이 상호 보완적인 여러 가지 테스트 방법을 사용하면 한두 가지 방법만 사용했을 때보다 프로그램에 대한 신뢰도를 크게 높일 수 있다.  

##### 12.4.1. 코드 리뷰
<br/>
병렬 프로그램의 오류를 찾아내고자 할 때 단위 테스트와 성능 테스트만큼이나 중요하고 또 효과적인 테스트 방법은 바로 여러 명이 모여서 코드를 하나하나 살펴보는 코드 리뷰이다(물론 코드 리뷰가 테스팅 장체를 대신할 수 있는 일은 아니다). 병렬 프로그램에 대한 테스트 프로그램을 작성해 안전성 오류를 최대한 찾아내도록 하는 일이나, 테스트 프로그램을 자주 실행하면서 계속해서 오류가 없다는 사실을 확인하는 것 외에도 코드를 직접 작성하지 않은 다른 사람에게 코드를 살펴보도록 하는 일도 게을리해서는 안 된다. 병렬 프로그램 전문가라 해도 항상 오류를 범할 수 있기 때문이다. 시간을 충분히 가치를 안겨주는 일이다. 아주 사소한 경쟁 조건(race condition)을 찾아내는 등의 일은 여러 개의 테스트 프로그램을 작성하는 것보다 병렬 프로그램 전문가가 코드를 들여다 보는 것으로 더 쉽게 찾아내는 경우가 많다(게다가 JVM의 구현 방법에 따라 다를 수 있는 세밀한 부분이나 프로세서 메모리 모델과 같은 문제 때문에 특정 하드웨어나 소프트웨어상에서는 오류가 발생하지 않을 수 있기 때문에 더 그렇다). 코드 리뷰를 하다 보면 그 외에도 더 많은 이득을 볼 수 있다. 예를 들어 단순하게 문제점을 찾아내는 것뿐만 아니라 코드 리뷰와 함께 소스코드의 주석문에 코드에 대한 더 자세한 설명을 추가하는 일을 함께 하면서 나중에 반드시 발생할 유지보수 비용을 낮출 수도 있다.  

##### 12.4.2. 정적 분석 도구
<br/>
정적 분석 도구(static analysis tools)가 점점 발전하면서 형식적인 테스팅은 물론 코드 리뷰에서 활용할 수 있는 효과적인 도구가 돼가고 있다. 정적 코드 분석 방법은 코드를 실행하지 않고 그 자체로 분석하며, 코드 감사(audit) 도구를 사용하면 클래스 파일 내부에 흔히 알려진 여러 가지 버그 패턴(bug patterns) 가운데 해당하는 부분이 있는지를 확인해준다. 오픈소스로 공개된 FindBugs와 같은 정적 분석 도구에는 버그 패턴 감지기(bug pattern detector)가 포함돼 있고, 단위 테스트나 섣능 테스트, 코드 리뷰 등의 과정에서 빼먹기 쉬운 다양한 종류의 일반적인 코딩 오류를 발견할 수 있다.  

정적 분석 도구를 실행시키면 경고할만한 부분을 목록으로 리포팅 해주며, 리포팅된 부분이 오류인지 아닌지는 반드시 사람이 직접 확인해야 한다. 전통적으로 보자면 lint와 같은 도구를 사용했을 때 개발자가 겁이나 먹게 만드는 너무나 많은 잘못된 경고 리포트를 출력하곤 했지만, FindBugs와 같은 도구는 잘못된 경고를 최대한 줄일 수 있도록 삳당 부분 튜닝을 거쳤다. 정적 분석 도구는 아직 굉장히 초창기를 벗어나지 못하고 있지만(특히 개발 툴과 연동하거나 개발 과정에 쉽게 통합되기 어려운 점 등), 프로그램 테스트 과정에서 상당히 쓸모있는 도구라고 인식되고 있다.  

현 시점에서 FindBugs에는 다음과 같은 일반적인 병렬 프로그램의 오류에 대한 감지기(detector)를 내장하고 있으며, 새로운 감지기가 계속해서 추가되고 또 업그레이드 되고 있다.  

+ 일관적이지 않은 동기화  
다수의 클래스는 클래스 내부의 변수를 자신의 암묵적인 락으로 동기화한다는 동기화 정책을 사용한다. 그런데 특정 변수가 동기화 블록 내부에서 사용되는 경우가 많으면서 일부는 동기화 블록 외부에서도 사용되는 경우가 있다면 해당 변수에 대해 동기화 정책이 정확하게 적용되지 않은 경우일 수 있다.  

지금까지는 자바 클래스에 구체적인 병렬 처리 스펙을 갖고 있지 않기 때문에 정적 분석 도구는 동기화 정책에 대해서 추측할 수밖에 없다. 나중에 &#64;GuardedBy와 같은 어노테이션(annotation)이 표준화된 이후에는 감사 도구에서 변수와 락 간의 관계를 분석하는 등의 방법으로 동기화 정책을 추측하는 대신 어노테이션을 직접 분석해 정적 분석 결과의 품질을 향상시킬 수 있을 것이다.  

+ Thread.run 호출  
Thread 클래스는 Runnable 인터페이스를 구현하고 있기 때문에 run 메소드를 갖고 있다. 그렇지만 일반적으로 Thread.start 메소드를 호출하는 대신 Thread.run 메소드를 호출하는 일은 잘못된 방법인 경우가 많다.  

+ 해제되지 않은 락  
암묵적인 락 대신 명시적인 락(explicit lock)은 해당 락을 확보한 블록이 실행을 마치고 빠져나갔다 해도 락이 자동으로 해제되지 않는다. 표준적으로 사용해야 하는 방법은 확보했던 락을 finally 구문에서 해제하는 방법이지만, 그렇지 않은 경우 실행 도중에 예외 상황이 발생했을 때 락이 해제되지 않을 가능성이 있다.  

+ 빈 synchronized 블록  
자바 메모리 모델상에서는 비어 있는 synchronized 블록이 의미가 있을 수 있겠지만, 실제로는 대부분 잘못 사용된 경우가 많다. 그리고 개발자가 어떤 의도에 빈 synchronized 블록을 사용했는지는 모르겠지만, 대부분의 경우 빈 synchronized 블록 대신 사용할만한 다른 방법이 있기 마련이다.  

+ 더블 체크 락(double-checked lock)  
더블 체크 락은 늦은 초기화(lazy initialization) 방법에서 발생하는 동기화 부하를 줄이기 위한 방법이다. 하지만 동기화 능력이 부족하기 때문에 공유된 변경 가능한 값을 읽어가는 등의 경우가 발생할 가능성이 있다.  

+ 생성 메소드에서 스레드 실행  
생성 메소드에서 새로운 스레드를 실행시키도록 한다면, 해당 클래스를 상속받았을 때 문제가 생길 수 있고, 또한 this 변수를 스레드에게 노출시킬 수 있다는 위험도 있다.  

+ 알림 오류  
notify나 notifyAll 메소드는 해당하는 조건 큐에서 대기하고 있는 스레드가 있다면, 객체의 상태가 변경돼 대기 중인 스레드가 대기 상태에서 풀려나도 좋다는 것을 알려주는 메소드이다. notify와 notifyAll 메소드는 항상 해당 조건과 관련된 상태가 변경됐을 때만 사용해야 한다. 예를 들어 synchronized 블록 내부에서 notify나 notifyAll 메소드를 호출하지만 상태를 변경하지 않은 상태라면 오류일 가능성이 높다.  

+ 조건부 대기 오류  
조건 큐(condition queue)에서 대기할 때는 필요한 락을 확보한 상태에서 상태 변수를 확인한 이후에 Object.wait나 Condition.await 메소드를 반복문으로 감싸는 구조로 구현해야 한다. 락을 확보하지 않은 상태이거나, 반복문으로 감싸지 않은 상태이거나, 상태 변수를 제대로 확인하지 않은 상황에서 Object.wait 메소드나 Condition.await 메소드를 호출하도록 돼 있다면 오류일 가능성이 높다.  

+ Lock과 Condition의 오용  
synchronized 블록에 락 인자를 넣을 때 Lock이라는 클래슬 이름을 지정한다거나, 아니면 Condition.await 메소드를 호출하는 대신 Condition.wait 메소드를 호출하는 등의 경우는 오타로 인해 동기화 구문을 잘못 작성하는 예이다(후자의 경우에는 실행할 때 애초에 IllegalMonitorStateException 예외를 띄울 것이기 때문에 테스트 프로그램을 통해 발견할 확률도 높다).  

+ 락을 확보하고 대기 상태 진입  
락을 확보한 상태에서 Thread.sleep 메소드를 호출하면 락을 필요로하는 다른 스레드 역시 아무 일도 못하고 멍하니 대기 상태에 들어가게 할 수 있으며, 따라서 나중에 활동성(liveness)에 심각한 영향을 줄 수 있다. 락을 두 개 확보한 상태에서 Object.wait 메소드를 호출하거나 Condition.await 메소드를 호출하는 경우 역시 비슷한 문제의 원인이 될 수 있다.  

+ 스핀 반복문  
아무런 일도 하지 않으면서 특정 변수의 값이 원하는 상태에 도달할 때까지 계속해서 반복하기만 하는 반복문(스핀 반복, spin loop)을 사용하면 CPU 자원을 엄청나게 소모할 뿐만 아니라 해당 변수가 volatile이 아니라면 심지어는 무한 반복에 빠질 수도 있다. 원하는 형태로 상태 변수 값이 변경되기를 기다리는 경우에는 래치(latch)나 여러 가지 조건부 대기 기능을 활용하는 편이 안전하다.  

##### 12.4.3. 관점 지향 테스트 방법
<br/>
지금 시점까지는 관점 지향 프로그래밍(AOP) 기법이 병렬 프로그래밍 분야에 적용되는 사례는 굉장히 제한적이었다. AOP 도구가 아직은 동기화 관련 지점에서 포인트컷(pointcut)을 지원하지 않고 있기 때문이다. 그렇다 해도 AOP를 사용하면 상태 변수의 값이 동기화 정책에 잘 맞는지를 확인하는 등의 작업을 하도록 적용해 볼 수 있겠다. 예를 들어 스레드 안전성이 보장되지 않는 스윙(Swing) 메소드를 호출하는지 확인하는 사례를 볼 수 있다. AOP의 특성상 코드를 따로 변경해야 할 필요가 없으니 이런 기법은 적용하기에도 간편하고, 사소한 변수 공개(publication) 상황이나 스레드 한정(confinement) 오류와 같은 부분을 찾아내기에 좋다.  

##### 12.4.4. 프로파일러와 모니터링 도구
<br/>
대부분의 상용 프로파일링 도구에는 스레드의 동작 상황을 살펴볼 수 있는 모듈이 포함돼 있다. 각 제품마다 기능과 효율성 등이 서로 다르긴 하지만 테스트 대상 프로그램이 도대체 무슨 일을 하고 있는지에 대한 내부적인 정보를 들여다 보는 좋은 방법이다(대부분의 프로파일링 도구는 대상 프로그램의 실행 성능과 실행 양상에 많은 악영향을 주는 단점이 있을 수도 있다). 대부분 각 스레드의 실행 상태(실행 가능 상태, 락 대기 상태, I/O 대기 상태 등)를 여러 가지 색으로 구분해 시간이 지나감에 따라 어떻게 실행되는지를 표시하는 기능을 갖고 있다. 이와 같은 결과 그래프를 보면 프로그램이 CPU를 얼마나 충분하게 활용하고 있으며, 만약 CPU를 충분히 사용하지 못한다면 어디에 그 원인이 있는지도 대략 알려준다(다수의 프로파일러는 또한 어느 락이 스레드 간의 경쟁을 유발하고 있는지도 알려준다고 한다. 하지만 이런 기능은 프로그램의 락 양상을 분석할 수 있을 만큼 날카로운 결과를 보여주지는 못한다고 생각된다).  

자바에 내장된 JMX 에이전트를 사용하는 것도 제한적이나마 스레드의 상태를 모니터링할 수 있는 방법이다. ThreadInfo 클래스를 보면 스레드의 현재 상태 ID를 갖고 있고, 만약 스레드가 대기 상태에 들어가 있다면 어떤 락을 놓고 대기 중인지도 알 수 있다. 그리고 '스레드 경쟁 모니터링' 기능이 켜져 있다면(성능이 크게 떨어질 수 있기 때문에 기본 값으로는 꺼져 있다) 락이나 알림을 대상으로 몇 번이나 대기 상태에 들어갔었는지의 값을 ThreadInfo 클래스에 저장하고, 대기 상태에서 소모한 시간의 전체 누적 값도 보관한다.  

### 요약
<br/>
병렬 프로그램이 올바르게 동작하는지 테스트하는 일은 굉장히 어려운 작업이다. 병렬 프로그램에서 발생하는 요류는 대부분 발생 확률이 아주 작은 타이밍 문제, 부하 문제, 아니면 기타 쉽게 발현되지 않는 여러 원인에 의해 발생하기 때문이다. 더군다나 테스트하는 대상에는 문제가 있었지만, 테스트 프로그램에서 발생하는 동기화 문제나 타이밍 문제 때문에 원래 테스트 대상이 갖고 있던 문제를 발견하지 못할 수도 있다. 병렬 프로그램이 충분한 성능을 내는지 테스트하는 작업도 굉장히 어렵다. 게다가 정적으로 컴파일되는 C와 같은 언어에 비해 동적으로 컴파일되는 자바 언어로 작성된 병렬 처리 프로그램은 성능을 측정하기가 더 어렵다. 성능을 측정할 때의 실행 시간이 동적인 컴파일, 가비지 컬렉션, 그리고 각종 최적화 방법 때문에 크게 변경될 수 있으며, 따라서 의도했던 대로 시간을 측정하기가 어렵기 때문이다.  

숨어 있는 버그를 상용 서비스 이전에 발견할 수 있는 가장 좋은 방법은 바로 전통적인 테스트 방법과 함께 코드 리뷰나 자동화된 분석 도구를 사용하는 방법이다. 각 방법은 모두 다른 방법이 잘 찾아내지 못하는 오류를 찾아낼 수 있으니, 다양한 방법을 동원해 테스트해야 오류를 최대한 줄일 수 있다.

### 13. 명시적인 락
<br/>
자바 5.0이 발표되기 전에는 공유된 데이터에 여러 스레드가 접근하려 할 때 조율할 수 있는 방법이라고는 synchronized 블록과 volatile 키워드뿐이었다. 이제 자바 5.0에는 또 다른 방법이 추가됐는데, 바로 ReentrantLock이다. 처음에는 마치 ReentrantLock이 암묵적인 락의 대용품인 정도로 생각할 수도 있겠지만, 암묵적인 락으로 할 수 없는 일도 처리할 수 있도록 여러 가지 고급 기능을 갖고 있다.  

#### 13.1. Lock과 ReentrantLock
<br/>
다음 예제에서 볼 수 있는 Lock 인터페이스는 여러 가지 락 관련 기능에 대한 추상 메소드(abstract method)를 정의하고 있다. Lock 인터페이스는 암묵적인 락과 달리 조건 없는(unconditional) 락, 폴링 락, 타임아웃이 있는 락, 락 확보 대기 상태에 인터럽트를 걸 수 있는 방법 등이 포함돼 있으며, 락을 확보하고 해제하는 모든 작업이 명시적이다. Lock을 구현하는 클래스는 항상 암묵적인 락과 비교해서 동일한 메모리 가시성(memory visibility)을 제공해야 하지만, 락을 거는 의미나 스케줄링 알고리즘, 순서를 지켜주는 기능, 성능 등의 측면에서 다른 면모를 갖고 있다.  

```java
public interface Lock {
  void lock();
  void lockInterruptibly() throws InterruptedException;
  boolean tryLock();
  boolean tryLock(lock timeout, TimeUnit unit) throws InterruptedException;
  void unlock();
  Condition newCondition();
}
```

ReentrantLock 클래스 역시 Lock 인터페이스를 구현하며, synchronized 구문과 동일한 메모리 가시성과 상호 배제 기능을 제공한다. ReentrantLock을 확보한다는 것은 synchronized 블록에 진입하는 것과 동일한 효과를 갖고 있고, ReentrantLock을 해제한다는 것은 synchronized 블록에서 빠져나가는 것과 동일한 효과를 갖는다. 그리고 ReentrantLock 역시 synchronized 키워드와 동일하게 재진입이 가능하도록 허용하고 있다. ReentrantLock은 Lock에 정의돼 있는 락 확보 방법을 모두 지원한다. 따라서 락을 제대로 확보하기 어려운 시점에 synchronized 블록을 사용할 때보다 훨씬 능동적으로 대처할 수 있다.  

이미 암묵적인 락 기능이 있는데 뭐하러 명시적인 락 클래스를 따로 만들었을까? 암묵적인 락만 사용해도 대부분의 경우에 별 문제 없이 사용할 수 있지만 기능적으로 제한되는 경우가 간혹 발생한다. 예를 들어 락을 확보하고자 대기하고 있는 상태의 스레드에는 인터럽트 거는 일이 불가능하고, 대기 상태에 들어가지 않으면서 락을 확보하는 방법 등이 꼭 필요한 상황이 있기 때문이다. 암묵적인 락은 또한 synchronized 블록이 끝나는 시점에 반드시 해제되도록 돼 있는데, 이런 구조는 코딩하기에 간편하고 예외 처리 루틴과 잘맞아 떨어지는 구조이긴 하지만 블록의 구조를 갖추지 않은 상황에서 락을 걸어야 하는 경우에는 적용하기가 불가능했다. 이런 단점이 있다고 해서 synchronized 키워드를 없애버려야 하는 것은 물론 아니지만, 일부 상황에서는 성능과 호라동성(liveness)을 높이려면 synchronized 구문보다 유연성이 높은 락 방법이 필요하다.  

Lock을 사용할 때 꼭 지켜야 하는, synchronized를 사용하는 암묵적인 락 보다 좀 복잡한 규칙도 있는데 바로 finally 블록에서 반드시 락을 해제해야 한다는 점이다. 락을 finally 블록에서 해제하지 않으면 try 구문 내부에서 예외가 발생했을 때 락이 해제되지 않는 경우가 발생한다. 이처럼 락을 사용할 때는 try 블록 내부에서 예외가 발생했을 때 어떤 일이 발생할 수 있는지에 대해 반드시 고민해봐야 한다. 만약 예외 때문에 해당 객체가 불안정한 상태가 될 수 있다면 try-catch 구문이나 try-finally 구문을 추가로 지정해 안정적인 상태를 유지하도록 해야 한다(명시적인 락뿐만 아니라 암묵적인 락을 사용할 때도 예외가 발생했을 때 어떤 결과가 수반될 수 있는지에 대해서 항상 고민해야 한다).  

락을 해제하는 기능을 finally 구문에 넣어두지 않은 코드는 언제 터질지 모르는 시한폭탄과 같다. 일단 그 상태로 만들고 나면 어디에서 언제 락을 해제해야 하는지에 대한 내용을 문서로 잘 남겨두지 않은 이상 나중에 폭탕이 터질 시점에 그 원인을 찾느라 코드를 모두 뒤져야 할 수도 있다. synchronized 구문을 제거하는 대신 기계적으로 ReentrantLock으로 대처하는 작업을 하지 말아야 하는 이유는 바로 이것이다. 즉 ReentrantLock을 사용하면 해당하는 블록의 실행이 끝나고 통제권이 해당 블록을 떠나는 순간 락을 자동으로 해제하지 않기 때문에 굉장히 위험한 코드가 될 가능성이 높다. 락을 블록이 끝나는 시점에 finally 블록을 사용해 해제해야 한다는 사실은 행동으로 지키기도 어렵지 않을 뿐더러 절대 잊어서는 안 되는 일이기도 하다. FindBugs 프로그램에는 '해제되지 않은 락'을 검출하는 기능, 즉 블록이 끝날 때 단 한 가지 경우라도 락이 해제되지 않을 가능성이 있는 부분을 찾아내는 기능이 포함돼 있다.  

##### 13.1.1. 폴링과 시간 제한이 있는 락 확보 방법
<br/>
tryLock 메소드가 지원하는 폴링 락 확보 방법이나 시간 제한이 있는 락 확보 방법은 오류가 발생했을 때 무조건적으로 락을 확보하는 방법돠 오류를 잡아내기에 훨씬 깔끔한 방법이라고 볼 수 있다. 암묵적인 락을 사용할 때에는 데드락이 발생하면 프로그램이 멈춰버리는 치명적인 상황에 이른다. 멈춘 프로그램을 다시 동작시키는 방법은 종료하고 다시 실행하는 방법뿐이고, 프로그램이 멈추지 않도록 하려면 올바르지 않은 락 순서를 제대로 맞춰 데드락이 발생하지 않도록 하는 수밖에 없다. 그런데 락을 확보할 때 시간 제한을 두거나 폴링을 하도록 하면 다른 방법, 즉 확률적으로 데드락을 회피할 수 있는 방법을 사용할 수 있다.  

락을 확보할 때 시간 제한을 두거나 폴링 방법(tryLock)을 사용하면 락을 확보하지 못하는 상황에도 통제권을 다시 얻을 수 있으며, 그러면 미리 확보하고 있던 락을 해제하는 등의 작업을 처리한 이후 락을 다시 확보하도록 재시도할 수 있다(아니면 최소한 데드락이 발생했다는 오류를 남기고 계속해서 실행하기라도 할 수 있다). 락 정렬 문제로 인해 데드락이 발생했을 때 이런 상황을 피해갈 수 있는 방법이 다음 예제에 소개돼 있다. 먼저 tryLock 메소드로 양쪽 락을 모두 확보하도록 돼 있지만, 만약 양쪽 모두 확보할 수 없다면 잠시 대기했다가 재시도하도록 돼 있다. 대기하는 시간 간격은 라이브락(livelock)이 발생할 확률을 최대한 줄일 수 있도록 고정된 시간 도는 임의의 시간만큼 대기한다. 만약 지정된 시간 이내에 락을 확보하지 못했다면 transferMoney 메소드는 오류가 발생했다는 정보를 리턴해주고, 적절한 통제하에서 오류를 처리할 수 있다.  

일정한 시간을 두고 객체를 관리하는 기능을 구현할 때 시간 제한이 있는 락을 적용하면 유용하다. 일정 시간 이내에 실행돼야 하는 코드에서 대기 상태에 들어갈 수 있는 블로킹 메소드를 호출해야 한다면 지정된 시간에서 현재 남아있는 시간만큼을 타임아웃으로 지정할 수 있겠다. 그러면 지정된 시간 이내에 결과를 내지 못하는 상황이 되면 알아서 기능을 멈추고 종료되도록 만들 수 있다. 반면 암묵적인 락을 사용했다면 일단 락을 확보하고자 시도하게 되면 멈출 수가 없기 때문에 정해진 시간 안에 처리해야 하는 작업을 맡기기엔 위험도가 높다.  

##### 13.1.2. 인터럽트를 걸 수 있는 락 확보 방법
<br/>
일정 시간 안에 처리해야 하는 작업을 실행하고 있을 때 타임아웃을 걸 수 있는 락 확보 방법을 유용하게 사용할 수 있는 것처럼, 작업 도중 최소시킬 수 있어야 하는 작업인 경우에는 인터럽트를 걸 수 있는 락 확보 방법을 유용하게 사용할 수 있다. 암묵적인 락을 확보하는 것과 같은 작업은 인터럽트에 전혀 반응하지 않는다. 이처럼 인터럽트에 전혀 반응하지 않는 방법밖에 없다면 작업 도중 취소시킬 수 있어야만 하는 기능을 구현할 때 굉장히 복잡해진다. lockInterruptibly 메소드를 사용하면 인터럽트는 그대로 처리할 수 있는 상태에서 락을 확보한다. 그리고 Lock 인터페이스에 lockInterruptibly 메소드를 포함하고 있기 때문에 인터럽트에 반응하지 않는 또 다른 종류의 블로킹 구조를 만들어야 할 필요가 없게 됐다.  

인터럽트에 대응할 수 있는 방법으로 락을 확보하는 코드의 구조는 일반적으로 락을 확보하는 모습보다 약간 복잡하긴 한데, 두 개의 try 구문을 사용해야 한다(인터럽트를 걸 수 있는 락 확보 방법에서 InterruptedException 예외를 던질 수 있도록 돼 있다면 표준적인 try-finally 락 구조를 그대로 사용할 수 있다). 타임아웃을 지정하는 tryLock 메소드 역시 인터럽트를 걸면 반응하도록 돼 있으며, 인터럽트를 걸어 취소시킬 수도 있어야 하면서 동시에 타임아웃을 지정할 수 있어야 한다면 tryLock을 사용하는 것만으로 충분하다.  

##### 13.1.3. 블록을 벗어나는 구조의 락
<br/>
암묵적인 락을 사용하는 경우에는 락을 확보하고 해제하는 부분이 완벽하게 블록의 구조에 맞춰져 있으며, 블록을 어떤 상태로 떠나는지에 관계 없이 락은 항상 자신을 확보했던 블록이 끝나는 시점에 자동으로 해제된다. 이렇게 자동으로 락을 해제하도록 돼있으면 프로그램 코드 분석 과정을 간략하게 줄일 수도 있고, 실수로 락을 해제하지 않아 발생하는 코드상의 오류를 줄일 수도 있다. 하지만 복잡한 구조의 프로그램에 락을 적용해야 할 때는 이보다 훨씬 유연한 방법으로 락을 걸 수 있어야 한다.  

락 스트라이핑(striping) 방법을 적용하면 해시 기반의 컬렉션 클래스에서 여러 개의 해시 블록을 구성해 블록별로 다른 락을 사용하기도 했다. 또한 연결 리스트(linked list) 역시 해시 컬렉션과 마찬가지로 락을 세분화할 수 있는데, 예를 들어 각각의 개별 노드마다 서로 다른 락을 적용할 수 있겠다. 그러면 각 스레드가 연결 리스트의 서로 다른 부분에 동시에 접근해 사용할 수 있다. 특정 노드에 대한 락은 해당 노드가 갖고 있는 링크 포인터와 실제 값을 보호한다. 따라서 링크를 따라가는 알고리즘을 실행하거나 리스트 연결 구조를 변경할 때는 특정 노드에 대한 락을 먼저 확보하고, 그 노드에 연결된 다른 노드에 대한 락을 확보한 다음 원래 노드에 대한 락을 해제해야 한다. 이런 방법은 핸드 오버 락(hand-over-hand locking) 또는 락 커플링(lock coupling)이라고 부른다.  

#### 13.2. 성능에 대한 고려 사항
<br/>
자바 5.0과 함께 ReentrantLock이 처음 소개됐을 때 암묵적인 락에 비해 훨씬 나은 경쟁 성능(contended performance)을 보여줬다. 여러 가지 동기화 기법에 있어서 경쟁 성능은 확장성을 높이는 데 가장 중요한 요소이다. 락과 그에 관련한 스케줄링을 관리하느라 컴퓨터의 자원을 많이 소모하면 할수록 실제 애플리케이션이 사용할 수 있는 자원은 줄어들 수밖에 없다. 좀더 잘 만들어진 동기화 기법일수록 시스템 호출을 더 적게 사용하고, 컨텍스트 스위치 횟수를 줄이고, 공유된 메모리 버스에 메모리 동기화 트래픽을 덜 사용하도록 하고, 시간을 많이 소모하는 작업을 줄여주며, 연산 자원을 프로그램에서 우회시킬 수도 있다.  

자바 6에서는 암묵적인 락을 관리하는 부분에 ReentrantLock에서 사용하는 것과 같이 좀더 향상된 알고리즘을 사용하며, 그에 따라 확장성에서 큰 차이가 나던 것이 많이 비슷해졌다. 다음 그림을 보면 솔라리스 운영체젲가 설치된 옵테론(Opteron) 프로세서 4개가 장착된 하드웨어에서 자바 5.0과 릴리즈 직전의 자바 6에서 각각 암묵적인 락과 ReentrantLock의 성능을 기뵤한 결과를 볼 수 있다. 그림에 표시된 곡선은 특정 자바 버전에서 암묵적인 락에 비해 ReentrantLock의 성능이 얼마나 좋아지는지를 그려내고 있다. 자바 5.0에서는 ReentrantLock의 성능이 훨씬 낫다는 점을 알 수 있지만, 자바 6에서는 ReentrantLock과 암묵적인 락의 차이가 많이 줄었다는 사실을 알 수 있다.  

자바 5.0에서는 암묵적인 락을 사용할 때 스레드 수가 1일 때(경쟁 없음)보다 스레드 개수가 늘어나면 성능이 크게 떨어진다. 대신 ReentrantLock을 사용하면 성능이 떨어지는 정도가 훨씬 덜하며, 따라서 확장성이 더 낫다고 볼 수 있겠다. 반면 자바 6에서는 얘기가 다르다. 암묵적인 락을 사용했다 해도 스레드 간의 경쟁이 있는 상황에서 성능이 그다지 떨어지지 않고, ReentrantLock을 사용할 때와 별반 차이가 없다.  

다음 그림을 보면 "X가 Y보다 더 빠르다"는 명제가 그다지 오래 가지 못한다는 사실을 알 수 있다. 성능과 확장성은 모두 CPU의 종류, CPU의 개수, 캐시의 크기, JVM의 여러 가지 특성 등에 따라 굉장히 민감하게 바뀌기 때문이며, 성능과 확장성에 영향을 주는 여러 가지 요인은 시간이 지나면서 계속해서 바뀌게 마련이다.  

<img src="/assets/images/Java_Concurrency_In_Practice/13-1.jpg" width="100%" height="100%"/>
<br/>

성능 측정 결과는 움직이는 대상이다. 바로 어제 X가 Y보다 빠르다는 결과를 산출했던 성능 테스트를 오늘 실행해보면 다른 결과를 얻을 수도 있다.  

#### 13.3. 공정성
<br/>
ReentrantLock 클래스는 두 종류의 공정성 설정을 지원한다. 하나는 불공정(nonfair) 락 방법이고, 다른 하나는 공정(fair)한 방법이다(기본 값은 불공정). 공정한 방법을 사용할 때는 요청한 순서를 지켜가면서 락을 확보하게 된다. 반면 불공정한 방법을 사용하는 경우에는 순서 뛰어넘기(barging)가 일어나기도 하는데, 락을 확보하려고 대기하는 큐에 대기 중인 스레드가 있다 하더라도 해제된 락이 있으면 대기자 목록을 뛰어 넘어 락을 확보할 수 있다(Semaphore 클래스 역시 공정하거나 불공정한 방법을 사용하도록 설정할 수 있다). 그렇다고 해서 불공정한 ReentrantLock이 일부러 순서를 뛰어넘도록 하지는 않으며, 대신 딱 맞는 타이밍에 락이 해제된다 해도 큐의 뒤쪽에 있어야 할 스레드가 순서를 뛰어넘지 못하게 제한하지 않을 뿐이다. 공정한 방법을 사용하면 확보하려는 락을 다른 스레드가 사용하고 있거나 동일한 락을 확보하려는 스레드가 큐에 대기하고 있으며 항상 큐의 맨 뒤에 쌓인다. 불공정한 방법이라면 락이 당장 사용 중인 경우에만 큐의 대기자 목록에 들어간다. tryLock 메소드를 통해 폴링하면 공정한 설정을 사용하고 있다고 해도 항상 순서 뛰어넘기가 가능하다.  

그런데 모든 락은 공정해야 하지 않을까? 일단 공정한 게 좋고 불공정한 건 좋지 않을 것 같은데, 락에서도 통하는 것일까? 히자만 락을 관리하는 입장에서 봤을 때 공정하게만 처리하다 보면 스레드를 반드시 멈추고 다시 실행시키는 동안에 성능에 큰 지장을 줄 수 있다. 실제로 보면 통계적인 공정함(대기 상태에 들어간 스레드는 언젠가는 반드시 락을 확보할 수 있다) 정도만으로도 충분히 괜찮은 결과를 얻을 수 있고, 그와 더불어 성능에도 훨씬 악영향이 적다. 일부 알고리즘은 제대로 동작하기 위해서는 반드시 순서를 지켜야 하는 경우도 있지만, 항상 공정하게 순서를 지켜야만 하는 것은 아니다. 대부분의 경우 공정하게 순서를 관리해서 얻는 장점보다 불공정하게 처리해서 얻는 성능상의 이점이 크다.  

<img src="/assets/images/Java_Concurrency_In_Practice/13-2.jpg" width="100%" height="100%"/>
<br/>

위의 그림을 보면 또 다른 성능 테스트 결과가 나타나 있다. 역시 Map을 대상으로 테스트 했는데 이번에는 HashMap을 놓고 공정한 락과 불공정한 락을 사용한 결과를 측정했다. 하드웨어는 4개의 옵테론 CPU가 장착되고 솔라리스가 설치된 시스템을 사용했으며, 결과 수치는 로그 스케일로 표시했다는 점을 알아두자. 스레드 개수가 4에서 8개 사이일 때 ConcurrentHashMap의 그래프가 출렁이는 모습을 볼 수 있다. 이렇게 결과 값에 변동이 생긴 이유는 측정상의 오류인 경우라고 생가되며, 우연히도 HashMap에 추가하는 항목의 해시 코드 값이 맞아 떨어지거나, 스레드 스케줄링에 약간 변화가 있거나, HashMap의 크기를 변경하는 작업이 진행됐거나, 가비지 컬렉션이 동작했거나, 기타 메모리 시스템에서 발생할 수 있는 추가 작업이 있었을 수 있다. 아니면 테스트가 실행되는 도중에 운영체제가 내부적으로 처리할 작업이 생겨서 실행 시간에 영향이 있었을 수도 있다. 실제로 성능을 테스트하는 경우에는 온갖 종류의 변수가 작용하기 마련이며, 이런 변수의 대부분은 그다지 신경 쓰거나 고려할 필요가 없는 종류이다. 실제 환경에서 테스트를 진행하면 이런 변수가 작용하지 않을 때가 없을 만큼 성능 측정 결과에는 항상 잡음이 섞이기 마련이다. 공정함을 얻기 위한 성능의 제약은 거의 두 자리수에 해당하는 수치이다. 꼭 그래야만 하는 경우가 아니라면 공정성을 일부러 지정해 성능을 떨어뜨리는 결과를 얻을 필요는 없다.  

스레드 간의 경쟁이 심하게 나타나는 상황에서 락을 공정하게 관리하는 것보다 불공정하게 관리하는 방법의 성능이 훨씬 빠른 이유는 대기 상태에 있던 스레드가 다시 실행 상태로 돌아가고 또한 실제로 실행되기까지는 상당한 시간이 걸리기 때문이다. 예를 들어 스레드 A가 락을 확보하고 있는 상태에서 스레드 B가 락을 확보하고자 한다고 해보자. 락은 현재 누군가가 사용하고 있기 때문에 스레드 B는 일단 대기 상태에 들어간다. 그리고 스레드 A가 락을 해제하면 스레드 B가 대기 상태에서 풀리면서 다시 락을 확보하고자 요청한다. 그러는 동안 스레드 C가 끼어들면서 동일한 락을 확보하고자 한다면 스레드 B 대신 스레드 C가 락을 밀 확보해버릴 확률이 꽤 높고, 더군다나 스레드 B가 본격적으로 실행되기 전에 스레드 C는 이미 실행을 마치고 락을 다시 해제시키는 경우도 가능하다. 이런 경우라면 모든 스레드가 원하는 성능을 충분히 발휘하면서 실행된 셈이다. 스레드 B는 사용할 수 있는 시점에 락을 확보할 수 있고, 스레드 C는 이보다 먼저 락을 사용할 수 있으니 처리량은 크게 늘어난다.  

공정한 방법으로 락을 관리할 때는 락을 확보하고 사용하는 시간이 상대적으로 걸거나 락 요청이 발생하는 시간 간격이 긴 경우에 유리하다. 락 사용 시간이 길거나 요청 간의 시간 간격이 길면 순서 뛰어넘기 방법으로 성능상의 이득을 얻을 수 있는 상태, 즉 락이 해제돼 있는 상테에서 다른 스레드가 해당 락을 확보하고자 대기  상태에서 깨어나고 있는 상태가 상대적으로 훨씬 덜 발생하기 때문이다.  

기본 ReentrantLock과 같이 암묵적인 락 역시 공정성에 대해 아무런 보장을 하지않는다. 히자만 통계적으로 공정하다는 사실을 놓고 보면 대부분의 락 구현 방법을 거의 모든 상황에 무리 없이 적용할 수 있다. 자바 언어 명세를 보면 JVM이 암묵적인 락을 구현할 때 반드시 공정하게 구현해야 한다고 명시하지는 않으며, 실제로 제품화돼 있는 JVM 가운데 공정하게 구현돼 있는 경우는 없다고 볼 수 있다. ReentrantLock 클래스가 공정성 문제를 불러 일으킨 건 아니다. 단지 계속 존재했던 문제를 명확하게 표현했을 뿐이다.  

#### 13.4. synchronized 또는 ReentrantLock 선택
<br/>
ReentrantLock은 락 능력이나 메모리 측면에서 synchronized 블록과 동일한 형태로 동작하면서도 락을 확보할 때 타임아웃을 지정하거나 대기 상태에서 인터럽트에 잘 반응하고 공정성 여부를 지정할 수도 있으며 블록의 구조를 갖추고 있지 않은 경우에도 락을 적용할 수 있는 유연함을 갖고 있다. ReentrantLock을 사용했을 때의 성능이 synchronized를 사용했을 때보다 낫다고 판단되는데, 자바 5.0에서는 아주 큰 차이로 성능이 앞섰지만 자바 6에서는 그다지 큰 차이가 있지는 않았다. 그렇다면 synchronized를 더 이상 사용하지 말고 모든 코드에서 ReentrantLock을 사용하도록 권장해야 하지 않을까? 일부 책이나 자료를 보면 이미 synchronized 블록을 '낡은' 방법이라고 보고 ReentrantLock을 사용하라고 권장하는 경우가 있다. 히지만 아직은 ReentrantLock의 장점을 너무 좋게 평가한 것이 아닐까 생각된다.  

암묵적인 락은 여전히 명시적인 락에 비해서 상당한 장점을 갖고 있다. 코드에 나타나는 표현 방법도 훨씬 익숙하면서 간결하고, 현재 만들어져 있는 대다수의 프로그램이 암묵적인 락을 사용하고 있으니 암묵적인 락과 명시적인 락을 섞어 슨다고 하면 코드를 읽을 때 굉장히 혼동될 뿐만 아니라 오류가 발생할 가능성도 더 높아진다. 분명히 ReentrantLock은 암묵적인 락에 비해 더 위험할 수도 있다. 만약 finally 블록에 unlock 메소드를 넣어 락을 헤제하도록 하지 않는다면 일단 프로그램이 제대로 동작하는듯 싶다가도 어디에선가 언젠가 분명히 터지고야 말 시한 폭탕을 심어두는 셈이다. ReentrantLock은 synchronized 블록에서 제공하지 않는 특별한 기능이 꼭 필요할 때만 사용하는 편이 안전하다고 본다.  

ReentrantLock은 암묵적인 락으로는 해결할 수 없는 복잡한 상황에서 사용하기 위한 고급 동기화 기능이다. 다음과 같은 고급 동기화 기법을 사용해야 하는 경우에만 ReentrantLock을 사용하도록 하자.  

1) 락을 확보할 때 타임아웃을 지정해야 하는 경우  
2) 폴링의 형태로 락을 확보하고자 하는 경우  
3) 락을 확보하느라 대기 상태에 들어가 있을 때 인터럽트를 걸 수 있어야 하는 경우  
4) 대기 상태 큐 처리 방법을 공정하게 해야 하는 경우  
5) 코드가 단일 블록의 형태를 넘어서는 경우  

그 외의 경우에는 synchronized 블록을 사용하도록 하자.  

자바 5.0에서는 synchronized 블록이 ReentrantLock에 비해 갖고 있는 장점이 하나 더 있다. 스레드 덤프를 떠보면 어느 스레드의 어느 메소드에서 어느 락을 확보하고 있고, 데드락에 걸린 스레드가 있는지, 어디에서 데드락에 걸렸는지도 표시해준다. 반면에 JVM 입장에서는 ReentrantLock이 어느 스레드에서 사용됐는지를 알 수 없기 때문에 동기화 관련 문제가 발생했을 때 JVM을 통해서 문제를 해결하는 데 도움이 될 정보를 얻기가 어렵다. 자바 6에서는 ReentrantLock의 이런 장점이 해소됐는데, 락이 등록할 수 있는 관리 및 모니터링 인터페이스가 추가됐다. 락을 관리 및 모니터링 인터페이스에 등록되고 나면 스레드 덤프에서 ReentrantLock의 상황을 알 수 있을 뿐만 아니라 외부의 관리나 디버깅 인터페이스를 통해 락의 움직임을 확인할 수도 있다. 이와 같은 정보를 디버깅에 활용할 수 있었다는 건 synchronized가 잠깐 동안이라도 가졌던 약간의 장점이긴 했다. 스레드 덤프에 출력되는 락 관련 정보가 없었더라면 많은 개발자 오류를 찾지 못해 막막해 한숨만 쉬는 경우가 많았을 것이다. 암묵적인 락을 사용할 때는 항상 특정 스택 프레임에 락이 연관돼 있었지만, ReentrantLock은 블록을 벗어나는 범위에도 사용할 수 있으며 따라서 특정 스택 프레임에 연결되지 않는다.  

이제 좀더 성능이 최적화되면 synchronized를 사용해도 ReentrantLock보다 성능이 더 나아지지 않을까 기대해본다. 특히 synchronized 구문은 JVM 내부에 내중돼 있기 때문에 ReentrantLock에 비해서 여러 가지 최적화를 적용하기가 쉽다. 예를 들어 스레드에 한정된 락 객체를 대상으로는 락 생략 기법을 적용할 수 있겠고, 락 확장 기법을 적용해 암묵적인 락으로 동기화된 부분에서 락을 사용하지 않도록 할 수도 있다. 자바 라이브러리에 포함된 클래스를 상대로 이런 최적화 기법을 적용한다는 것은 실현 가능성이 떨어진다. 머지 않은 시점에 자바 5.0으로 넘어갈 예정이거나 자바 5.0에서 ReentrantLock이 제공하는 확장성을 꼭 사용해야만 하는 경우가 아니라면, 다시 말해 단수히 성능이 나아지기를 기대하면서 synchronized 대신 ReentrantLock을 사용하는 일은 그다지 좋은 선택이 아니다.  

#### 13.5. 읽기-쓰기 락
<br/>
ReentrantLock은 표준적인 상호 배제(mutual exclusion) 락을 구현하고 있다. 즉 한 시점에 단 하나의 스레드만이 락을 확보할 수 있다. 하지만 이와 같은 상호 배제 규칙은 일반적으로 데이터의 오나전성을 보장하는 데 충분한 정도를 넘어서는 너무 엄격한 특징을 갖고 있으며, 따라서 병렬 프로그램의 장점을 필요 이상으로 제한하기도 한다. 상호 배제 규칙은 다시 말하자면 너무 보수적인 규칙이며, 쓰기 연산과 쓰기 연산이 동시에 일어나거나 쓰기와 일기 연산이 동시에 일어나는 경우를 제한할 뿐만 아니라 읽기와 읽기 연산이 동시에 일어나는 경우도 제한한다. 그런데 대부분의 경우 사용하는 데이터 구조는 읽기 작업이 많이 일어난다. 즉 데이터 내용은 변경될 수 있으며 간혹 변경되기도 하지만 대다수의 작업은 데이터 변경이 아닌 데이터 읽기 작업이다. 이런 상황에서는 락의 조건을 좀 풀어서 읽기 연산은 여러 스레드에서 동시에 실행할 수 있도록 해주면 성능을 크게 높일 수 있지 않을까? 해당 데이터 구조를 사용하는 모든 스레드가 가장 최신의 값을 사용하도록 보장해주고, 데이터를 읽거나 보고 있는 상테에서는 다른 스레드가 변경하지 못하도록 하면 아무런 문제가 없겠다. 이런 작업, 즉 읽기 작업은 여러 개를 한꺼번에 처리할 수 있지만 쓰기 작업은 혼자만 동작할 수 있는 구조의 동기화를 처리해주는 락이 바로 읽기-쓰기 락(read-write lock)이다.  

ReadWriteLock을 보면 두 개의 Lock 객체를 볼 수 있다. 하나는 읽기 작업용 락이고 다른 하나는 쓰기 작업용 락이다. ReadWriteLock으로 동기화된데이터를 읽어가려면 읽기 락을 확보해야 하고, 해당 데이터를 변경하고자 한다면 쓰기 락을 확보해야 한다. 메소드 모양만 본다면 두 개의 락 객체가 있는 게 아닌가 싶기도 하지만, 실제로 내부적으로는 하나의 ReadWriteLock 객체가 사용된다.  

```java
public interface ReadWriteLock {}
  Lock readLock();
  Lock writeLock();
}
```

ReadWriteLock에서 구현하고 있는 동기화 정책은 이미 소개한 것처럼 여러 개의 일기 작업은 동시에 처리할 수 있지만, 쓰기 작업은 한 번에 단 하나만 동작할 수 있다. Lock 인터페이스와 같이 ReadWriteLock 역시 성능이나 스케줄링 특성, 락 확보 방법의 특성, 공정성 문제, 기타 다른 락 관련 의미가 서로 다르게 반영되도록 새로운 클래스를 구현할 수 있게 돼 있다.  

ReadWriteLock은 특정 상황에서 병렬 프로그램의 성능을 크게 높일 수 있도록 최적화된 형태로 설계된 락이다. 실제로 멀티 CPU 시스템에서 읽기 작업을 많이 사용하는 데이터 구조에 ReadWriteLock을 사용하면 성능을 크게 높일 수 있다. 하지만 ReadWriteLock은 구현상의 복잡도가 약간 높기 때문에 최적화된 상황이 아닌 곳에 적용하면 상호 배제시키는 일반적인 락에 비해서 성능이 약간 떨어지기도 한다. 특정 상황을 놓고 ReadWriteLock을 사용하는 것이 적절한 것인지에 대한 대답은 성능 프로파일링을 통해서만 얻을 수 있다. 또한 ReadWriteLock 역시 읽기와 쓰기 작업을 동기화하는 부분에 Lock을 사용하기 때문에 성능을 측정해봤을 때 ReadWriteLock이 더 느리다고 판단되면 손쉽게 ReadWriteLock을 걷어내고 일반 Lock을 사용하도록 변경할 수 있다.  

읽기 락과 쓰기 락 간의 상호작용을 잘 활용하면 여러 가지 특성을 갖는 다양한 ReadWriteLock을 구현할 수 있다. ReadWriteLock을 구현할 때 적용할 수 있는 특성에는 다음과 같은 것이 있다.  

+ 락 해제 방법  
쓰기 작업에서 락을 해제했을 때, 대기 큐에 읽기 작업뿐만 아니라 쓰기 작업도 대기중이었다고 하면 누구에게 락을 먼저 넘겨줄 것인가의 문제. 읽기 작업을 먼저 할 것인지, 쓰기 작업을 먼저 처리할 것인지, 아니면 그냥 큐에 먼저 대기하고 있던 작업을 먼저 처리하도록 해도 좋다.  

+ 읽기 순서 뛰어넘기  
읽기 작업에서 락을 사용하고 있고 대기 큐에 쓰기 작업이 대기하고 있다면, 읽기 작업이 추가로 실행됐을 때 읽기 작업을 그냥 실행할 것인지? 아니면 대기 큐의 쓰기 작업 뒤에서 대기하도록 할 것인지 정할 수 있다.  

+ 재진입 특성  
읽기 작업과 쓰기 작업 모두 재진입(reentrant)이 가능한지?  

+ 다운그레이드  
특정 스레드에서 쓰기 락을 확보하고 있을 때, 쓰기 락을 해제하지 않고도 읽기 락을 확보할 수 있는지? 만약 가능하다면 쓰기 작업을 하던 스레드가 읽기 락을 직접 확보하고 읽기 작업을 할 수 있다. 즉 읽기 락을 확보하려는 사이에 다른 쓰기 작업이 실행되지 못하게 할 수 있다.  

+ 업그레이드  
읽기 락을 확보하고 있는 상태에서 쓰기 락을 확보하고자 할 때 대기 큐에 들어 있는 다른 스레드보다 먼저 쓰기 락을 확보하게 할 것인지? 직접적인 업그레이드 연산을 제공하지 않는 한 자동으로 업그레이드가 일어나면 데드락의 위험이 높기 때문에 ReadWriteLock을 구현하는 대부분의 경웨 업그레이드를 지원하지는 않는다(읽기 작업을 진행하던 두 개의 스레드가 동시에 쓰기 락으로 업그레이드하고자 한다면 양쪽 모두 읽기 락을 놓지 않게 된다).  

ReentrantReadWriteLock 클래스를 사용하면 읽기 락과 쓰기 락 모두에게 재진입 가능한 락 기능을 제공한다. ReentrantReadWriteLock 역시 ReentrantLock처럼 공정성 여부도 지정할 수 있다(기본 값은 불공정). 공정하게 설정한 락을 사용하는 경우에는 대기 큐에서 대기한 시간이 가장 긴 스레드에게 우선권이 돌아가는데, 즉 읽기 락을 확보하고 있는 상태에서 다른 스레드가 쓰기 락을 요청하는 경우, 쓰기 락을 요청한 스레드가 쓰기 락을 확보하고 해제하기 전에는 다른 스레드에서 읽기 락을 가져가지 못한다. 불공정하게 설정된 락을 사용하면 어느 스레드가 락을 가져가게 될지 알 수 없다. 쓰기 락을 호가보한 상태에서 읽기 락을 사용하는 다운그레이드는 허용되며, 읽기 락을 확보한 상태에서 쓰기 락을 사용하는 업그레이드는 제한된다(업그레이드를 시도하면 데드락이 발생한다).  

ReentrantLock과 동일하게 ReentrantReadWriteLock 역시 쓰기 락을 확보한 스레드가 명확하게 존재하며, 쓰기 락을 확보한 스레드만이 쓰기 락을 해제할 수 있다. 자바 5.0에서 읽기 락은 락이라기보다는 Semaphore와 같이 동작하는데, 즉 읽기 작업을 하고 있는 스레드의 개수만 세고 실제로 어느 스레드가 읽고 있는지는 상관없다. 이런 특성은 자바 6에서 변경됐는데, 어느 스레드가 읽기 락을 확보했는지 추적하도록 돼 있다. 이렇게 변경된 이유는 자바 5.0에서는 읽기 락을 최초로 요청했는지, 아니면 재진입 상황에서 읽기 락을 요청하는 것인지 구분이 되지 않았다. 그러면 락을 공정하게 동작하도록 설정했을 때 읽기-쓰기 락에서 데드락이 발생할 위험이 높다.  

읽기-쓰기 락은 락을 확보하는 시간이 약간은 길면서 쓰기 락을 요청하는 경우가 적을 때에 병렬성(concurrency)을 크게 높여준다.  

<img src="/assets/images/Java_Concurrency_In_Practice/13-3.jpg" width="100%" height="100%"/>
<br/>

위의 그림을 보면 ArrayList를 ReentrantLock으로 돋기화시킨 클래스와 ReentrantReadWriteLock으로 동기화시킨 클래스의 실행 속도를 비교하고 있다. 양쪽 모두 4개 옵테론(Opteron) 프로세서가 장착된 시스템에 솔라리스 운영체제가 설치된 상태에서 테스트했다.  

### 요약
<br/>
명시적으로 Lock 클래스를 사용해 스레드를 동기화하면 암묵적인 락보다 더 많은 기능을 활용할 수 있다. 예를 들어 락을 확보할 수 없는 상황에 유연하게 대처하는 방법이나 대기 큐에서 기다리는 방법과 규칙도 원하는 대로 정할 수 있다. 그렇다고해서 synchronized 구문 대신 기계적으로 ReentrantLock을 사용해야 할 필요는 없고, 단지 ReentrantLock에서만 제공되고 synchronized 구문은 제공하지 않는 동기화 관련 기능이 꼭 필요한 경우에만 ReentrantLock을 사용하도록 하자.  

읽기-쓰기 락을 사용하면 읽기 작업만 처리하는 다수의 스레드는 동기화된 값을 얼마든지 동시에 읽어갈 수 있다. 따라서 읽기 작업이 대부분인 데이터 구조에 읽기-쓰기 락을 사용하면 확장성을 높여주는 훌륭한 도구가 된다.  

### 14. 동기화 클래스 구현
<br/>
#### 14.1. 상태 종속성 관리
<br/>
단일 스레드로 동작하는 프로그램에서는 메소드를 호출했을 때 상태 기반의 조건(예를 들어 "연결 풀에 남는 연결 인스턴스가 있는지?")이 만족되지 않는다면, 해당 조건은 앞으로도 절대로 만족될 가능성이 없다. 따라서 순차적으로 실행되는 프로그램은 원하는 상태를 만족시키지 못하는 부분이 있다면 반드시 오류가 발생하게 된다. 히지만 병렬 프로그램에서는 상태 기반의 곶건은 다른 스레드를 통해서 언제든지 마음대로 변경될 수 있다. 바로 직전에 실행할 때는 비어 있던 풀에 다른 스레드가 사용하고 남은 객체가 반환돼 풀에 항목이 들어오기도 한다. 병렬 객체의 상태 족속적인 메소드는 선행 조건이 만족하지 않았을 때 오류가 발생하는 문제에서 비켜날 수도 있겠지만, 비켜나는 일보다는 선행 조건을 만족할 때까지 대기하는 경우가 많아진다.  

상태 종속적인 기능을 구현할 때 원하는 선행 조건이 만족할 때까지 작업을 멈추고 대기하도록 하면 조건이 맞지 않았을 때 프로그램이 멈춰버리는 방법보다 훨씬 간편하고 오류도 적게 발생한다. 자바에 내장된 조건 큐 메커니즘(condition queue mechanism)은 실행 중인 스레드가 특정 객체가 원하는 상태에 진입할 때까지 대기할 수 있도록 도와주며, 원하는 상태에 도달해서 스레드가 계속해서 실행할 수 있게 되면 대기 상태에 들어가 있던 스레드를 깨워주는 역할도 담당한다. 조건 큐는 원하는 상태에 다다를 때까지 폴링하고 잠깐 기다리고 다시 폴링하고 다시 잠깐 기다리는 (고통스러운) 반복문을 사용하는 대신 조건 큐를 사용하면 얼마나 많은 이득을 얻을 수 있는지를 약간 소개하고자 한다.  

상태 종속적인 블로킹(blocking) 작업은 다음 예제와 같은 모양을 갖고 있다. 락을 활용하는 형태가 그다지 일반적이지 않은데, 이를테면 작업하고자 확보했던 락을 그 내부에서 다시 풀어주고 또 다시 확보하는 우스꽝스런 모습이다. 어쨌거나 선행 조건에 해당하는 클래스 내부의 상태 변수는 값을 확인하는 동안에도 적절한 락으로 반드시 동기화해야 올바른 값을 확인할 수 있다. 하지만 일단 선행 조건을 만족하지 않았다면 락을 다시 풀어줘야 다른 스레드에서 상태 변수를 변경할 수 있다. 만약 락을 풀어주지 않고 계속 잡고 있다면 다른 스레드에서 상태 변수의 값을 변경할 수 없기 때문에 선행 조건을 영원히 만족시키지 못한다. 물론 다음 번에 선행 조건을 확인하기 직전에는 락을 다시 확보해야만 한다.  

```java
void blockingAction() throws InterruptedException {
  상태 변수에 대한 락 확보
  while (선행 조건이 만족하지 않음) {
    확보했던 락을 풀어줌
    선행 조건이 만족할만한 시간만큼 대기
    인터럽트에 걸리거나 타임아웃이 걸리면 멈춤
    락을 다시 확보
  }
  작업 실행
  락 해제
}
```

프로듀서-컨슈머 패턴으로 구현된 애플리케이션에서는 ArrayBlockingQueue와 같이 크기가 제한된 큐를 많이 사용한다. 크기가 제한된 큐는 put과 take 메소드를 제공하며, put과 take 메소드에는 다음과 같은 선행 조건이 있다. 버퍼 내부가 비어 있다면 값을 take 할 수 없고, 버퍼가 가득 차 있다면 값을 put 할 수 없다. 상태 종속적인 메소드에서 선행 조건과 관련한 오류가 발생하면 예외를 발생시키거나 오류 값을 리턴하기도 하고(이 두가지 경우는 호출한 쪽에서 오류로 처리해야 한다), 아니면 선행 조건이 원하는 상태에 도달할 때까지 대기하기도 한다.  

```java
@ThreadSafe
public abstract class BaseBoundedBuffer<V> {
  @QuardedBy("this") private final V[] buf;
  @QuardedBy("this") private int tail;
  @QuardedBy("this") private int head;
  @QuardedBy("this") private int count;

  protected BaseBoundedBuffer(int capacity) {
    this.buf = (V[]) new Object[capacity];
  }

  protected synchronized final void doPut(V v) {
    buf[tail] = v;
    if (++tail == buf.length)
      tail = 0;
    ++count;
  }

  protected synchronized final V doTake() {
    V v = buf[head];
    buf[head] = null;
    if (++head == buf.length)
      head = 0;
    --count;
    return v;
  }

  public synchronized final boolean isFull() {
    return count == buf.length;
  }

  public synchronized final boolean isEmpty() {
    return count == 0;
  }
}
```

##### 14.1.1. 선행 오류를 호출자에게 그대로 전달
<br/>
선행 오류를 호출자에게 그대로 전달하는 방법으로 구현하면 만들기는 편리하지만 사용할 때는 여간 짜증나는 게 아니다. 예외는 예외적인 상호아에서만 사용하는게 정상이다. 발생할 가능성이 있는 예외 상황을 매번 처리해줘야 한다. 이처럼 상태 종속성을 호출자에게 넘기는 방법을 쓰면 FIFO 큐에서 값의 순서를 정확하게 유지하는 것과 같은 일이 분가능해진다. 외부의 호출자가 계속해서 재시도해야 하기 때문에 어느 값이 먼저 도착했는지를 제대로 알아낼 수 없다.  

```java
@ThreadSafe
public class GrumpyBoundedBuffer<V> extends BaseBoundedBuffer<V> {
  public GrumpyBoundedBuffer(int size) { super(size); }

  public synchronized void put(V v) throws BufferFullException {
    if (isFull())
      throw new BufferFullException();
    doPut(v);
  }

  public synchronized V take() throws BufferEmptyException {
    if (isEmpty())
      throw new BufferEmptyException();
    return doTake();
  }
}
```

```java
while (true) {
  try {
    V item = buffer.take();
    // 값을 사용한다
    break;
  } catch (BufferEmptyException e) {
    Thread.sleep(SLEEP_GRANULARITY);
  }
}
```

이와 유사한 또 다른 방법으로는 원하는 상태가 아닐 때 오류 값을 리턴하는 방법이 있다. 오류 값을 리턴하는 방법은 예외 상황이 아님에도 불구하고 "미안합니다 디시 시도하세요"라는 의미로 예외를 던지지는 않으니 약간 나은 방법이라고 볼 수도 있겠다. 하지만 선행 조건이 맞지 않다고 해서 호출자가 오류를 맡아서 처리해야 하는 원론적인 방법상의 문제를 해결하지는 못한다. JDK 라이브러리의 Queue 클래스는 두 가지 방법을 모두 사용할 수 있도록 하고 있다. 큐가 비어 있는 경웨 poll 메소드는 null을 리턴하고 remove 메소드는 예외를 던진다. 히지만 프로듀서-컨슈머 패턴에서는 Queue를 사용하는 것이 그다지 적잘하지는 않다. 프로듀서-컨슈머 패턴에는 BlockingQueue와 같이 작업을 계속 진행하기에 적절한 상태가 아니라면 계속해서 대기하는 클래스가 더 적당한 선택이다.  

재시도하는 논리를 구현하는 방법에 있어서 호출자 측 코드 말고 다른 방법도 있다. 호출자가 잠자는 대기 시간 없이 take 메소드를 즈깃 다시 호출하는 방법인데, 흔히 스핀 대기(spin waiting 또는 busy waiting) 방법이라고 한다. 이 방법을 사용했는데 버퍼의 상태가 원하는 값으로 얼른 돌아오지 않는다면 상당한 양의 CPU 자원을 소모하게 된다. 반대로 CPU 자원을 덜 소모하도록 하고자 일정 시간 동안 대기하게 할 수 있는데, 이렇게 하면 버퍼의 상태가 원하는 값으로 돌아왔음에도 불구하고 계속해서 대기 상태에 빠져있는 '과다 대기' 문제가 생기기도 한다. 따라서 호출자는 CPU를 덜 사용하되 응답성에서 손해를 보거나, 응답성은 좋지만 CPU를 엄청나게 소모하는 두 가지 방법 가운데 어느 것을 사용할지 선택해야 한다(Thread 클래스의 yield 메소드를 호출하면 시스템의 스케줄러에게 '다른 스레드를 실행하려면 지금이 괜찮은 시점이다'라는 사실을 알리는 것과 같다. 따라서 Thread.yield 메소드를 반복문 내부에서 매번 호출하는 방법도 생각해볼 수 있겠는데, 이 방법은 스핀 대기 방법과 '과다 대기' 방법의 사이에 존재한다고 봐도 되겠다. 즉 다른 스레드가 뭔가 작업을 해주기를 기다리고 있는 상태라면 할당받은 스케줄 시간을 모두 사용하기 전에 다른 스레드를 먼저 실행시키는 방법도 나쁘지는 않다).  

##### 14.1.2. 폴링과 대기를 반복하는 세련되지 못한 대기 상태
<br/>
다음 예제의 SleepyBoundedBuffer 클래스는 '폴링하고 대기하는' 재시도 반복문을 put 메소드와 take 메소드 내부에 내장시켜서 외부의 호출 클래스가 매번 직접 재시도 반복문을 만들어 사용해야 하는 불편함을 줄여주고자 하고 있다. 만약 버퍼가 비어 있다면 take 메소드는 다른 스레드가 버퍼에 값을 집어 넣을 때까지 대기하고, 버퍼가 가득 차 있다면 put 메소드는 다른 스레드가 값을 꺼내 버퍼에 빈 공간이 생길 때까지 대기한다. 이 방법은 선행 조건 관리하는 부분을 버퍼 내부에 내장했기 때문에 외부에서 버퍼를 훨씬 간편하게 사용할 수 있다. 외부에서 간단하게 사용할 수 있다는건 버퍼를 구현하는 입장에서 굉장히 중요하고 그래야만 하는 요건이다.  

SleepyBoundedBuffer 클래스의 구현 내용을 보면 이전에 구현했던 방법보다 약간 더 복잡한 모양을 갖추고 있다. 버퍼 내부를 보면 상태 조건을 나타내는 변수가 버퍼 락으로 동기화돼 있기 대문에 버퍼의 락을 확보한 상태에서 상태 조건이 적절한지 먼저 확인한다. 만약 상태 조건이 적절하지 않다면 실행 중이던 스레드가 잠시 대기 상태에 들어가고, 대기 상태에 들어가기 직전에 락을 풀어서 다른 스레드가 버퍼의 상태 변수를 사용할 수 있도록 한다. 일반적으로 락을 확보한 상태에서 sleep 메소드를 호출하거나 기타 여러 가지 방법으로 대기 상태에 들어가는 일은 그다지 추천하지 않으며, 특히나 지금 보고 있는 버퍼의 경우에는 락을 확보하고 있는 한 다른 스레드가 상태 변수의 값을 변경할 수 없기 때문에 원하는 상태 조건(예를 들어 버퍼에 값이 들어오거나, 버퍼에 빈 공간이 생기는 일)에 도달할 수 없다. 대기 상태에 있던 스레드가 깨어나면 락을 다시 확보한 다음 상태 조건을 다시 확인한다. 이렇게 잠시 대기하고 상태 조건을 확인하는 반복문을 계속해서 실행하다 조건이 적절해지면 반복문을 빠져나와 작업을 처리한다.  

기능을 호출하는 호출자의 입장에서 보면 일단 그럴듯하게 동작한다. 만약 상태 조건이 이미 적절하게 갖춰져 있었다면 작업 역시 즈깃 실행할 수 있고, 그렇지 않다면 대기 상태에 들어간다. 물론 호출자는 반복문 내부의 구조를 알아야 할 필요도 없고 오류가 발생하는지 보다가 재시도해야 할 필요도 없다. 잠자기 대기 상태에 들어가는 시간을 길게 잡거나 짧게 잡으면 응답 속도와 CPU 사용량 간의 트레이드 오프(trade off)가 발생한다. 대기 시간을 짧게 잡으면 응답성은 좋아지지만 CPU 사용량은 줄어들지만 응답 속도가 떨어진다. 다음 그림을 보면 대기 시간에 따라 응답 속도가 어떻게 변하는지를 그래프로 보여주고 있다. 버퍼에 공간이 생긴 이후에 스레드가 대기 상태에서 빠져나와 상태 조건을 확인하기까지 약간의 시간 차이가 발생하기도 한다는 점을 주의하자.  

```java
@ThreadSafe
public class SleepyBoundedBuffer<V> extends BaseBoundedBuffer<V> {
  public SleepyBoundedBuffer(int size) { super(size); }

  public void put(V v) throws InterruptedException {
    while (true) {
      synchronized (this) {
        if (!isFull()) {
          doPut(v);
          return;
        }
      }
      Thread.sleep(SLEEP_GRANULARITY);
    }
  }

  public V take() throws InterruptedException {
    while (true) {
      synchronized (this) {
        if (!isEmpty())
          return doTake();
      }
      Thread.sleep(SLEEP_GRANULARITY);
    }
  }
}
```

SleepyBoundedBuffer를 사용하는 호출자는 챙겨야 할 일이 하나 더 있다. 바로 InterruptedException이 발생하는 경우를 처리하는 일이다. 메소드 내부에서 원하는 조건을 만족할 때까지 대기해야 한다면 작업을 취소할 수 있는 기능을 제공하는 ㅍ녀이 좋다. 대부분의 깔끔하게 만들어진 JDK 라이브러리 메소드처럼 SleepyBoundedBuffer 역시 인터럽트를 걸면 즉시 리턴되면서 InterruptedException을 던지는 작업 취소 방법을 적용하고 있다.  

이와 같이 폴링하고 대기하는 반복 작업을 통해 블로킹 연산을 구현하는 일은 상당히 고생스러운 일이다. 조건이 맞지 않으면 스레드를 멈추지만, 만약 원하는 조건에 도달(버퍼에 빈 공간이 생겨서 put 할 수 있게 되는 상태)하면 그 즉시 스레드를 다시 실행시킬 수 있는 방법이 있다면 좋지 않을까? 이런 일을 당당하는 구조가 바로 조건 큐(condition queue)이다.  

##### 14.1.3. 조건 큐 - 문제 해결사
<br/>
조건 큐는 주방에 놓여 있는 토스트 기계에서 "토스트가 다 됐습니다"라고 울리는 벨과 같다. 토스트가 구워지는 동안 벨 소리에 귀를 기울이고 있다가 토스트가 다 구워지면 즉시 알 수 있다. 그러면 현재 하던 신문 보는 일을 멈추고(또는 신문을 먼저 모두 읽어도 상관 없다.) 토스트를 꺼내 맛있게 먹을 수 있다. 그런데 토스트 기계의 벨 소리에 신경을 쓰지 않고 있다면(예를 들어 신문을 가지러 잠시 밖에 나갔다고 해보자) 벨 소리를 놓칠 수도 있다. 하지만 주방에 돌아와서 토스터의 상태를 확인할 수 있고, 상태를 본 결과 토스트가 다 구워졌으면 꺼내고, 아니면 다시 벨 소리에 귀를 기울이고 대기 상태에 들어갈 수 있다.  

<img src="/assets/images/Java_Concurrency_In_Practice/14-1.jpg" width="100%" height="100%"/>
<br/>

조건 큐는 여러 스레드를 한 덩어리(대기 집합 wait set이라고 부른다)로 묶어 특정 조건이 만족할 때까지 한꺼번에 대기할 수 있는 방법을 제공하기 때문에 '조건 큐'라는 이름으로 불린다. 데이터 값으로 일반적인 객체를 담아두는 보통의 큐와 달리 조건 큐에는 특정 조건이 만족할 때까지 대기해야 하는 스레드가 값으로 들어간다.  

자바 언어에서 사용하는 모든 객체를 락으로 활용할 수 있는 것처럼 모든 객체는 스스로를 조건 큐로 사용할 수 있으며, 모든 객체가 갖고 있는 wait, notify, notifyAll 메소드는 조건 큐의 암묵적인 API라고 봐도 좋다. 자바 객체의 암묵적인 락과 암묵적인 조건 큐는 서로 관련돼 있는 부분이 있는데, 이를테면 X라는 객체의 조건 큐 API를 호출하고자 하면 반드시 객체 X의 암묵적인 락을 확보하고 있어야만 한다. 상태 기반의 조건이 만족하기를 기다리도록 구현된 부분이 객체 내부의 상태를 일관적으로 유지하도록 구현된 코드와 필연적으로 굉장히 밀접하게 관련돼 있기 때문이다. 객체 내부의 상태를 확인하기 전에는 조건이 만족할 때까지 대기할 수가 없고, 객체 내부의 상태를 변경하지 못하는 한 해당 객체의 조건 큐에서 대기하고 있는 객체를 풀어줄 수 없으니 당연하다.  

Object.wait 메소드는 현재 확보하고 있는 락을 자동으로 해제하면서 운영체제에게 현재 스레드를 머춰달라고 요청하고, 따라서 다른 스레드가 락을 확보해 객체 내부의 상태를 변경할 수 있도록 해준다. 대기 상태에서 깨어나는 순간에는 해제했던 락을 다시 확보한다. 풀어서 말하자면 Object.wait 메소드는 "나는 대기 상태에 들어갈 예정인데, 만약 뭔가 재미있는 일이 생기면 깨워주기 바랍니다"라는 뜻이다. 이와 유사하게 notify 또는 notifyAll 메소드는 "뭔가 재미있는 일이 발생했습니다"라고 알려주는 셈이다.  

다음 예제는 BoundedBuffer 클래스는 wait와 notifyAll 메소드를 사용해 크기가 제한된 버퍼를 구현하고 있다. 이전에 sleep 메소드로 대기 상태에 들어가던 메소드보다 구현하기도 훨씬 간편하고, 훨씬 효율적(버퍼의 내부 상태가 변하지 않으면 거의 깨어나지 않는다)이면서 응답성도 훨씬 좋다(버퍼 내부 상태에 봐야할 만한 변화가 발생하면 그 즉시 깨어난다). 이런 구조는 굉장히 많이 발전한 모습이라고 할 수 있지만, 조건 큐를 사용했다고 해서 잠자기 대기 상태에 들어가던 버전과 비교해봤을 때 그 작동하는 모습에는 변화가 없다는 점을 알아두자. 여러 가지 측면, 즉 CPU 사용의 호율성, 컨텍스트 스위치 관련 부하, 응답 속도 등의 측면에서 봤을 때 그저 잠자기 대기 상태에 들어가던 버전에 비해 몇 가지 최적화 작업을 한 것뿐이라고 봐야 한다. 다시 말해 조건 큐를 사용한다고 해서 폴링과 대기 상태를 반복하던 버전에서 할 수 없던 일을 할 수 있게 되는 경우는 없다. 알고보면 이 말이 딱 맞는 것도 아니다. 공정한 조건 큐는 대기 집합에서 어느 스레드가 먼저 풀려 나오는지의 순서가 공정하게 보장된다. 반면 암묵적인 조건 큐 역시 공정한 큐를 제공하지는 않는다. 만약 공정한 조건 큐를 사용해야만 하는 경우라면 Condition 클래스를 사용해 공정함 또는 불공정함을 선택할 수 있다. 하지만 조건 큐를 사용하면 상태 종속성을 관리하거나 표현하는 데 있어서 훨신 효율적이면서 간편한 방법이긴 하다.  

```java
@ThreadSafe
public class BoundedBuffer<V> extends BaseBoundedBuffer<V> {
  // 조건 서술어: not-full (!isFull())
  // 조건 서술어: not-empty (!isEmpty())

  public BoundedBuffer(int size) { super(size); }

  // 만족할 때까지 대기: not-full
  public synchronized void put(V v) throws InterruptedException {
    while (isFull())
      wait();
    doPut(v);
    notifyAll();
  }

  // 만족할 때까지 대기: not-empty
  public synchronized V take() throws InterruptedException {
    while (isEmpty())
      wait();
    V v = doTake();
    notifyAll();
    return v;
  }
}
```

BoundedBuffer는 이제 쓸만하게 구현됐다. 사용하기도 편리하고 상태 종속성도 깔끔하게 관리한다. ConditionBoundedBuffer는 notifyAll 대신 notify를 사용하기 때문에 더 효율적이라고 볼 수 있다. 상용으로 활용할 버전에는 put 메소드와 take 메소드에 타임아웃을 걸 수 있는 기능도 추가해서 일정 시간 동안 작업을 처리하지 못할 경우 대기 중이던 작업을 자동으로 멈출 수 있도록 준비하는 것도 좋다. 오버로드된 Object.wait 메소드 가운데 타임아웃을 지정할 수 있는 메소드도 있는데, 이 메소드를 사용하면 put과 take 메소드에 타임아웃을 쉽게 추가할 수 있다.  

#### 14.2. 조건 큐 활용
<br/>
조건 큐를 사용하면 효율적이면서 응답속도도 빠른 상태 종속적인 클래스를 구현할 수 있지만, 올바르지 않은 방법으로 사용할 가능성도 높다. 컴파일러나 자바 플랫폼에서 정의하고 있지는 않지만, 조건 큐를 제대로 활용하려면 꼭 지켜야만 하는 몇 가지 규칙이 있다(가능한 한 LinkedBlockingQueue, CountDownLatch, Semaphore, FutureTask 등의 클래스를 기반으로 원하는 기능을 구현하라고 하는데 바로 이런 원인이 있다. 만약 이런 클래스만으로 원하는 기능을 구현할 수 있다면, 프로그래밍 작업이 굉장히 간편해진다).  

##### 14.2.1. 조건 서술어
<br/>
조건 큐를 올바로 사용하기 위한 가장 핵심적인 요소는 바로 해당 객체가 대기하게 될 조건 서술어(predicate)를 명확하게 구분해내는 일이다. wait와 notify를 사용함에 있어서 가장 많은 혼란을 줄 수 있는 요소가 바로 조건 서술어인데, JDK 라이브러리 API에도 전혀 언급되지 않고, 조건 서술어를 올바르게 사용하는 데 꼭 필요한 내용이 자바 언어 명세나 JVM 구현 메뉴얼 어디에도 소개돼 있지 않기 때문이다. 실제로 자바 언어 명세나 API 문서에 조건 서술어라는 단어가 전혀 명시된 바가 없다. 하지만 조건 서술어가 없으면 대기 기능이 동작할 수 없다.  

조건 서술어는 애초에 특정 기능이 상태 종속적이 되도록 만드는 선행 조건을 의미한다. 크기가 제한된 버퍼를 예로 들면 take 메소드는 버퍼에 값이 들어있는 경우에만 작업을 진행할 수 있고, 버퍼가 비어 있다면 대기해야 한다. 그러면 take 메소드의 입장에서는 작업을 진행하기 전에 확인해야만 하는 "버퍼에 값이 있어야 한다"는 것이 조건 서술어이다. 이와 유사하게 put 메소드의 입장에서 조건 서술어는 바로 "버퍼에 빈 공간이 있다"는 것이다. 조건 서술어는 클래스 내부의 상태 변수에서 유추할 수 있는 표현식이다. BaseBoundedBuffer 클래스는 "버퍼에 값이 있어야 한다"는 조건 서술어에 대해 count 변수가 0보다 큰지 비교하고, "버퍼에 빈 공간이 있어야 한다"는 조건 서술어에 대해서는 count 변수의 값이 버퍼의 크기보다 작은지를 확인한다.  

조건 큐와 연결된 조건 서술어를 항상 문서로 남겨야 하며, 그 조건 서술어에 영향을 받는 메소드가 어느 것인지도 명시해야 한다.  

조건부 대기와 관련된 락과 wait 메소드와 조건 서술어는 중요한 삼각 관계를 유지하고 있다. 조건 서술어는 상태 변수를 기반으로 하고 있고, 상태 변수는 락으로 동기화돼 있으니 조건 서술어를 만족하는지 확인하려면 반드시 락을 확보해야만 한다. 또한 락 객체와 조건 큐 객체(wait와 notify 메소드를 호출하는 대상 객체)는 반드시 동일한 객체여야만 한다.  

BoundedBuffer 클래스는 버퍼 락으로 버퍼의 상태를 동기화하고 있으며 퍼버 객체 자체를 조건 큐로 사용하고 있다. take 메소드를 보면 먼저 버퍼 락을 확보한 다음 조건 서술어(버퍼에 값이 있어야 한다)를 확인한다. 만약 버퍼에 값이 하나라도 있었다면 take 메소드는 첫 번째 값을 뽑아내는데, 값을 뽑아내는 작업 역시 이미 락을 확보하고 있기 때문에 버퍼의 상태를 올바르게 유지하면서 문제없이 처리할 수 있다.  

반대로 조건 서술어를 확인해봤을 때 만족하지 않았다면(버퍼에 값잉 없었다) take 메소드는 다른 스레드에서 put 메소드를 통해 버퍼에 값을 추가할 때까지 대기해야 한다. take 메소드는 이렇게 대기하는 방법으로 해당 버퍼 객체의 wait 메소드를 호출해 암묵적인 조건 큐를 활용하며, 이 작업을 하는 과정에도 역시 조건 큐 객체에 대한 락을 확보한 상태여야 한다. 주의 깊게 설계하면 당연히 그렇겠지만 take 메소드는 이미 조건 서술어를 확인하는 시점에 필요한 락을 확보한 상태이다(물론 조건 서술어를 만족해 원하는 작업을 수행할 때에도 이미 락을 확보하고 있었기 때문에 단일 연산으로 해당 작업을 처리할 수 있다). wait 메소드는 먼저 락을 해제하고 현재 스레드를 대기 상태에 두고, 일정 시간 이후에 타임아웃이 발생하거나 스레드에 인터럽트가 걸리거나 notify 또는 notifyAll을 통해 알림을 받을 때까지 대기한다. 대기 상태에 있던 스레드가 깨어나면 wait 메소드는 리턴되기 전에 락을 다시 확보한다. wait 메소드에서 깨어나는 스레드라고 해도 락을 다시 확보함에 있어서 별다른 우선 순위를 갖지는 않으며, 일반적인 다른 스레드와 같이 락을 확보하는 경쟁에 참여해 공정하거나 불공정한 방법을 거쳐 락을 확보한다.  

wait 메소드를 호출하는 모든 경우에는 항상 조건 서술어가 연결돼 있다. 특정 조건 서술어를 놓고 wait 메소드를 호출할 때, 호출자는 항상 해당하는 조건 큐에 대한 락을 이미 확보한 상태여야 한다. 또한 확보한 락은 조건 서술어를 확인하는 데 필요한 모든 상태 변수를 동기화하고 있어야 한다.  

##### 14.2.2. 너무 일찍 깨어나기
<br/>
앞에서 락과 조건 서술어와 조건 큐 간의 삼각 관계가 있음을 어렵지 않게 이해할 수 있었다. wait 메소드를 호출하고 리턴됐다고 해서 반드시 해당 스레드가 대기하고 있던 조건 서술어를 만족한다는 것은 아니다.  

하나의 암묵적인 조건 큐를 두 개 이상의 조건 서술어를 대상으로 사용할 수도 있다. 어디에선가 notifyAll을 호출해서 대기 상태에 있던 스레드가 깨어났다면, wait 메소드가 리턴됐다고 해서 wait하기 직전에 확인했던 조건 서술어를 만족하게 됐다는 것으로 이해해서는 안 된다(이것은 마치 토스터와 커피 메이커가 하나의 벨을 공유해 사용하는 것과 비슷하다. 벨이 울렸다고 해서 어느 작업이 끝난 건지 알 수는 없으며, 어느 작업이 끝났는지를 확인하려면 직접 호가인하는 과정을 거쳐야 한다). 게다가 wait 메소드는 누군가가 notify 해주지 않아도 리턴되는 경우까지 있다.  

wait 메소드를 호출했던 스레드가 대기 상태에서 깨어나 다시 실행된다고 보면, 조건 큐와 연결돼 있는 락을 다시 확보한 상태이다. 그렇다면 조건 서술어는 이제 만족됐는가? 그럴 수도 있다. 다른 스레드가 notifyAll을 호출하는 시점에는 조건 서술어가 만족하는 상태였다고 할 수도 있지만, 락을 확보하고 보니 다시 조건 서술어를 만족하지 않는 상태가 됐을 가능성도 있다. 다시 말해서 스레드가 깨어난 이후 락을 다시 확보하기 직전까지 다른 스레드가 락을 미리 확보하고는 조건 서술어와 관련된 상태 변수의 값을 변경시킬 가능성도 있다. 다시 말해서 스레드가 깨어난 이후 락을 다시 확보하기 직전까지 다른 스레드가 락을 미리 확보하고는 조건 서술어와 관련된 상태 변수의 값을 변경시킬 가능성도 있기 때문이다. 아니면 아예 wait 메소드를 처음 호출한 이후 한 번도 조건 서술어를 만족했던 적이 없었을 수도 있다. 다른 스레드가 notify 또는 notifyAll 메소드를 왜 호출했는지는 알 방법이 없다. 아마도 동일한 조건 큐를 대상으로 하는 다른 조건 서술어가 만족돼 호출한 것일 수 있다. 이처럼 하나의 조건 큐에 여러 개의 조건 서술어를 연결해 사용하는 일은 굉장히 흔한 방법이다.  

BoundedBuffer 역시 하나의 조건 큐를 놓고 "버퍼에 값이 있어야 한다"와 "버퍼에 빈 공간이 있다"는 두 개의 조건 서술어를 한꺼번에 연결해 사용하고 있다. 더군다나 "버퍼에 빈 공간이 있다"는 조건과 "버퍼에 값이 있어야 한다"는 양쪽 조건 모두에 스레드가 대기할 수도 있다. 이런 현상은 프로듀서-컨슈머 패턴에서 프로듀서와 컨슈머의 개수가 버퍼의 용량을 초과할 때 발생하는 경우가 있다.  

이런 모든 원인 때문에 wait 메소드가 깨어나 리턴되고 나면 조건 서술어를 한 번 더 확인해야만 하며, 만약 이벤에도 조건 서술어를 만족하지 않으면 물론 다시 wait 메소드를 호출해 대기 상태에 들어가야 한다. 또한 조건 서술어를 만족하지 않은 상태에서 wait 메소드가 여러 차례 리턴될 가능성도 있기 때문에 wait 메소드를 반복문 안에 넣어 사둉해야 하며, 매번 반복할 때마다 계속해서 조건 서술어를 확인해야 한다. 조건부 wait 메소드를 사용하는 표준적인 방법이 다음 예제에 소개돼 있다.  

```java
void stateDependentMethod() throws InterruptedException {
  // 조건 서술어는 반드시 락으로 동기화된 이후에 확인해야 한다.
  synchronized(lock) {
    while (!conditionPredicate())
      lock.wait();
    // 객체가 우언하는 상태에 맞춰졌다.
  }
}
```

조건부 wait 메소드(Object.wait 또는 Condition.wait)를 사용할 때에는,  

+ 항상 조건 서술어(작업을 계속 진행하기 전에 반드시 확인해야 하는 확인 절차)를 명시해야 한다.  
+ wait 메소드를 호출하기 전에 조건 서술어를 확인하고, wait에서 리턴된 이후에도 조건 서술어를 확인해야 한다.
+ wait 메소드는 항상 반복문 내부에서 호출해야 한다.
+ 조건 서술어를 확인하는 데 관련된 모든 상태 변수는 해당 조건 큐의 락에 의해 동기화돼 있어야 한다.
+ wait, notify, notifyAll 메소드를 호출할 때는 조건 큐에 해당하는 락을 확보하고 있어야 한다.
+ 조건 서술어를 확인한 이후 실제로 작업을 실행해 작업이 끝날 때까지 락을 해제해서는 안 된다.

##### 14.2.3. 놓친 신호
<br/>
특정 스레드가 아미 참(true)을 만족하는 조건을 놓고 조건 서술어를 제대로 확인하지 못해 대기 상태에 들어가는 상황을 놓친 신호라고 한다. 즉 놓친 신호 문제가 발생한 스레드는 이미 지나간 일에 대한 알림을 받으려 대기하게 된다. 말하자면 토스트 기계에 빵을 오려놓고는 신문을 가지러 밖으로 나가고, 밖에서 돌아오기 전에 토스트가 끝나 벨이 울렸는데 돌아온 이후에도 계속해서 벨소리가 들릴 때까지 기다리는 것과 같다. 아마도 상당히 긴 시간을 기다리게 될 수도 있고, 심지어는 영원히 대기 상태에서 나오지 않을 수도 있다. 이와 같은 대기 상태에서 빠져나오려면 다른 누군가가 토스트 기계를 다시 동작시켜야 할 텐데, 아마도 그렇게 되면 토스트 기계 속에 있는 빵의 주인이 누구인지를 놓고 다투게 될지도 모른다. 이런 놓친 신호 현상이 발생하는 원인은 스레드에 대한 알림이 일시적이라는 데에 있다. 스레드 A가 조건 큐에 신호를 보내주고, 신호가 지나간 이후에 스레드 B가 동일한 조건 큐에서 대기한다면 스레드 B는 대기 상태에서 깨어나지 못한다. 스레드 B가 대기 상태에서 빠져나오려면 신호가 한 번 더 지나가야 한다. 놓친 신호는 프로그램을 작성할 때 앞에서 소개한 여러 가지 주의 사항을 지키지 않아서 발생한다. 예를 들어 wait 메소드를 호출하기 전에 조건 서술어를 확인하지 못하는 경우가 생길 수 있다면 놓친 신호 문제가 발생할 가능성도 있다. wait 메소드를 호출하기 전에 조건을 확인하는 부분은 위의 예제와 같은 형태로 작성하면 놓친 신호 문제에 대해서 걱정하지 않아도 된다.  

##### 14.2.4. 알림
<br/>
지금까지는 조건부 대기 관련 기능에서 '대기'라는 한쪽에 해당하는 부분을 살펴봤다. '대기'가 아닌 다른 한쪽은 '알림(notification)'이다. 크기가 제한된 버퍼 클래스에 값이 전혀 들어 있지 않은 상태에서 take 메소드를 호출하면 대기 상태에 들어간다. take 메도그ㅏ 대기 상태에 들어간 이후 버퍼 클래스에 값이 들어왔을 때 대기 상테에서 다시 빠져나오게 하려면 버퍼 클래스에 값이 추가되는 모든 실행 경로의 코드에서 뭔가 알림 조치를 취해야 한다. BoundBuffer 클래스에는 값이 추가되는 코드가 딱 한군데 있는데, 바로 put 메소드이다. put 메소드의 코드를 보면 값을 성공적으로 추가한 이후에 notifyAll 메소드를 호출하게 돼 있다. 이와 비슷하게 take 메소드 역시 버퍼에서 값을 뽑아낸 직후에 notifyAll 메소드를 호출하도록 돼 있는데, 이는 take에서 값을 제거해 공간이 남기 때문에 공간이 모자라 값을 추가하지 못하고 대기상태에 들어가 있던 스레드에게 추가 작업을 다시 시도해 보라는 의미이다.  

특정 조건을 놓고 wait 메소드를 호출해 대기 상태에 들어간다면, 해당 조건을 만족하게 된 이후에 반드시 알림 메소드를 사용해 대기 상태에서 빠져나오도록 해야 한다.  

조건 큐 API에서 아릴미 기능을 제공하는 메소드에는 두 가지가 있는데 하나는 notify이고 다른 하나는 notifyAll이다. notify 또는 notifyAll 어느 메소드를 호출하더라도 해당하는 조건 큐 객체에 대한 락을 확보한 상태에서만 호출할 수 있다. notify 메소드를 호출하면 JVM은 해당하는 조건 큐에서 대기 상태에 들어가 있는 다른 스레드 하나를 골라 대기 상태를 풀어준다. notify나 notifyAll을 호출할 때는 반드시 해당하는 조건 큐 객쳉 대한 락을 확보해야 하고 wait 메소드를 호출했던 스레드 역시 조건 큐에 대한 락을 확보하지 못하면 대기 상태에서 깨어날 쑤 없기 때문에 notify 또는 notifyAll을 호출한 이후에는 최대한 빨리 락을 풀어줘야 대기 상테에서 깨어난 스레드가 얼른 동작할 수 있다.  

여러 개의 스레드가 하나의 조건 큐를 놓고 대기 상태에 들어갈 수 있는데, 대기 상태에 들어간 조건이 서로 다를 수 있기 때문에 notifyAll 대신 notify 메소드를 사용해 대기 상태를 풀어주는 방법은 위험성이 높다. 단 한번만 알림 메시지를 전달하게 되면 앞서 소개했던 '놓친 신호'와 유사한 무제가 생길 가능성이 높다.  

BoundedBuffer 클래스를 보면 특별한 경우가 아닌 이상 notify 메소드 대신 notifyAll 메소드를 사용하는 편이 안전한 이유를 쉽게 알 수 있다. 즉 BoundedBuffer 메소드를 사욯하는 편이 안전한 이유를 쉽게 알 수 있다. 즉 BoundedBuffer에서 대기 상테에 들어가는 조건으로 "버퍼에 공간이 없다"와 "버파가 비어 있다"는 두 가지를 사용하고 있다. A라는 스레드가 PA라는 조건을 놓고 조건 큐에서 대기 중이고, 스레드 B는 PB 조건을 놓고 동일한 조건 큐에서 대기 중이라고 가정해보자. 그런데 조건 PB가 먼저 만족하게 되고 스레드 C가 notify 메소드를 호출한다. 그러면 JVM은 조건 큐에서 대기 중인 스레드 하나를 골라서 깨우는데, 만약 스레드 A를 깨운다면 스레드 A는 조건 PA를 확인해보고 조건을 만족하지 않는다는 사실을 확인하고는 다시 대기 상태에 들어간다. 그러는 동안 실제로 작업을 진행할 수 있는 상태가 된 스레드 B는 깨지도 못한 상태에서 계속 대기해야 한다. 이런 상황이 앞서 설명했던 '놓친 신호'와 일치하는 상황은 아니지만(오히려 '빼앗긴 신호'라고 해야 맞겠다) 그 결과 발생하는 문제, 즉 이미 발생해서 날아가버린 신호를 기다리느라 대기 상테에서 깨어나지 못한다는 문제는 동일하다.  

notifyAll 대신 notify 메소드를 사용하려면 다음과 같은 조건에 해당하는 경우에만 사용하는 것이 좋다.  

+ 단일 조건에 따른 대기 상태에서 깨우는 경우  
해당하는 조건 큐에 단 하나의 조건만 사용하고 있는 경우이고, 따라서 각 스레드는 wait 메소드에서 리턴될 때 동일한 방법으로 실행된다.  

+ 한 번에 하나씩 처리하는 경우  
조건 변수에 대한 알림 메소드를 호출하면 하나의 스레드만 실행시킬 수 있는 경우  

BoundedBuffer 클래스는 한 번에 하나씩 처리하는(one-in, one-out) 조건은 만족하지만 "공간이 없다" 또는 "비어 있다"는 두 가지 조건을 사용하기 때문에 단일 조건에 따라 대기 상태에 들어가는 경우라는 조건에는 해당되지 않는다.  

대부분의 클래스는 위의 두 가지 조건을 모두 만족하지는 못한다. 따라서 일반적인 경우에는 notify 대신 notifyAll을 사용하는 편이 더 낫다. 물론 notifyAll 메소드가 notify보다 덜 효율적일 수는 있으나 클래스를 제대로 동작시키려면 notify를 사용하기보다 notifyAll을 사용하는 쪽이 더 쉽다.  

이런 일반적인 방법을 쓰려고 보니 아무래도 마음에 걸리는 사람도 있을 것이며 마음에 걸리는 충분한 이유를 갖고 있기도 하다. 대기 상태에 들어간 스레드 가운데 단 하나의 스레드만이 동작할 수 있는 상황이라면 notifyAll을 사용하는 게 비효율적이라고 볼 수 있다. 어떨 때는 그나마 효율성이 덜 떨어질 수도 있지만, 효율성이 상당히 떨어지기도 한다. 조건 큐에서 열 개의 스레드가 대기하고 있는 상태인데 notifyAll을 호출하면 열 개의 스레드가 모두 깨어나서 다시 락을 잡으려고 경쟁하게 된다. 그리고 락을 확보한 하나의 스레드를 제외하고는 모두 다시 대기 상태에 들어간다. 그러면 단지 하나의 스레드만 대기 상태에서 깨우려는 목적을 달성하기 위해 대기 중인 스레드를 모두 깨우면서 컨텍스트 스위칭이 빈번하게 일어나고, 상당량의 락 확보 경쟁이 벌어진다(최악의 경우에는 n이 충분히 큰 값일 때 notifyAll을 호출하면 최고 O(n²) 만큼 대기 상태에서 깨어나는 경우도 생길 수 있다). 이런 상황은 성능을 높이거나 안전성을 높이는 두 가지 목표가 서로 상충되는 상황이라고 볼 수 있다.  

BoundedBuffer 클래스의 put과 take 메소드에서 실행되는 알림 기능은 보수적인 편이다. 즉 버퍼에 객체가 들어가거나 객체를 뽑아낼 때마다 무조건 알림 메소드를 호출한다. 여기에서 버퍼가 비어 있다가 값이 들어오거나 가득 찬 상태에서 값을 뽑아내는 경우에만 대기 상태에서 빠져나올 수 있다는 점을 활용해 take나 put 메소드가 대기 상태에서 빠져나올 수 있는 상태를 만들어주는 경우에만 알림 메소드를 호출하도록 하면 이런 보수적인 측면을 최적화할 수 있다. 이런 최적화 방법을 조건부 알림(conditional notification)이라고 부른다. 조건부 알림 방법을 사용하면 성능은 향상시킬 수 있겠지만 제대로 동작하도록 만드는 과정은 꽤나 복잡하고 섬세한 면이 있다(더군다나 해당 클래스를 상속받은 하위 클래스를 구현해야 하는 시점에는 훨씬 더 복잡해지기도 한다). 다음 예제를 보면 BoundedBuffer 클래스의 put 메소드에 조건부 알림 방법을 적용한 모습을 볼 수 있다.  

단일 알림 방법이나 조건부 알림 방법은 일반적인 방법이라기보다는 최적화된 방법이다. 단일 알림 방법이나 조건부 알림 방법을 사용하기 전에 항상 그랬던 것처럼 "일단 제대로 동작하게 만들어라. 그리고 필요한 만큼 속도가 나지 않는 경우에만 최적화를 진행하라"는 원칙을 먼저 지킬 필요가 있다. 최적화 방법을 적절치 못하게 적용하고 나면 이상하게 발생하는 프로그램 오류를 만나게 될지도 모른다.  

```java
public synchronized void put(V v) throws InterruptedException {
  while (isFull())
    wait();
  boolean wasEmpty = isEmpty();
  doPut(v);
  if (wasEmpty)
    notifyAll();
}
```

##### 14.2.5. 예제: 게이트 클래스
<br/>
1부에서 살펴봤던 시작 게이트 래치는 최초에 1이라는 값을 갖고 있으며 바이너리 래치로 동작하도록 설계돼 있다. 여기서 바이너리 래치라 함은 초기와 종료의 두 가지 상태를 갖는다는 의미이다. 시작 게이트 래치는 문이 열리기 전까지는 모든 스레드가 통과하지 못하도록 막고 있다가 특정 조건이 만족하는 시점에 한꺼번에 통과하도록 문을 열어준다. 대부분의 경우에는 이와 같이 동작하는 래치만 갖고 충분히 사용할 수가 있지만 이와 같은 형태의 래치는 한 번 열리고 나면 다시는 닫을 수 없다는 특징이 있다.  

다음 예제에서 보다시피 조건부 대기 기능을 활용하면 여러 거번 닫을 수 있는 ThreadGate와 같은 클래스를 어렵지 않게 구현할 수 있다. ThreadGate 클래스는 문을 열었다 닫았다 할 수 있는 구조로 돼 있으며, 문이 열릴 때까지 대기하도록 하는 await 메소드를 제공한다. open 메소드에서 스레드를 대기 상태에서 풀어줄 때 알림 방법으로 notifyAll 메소드를 사용하는데 앞서 설명했던 한 번에 하나씩 처리하는 조건에 해당되지 않기 때문에 notify 메소드를 사용할 수는 없다.  

await 메소드에서 사용하는 조건은 단순하게 isOpen 메소드를 사요해 문이 열렸는지를 확인하는 것보다 좀더 복잡하게 돼 있다. 문이 열리는 시점에 N개의 스레드가 문이 열리기를 기다리고 있었다면 대기 중이던 스레드 N개가 모두 대기 상태에서 빠져나오도록 해야 한다. 하지만 문이 열린 이후 굉장히 짧은 시간 후에 다시 닫히는 상황이 발생하면 await 메소드에서 단순하게 isOpen 메소드만을 기준으로 대기 상태에서 깨어나도록 하는 방법이 충분하지 않을 수 있다. 다시 말해서 대기 중이던 스레드가 알림을 받고는 대기 상태에서 깨어나 락을 확보하고 wait 메소드에서 리턴하고 나니 이미 isOpen 값이 다시 "닫힘"으로 바뀌어 있을 수 있다는 말이다. 그래서 ThreadGate에서는 좀더 복잡한 조건을 사용한다. 일단 문이 닫힐 때마다 ThreadGate 클래스는 내부의 일련번호 값을 증가시킨다. 그리고 isOpen 값이 '열림'으로 설정돼 있거나 일련번호를 사용해 해당 스레드가 문 앞에 온 이우헹 문이 열려 있었는지 확인하고, 문이 열려 있었다면 awiat 메소드를 그냥 통과한다.  

ThreadGate 클래스는 문이 열릴 때까지 대기하는 기능만 갖고 있으므로 알림 기능은 open 메소드에서만 호출한다. 만약 '문이 열리기를 기다림'이나 '문이 닫히기를 기다림'의 두 가지 기능을 모두 지원하려면 open과 close 양쪽에서 알림 메소드를 호출해야 할 수 있다. 이런 면에서 보면 내부에 상태를 관리하는 클래스를 유지보수하기가 쉽지 않은 이유를 알 수 있다. 즉 클래스 내부 상태를 기반으로 하는 기능을 하나 추가하려면 해당 기능이 제대로 돌아가도록 상당 부분의 클래스 코드를 수정할 필요가 있기 때문이다.  

```java
@ThreadSafe
public class ThreadGate {
  // 조건 서술어: opened-since(n) (isOpen || generation>n)
  @GuardedBy("this") private boolean isOpen;
  @GuardedBy("this") private int generation;

  public synchronized void close() {
    isOpen = false;
  }

  public synchronized void open() {
    ++generation;
    isOpen = true;
    notifyAll();
  }

  // 만족할 때까지 대기: opened-since(generation on entry)
  public synchronized void await() throws InterruptedException {
    int arrivalGeneration = generation;
    while (!isOpen && arrivalGeneration == generation)
      wait();
  }
}
```

##### 14.2.6. 하위 클래스 안전성 문제
<br/>
조건부 알림 기능이나 단일 알림 기능을 사용하고 나면 해당 클래스의 하위 클래스를 구현할 때 상당히 복잡해지는 문제가 생길 수 있다. 일단 하위 클래스를 구현할 수 있도록 하려면 상위 클래스를 구현할 때 상위 클래스에서 구현했던 조건부 또는 단일 알림 방법을 벗어나는 방법을 사용해야만 하는 경우가 있을 수 있으며, 이런 경웨 상위 클래스 대신 하위 클래스에서 적절한 알림 방법을 사용할 수 있도록 갖춰둬야 한다.  

상태 기반으로 동작하는 클래스는 하위 클래스에게 대기와 알림 구조를 완전하게 공개하고 그 구조를 문서로 남기거나, 아니면 아예 하위 클래스에게 대기와 알림 구조에 전혀 접근할 수 없도록 깔끔하게 제한해야 한다(이 부분은 "하위 클래스에서 사용하기 좋게 설계하고 구현하거나, 아니면 아예 막아버려라"는 원칙의 확장판인 셈이다). 최소한 상태 기반으로 동작하면서 하위 클래스가 상속받을 가능성이 높은 클래스를 구현하려면 조건 큐에 락 객체 등을 하위 클래스에게 노출시켜 사용할 수 있도록 해야 하고, 그와 함께 조건과 동기화 정책 등을 문서로 남겨둬야 한다. 그러다보면 조건 큐와 락 객체뿐만 아니라 상태 변수 자체를 하위 클래스에게 열어줘야 할 가능성도 있다(상태 기반의 클래스를 구현할 때 저지를 수 있는 가장 큰 잘못은 클래스 내부의 상태를 하위 클래스가 볼 수 있도록 열어둔 상태에서 대기하거나 알림으로 깨어나는 규칙을 전혀 설명하지 않는 것이다. 이런 상황은 클래스의 상태 변수를 외부에 노출시켜두고 그에 대한 사용 조건을 전혀 명시하지 않는 것과 같다).  

클래스를 상속받는 과정에서 발생할 수 있는 오류를 막을 수 있는 간단한 방법 가운데 하나는 클래스를 final로 선언해 상속 자체를 금지하거나 조건 큐, 락, 상태 변수 등을 하위 클래스에서 접근할 수 없도록 막아두는 방법이 있다. 이런 조치를 취해두지 않았다면 하위 클래스가 상위 클래스에서 notify 메소드를 호출하는 방법을 추적해 않았다면 하위 클래스가 상위 클래스에서 notify 메소드를 호출하는 방법을 추적해 잘못 사용하는 경우가 생기고, 잘못 움직여버린 상태 변수의 값을 고쳐야 할 수도 있다. 예를 들어 크기에 제한이 없는 스택 클래스를 생각해보자. 크기 제한이 없기 때문에 스택에서 값을 뽑아내는 pop 연산은 스택이 비어 있는 경우에 대기 상태에 들어가도록 돼 있고, 값을 추가하는 push 연산은 언제든지 대기 상태에 들어가지 않으면서 실행될 수 있다. 이런 스택 자체는 단일 알림 방법을 사용할 수 있는 조건에 부합된다. 그래서 단일 알림 방법을 사용해 스택을 구현한 이후, 하위 클래스에서 "두 개의 값을 한꺼번에 pop하는" 메소드를 새로 추가한다고 해보자. 그러면 대기 상테에 들어가는 조건이 두 가지로 늘어난다. 하나는 값이 하나 이상 있을 때까지 대기하는 기존 조건이고, 새로 추가된 메소드 때문에 두 개 이상의 값이 남아 있을 때가지 대기하는 조건이 더 생긴다. 만약 상위 클래스에서 조건 큐와 대기 및 알림 규칙을 잘 설명해뒀다면 하위 클래스에서는 push 메소드에서 notify 대신 notifyAll을 사용해 프로그램의 안전성을 유지할 수 있다.  

##### 14.2.7. 조건 큐 캡슐화
<br/>
일반적으로 조건 큐를 클래스 내부에 캡슐화해서 클래스 상속 구조의 외부에서는 해당 조건 큐를 사용할 수 없도록 막는 게 좋다. 그렇지 않으면 클래스를 사용하는 외부 프로그램에서 조건 큐에 대한 대기와 알림 규칙을 '추측'한 상태에서 클래스를 처음 구현할 때 설계했던 방법과 다른 방법으로 호출할 가능성이 있다(조건 큐를 외부에서 사용하지 못하도록 막지 않는 이상 대기 상태에 들어가는 모든 스레드에게 단일 알림 방법을 기준으로 동작한다는 점을 강제할 방법은 전혀 없다. 외부의 프로그램이 조건 큐에 접근할 수 있다고 하면 의도하지 않은 스레드가 내부의 조건 큐를 대상으로 대기 상태에 들어갈 수 있고, 그러다보면 알림 규칙을 흐트러뜨리거나 '빼앗긴 신호' 문제점을 발생시킬 수 있다).  

하지만 조건 큐로 사용하는 객체를 클래스 내부에 캡슐화하라는 방법은 스레드 안전한 클래스를 구현할 때 적용되는 일반적인 디자인 패턴과 비교해 볼 때 일관적이지 않은 부분이 있다, 다시 말해, 해당하는 객체 내부에서 객체 자체를 기준으로 한 암묵적인 락을 사용하는 경우가 바로 그렇다. BoundedBuffer 클래스 역시 버퍼 객체 자체가 락이고 또한 조건 큐로 동작하는 일반적인 패턴으로 구현돼 있다. 어쨌거나 BoundedBuffer 클래스도 객체 자체 대신 그 내부에 락과 조건 큐로 사용할 객체를 따로 두는 모습으로 구현 형태를 쉽게 변경할 수 있다. 대신 내부에 락 객체를 다로 두게되면 어떤 형태로든 클라이언트 측 락 기능을 제공하지는 못한다.  

##### 14.2.8. 진입 규칙과 완료 규칙
<br/>
웰링스 Wellings(Wellings 2004)는 wait와 notify를 적용하는 규칙을 진입 규칙과 완료 규칙으로 표헌하고 있다. 즉 상태를 기반으로 하는 모든 연산과 상태에 의존성을 갖고 있는 또 다른 상태를 변경하는 연산을 수행하는 경우에는 항상 진입 규칙과 완료 규칙을 정의하고 문서화해야 한다는 말이다. 진입 규칙은 해당 연산의 조건을 뜻한다. 완료 규칙은 해당 연산으로 인해 변경됐을 모든 상태 값이 변경되는 시점에 다른 연산의 조건도 함께 변경됐을 가능성이 있으므로, 만약 다른 연산의 조건도 함께 변경됐다면 해당 조건 큐에 알림 메시지를 보내야 한다는 규칙이다.  

java.util.concurrent 패키지에 들어 있는 대부분의 상태 기반 클래스를 하위 클래스로 거느리고 있는 AbstractQueuedSynchronizer 클래스를 보면 완료 규칙의 개념을 좀더 쉽게 이해할 수 있다. 동기화 클래스에서 직접 스스로의 규칙에 따라서 알림 기능을 실행하도록 하는 대신 동기화 클래스의 메소드에서 기능을 길행한 결과로 하나 이상의 대기 중인 스레드가 깨어났는지의 여부를 넘겨주도록 하고 있다. API와 이와 같이 정의돼 있기 때문에 일부 상태 변화 과정에서 알림 메소드를 호출하는 일은 '까먹는' 일이 훨씬 줄어든다.  

#### 14.3. 명시적인 조건 객체
<br/>
명시적으로 Lock 객체를 사용하면 암묵적인 락이 활용 형태가 지극히 제한돼 있어 처리할 수 없던 동기화 기능도 수행할 수 있다. 암묵적인 락을 일반화한 형태가 Lock 클래스인 것처럼 암묵적인 조건 큐를 일반화한 형태는 바로 Condition 클래스이다.  

암묵적인 조건 큐에는 여러 가지 단점이 있다. 모든 암묵적인 락 하나는 조건 큐를 단 하나만 가질 수 있다. 따라서 BoundedBuffer와 같은 클래스에서 여러 개의 스레드가 하나의 조건 큐를 놓고 여러 가지 조건을 기준으로 삼아 대기 상태에 들어갈 수 있다는 말이다. 그리고 락과 관련해 가장 많이 사용되는 패턴을 보면 바로 조건 큐 객체를 스레드에게 노출시키도록 돼 있다, 이 두 가지를 놓고 보면 notify를 사용할 수 있도록 해주는 조건 가눙데 하나인 단일 대기 조건을 만족시키기가 불가능하다. 암묵적인 락이나 조건 큐 대신 Lock 클래스와 Condition 클래스를 활용하면 여러 가지 종류의 조건을 사용하는 병렬 처리 객체를 구현하거나 조건 큐를 노출시키는 것에 대한 공부를 할 때 훨신 유연하게 대처할 수 있다.  

암묵적인 조건 큐가 암묵적인 락 객체를 사용해 동기화하는 것처럼 Condition 클래스 역시 내부적으로 하나의 Lock 클래스를 사용해 동기화를 맞춘다. Condition 인스턴스를 생성하려면 Lock.newCondition 메소드를 호출한다. 앞서 설명했지만 Lock 클래스가 암묵적인 락보다 훨씬 다양한 기능을 제공하는 것처럼 Condition 클래스 역시 하나의 락에 여러 조건으로 대기하게 할 수 있고 또한 인터럽트에 반응하거나 반응하지 않는 대기 상태, 데드라인을 정해둔 대기 상태, 공정하거나 공정하지 않은 큐 처리 방법 등 조건 큐보다 훨씬 다양한 기능을 제공한다.  

```Java
public interface Condition {
  void await() throws InterruptedException;
  boolean await(long time, TimeUnit unit) throws InterruptedException;
  long awaitNanos(long nanoTimeout) throws InterruptedException;
  void awaitUninterruptibly();
  void awaitUntil(Date deadline) throws InterruptedException;

  void signal();
  void signalAll();
}
```

Condition 객체는 암묵적인 조건 큐와 달리 Lock 하나를 대상으로 필요한 만큼 몇 개라도 만들 수 있다. Condition 객체는 자신을 생성해준 Lock 객체의 공정성을 그대로 물려받는데, 이를테면 공정한 Lock에서 생성된 Condition 객체의 경우에는 Condition.await 메소드에서 리턴될 때 정확하게 FIFO 순서를 따른다.  

+ 위험성 경고  
암묵적인 락에서 사용하던 wait, notify, notifyAll 메소드의 기능은 Condition 클래스에서는 각각 await, signal, signalAll 메소드이다. 자바에서 모든 클래스가 그렇지만 Condition 클래스 역시 Object를 상속받기 때문에 Condition 객체에도 wait, notify, notifyAll 메소드가 포함돼 있다. 따라서 실수로 await 대신 wait 메소드를 사용하거나 notify 대신 signal 메소드를 사용하면 동기화 기능에 큰 문제가 생길 수 있다.  

다음 예제에는 크기가 제한된 버퍼를 또다른 형태로 구현한 예가 있다. 여기에 구현된 내용을 보면 두 개의 Condition 객체를 사용해 "버퍼가 가득 차지 않았다"는 notFull 조건과 "버퍼가 비어 있지 않다"는 notEmpty 조건을 처리한다. 버퍼의 take 메소드에서 버퍼의 큐가 비어서 대기해야 한다면 notEmpty 조건에서 대기한다. 그러면 put 메소드에서는 notEmpty 조건에 신호를 보내서 대기 중이던 take 메소드를 대기 상태에서 깨운다.  

ConditionBoundedBuffer가 동작하는 모습은 기존의 BoundedBuffer 클래스와 동일하지만, 내부적으로 조건 큐를 사용하는 모습은 훨신 읽기 좋게 작성돼 있다. 하나의 암묵적인 조건 큐를 사용해 여러 개의 조건을 처리하느라 복잡해지는 것보다 조건별로 각각의 Condition 객체를 생성해 사용하면 클래스 구조를 분석하기도 쉽다. Condition 객체를 활용하면 대기 조건들을 각각의 조건 큐로 나눠 대기하도록 할 수 있기 때문에 단일 알림 조건을 간단하게 만족시킬 수 있다. 따라서 signalAll 대신 그보다 더 효율적인 signal 메소드를 사용해 동일한 기능을 처리할 수 있으므로, 컨텍스트 스위치 횟수도 줄일 수 있고 버퍼의 기능이 동작하는 동안 각 스레드가 락을 확보하는 횟수 역시 줄일 수 있다.  

암묵적인 락이나 조건 큐와 같이 Lock 클래스와 Condition 객체를 사용하는 경우에도 락과 조건과 조건 변수 간의 관계가 동일하게 유지돼야 한다. 조건에 관련된 모든 변수는 Lock의 보호 아래 동기화돼 있어야 하고, 조건을 확인하거나 await 또는 signal 메소드를 호출하는 시점에는 반드시 Lock을 확보한 상태여야 한다. ReentrantLock 클래스에서 생성한 Condition 객체의 signal 또는 signalAll 메소드를 호출할 때는 반드시 Lock을 확보한 상태여야 한다. 하지만 Lock 클래스에서 이와 같이 signal 또는 signalAll을 호춣할 때 반드시 Lock을 확보한 상태여야 한다는 건이 없는 Condition 객체를 생성할 수도 있다.  

Condition 객체를 사용할 것이냐 아니면 암묵적인 조건 큐를 사용할 것이냐의 문제는 ReentrantLock을 사용할 것이냐 아니면 synchronized 구문을 사용할 것이냐의 선택과 같은 문제이다. 공정한 큐 관리 방법이나 하나의 락에서 여러 개의 조건 큐를 사용할 필요가 있는 경우라면 Condition 객체를 사용하고, 그럴 필요가 없다면 암묵적인 조건 큐를 사용하는 편이 더 낫다(이미 이런 요구사항 때문에 암묵적인 락 대신 ReentrantLock을 사용하고 있었다면, 이미 답이 나온 것이나 다름 없다).  

```java
@ThreadSafe
public class ConditionBoundedBuffer<T> {
  protected final Lock lock = new ReentrantLock();
  // 조건 서술어: notfull (count < itmes.length)
  private final Condition notFull = lock.newCondition();
  // 조건 서술어: notEmpty (count > 0)
  private final Condition notEmpty = lock.newCondition();
  @GuardedBy("lock")
  private final T[] items = (T[]) new Object[BUFFER_SIZE];
  @GuardedBy("lock") private int tail, head, count;

  // 만족할 때까지 대기: notfull
  public void put(T x) throws InterruptedException {
    lock.lock();
    try {
      while (count == itmes.length)
        notFull.await();
      items[tail] = x;
      if (++tail == items.length)
        tail = 0;
      ++count;
      notEmpty.signal();
    } finally {
      lock.unlock();
    }
  }

  // 만족할 때까지 대기: notEmpty
  public T take() throws InterruptedException {
    lock.lock();
    try {
      while (count == 0)
        notEmpty.await();
      T x = items[head];
      items[head] = null;
      if (++head == items.length)
        head = 0;
      --count;
      notFull.signal();
      return x;
    } finally {
      lock.unlock();
    }
  }
}
```
#### 14.4. 동기화 클래스의 내부 구조
<br/>
ReentrantLock과 Semaphore의 인터페이스는 비슷한 부분이 많다. 양쪽 클래스 모두 일종의 '문'의 역할을 하며, 특정 시점에 제한된 개수의 스레드만이 문을 통과할 수 있다. 문 앞에 도착한 스레드는 문을 통과할 수도 있고(lock 또는 acquire 메소드가 성공적으로 리턴된 경우), 문 앞에서 대기해야 할 수도 있고(lock 또는 acquire 메소드에서 대기하는 경우), 아니면 문 앞에서 되돌아 가야 할 수도 있다(지정된 시간 안에 조건을 만족하지 않아서 tryLock 또는 tryAcquire 메소드가 false 값을 리턴한 경우). 또한 양쪽 클래스 모두 인터럽트가 가능하거나 아니면 이터럽트를 걸 수 없는 조건 확인 기능을 제공하며, 조건 확인에 시간을 지정할 수도 있고, 큐 관리 방법의 공정성 역시 지정할 수 있다.  

이렇게 중복되는 부분이 많다보니 혹시라도 ReentrantLock을 기반으로 Semaphore를 만든 것은 아닌가, 반대로 Semaphore를 기반으로 ReentrantLock을 만든건 아닌가 하고 의심할 수도 있겠다. 이런 의심은 굉장히 현실적인 면이 있는데, 락을 사용해 카운팅 세마포어(counting semaphore)를 쉽게 만들 수 있다거나 카운팅 세마포어를 사용해 락을 구현할 수 있다는 점은 이미 널리 알려진 사실이다.  

실제로 양쪽 클래스 모두 다른 여러 동기화 클래스와 같이 AbstractQueuedSynchronizer(AQS)를 상속받아 구현돼 있다. AQS는 락이나 기타 동기화 클래스를 만들 수 있는 프레임웍 역할을 하며 AQS를 기반으로 하면 엄청나게 다양한 종류의 동기화 클래스를 간단하면서 효율적으로 구현할 수 있다. ReentrantLock이나 Semaphore 클래스뿐만 아니라 CountDownLatch, ReentrantReadWriteLock, SynchronousQueue, FutureTask 등의 클래스 역시 AQS 기반으로 만들어져 있다. 자바 6에서는 SynchronousQueue가 AQS 기반 대신 대기 상태에 들어가지 않아 훨씬 확장성이 좋은 구조로 변경됐다.  

AQS를 사용해보면 동기화 클래스를 구현할 때 필요한 다양한 잡무, 예를 들어 대기 중인 스레드를 FIFO 큐에서 관리하는 기능 등을 AQS에서 처리해준다. AQS 기반으로 만들어진 개별 동기화 클래스는 스레드가 대기 상태에 들어가야 하는지 아니면 그냥 통과해야 하는지의 조건을 유연하게 정의할 수 있다.  

동기화 클래스를 작성할 때 AQS 기반으로 작성하면 여러 가지 장점이 있다. 구현할 때 필요한 노력을 좀 줄여준다는 장점뿐만 아니라 동기화 클래스 하나를 기반으로 다른 동기화 클래스를 구현할 때 여러 면에서 신경 써야 하는 부분이 줄어든다. AQS 기반으로 만들어진 동기화 클래스는 대기 상태에 들어갈 수 있는 지점이 단 한군데이기 때문에 컨텍스트 스위칭 부하를 줄일 수 있고 결과적으로 전체적인 성능을 높일 수 있다. AQS 자체도 원래부터 확장성을 염두에 두고 만들어졌으며, AQS를 기반으로 만들어진 java.util.concurrent 패키지의 동기화 클래스 모두가 이런 장점을 그대로 불려받았다.  

#### 14.5. AbstractQueuedSynchronizer
<br/>
개발자가 AQS를 직접 사용할 일은 거의 없을 것이다. JDK에 들어 있는 표준 동기화 클래스만으로도 거의 모든 경우의 상황에 대처할 수 있기 때문이다. 그렇다 해도 표준 동기화 클래스가 어떻게 만들어졌는지를 살펴본다면 좋은 공부가 되겠다.  

AQS 기반의 동기화 클래스가 담당하는 작업 가운데 가장 기본이 되는 연산은 바로 확보(acquire)와 해제(release)이다. 확보 연산은 상태 기반으로 동작하며 항상 대기 상태에 들어갈 가능성이 있다. 락이나 세마포어 등의 입장세너는 확보라는 연산은 락이나 퍼밋을 확보한다는 것으로 그 의미가 굉장히 명확하다. 이 연산을 사용하는 호출자는 항상 원하는 상태에 다다를 때까지 대기할 수 있다는 가능성을 염두에 둬야 한다. CountDownLatch 클래스를 놓고 보면 확보라는 연산은 "래치가 완료 상태에 다다를 때까지 대기하라"는 의미이다. 그리고 FutureTask 클래스에서는 확보가 "작업이 끝날 때까지 대기하라"는 뜻이다. 해제 연산은 대기 상태에 들어가지 않으며, 대신 확보 연산에서 대기 중인 스레드를 풀어주는 역할을 한다.  

특정 클래스가 상태 기반으로 동작하려면 반드시 상태 변수를 하나 이상 갖고 있어야 한다. AQS는 동기화 클래스의 상태 변수를 관리하는 작업도 어느 정도 담당하는데 getState, setState, compareAndSetState 등의 메소드를 통해 단일 int 변수 기반의 상태 정보를 관리해준다. 이 기능만 사용해도 다양한 상태를 간단하게 표현할 수 있다. 예를 들어 ReentrantLock 클래스는 이 상태를 사용해 소속된 스레드에서 락을 몇 번이나 확보했었는지를 관리하고, Semaphore 클래스는 남아 있는 퍼밋의 개수를 관리하고, FutureTask 클래스는 작업의 실행 상태(시작 전, 실행 중, 완료, 취소)를 관리한다. 동기화 클래스는 int 상태 변수 말고도 각자 필요한 상태 변수를 추가해 관리한다. 예를 들어 ReentrantLock 클래스는 락을 다시 확보하려는 것인지(reentrant) 아니면 서로 다른 스레드가 경쟁하고 있는 상태인지(contended)를 확인할 수 있도록 현재 락을 확보하고 있는 스레드의 목록을 관리한다.  

AQS 내부의 확보와 해제 연산은 다음 예제에 소개한 것과 같은 형태를 갖고 있다. AQS를 구현한 동기화 클래스에 따라 다르지만 확보 연산은 ReentrantLock에서와 같이 배타적(exclusive)으로 동작할 수도 있고, Semaphore나 CountDownLatch 클래스에서와 같이 배타적이지 않을 수도 있다. 확보 연산은 두 가지 부분으로 나눠 볼 수 있다. 첫 번째 부분은 동기화 클래스에서 확보 연산을 허용할 수 있는 상태인지 확인하는 부분이다. 만약 허용할 수 있는 상태라면 해당 스레드는 작업을 계속 진행하게 되고, 그렇지 않다면 확보 연산에서 대기 상태에 들어가거나 실패하게 된다. 이와 같은 판단은 동기화 클래스의 특성에 따라 다르게 나타난다. 예를 들어 락이 풀려 있는 경우에는 락을 확보하는 연산이 성공하고, 래치가 완료 상태에 도달해 있었다면 래치 확보 연산이 성공한다.  

두 번째 부분은 동기화 클래스 내부의 상태를 업데이트 하는 부분이다. 특정 스레드 하나가 동기화 클래스의 확보 연산을 호출하면 다른 스레드가 해당 동기화 클래스의 확보 연산을 호출했을 때 성공할지의 여부가 달라질 수 있다. 예를 들어 락을 확보하면 락의 상태가 "해제됨"에서 "확보됨"으로 변한다. 또한 Semaphore에서 퍼밋을 확보하면 남은 퍼밋의 개수가 줄어든다. 반면 스레드 하나가 래치의 확보 연산을 호출했다는 것으로는 다른 스레드가 해당 래치의 확보 연산을 호출하는 결과에 영향을 주지 못하므로, 래치에 대한 확보 연산은 그 내부의 상태 변수를 변경하지 않는다.  

```java
boolean acquire() throws InterruptedException {
  while (확보 연산을 처리할 수 없는 상태이다) {
    if (확보 연산을 처리할 때까지 대기하길 원한다) {
      현재 스레드가 큐에 들어 있지 않다면 스레드를 큐에 넣는다
      대기 상태에 들어간다
    }
    else
      return 실패
  }
  상황에 따라 동기화 상태 업데이트
  스레드가 큐에 들어 있었다면 큐에서 제거한다
  return 성공
}

void release() {
  동기화 상태 업데이트
  if (업데이트된 상태에서 대기 중인 스레드를 풀어줄 수 있다)
    큐에 쌓여 있는 하나 이상의 스레드를 풀어준다
}
```

배타적인 확보 기능을 제공하는 동기화 클래스는 tryAcquire, tryRelease, isHeldExclusively 등의 메소드를 지원해야 하며, 배타적이지 않은 확보 기능을 지원하는 동기화 클래스는 tryAcquireShared, tryReleaseShared 메소드를 제공해야 한다. AQS에 들어 있는 acquire, acquireShared, release, releaseShared 메소드는 해당 연산을 실행할 수 있는지를 확인할 때 상속받은 클래스에 들어 있는 메소드 가운데 이름 앞에 try가 붙은 메소드를 호출한다. 동기화 클래스는 물론 getState, setState, compareAndSetState 등의 메소드를 상요해 자신의 확보와 해제 조건에 맞춰 상태 변수 값을 읽어가거나 변경할 수 있다. 그리고 확보나 해제 작업이 끝난 후에는 시도했던 연산이 성공적이었는지를 리턴 값으로 알려준다. 예를 들어 tryAcquireShared 메소드에서 리턴 값으로 0보다 작은 값이 넘어오면 확보 연산이 실패했다는 의미이고, 0을 리턴하념 배타적인 확보 연산이 성공했다는 의미이고, 마지막으로 0보다 큰 값을 리턴하면 배타적이지 않은 확보 연산이 성공했다는 의미이다.  

AQS에서는 조건 큐 기능을 지원하는 락(예를 들면 ReentrantLock과 같이)을 간단하게 구현할 수 있도록 동기화 클래스와 연동된 조건 변수를 생성하는 방법을 제공한다.  

##### 14.5.1. 간단한 래치
<br/>
다음 예제의 다음 예제의 OneShotLatch 클래스는 AQS를 기반으로 구현한 바이너리 래치이다. OneShotLatch 클래스에는 두 개의 public 메소드가 있는 데 하나는 확보 연산을 실행하는 await이고 다른 하나는 해제 연산을 담당하는 signal이다. OneShotLatch 클래스는 초기에 닫힌 상태로 생성된다. await 메소드를 호춣하는 모든 스레드는 래치가 열린 상태로 넘어가기 전가지 모두 대기 상태에 들어간다. 누군가가 signal을 호출해 해제 연산을 실행하면 그 동안 await에서 대기하던 스레드가 모두 해제되고 signal 호출 이후에 await를 호출하는 스레드는 대기 상태에 들어가지 않고 바로 실행된다.  

```java
@ThreadSafe
public class OneShotLatch {
  private final Sync sync = new Sync();

  public void signal() { sync.releaseShared(0); }

  public void await() throws InterruptedException {
    sync.acquireSharedInterruptibly(0);
  }

  private class Sync extends AbstractQueuedSynchronizer {
    protected int tryAcquireShared(int ignored) {
      // 래치가 열려 있는 상태(state==1)라면 성공, 아니면 실패
      return (getState() == 1) ? 1 : -1;
    }

    protected boolean tryReleaseShared(int ignored) {
      setState(1); // 래치가 열렸다
      return true; // 다른 스레드에서 확보 연산에 성공할 가능성이 있다
    }
  }
}
```

OneShotLatch 클래스에서는 래치의 상태를 AQS 상태 변수로 표현하는데, 0이면 닫힌 상태이고 1이면 열린 상태이다. await 메소드는 내부에서 AQS의 acquireSharedInterruptibly 메소드를 호출하며, acquireSharedInterruptibly 메소드는 결국 OneShotLatch 클래스의 tryAcquireShared 메소드를 호출해 그 기능을 사용한다. 앞서 설명했지만 tryAcquireShared 메소드를 호출해 그 기능을 사용한다. 앞서 설명했지만 tryAcquireShared 메소드는 확보 연산을 진행할 수 있는 상태인지 확인해서 그 여부를 리턴해줘야 한다. 만약 래치가 이전에 열려 있던 상태였다면 확보 연산을 진행하도록 결과를 리턴해 스레드가 진행할 수 있도록 하고, 아니면 확보 연산이 실패했다는 결과를 리턴한다. 만약 래치가 이전에 열려 있던 상태였다면 확보 연산이 실패했다는 결과를 리턴한다. 확보 연산이 실패한 경우는 acquireSharedInterruptibly 메소드에서 현재 스레드가 대기 큐에 들어가야 하는 상화응로 해석한다. 이와 비슷하게 signal 메소드는 releaseShared 메소드를 호출하고, releaseShared 메소드는 tryReleaseShared 메소드를 호출한다. tryReleaseShared 메소드는 래치의 상태를 무조건 '열림'으로 돌려놓고 래치 클래스가 완전히 열린 상태라는 결과를 리턴 값으로 알려준다. 그러면 AQS는 대기하던 모든 스레드에게 확보 연산을 실행하라는 신호를 보낼 것이고, 이제부터는 tryAcquireShared 메소드에서 확보 연산이 성공했다는 결과를 리턴할 것이다.  

OneShotLatch는 겨우 20여 줄의 코드만으로 구현돼 있지만 정의된 기능을 모두 구현하고 있으며 제품 구현 작업에 직접 사용해도 좋고 성능도 충분한 동기화 클래스이다. 물론 몇 가지 추가해볼 만한 기능이 더 있기는 하다. 예를 들어 시간 제한을 걸고 확보 연산을 실행하는 기능이라든가 래치의 현재 상태를 확인하는 기능 등이 가능하겠다. AQS에는 확보 연산에 시간 제한을 할 수 있는 기능이나 몇 가지 상태 확인 기능 역시 만들어져 있는 상태이기 때문에 이런 추가 기능 역시 간단하게 구현할 수 있다.  

OneShotLatch는 AQS의 핵심 기능을 위임(delegate)하는 형식으로 구현했는데, 대신 AQS를 직접 상솓받는 방법으로 구현하는 것도 가능하다. 하지만 이런 경우에 상속을 통한 구현 방법은 그다지 권장할만하지 않다. 만약 AQS를 직접 상속받아 구현했다면 지금의 OneShotLatch와 같이 단 2개의 메소드로 이뤄져 있다는 단순함을 잃을 수밖에 없으며, AQS에 정의돼 있지만 사용하지는 않는 메소드가 public으로 노출돼 있기 때문에 여러 가지 비슷한 메소드에 혼동돼 잘못 사용할 위험도 높다. 실제로 java.util.concurrent 패키지에 들어 있는 동기화 클래스 가운데 AQS를 직접 상속받는 클래스는 하나도 없고, 모두 AQS를 private인 내부 클래스로 선언해 위임 기법을 사용하고 있다.  

#### 14.6. java.util.concurrent 패키지의 동기화 클래스에서 AQS 활용 모습
<br/>
##### 14.6.1. ReentrantLock
<br/>
ReentrantLock은 배타적인 확보 연산만 제공하기 때문에 tryAcquire, tryRelease, isHeldExclusively와 같은 메소드만 구현하고 있다. 공정하지 않은 형태로 동작하는 tryAcquire 메소드의 코드는 다음 예제에서 볼 수 있다. ReentrantLock에서는 동기화 상태 값을 확보된 락의 개수를 확인하는 데 사용하고, owner라는 변수를 통해 락을 가져간 스레드가 어느 스레드인지도 관리한다. owner 변수에는 현재 스레드에서 락을 확보할 때 현재 스레드를 추가하고, 해제되는 시점에 owner에서 현재 스레드를 제거하도록 돼 있다. 내부 상태를 변경하는 외부에 노출되지 않은 메소드가 volatile 메모리 특성을 갖고 있으며, ReentrantLock 역시 getState 메소드를 호출한 이후에만 owner 변수의 값을 읽어가고 setState 메소드를 호출하기 전에 owner 변수에 값을 추가하도록 섬세하게 구현돼 있다. 이렇게 ReentrantLock은 동기화 상태가 메모리에서 동작하는 구조를 잘 활용하고 있으며, 따라서 추가적인 동기화 구조를 갖추지 않아도 충분하다. tryRelease 메소드에서는 unlock 메소드를 호출하기 전에 ownver 변수에 들어 있는 내용을 들여다보고 해당 락을 확보하고 있는 스레드가 현재 스레드인지를 확인한다. tryAcquire 메소드에서는 락을 확보하려는 시도가 재진입 시도인지 아니면 최초로 락을 확보하려는 것인지 구분하기 위한 용도로 owver 변수의 내용을 사용한다.  

스레드에ㅓㅅ 락을 확보하려고 하면 tryAcquire 메소드는 먼저 락의 상태를 확인한다. 락이 풀려 있는 상태라면 락을 확보했다는 사실을 알릴 수 있도록 상태 값을 업데이트해본다. 락의 상태를 확인하고 값을 업데이트하는 동안에 다른 스레드에서 락의 상태를 변경할 가능성이 있기 때문에 tryAcquire 메소드는 compareAndSetState 메소드를 사용해 상태 값을 단일 연산으로 업데이트하며, 이런 방법을 사용하면 락 확보 여부를 확인하고 값을 업데이트하는 사이에 다른 스레드에서 값을 사용하는 경우를 방지할 수 있다. 락의 상태를 확인했는데 이미 확보된 상태라고 판단되면, 락을 확보하고 있는 스레드가 현재 스레드인지를 확인하고 만약 그렇다면 락 확보 개수를 증가시킨다. 만약 락을 확보하고 있는 스레드가 현재 스레드가 아니라면 확보 시도가 실패한 것으로 처리한다.  

```java
protected boolean tryAcquire(int ignored) {
  final Thread current = Thread.currentThread();
  int c = getState();
  if (c == 0) {
    if (compareAndSetState (0, 1)) {
      owner = current;
      return true;
    }
  } else if (current == owner) {
    setState(c+1);
    return true;
  }
  return false;
}
```

ReentrantLock은 AQS가 기본적으로 제공하는 기능이라고 할 수 있는 다중 조건 변수와 대기 큐도 그대로 사용하고 있다. Lock.newCondition 메소드를 호출하면 AQS의 내부 클래스인 ConditionObject 객체를 받아서 사용할 수 있다.  

##### 14.6.2. Semaphore와 CountDownLatch
<br/>
Semaphore는 AQS의 동기화 상태를 사용해 현재 남아 있는 퍼밋의 개수를 관리한다. 다음 예제에 소개돼 있는 tryAcquireShared 메소드는 먼저 현재 남아 있는 퍼밋의 개수를 알아내고, 남아 있는 퍼밋의 개수가 모자란다면 확보에 실패했다는 결과를 리턴한다. 반대로 충분한 개수의 퍼밋이 남아 있었다면 compareAndSetState 메소드를 사용해 단일 연산으로 퍼밋의 개수를 필요한 만큼 줄인다. 퍼밋의 개수를 줄이는 작업이 성공하면(다시 말해 퍼밋의 개수를 확인한 이후에 개수를 줄이는 작업이 시작되기 전에 다른 스레드에서 퍼밋을 사용해 버리지 않았다면) 확보 연산이 성공했다는 결과를 리턴한다. 리턴되는 결과 값에는 성공 여부와 함께 다른 스레드에서 실행하던 확보 연산을 처리할 수 있을지의 여부도 포함돼 있는데, 그렇다면 다른 스레드 역시 대기 상태에서 풀려날 수 있다.  

메소드 내부의 while 반복문은 충분한 개수의 퍼밋이 없거나 tryAcquireShared 메소드가 확보 연산의 결과로 퍼밋 개수를 단일 연산으로 변경할 수 있을 때까지 반복한다. compareAndSetState 메소드를 호출했을 때 다른 스레드와 경쟁하는 상태였다면 값을 변경하지 못하고 실패할 수도 있으며, 실패했다면 계속해서 재시도하게 되고, 허용할 만한 횟수 이내에서 재시도를 하다 보면 위의 두 가지 조건 가운데 하나라도 만족하게 된다. 이와 비슷하게 tryReleaseShared 메소드는 퍼밋의 개수를 증가시키며, 따라서 현재 대기 상태에 들어가 있는 스레드를 풀어줄 가능성도 있고, 성공할 때까지 상태 값 변경 연산을 재시도한다. tryReleaseShared 메소드의 리턴 결과를 보면 해제 연산에 따라 다른 스레드가 대기 상태에서 풀려났을 가능성 여부를 알 수 있다.  

CountDownLatch 클래스도 동기화 상태 값을 현재 개수로 사용하는, Semaphore와 비슷한 형태로 AQS를 활용한다. countDown 메소드는 release 메소드를 호춯하고, release 메소드에서는 개수 값을 줄이고 개수가 0에 이르렀다면 대기 중이던 스레드를 대기 상태에서 풀어준다. await 메소드는 acquire 메소드를 호출하며 클래스 내부의 개수가 0이라면 즉시 리턴되고, 0보다 큰 값이라면 대기 상태에 들어간다.  

```java
protected int tryAcquireShared(int acquires) {
  while (true) {
    int available = getState();
    int remaining = available - acquire;
    if (remaining < 0 || compareAndSetState(available, remaining))
      return remaining;
  }
}

protected boolean tryReleaseShared(int releases) {
  while (true) {
    int p = getState();
    if (compareAndSetState(p, p + releases))
      return true;
  }
}
```

##### 14.6.3. FutureTask
<br/>
그냥 겉으로 보기에 FutureTask 클래스는 동기화 클래스가 아닌 것처럼 보인다. 하지만 Future.get 메소드를 보면 래치 클래스와 굉장히 비슷한 기능을 갖고 있다. 바로 특정 이벤트(FutureTask가 담당하고 있는 작업이 완료되거나 아니면 취소되는 이벤트)가 발생하면 해당 스레드가 계속 진행할 수 있고, 아니면 원하는 이벤트가 발생할 때까지 스레드가 대기 상태에 들어간다.  

FutureTask는 작업의 실행 상황, 즉 실행 중이거나 완료됐거나 취소되는 등의 상황을 관리하는 데 AQS 내부의 동기화 상태를 활용한다. 그에 덧붙여 작업이 끝나면서 만들어낸 결과 값이나 작업에서 오류가 발생했을 때 해당하는 예외 객체를 담아둘 수 있는 추가적인 상태 변수도 가지고 있다. 게다가 실제 작업을 처리하고 있는 스레드에 대한 참조(현재 작업이 실행 중인 상태라면)도 갖고 있으며, 그래야만 인터럽트 요청이 들어왔을 때 해당 스레드에 인터럽트를 걸 수 있다.  

##### 14.6.4. ReentrantReadWriteLock
<br/>
ReadWriteLock 인터페이스를 보면 읽기 작업용과 쓰기 작업용의 두 개의 락을 사용하고 있다고 추측해 볼 수 있다. 하지만 AQS를 기반으로 구현된 ReentrantReadWriteLock 클래스는 AQS 하위 클래스 하나로 읽기 작업과 쓰기 작업을 모두 담당한다. ReentrantReadWriteLock은 상태 변수의 32개 비트 가운데 16비트로는 쓰기 락에 대한 개수를 관리하고, 나머지 16비트로는 읽기 락의 개수를 관리한다. 읽기 락에 대한 기능은 독점적이지 않은 확보와 해제 연산으로 구현돼 있고, 쓰기 락에 대한 기능은 독점적인 확보와 해제 연산을 사용한다.  

내부적으로 보면 AQS를 상속받은 클래스는 대기 중인 스레드의 큐를 관리하고, 스레드가 독점적인 연산을 요청했는지 아니면 독점적이지 않은 연산을 요청했는지도 관리한다. ReentrantReadWriteLock은 락에 여유가 생겼을 때 대기 큐의 맨 앞에 들어 있는 스레드가 쓰기 락을 요청한 상태였다면 해당 스레드가 락을 독점적으로 가져가고, 만약 맨 앞에 있는 스레드가 읽기 락을 요청한 상태였다면 쓰기 락을 요청한 다음 스레드가 나타나기 전까지 읽기 락을 요청하는 모든 스레드가 독점적이지 않은 락을 가져간다. 읽기 스레드나 쓰기 스레드 어느 한쪽에 우선 순위를 주는 최적화 기능을 지원하는 읽기-쓰기 락이 있긴 하지만, 여기에서 설명한 ReentrantReadWriteLock과 같은 구조에서는 읽기와 쓰기 어느 쪽에도 우선 순위를 주기가 쉽지 않다. 이런 최적화 기능을 제공하려면 AQS에서 제공하는 스레드 대기 큐가 단순한 FIFO 구조가 아닌 다른 형태로 구현돼 있거나, 아니면 아예 읽기 작업을 요청하는 스레드용 대기 큐와 쓰기 작업용 대기 큐를 따로 운영해야 할 수도 있다. 어쨌거나 이와 같은 기능을 꼭 필요로 하는 경우는 거의 없다고 봐도 좋다고 생각된다. 만약 공정하지 않은 버전의 ReentrantReadWriteLock 클래스에서 적절한 수준의 활동성을 제공하지 못한다면, 공정한 버전을 사용해보자. 그러면 작업 스레드가 진행되는 순서에도 만족할 수 있을 것이고 또한 읽기 스레드와 쓰기 스레드 어느 쪽도 멈춰서는 일이 발생하지 않는다는 점을 보장한다.  

### 요약
<br/>
상태 기반으로 동작하는 클래스, 즉 메소드 가운데 하나라도 상태 값에 따라 대기 상태에 들어갈 가능성이 있는 클래스를 작성해야 할 때 가장 좋은 방법은 바로 Semaphore, BlockingQueue, CountDownLatch 등을 활용해 구현하는 방법이다. 이미 많은 종류의 동기화 클래스가 제공되고 있음에도 불구하고 적절한 기능을 찾을 수 없다면, 암묵적인 조건 큐나 명시적인 Condition 클래스 또는 AbstractQueuedSynchronizer 클래스 등을 활용해 직접 원하는 기능의 동기화 클래스를 작성할 수도 있겠다. 상태 의존성을 관리하는 작업은 상태의 일관성을 유지하는 방법과 맞물려 있기 때문에 암묵적인 조건 큐 역시 암묵적인 락과 굉장히 밀접하게 관련돼 있다. 이와 비슷하게 명시적인 조건 큐인 Condition 클래스도 명시적인 Lock 클래스와 밀접하게 관련돼 있으며, 락 하나에서 다수의 대기 ㅅ큐를 활용하거나 대기 상태에서 인터럽트에 어떻게 반응하는지를 지정하는 기능, 스레드 대기 큐의 관리 방법에 대한 공정성 여부를 지정하는 기능, 대기 상태에서 머무르는 시간을 제한할 수 있는 기능 등과 같이 암묵적인 버전의 조건 큐나 락보다 훨씬 다양한 기능을 제공한다.  

### 15. 단일 연산 변수와 넌블로킹 동기화
<br/>
Semaphore, ConcurrentLinkedQueue와 같이 java.util.concurrent 패키지에 들어 있는 다수의 클래스는 단순하게 synchronized 구문으로 동기화를 맞춰 사용하는 것에 비교하면 속도도 빠르게 확장성도 좋다.  

병렬 알고리즘과 관련한 최근의 연구 결과를 보면 대부분이 넌블로킹 알고리즘, 즉 여러 스레드가 동작하는 환경에서 데이터의 안전송을 보장하는 방법으로 락을 사용하는 대신 저수준의 하드웨어에서 제공하는 비교 후 교환(compare-and-swap) 등의 명령을 사용하는 알고리즘을 다루고 있다. 넌블로킹 알고리즘은 운영체제나 JVM에서 프로세스나 스레드를 스케줄링 하거나 가비지 컬렉션 작업, 그리고 락이나 기타 병렬 자료 구조를 구현하는 부분에서 굉장히 많이 사용하고 있다.  

넌블로킹 알고리즘은 락을 기반으로 하는 방법보다 설계와 구현 모두 훨씬 복잡하며, 대신 확장성과 활동성을 엄청나게 높여준다. 넌블로킹 알고리즘은 훨씬 세밀한 수준에서 동작하며, 여러 스레드가 동일한 자료를 놓고 경재하는 과정에서 대기 상태에 들어가는 일이 없기 때문에 스케줄링 부하를 대폭 줄여준다. 더군다나 데드락이나 기타 활동성 문제가 발생할 위험도 없다. 락을 기반으로 하는 알고리즘은 특정 스레드가 락을 확보한 상태에서 잠자기 상태에 들어가거나 반복문을 실행하면 다른 스레드는 그 시간 동안 각자의 작업 가운데 락이 필요한 부분을 전혀 실행할 수 없다. 반면 넌블로킹 알고리즘을 사용하는 경우에는 개별 스레드에서 발생하는 오류에 의해 영향을 받는 일이 없다. 자바 5.0부터는 AtomicInteger나 AtomicReference 등의 단일 연산 변수(atomic variable)를 사용해 넌블로킹 알고리즘을 효율적으로 구현할 수 있게 됐다.  

단일 연산 변수는 본격적인 넌블로킹 알고리즘을 구현하는 일이 아니라 해도 '더 나은 volatile 변수'의 역할만으로 사용할 수도 있다. 단일 연산 변수는 volatile 변수와 동일한 메모리 유형을 갖고 있으며 이에 덧붙여 단일 연산으로 값을 변경할 수 있는 기능을 갖고 있다. 따라서 이런 특성을 사용해 숫자 카운터, 일련번호 생성기, 통계 수치 추출기 등으로 활용하면 락 기반의 구조에 비해 높은 확장성을 얻을 수 있다.  

#### 15.1. 락의 단점
<br/>
공유된 상태에 접근하려는 스레드에 일관적인 락 구조를 적용해 동기화하면 특정 변수를 보호하고 있는 락을 확보한 스레드가 해당 변수에 대한 독점적인 접근 권한을 갖게 되며, 변수의 값을 변경했다고 하면 다음 스레드가 락을 확보했을 때 모든 변경된 사항을 완벽하게 볼 수 있다.  

최근 사용하는 JVM은 스레드 간의 경쟁이 없는 상태에서 락을 확보하는 부분을 최적화하는 기능을 갖고 있으며 락을 해제하는 부분도 굉장히 효율적이다. 하지만 락 확보 경쟁이 벌어지는 상황에서는 JVM 역시 운영체제의 도움을 받는다. 이런 경우 락을 확보하지 못한 스레드는 실행을 멈춰야 하며 나중에 조건이 충족되면 다시 실행시켜야 한다. 최적화된 JVM이라면 락 확보 경쟁에서 밀려난 스레드라 해도 반드시 실행을 멈추지 않기도 한다. 즉 이전에 동일한 코드를 실행하는 데 얼마만큼의 시간이 걸렸는지를 파악한 다음 경쟁에서 밀린 스레드의 실행을 잠시 멈출 것인지, 아니면 임시로 반목문을 추가해 실행을 중단한 효과만 내도록 할 것인지를 결정해서 동작한다. 실행을 잠시 멈추고 있던 스레드가 다시 실행하게 됐다 해도 실제 CPU를 할당받기 전에 이미 CPU를 사용하고 있는 다른 스레드가 CPU 할당량을 모두 사용하고 CPU 스케줄을 넘겨줄 때까지 대기해야 할 수도 있다. 이와 같이 스레드의 실행을 중단했다가 계속해서 실행하는 작업은 상당한 부하를 발생시키며 일반적으로 적지 않은 시간 동안 작업이 중단되게 된다. 락을 기반으로 세밀한 작업(예를 들어 대부분의 메소드가 몇 줄 되지 않는 짧은 코드로 동작하는 컬렉션 클래스의 동기화를 맞추는 등)을 주로 하도록 구현돼 있는 클래스는 락에 대한 경쟁이 심해질수록 실제로 필요한 작업을 처리하는 시간 대비 동기화 작업에 필요한 시간의 비율이 살당한 수치로 높아질 가능성이 있다.  

volatile 변수는 락과 비교해 봤을 때 컨텍스트 스위칭이나 스레드 스케줄링과 아무런 연관이 없기 대문에 락보다 훨씬 가벼운 동기화 방법이라고 볼 수 있다. 반면 volatile 변수는 락과 비교할 때 가시성 측면에서는 비슷한 수준을 보장하긴 하지만, 복합 연산을 하나의 단일 연산으로 처리할 수 있게 해주는 기능은 전혀 갖고 있지 않다. 해당 변수의 새로운 값이 이전 값과 관련이 있다면 volatile 변수를 사용할 수가 없다. 이런 특성 때문에 volatile 변수로는 카운터나 뮤텍스(mutex)를 구현할 수 없으며, 따라서 전체적으로 volatile 변수를 사용할 수 있는 부분이 상당히 제한된다. volatile 변수를 사용한다 해도 뮤텍스나 기타 다른 동기화 구조를 구현하지 못하는 것은 아니다. 단지 이론적으로 구현이 가능하긴 하지만 전혀 실용적이지 못할 뿐이다.  

예를 들어 (++i)와 같은 증가 연산이 단일 연산인 것처럼 보이기는 하지만, 실제로는 변수의 현재 값을 읽어오고, 읽어온 값에 1을 더하고, 더해진 값을 변수에 다시 설정하는 세 가지 연산의 조합으로 구현돼 있다. 여러 스레드가 동작하는 과정에서 값을 제대로 변경하려면 읽고 변경하고 쓰는 세 가지 작업 전체가 하나의 단일 연산으로 동작해야 한다. 지금까지는 두 개 이상의 작업을 하나의 단일 연산으로 묶으려면 락을 사용해야만 했다.  

락 기반의 동기화 방법에는 또 다른 단점도 있다. 스레드가 락을 확보하기 위해 대기하고 있는 상태에서 대기 중인 스레드는 다른 작업을 전혀 못한다. 이런 상태에서 락을 확보하고 있는 스레드의 작업이 지연되면(메모리 페이징이나 스케줄링 문제 등에 의해 지연될 가능성이 충분히 있다) 해당 락을 확보하기 위해 대기하고 있는 모든 스레드의 작업이 전부 지연된다. 더군다나 락을 확보하고 지연되는 스레드의 우선 순위가 떨어지고 대기 상태에 있는 스레드의 우선 순위가 높다면 프로그램의 성능에 심각한 영향을 미칠 수 있으며, 이런 현상을 우선 순위 역전(priority inversion)이라고 부른다. 즉 대기 중인 스레드의 우선 순위가 높음에도 불구하고 락이 해제될 때까지 대기해야 하며, 결과적으로 락을 확보하고 있는 스레드의 우선 순위보다 더 낮은 우선 순위를 가진 것처럼 동작한다. 최악의 상황에서 락을 홥고하고 있던 스레드가 영원히 멈추는 상황(무한 반복문에 빠지거나 데드락이 걸리거나 기타 다른 활동성 문제)이 발생하면 대기 중이던 모든 스레드 역시 영원히 대기하고 동작을 멈추게 된다.  

이와 같은 오류까지 생각하지 않더라도 카운터 값을 증가시키는 등의 세밀한 작은 연산을 동기화하기에는 락이 너무 무거운 방법이다. 따라서 스레드 간의 경쟁을 관리할 수 있는 훨씬 가볍고 세밀한 연산에도 적당한 방법이 있으면 좋지 않을까? 다시 말해 volatile 변수와 같이 가벼우면서 단일 연산 조건까지 충족시키는 그런 방법 말이다. 다행스럽게도 요즘 사용되는 대부분의 프로세서는 그와 같은 방법을 제공하고 있다.  

#### 15.2. 병렬 연산을 위한 하드웨어적인 지원
<br/>
배타적인 락 방법은 보수적인 동기화 기법이다. 즉 가장 최악의 상황을 가정하고 완전하게 확실한 조치를 취하기 전에는 더 이상 진행하지 않는 방법을 택하고 있는데, 바로 락을 확보하고 나면 다른 스레드가 절대 간섭하지 못하는 구조이다.  

세밀하고 단순한 작업을 처리하는 경우에는 일반적으로 훨씬 효율적으로 동작할 수 있는 낙관적인 방법이 있는데, 일단 값을 변경하고 다른 스레드의 간섭 없이 값이 제대로 변경되는 방법이다. 이 방법에는 충돌 검출(collision detection) 방법을 사용해 값을 변경하는 동안 다른 스레드에서 간섭이 있었는지를 확인할 수 있으며, 만약 간섭이 있었다면 해당 연산이 실패하게 되고 이후에 재시도하거나 아ㅖ 재시도조차 하지 않기도 한다. 이 방법은 어른들이 "허락을 받기보다는 나중에 용서를 구하는 편이 더 쉽다"라고 하는 말씀에서 "쉽다" 대신 "더 효율적이다"를 사용한 것과 같다.  

멀티프로세서 연산을 염두에 두고 만들어진 프로세서는 공유된 변수를 놓고 동시에 여러 작업을 해야 하는 상황을 간단하게 관리할 수 있도록 특별한 명령어를 제공한다. 초기의 프로세서는 확인하고 값 설정(test-and-set), 값을 읽어와서 증가(fetch-and-increment), 치환(swap) 등의 단일 연산을 하드웨어적으로 제공했으며, 이런 연산을 기반으로 더 복잡한 병렬 클래스를 쉽게 만드는 데 도움이 되는 뮤텍스(mutex)를 충분히 구현할 수 있었다. 최근에는 거의 모든 프로세서에서 읽고-변경하고-쓰는 단일 연산을 하드웨어적으로 제공하고 있다. 예를 들어 비교하고 치환(compare-and-swap), LL(load-linked/)/SC(store-conditional) 등의 연산 등이 있다. 운영체제와 JVM은 모두 이와 같은 연산을 사용해 락과 여러 가지 병렬 자료 구조를 작성했지만, 자바 5.0 이전에는 자바 클래스에서 직접 이런 기능을 사용할 수는 없었다.  

##### 15.2.1. 비교 후 치환
<br/>
IA₃₂나 Sparc과 같은 프로세서에서 채택하고 있는 방법은 비교 후 치환(CAS, compare and swap) 명렬을 제공하는 방법이다(PowerPC와 같은 다른 프로세서에서는 CAS와 동일한 기능을 LL/SC와 같이 두 개의 단일 연산으로 제공한다). CAS 연산에는 3개의 인자를 넘겨주는데, 작업할 대상 메모리의 위치인 V, 예상하는 기존 값인 A, 새로 설정할 값인 B의 3개이다. CAS 연산은 V 위치에 있는 값이 A와 같은 경우에 B로 변경하는 단일 연산이다. 만약 이전 값이 A와 달랐다면 아무런 동작도 하지 않는다. 그리고 값을 B로 변경했건 못했건 간에 어떤 경우라도 현재 V의 값을 리턴한다(비교 후 설정(compare and set)이라는 약간 다른 연산의 경우 리턴되는 값은 설정 연산이 성공했는지의 여부라는 점을 참고하자). 즉 CAS 연산의 동작하는 모습을 말로 풀어보면, "V에 들어 있는 값이 A라고 생각되며, 만약 실제로 V의 값이 A라면 B라는 값으로 바꿔 넣어라. 만약 V의 값이 A가 아니라면 아무 작업도 하지 말고, V의 값이 뭔지를 알려달라"는 것이다. 앞서 소개한 것처럼 CAS 연산은 낙관적인 기법이다. 다시 말해 일단 성공적으로 치환할 수 있을 것이라고 희망하는 상태에서 연산을 실행해보고, 값을 마지막으로 확인한 이후에 다른 스레드가 해당하는 값을 변경했다면 그런 사실이 있는지를 확인이나 하자는 의미아다.  

만약 여러 스레드가 동시에 CAS 연산을 사용해 한 변수의 값을 변경하려고 한다면, 스레드 가운데 하나만 성공적으로 값을 변경할 것이고, 다른 나머지 스레드는 모두 실패한다. 대신 값을 변경하지 못했다고 해서 락을 확보하는 것처럼 대기 상태에 들어가는 대신, 이번에는 값을 변경하지 못했지만 다시 시도할 수 있다고 통보를 받는 셈이다. CAS 연산에 실패한 스레드도 대기 상태에 들어가지 않기 때문에 스레드 마다 CAS 연산을 다시 시도할 것인지, 아니면 다른 방법을 취할 것인지, 아니면 아예 아무 조치도 취하지 않을 것인지를 결정할 수 있다. CAS 연산이 실패했을 때 아무 조치도 취하지 않는 방법이야말로 올바른 처리 방법일 수 있다. CAS 연산에서 실패했다는 건 바로 현재 스레드에서 하려고 했던 일을 다른 스레드에서 먼저 처리했다는 결과로 받아들일 수 있기 때문이다. 이와 같은 CAS 연산의 유연성 때문에 락을 사용하면서 발생할 수밖에 없었던 여러 가지 활동성 문제를 미연에 방지할 수 있다.  

CAS를 활용하는 일반적인 방법은 먼저 V에 들어 있는 값 A를 읽어내고, A 값을 바탕으로 새로운 값 B를 만들어 내고, CAS 연산을 사용해 V에 들어 있는 A 값을 B값으로 변경하도록 시도한다. 그러면 다른 스레드에서 그 사이에 V의 값을 A가 아닌 다른 값으로 변경하지 않은 한 CAS 연산이 성공하게 된다. 이처럼 CAS 연산을 사용하면 다른 스레드와 간섭이 발생했는지를 확인할 수 있기 때문에 락을 사용하지 않으면서도 읽고-변경하고-쓰는 연산을 단일 연산으로 구현해야 한다는 문제를 간단하게 해결해준다.  

##### 15.2.2. 넌블로킹 카운터
<br/>
```java
@ThreadSafe
public class SimulatedCAS {
  @GuardedBy("this") private int value;

  public synchronized int get() { return value; }

  public synchronized int compareAndSwap(int expectedValue, int newValue) {
    int oldValue = value;
    if (oldValue == expectedValue)
      value = newValue;
    return oldValue;
  }

  public synchronized boolean compareAndSet(int expectedValue, int newValue) {
    return (expectedValue == compareAndSwap(expectedValue, newValue));
  }
}
```

```java
@ThreadSafe
public class CasCounter {
  private SimulatedCAS value;

  public int getValue() {
    return value.get();
  }

  public int increment() {
    int v;
    do {
      v = value.get();
    } while (v != value.compareAndSwap(v, v + 1));
    return v + 1;
  }
}
```

위의 예제의 CasCounter 클래스는 CAS 연산을 사용해 대기 상태에 들어가지 않으면서도 스레드 경쟁에 안전한 카운터 클래스이다. 카운터 증가 연산은 표준적인 형태를 그대로 따른다. 즉 이전 값을 가져오고, 1을 더해 새로운 값으로 변경하고, CAS 연산으로 새 값을 설정한다. 만약 CAS 연산이 실패하면 그 즉시 전체 작업을 재시도한다. 스레드 간의 경쟁이 심한 경우에는 물론 라이브락 현상을 방지할 수 있도록 잠시 대기 상태에 들어가거나 약간 물러서는 전략을 취하는 방법이 좋을 수도 있지만, 그럼에도 불구하고 계속해서 재시도하는 전략도 괜찮은 방법이다.  

CasCounter 클래스는 대기 상태에 들어가는 일이 엇는데 그 대신 다른 스레드에서 역시 같은 카운터 객체를 계속해서 업데이트하고 있다면 여러 차례 재시도해야할 수도 있다(필요로 하는 기능이 카운터나 일련번호를 생성하는 기능이라면 AtomicInteger나 AtomicLong 클래스를 사용하는 편이 좋다. 이런 클래스는 몇 가지 산술 연산을 이미 단일 연산으로 구현해 갖고 있다).  

그냥 보기에는 CAS 기반의 카운터 클래스가 락 기반의 카운터 클래스보다 훨씬 성능이 떨어질 것처럼 보인다. 코드도 훨씬 길고 코드의 흐름도 더 복잡한데다 복잡한 CAS 연산까지 사용하고 있다. 하지만 실제 사용한 예를 보면 많지 않은 양의 경쟁이 있는 상황에서도 CAS 기반의 클래스가 락 기반의 클래스보다 성능이 훨씬 좋고, 경쟁이 없는 경우에도 락 기반의 방법보다 나은 경우가 있다. 경쟁이 없는 상태에서 락을 확보하는 가장 빠른 경로를 생각해보면 최소한 한 번의 CAS 연산이 실행돼야 하고 락과 관련된 기본적인 작업 몇 가지도 함께 실행해야 한다. 따라서 락 기반으로 구현한 카운터 클래스에서 가장 최적의 조건으로 실행되는 경우에도 CAS 기반의 카운터 클래스에서 일반적인 경우에 해당하는 경우보다 더 많은 작업을 하는 셈이다. (경쟁이 적거나 어느 정도의 경쟁이 발생할 때까지는) CAS 연산은 대부분 성공하는 경우가 많기 때문에 하드웨어에서 분기 지점과 흐름을 예측해 코드에 들어 있는 while 반복문을 포함한 복잡한 논리적인 작업 흐름 구조에서 발생할 수 있는 부하를 최소화할 수 있다.  

락 기반의 프로그램을 보면 언어적인 문법은 훨씬 간결하지만, JVM과 운영체제가 그 락을 처리하기 위한 작업은 겉보기와 달리 그렇게 간단하지 않다. 락을 사용하면 JVM 내부에서 상당히 복잡한 코드 경로를 따라 실행하게 되고, 운영체제 수준의 락이나 스레드 대기, 컨텍스트 스위칭 등의 기능을 불러다 쓰기도 한다. 최적의 경우라면 락을 사용한 부분에서 CAS 연산을 단 한 번만 사용하면 되는데, 이와 같은 최적의 경우에는 락을 사용해서 CAS 연산을 눈에 보이지 않는 부분에 숨기긴 했지만 그래서는 실행상의 어떤 이점도 얻을 수 없다. 반면 CAS 연산을 프로그램에서  직접 사용하면 JVM에서 특별한 루틴을 실행해야 할 필요도 없고, 운영체제의 함수를 호출해야 할 필요도 없고, 스케줄링 관련 작업을 따로 조절해야 할 필요도 없다. 애플리케이션 수준에서는 코드가 더 복잡해 보이지만, JVM이나 운영체제의 입장에서는 훨씬 적은 양의 프로그램만 실행하는 셈이다. CAS 연산의 가장 큰 단점은 호출하는 프로그램에서 직접 스레드 경쟁 조건에 대한 처리(즉 재시도하거나 나중에 처리하거나 무시해 버리는 등)를 해야 한다는 점이 있는데, 반면 락을 사용하면 락을 사용할 수 있을 때까지 대기 상태에 들어가도록 하면서 스레드 경쟁 문제를 알아서 처리해준다는 차이점이 있다. 실제적으로 CAS를 사용할 때 가장 큰 단점은 CAS 연산 주변의 프로그램 흐름을 올바르게 구현하기가 어렵다는 점이다.  

CAS 연산의 성능은 프로세서마다 크게 차이가 난다. CPU가 하나인 시스템에서는 CAS 연산을 수행할 때 다중 시스템과 같이 CPU 간의 동기화 작업이 필요 없기 때문에 몇 번의 CPU 사이클이면 충분하다. 이 글을 쓰는 시점에 다중 CPU 시스템에서 스레드 간의 경쟁이 없는 경우에 CAS 연산은 약 10~150번의 CPU 사이클을 소모한다. CAS 연산의 성능은 프로세서의 구조나 심지어는 같은 프로세서의 서로 다른 버전 간에도 너무나 다양한 차이를 보이고 있다. 여러 종류의 프로세서가 경쟁을 벌이다보면 앞으로 수년간은 CAS 연산의 성능이 크게 나아질 것으로 예상된다. 대략 정리하자면 스레드 간의 경쟁이 없는 상태에서 락을 가장 빠른 경로로 확보하고 해제하는 데 드는 자원은 CAS 연산을 사용할 때보다 약 2배 정도 된다고 보면 무리가 없다.  

##### 15.2.3. JVM에서의 CAS 연산 지원
<br/>
그렇다면 자바 프로그램에서 어떻게 하드웨어 프로세서의 CAS 연산을 호출할 수 있을까? 자바 5.0 이전에는 짧더라도 네이티브 코드를 작성하지 않는 한 불가능한 일이었다. 자바 5.0부터는 int, long 그리고 모든 객체의 참조를 대상으로 CAS 연산이 가능하도록 기능이 추가됐고, JVM은 CAS 연산을 호출받았을 때 해당하는 하드웨어에 적당한 가장 효과적인 방법으로 처리하도록 돼 있다. CAS 연산을 직접 지원하는 플랫폼의 경우라면 자바 프로그램을 실행할 때 CAS 연산 호출 부분을 직접 해당하는 기계어 코드로 변환해 실행한다. 하드웨어에서 CAS 연산을 지원하지 않는 최악의 경우에는 JVM 자체적으로 스핀 락을 사용해 CAS 연산을 구현한다. 이와 같은 저수준의 CAS 연산은 단일 연산 변수 클래스, 즉 AtomicInteger와 같이 java.util.concurrent.atomic 패키지의 AtomicXxx 클래스를 통해 제공한다. java.util.concurrent 패키지의 클래스 대부분을 구현할 때 이와 같은 AtomicXxx 클래스가 직간접적으로 사용됐다.  

#### 15.3. 단일 연산 변수 클래스
<br/>
단일 연산 변수(atomic variable)는 락보다 훨씬 가벼우면서 세밀한 구조를 갖고 있으며, 멀티프로세서 시스템에서 고성능의 병렬 프로그램을 작성하고자 할대 핵심적인 역할을 한다. 단일 연산 변수를 사용하면 스레드가 경쟁하는 버위를 하나의 변수로 좁혀주는 효과가 있으며, 이 정도의 범위는 프로그램에서 할 수 있는 가장 세밀한 범위이다(사용하는 알고리즘이 개별 변수 수준으로 세밀한 연산을 필요로 하는 경우에만 해당되는 말이긴 하다). 경쟁이 없는 상태에서 단일 연산 변수의 값을 변경하는 실행 경로는 락을 확보하는 가장 빠른 코드 실행 경로보다 느릴 수 없으며, 대부분 단일 연산 변수 쪽이 더 빠르게 실행된다. 느리게 실행되는 경우를 비교해보면, 락을 사용해 구현된 부분과 같이 대기 상태에 들어가거나 스레드 스케줄링과 관련된 문제가 발생하지 않기 대문에 단일 연산 변수를 사용하는 쪽이 명백하게 더 빠르게 실행된다. 따라서 락 대신 단일 연산 변수를 기반의 알고리즘으로 구현된 프로그램은 내부의 스레드가 지연되는 현상이 거의 없으며, 스레드 간의 경쟁이 발생한다 해도 훨씬 쉽게 경쟁 상황을 헤쳐나갈 수 있다.  

단일 연산 변수 클래스는 volatile 변수에서 읽고-변경하고-쓰는 것과 같은 조건부 단일 연산을 지원하도록 일반화한 구조이다. AtomicInteger 클래스는 int 값을 나타내며, 일반적인 volatile 변수로 사용할 때 변수의 값을 읽거나 쓰는 연산과 동일한 기능을 하는 get 메소드와 set 메소드를 제공한다. 또한 단일 연산으로 실행되는 compareAndSet 메소드(volatile 변수를 놓고 읽기 연산과 쓰기 연산을 실행한 것과 동일한 기능이다)도 제공하며, 그 외의 편의 사항인 단일 연산으로 값을 더하거나, 증가시키거나, 감소시키는 등의 메소드도 제공한다. 겉으로 보기에 AtomicInteger는 Counter 클래스와 굉장히 비슷한 모습을 갖고 있다. 하지만 동기화를 위한 하드웨어의 기능을 직접적으로 활용할 수 있기 때문에 경쟁이 발생하는 상황에서 훨씬 높은 확장성을 제공한다.  

일단 12개의 단일 연산 변수 클래스가 제공되며, 대략 일반 변수, 필드 업데이터(field updater), 배열, 르기로 조합 변수의 4개 그룹으로 나눠 볼 수 있다. 가장 많이 사용하는 형태는 바로 일반 변수의 형태를 그대로 갖고 있는 AtomicInteger, AtomicLong, AtomicBoolean, AtomicReference 클래스이다. 네 가지 모두 CAS 연산을 제공하며 AtomicInteger나 AtomicLong은 간단한 산술 기능도 제공한다(제공되지 않는 형태의 변수를 단일 연산 변수로 사용하려면, short나 byte의 경우에는 int로 강제 형변환해서 읽거나 쓰면 되고, 정수가 아닌 숫자를 사용하려는 경우에는 Float.floatToIntBits 메소드나 Double.doubleToLongBits 메소드를 사용하면 된다).  

단일 연산 배열 변수 클래스(Integer 형, Long 형, 그리고 Reference 형이 준비돼 있다)는 배열의 각 항목을 단일 연산으로 업데이트할 수 있도록 구성돼 있는 배열 클래스이다. 단일 연산 배열 클래스는 배열의 각 항목에 대해 volatile 변수와 같은 구조의 접근 방법을 제공하며, 일반적인 배열에서는 제공하지 않는 기능이다. 다시 말해 일반적인 배열 변수가 volatile이라 해도 배열 변수 자체에 대한 참조가 volatile일 뿐 각 항목까지 volatile 특성을 갖고 있지는 않았다.  

단일 연산 변수가 Number 클래스를 상속받고 있기는 하지만 Integer나 Long과 같은 클래스는 상속받지 않고 있다. 사실 Integer나 Long을 상속받을 수는 없다는 것이 정확한 표현이다. Integer나 Long과 같은 클래스는 변경 부가능한 클래스이지만, AtomicInteeger나 AtomicLong과 같은 단일 연산 클래스는 그 값을 변경할 있는 특징이 있기 때문이다. 단일 연산 클래스는 도한 hashCode 메소드나 equals 메소드를 재정의하고 있는 않으며, 모든 인스턴스가 서로 다르다. 내부 값을 변경할 수 있는 모든 클래스가 그렇지만, 해시 값을 기반으로 하는 컬렉션 클래스에 키 값으로 사용하기에는 적절하지 않다는 점도 잊지 말자.  

##### 15.3.2. 성능 비교: 락과 단일 연산 변수
<br/>
락과 단일 연산 변수 간의 확장성의 차이점을 확인할 수 있도록 여러 가지 방법으로 구현한 난수 발생기(PRNG, pseudo random number generator)의 처리 속도를 비교하는 벤치마크 테스트를 준비했다. 난수 발생기에서 만들어내는 다음 '임의'의 난수는 이전에 발생했던 난수를 기반으로 확정적인 함수(deterministic function)를 통해 만들어낸 결과 값이다. 따라서 난수 발생기는 항상 이전 결과 값을 내부 상태로 보존하고 있어야 한다.  

다음 예제들을 보면 스레드 안전한 난 수 발생 함수의 코드가 소개돼 있는데, 하나는 ReentrantLock을 사용해 구현했고, 또 하나는 AtomicInteger를 사용해 구현했다. 난수 발생기 테스트 프로그램은 각각의 함수를 계속해서 호출하며, 매번 반복할 때마다 난수 하나를 생성하고(난수 발생 작업은 공유된 seed 변수의 값을 읽어와서 변경하는 형태로 구성돼 있다). 스레드 내부의 값만을 사용해 '복잡한 작업'에 해당하는 반복 작업도 수행한다. 이렇게 구성돼 있는 이유는 공유된 자원을 놓고 동작하는 부분과 스레드 내부의 값만을 갖고 동작하는 부분을 함께 갖고 있는 일반적인 작업 형태를 묘사하기 위함이다.  

다음 그림들을 보면 매번 반복될 대마다 각각 적은 양과 많은 양에 해당하는 작업을 처리하는 경우의 성능을 볼 수 있다. 스레드 내부의 데이터만으로 처리하는 작업량이 적은 경우에는 락이나 단일 연산 변수 쪽에서 상당한 경쟁 상황을 겪고, 반대로 스레드 내부 작업의 양이 많아지면 상대적으로 락이나 단일 연산 변수에서 경쟁 상황이 덜 벌어진다.  

```java
@ThreadSafe
public class ReentrantLockPseudoRandom extends PseudoRandom {
  private final Lock lock = new ReentrantLock(false);
  private int seed;

  ReentrantLockPseudoRandom(int seed) {
    this.seed = seed;
  }

  public int nextInt(int n) {
    lock.lock();
    try {
      int s = seed;
      seed = calculateNext(s);
      int remainder = s % n;
      return remainder > 0 ? remainder : remainder +
    } finally {
      lock.unlock();
    }
  }
}
```

```java
@ThreadSafe
public class AtomicPseudoRandom extends PseudoRandom {
  private AtomicInteger seed;

  AtomicPseudoRandom(int seed) {
    this.seed = new AtomicInteger(seed);
  }

  public int nextInt(int n) {
    while (true) {
      int s = seed.get();
      int nextSeed = calculateNext(s);
      if (seed.compareAndSet(s, nextSeed)) {
        int remainder = s % n;
        return remainder > 0 ? remainder : remainder + n;
      }
    }
  }
}
```

<img src="/assets/images/Java_Concurrency_In_Practice/15-1.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Java_Concurrency_In_Practice/15-2.jpg" width="100%" height="100%"/>
<br/>

그래프에서 보다시피 경쟁이 많은 상황에서는 단일 연산 변수보다 락이 더 빠르게 처리되는 모습을 볼 수 있지만, 훨씬 실제적인 경쟁 상황에서는 단일 연산 변수가 락보다 성능이 더 좋다. 이런 결과는 전혀 다른 분야에서도 동일하게 나타난다. 예를 들어 교차로에 교통량이 많은 상황에는 신호등을 설치하는 것이 교통소통에 도움이 되고, 교통량이 적은 경우에는 로터리 구조로 만드는 것이 도움이 된다. 이더넷 네트웍에서 사용하는 경쟁 처리 알고리즘은 네트웍 트래픽의 양이 많지 않을 때 효과적이지만, 토큰링 네트웍에서 사용하는 토큰 절달 알고리즘을 사용하면 트래픽이 많은 경우에 더 효과적이다. 이런 현상은 락의 특성 때문에 나타난다. 즉 락을 두고 경쟁이 발생하면 대기 상태에 들어가는 스레드가 나타나는데, 일부 스레드가 대기 상태에 들어가면 전체적인 CPU 사용률과 공유된 메모리 버스의 동기화 트래픽이 줄어드는 효과에 의해 처리 속도가 높아진다(이런 작업 흐름은 프로듀서-컨슈머 디자인 패턴에서 프로듀서가 잠시 대기 상태에 들어가면그로 인해 컨슈머에게 돌아가는 부하가 줄어들고 큐에 쌓여 있는 작업을 처리하는 기회가 되는 것과 비슷한 구조라고 볼 수 있다). 반면 단일 연산 변수를 사용하면 경쟁 조건에 대한 처리 작업의 책임이 경쟁하는 스레드에게 넘어간다. CAS 연산 기반의 알고리즘이 대부분 그렇지만 AtomicPseudoRandom 클래스는 경쟁이 발생하면 그 즉시 재시도하는 것으로 대응하며, 일반적으로는 괜찮은 방법이긴 하지만 경쟁이 심한 겯우에는 경쟁을 계속해서 심하게 만드는 요인이 되기도 한다.  

AtomicPseudoRandom 클래스가 잘못 만들어졌다거나 단일 연산 변수가 락에 비해서 좋지 않은 선택이라고 단정짓고 탓하기 전에, 위의 그림의 결과를 만들었던 경쟁의 수준이 비상식적으로 높았다는 점을 알아야 한다. 정상적인 프로그램에서는 아무 일도 하지 않으면서 락이나 단일 연산 변수에 경쟁 상황만 만들어 내는 경우는 없다고 봐야 한다. 실제로 단일 연산 변수는 일반적인 경쟁 수준에서 경쟁 상황을 더 효율적으로 처리하기 때문에 단일 연산 변수가 락에 비해서 확장성이 좋다.  

어쨌거나 경쟁 수준에 따라 락과 단일 연산 변수의 처리 능력이 반화하는 모습을 보면 각각의 장점과 단점을 쉽게 파악할 수 있다. 경쟁이 적거나 보통의 경쟁 수준에서는 단일 연산 변수를 사용해야 확장성을 높일 수 있다. 경쟁 수준이 아주 높은 상황에서는 락을 사용하는 쪽이 경쟁에 더 잘 대응하는 모습을 보인다(단일 CPU 시스템에서는 CAS 연산의 읽고-변경하고-쓰는 과정에서 해당 스레드가 멈춰버리지 않는 이상 CAS 연산이 항상 성공하기 때문에, 단일 CPU 시스템에서는 CAS 기반의 알고리즘이 락 기반의 알고리즘보다 성능이 더 좋게 나타난다).  

위의 그림 양쪽 모두 ReentrantLock과 AtomicInteger 말고 세 번째의 수치가 나타나 있다. 바로 Lock이나 Atomic 변수를 사용하지 않고 대신 ThreadLocal을 사용하면 난수 발생 기능이 동작하는 구조를 변경하는 셈이다. 즉 락이나 단일 연산 변수를 사용하는 구조에서 하나의 값을 모든 스레드가 공유하는 것과 달리 각 스레드는 서로 각자에게 소속된 난수만을 볼 수 있다. 결국 상태 변수를 공유하지 않고 동작하는 방법이 있다면 최대한 공유하지 않는 쪽이 더 낫다는 사실을 보여준다. 스레드 간의 경쟁을 최대한 적절하게 처리하면 확장성을 어느 정도 향상시킬 수 있다. 하지만 최종적을 확장성을 가장 높일 수 있는 방법은 스레드 간의 경쟁이 발생하지 않도록 미연에 방지하는 방법이라는 점을 알아두자.  

### 15.4. 넌블로킹 알고리즘
<br/>
락 기반으로 동작하는 알고리즘은 항상 다양한 종류의 가용성 문제에 직면할 위험이 있다. 락을 현재 확보하고 있는 스레드가 I/O 작업 때문에 대기 중이라거나, 메모리 페이징 때문에 대기 중이라거나, 기타 어떤 원인 때문에라도 대기 상태에 들어간다면 다른 모든 스레드가 전부 대기 상태에 들어갈 가능성이 있다. 특정 스레드에서 작업이 실패하거나 또는 대기 상태에 들어가는 경우에, 다른 어떤 스레드라도 그로 인해 실패하거나 또는 대기 상테에 들어가지 않는 알고리즘을 대기 상태에 들어가지 않는 알고리즘, 즉 넌블로킹(non-blocking) 알고리즘이라고 한다. 또한 각 작업 단계마다 일부 스레드는 항상 작업을 진행할 수 있는 경우 락 프리(lock-free) 알고리즘이라고 한다. 스레드 간의 작업 조율을 위해 CAS 연산을 독점적으로 사용하는 알고리즘을 올바로 구현한 경우에는 대기 상태에 들어가지 않는 특성과 락 프리 특성을 함께 가지게 된다. 여러 스레드가 경쟁하지 않는 상황이라면 CAS 연산은 항상 성공하고, 여러 스레드가 경쟁을 한다고 해도 최소한 하나의 스레드는 반드시 성공하기 때문에 성공한 스레드는 작업을 진행할 수 있다. 넌블로킹 알고리즘은 데드락이나 우선 순위 역전(priority inversion) 등의 문제점이 발생하지 않는다(물론 지속적으로 재시도만 하고 있을 가능성도 있기 대문에 라이브락 등의 문제점이 발생할 가능성도 있기는 하다). 지금까지 넌블로킹 알고리즘 하나를 소개한 적이 있는데, 바로 CasCounter 클래스이다. 스택, 큐, 우선순위 큐, 해시 테이블 등과 같은 일반적인 데이터 구조를 구현할 때 대기 상태에 들어가지 않는 좋은 알고리즘이 많이 공개돼 있다. 아주 특별한 경우에 새로운 알고리즘을 개발하는 일 은 전문가에게 맡기는 편이 좋다.  

##### 15.4.1. 넌블로킹 스택
<br/>
대기 상태에 들어가지 않도록 구현한 알고리즘은 락 기반으로 구현한 아고리즘에 비해 상당히 복잡한 경우가 많다. 넌블로킹 알고리즘을 구성할 대 가장 핵심이 되는 부분은 바로 데이터의 일관성을 유지하면서 단일 연산 변경 작업의 범위를 단 하나의 변수로 제한하는 부분이다. 큐와 같이 연결된 구조를 갖는 컬렉션 클래스에서는 상태 전환을 개별적인 링크에 대한 변경 작업이라고 간주하고, AtomicReference로 각 연결 부분을 관리해서 단일 연산으로만 변경할 수 있도록 하면 어느 정도 구현이 가능하다.  

스택은 연결 구조를 갖는 자료 구조 가운데 가장 간단한 편에 속한다. 각 항목은 각자 단 하나의 다른 항목만을 연결하고 있고, 반대로 각 항목은 단 하나의 항목에서만 참조된다. 다음 예제의 ConcurrentStack 클래스는 단일 연산 참조를 사용해 스택을 어떻게 구현하는지를 보여주는 좋은 예이다. 스택 자체는 Node 클래스로 구성된 연결 리스트이며, 최초 항목은 top 벼수에 들어 있고, 각 항목마다 자신의 값과 다음 항목에 대한 참조를 갖고 있다. push 메소드에서는 새로운 Node 인스턴스를 생성하고, 새 Node의 next 연결 값으로 현재의 top 항목을 설정한 다음, CAS 연산을 통해 새로운 Node를 스택의 top으로 설정한다. CAS 연산을 시작하기 전에 알고 있던 top 항목이 CAS 연산을 시작한 이후에도 동일한 값이었다면 CAS 연산이 성공한다. 반대로 다른 스레드에서 그 사이에 top 항목을 변경했다면 CAS 연산이 실패하며, 현재의 top 항목을 기준으로 다시 새로운 Node 인스턴스를 top으로 설정하기 위해 CAS 연산을 재시도한다. CAS 연산이 성공하거나 실패하는 어떤 경우라 해도 스택은 항상 안전적인 상태를 유지한다.  

CasCounter와 ConcurrentStack 클래스는 대기 상태에 들어가지 않는 알고리즘의 여러 가지 특성을 모두 보여주고 있다, 즉, 작업이 항상 성공하는 것은 아니며, 재시도해야 할 수도 있다, ConcurrentStack에서는 새로 추가된 항목을 나타내는 Node 인스턴스를 새로 만들 때 next 변수에 지정한 항목이 스택에 모두 연결된 이후에도 정상적인 항목으로 연결돼 있기를 기대하고 있는 셈이고, 다만 경쟁이 있는 상황이라면 그렇지 못할 수도 있으니 재시도할 준비를 하고 있을 뿐이다.  

ConcurrentStack에서와 같이 대기 상태에 들어가지 않는 알고리즘은 락과 같이 compareAndSet 연산을 통해 단일 연산 특성과 가시성을 보장하기 대문에 스레드 안전성을 보장한다. 특정 스레드에서 스택의 상태를 변경했다면 상태를 변경할 때 volatile 쓰기 특성이 있는 compareAndSet 연산을 사용해야만 한다. 특정 스레드에서 스택의 값을 읽어간다면 변경을 가할 대와 동일한 AtomicReference 객체에 대해서 get 메소드를 호출하게 되며, 이는 정확하게 volatile 읽기 특성을 갖고 있다. 따라서 어느 스레드에서건 스택의 내용을 변경하면 스택의 내용을 확인하는 모든 스레드가 변경된 내용을 즉시 볼 수 있다. 그리고 스택 내부의 리스트는 단일 연산으로 top 항목을 변경하거나 또는 다른 스레드와의 경쟁 관계에서 아예 실패하는 compareAndSet 메소드를 사용해 변경하도록 돼 있다.  

```Java
@ThreadSafe
public class ConcurrentStack<E> {
  AtomicReference<Node<E>> top = new AtomicReference<Node<E>>();

  public void push(E item) {
    Node<E> newHead = new Node<E>(item);
    Node<E> oldHead;
    do {
      oldHead = top.get();
      newHead.next = oldHead;
    } while (!top.compareAndSet(oldHead, newHead));
  }

  public E pop() {
    Node<E> oldHead;
    Node<E> newHead;
    do {
      oldHead = top.get();
      if (oldHead == null)
        return null;
      newHead = oldHead.next;
    } while (!top.compareAnd(oldHead, newHead));
    return oldHead.item;
  }

  private static class Node<E> {
    public final E item;
    public Node<E> next;

    public Node(E item) {
      this.item = item;
    }
  }
}
```

##### 15.4.2. 넌블로킹 연결 리스트
<br/>
지금까지 카운터와 스택이라는 두 가지 자료 구조를 놓고 대기 상태에 들어가지 않는 알고리즘을 살펴봤다. 양쪽 모두 반드시 성공하리라는 보장이 없는 CAS 연산을 사용해 값을 변경하고 만약 변경하지 못했다면 재시도하는, CAS 연산을 사용하는 알고리즘이 갖고 있는 일반적인 구조를 살펴봣다, 앞서 설명했지만 넌블로킹 랄고리즘을 작성할 때의 핵심은 바로 단일 연산의 범위를 단 하나의 변수로 제한하는 부분이다 그런데 카운터 정도의 클래스를 구현할 때는 단일 연산의 범위를 좁히는 일이 간단했고, 스택의 경우에도 그다지 어려운 편은 아니었다. 하지만 큐, 해시 테이블 또는 트리와 같이 약간 복잡한 구조를 놓고 보면 복잡하면서도 편법을 많이 사용하게 된다.  

연결 큐는 리트스의 머리와 고리 부분에 직접적으로 접근할 수 있어야 하기 때문에 스택보다 훨씬 복잡한 구조를 갖고 있다. 일단 머리와 꼬리 부분에 직접 접근하려면 각 항목에 대한 참조를 서로 다른 두 개의 변수에 각자 보관해야 한다. 따라서 꼬리 항목을 가리키는 참조가 두개가 존재하게 되는데, 하나는 현재 가장 마지막에 있는 항목의 next 값이고, 또 하나는 직접 접근하기 위해 따로 보관하고 있는 변수이다. 새로운 항목을 연결 큐에 추가하려면 마지막 항목을 가리키는 두 개의 참조가 동시에 단일 연산으로 변경돼야 한다. 일단 생각하기에 이런 작업은 단일 연산 변수로는 처리할 수가 없다. 두 개의 참조를 업데이트할 때 두 번의 CAS 연산이 필요한데, 만약 첫 번째 CAS 연산은 성공했지만 두 번째 CAS 연산은 실패했다고 하면 연결 큐가 올바르지 않은 상태에 놓이게 된다. 따라서 연결 큐를 대기 상태에 들어가지 않도록 구현할 수 있는 알고리즘은 이와 같은 두 가지 경우를 모두 처리할 수 있어야 한다.  

이와 같은 기능을 구현하려면 여러 가지 전략을 동원해야 한다. 첫 번째로는 데이터 구조가 여러 단계의 변경 작업을 거치는 과정을 포함해 언제라도 일관적인 상태를 유지하도록 해야 한다. 그래야만 스레드 B가 등장하는 시점에 스레드 A가 값을 변경하고 있었다고 해도 현재 다른 스레드에서 변경 작업을 진행 중이라는 사실을 스레드 B가 알 수 있어야 하며, 스레드 B가 하고자 하는 변경 작업을 당장 시작하지 않도록 조율할 수 있어야 한다. 다시 말해 스레드 B는 스레드 A의 변경 작업이 마무리 될 때까지 기다리도록 하면 (반복적으로 큐의 상태를 확인하면서) 한쪽 스레드에서 변경 작업을 하고 있을 때 다른 스레드가 끼어들지 않게 된다.  

일단 이런 전략을 사용하면 일단 여러 스레드가 큐 내부의 데이터를 흐트러뜨리지 않으면서 차례대로 접근할 수 있기는 하지만, 만약 차례대로 작업 중인 스레드 하나에서 오류가 발생한다면 다음 차례로 대기하던 스레드는 큐의 데이터를 사용하지 못하게 된다. 알고리즘이 실제로 대기 상태에 들어가지 않도록 구현하려면 특정 스레드에서 오류가 발생한다 해도 다른 스레드의 작업을 멈추게 해서는 안 된다는 점을 보장해야 한다. 따라서 두 번째 전략은 스레드 A가 값을 변경하는 와중에 스레드 B가 데이터 구조를 사용하고자 접근하는 경우에 "스레드 A가 처리 중인 작업을 마쳐야 한다"는 사실을 알 수 있는 충분한 정보를 데이터 구조에 넣어두는 방법이다. 만약 스레드 A가 작업을 마칠 때까지 기다릴 필요 없이 자신이 해야 할 일을 계속해서 진행할 수 있게 된다. 그리고 스레드 A가 마무리 작업을 할 시점이 되면 스레드 B가 마무리 작업을 밀 처리했음을 알게 된다.  

다음 예제의 LinkedQueue 클래스를 보면 마이클-스콧(Michael-Scott)의 넌블로킹 연결 큐 알고리즘(Michael and Scott, 1996) 가운데 값을 추가하는 부분의 코드가 소개돼있다. 마이클-스콧 알고리즘은 ConcurrentLinkedQueue 클래스에서도 사용하고 있다. 여러 종류의 큐 알고리즘이 그렇지만 값이 없는 비어 있는 큐는 '표식' 또는 '의미없는' 노드만 갖고 있고, 머리와 꼬리 변수는 이와 같은 표식 노드를 참조하고 있다. 꼬리 변수는 항상 표식 노드, 큐의 마지막 항목, 또는 맨 뒤에서 두번째 항목(변경 작업이 진행 중인 경우)을 가리킨다. 다음 그림을 보면 두 개의 항목을 갖고 있는 정산적인, 또는 '평온'한 상태의 큐를 볼 수 있다.  

새로운 항목을 추가하려면 두 개의 참조를 변경해야 한다. 첫 번째는 현재 큐의 마지막 항목이 갖고 있는 next 참조 값을 변경해서 새로운 항목을 큐의 끝에 연결하는 작업이다. 두 번재는 꼬리를 가리키는 변수가 새로 추가된 항목을 가리키도록 참조를 변경하는 작업이다. 두 번째는 꼬리를 가리키는 변수가 새로 추가된 항목을 가리키도록 참조를 변경하는 작업이다. 이 두 작업의 사이에서는 다음 그림과 같이 큐가 '중간' 상태에 놓이게 된다. 두 번째 작업가지 처리하고 나면 큐는 다시 '평온'한 상태로 돌아간다.  

<img src="/assets/images/Java_Concurrency_In_Practice/15-4.jpg" width="100%" height="100%"/>
<br/>

<img src="/assets/images/Java_Concurrency_In_Practice/15-5.jpg" width="100%" height="100%"/>
<br/>

위에서 소개했던 두 가지 전략을 모두 성공적으로 구현하고자 할 때 꼭 필요한 전략이 있다. 즉 큐가 평온한 상태에 있을 때 tail 변수가 가리키는 항목의 next 값이 null을 유지하도록 하고, 반대로 중간 상태인 경우에는 tail이 가리키는 항목의 next 값이 null이 아닌 값을 갖도록 하는 전략이다. 따라서 어느 스레드건 간에 tail.next 값을 확인해보면 해당 큐가 어떤 상태에 놓여 있는지를 확인할 수 있다. 또한 큐가 중간 상태에 있다는 사실을 알고 나면 tail이 바로 다음 항목을 가리키도록 변경해서 다시 평온한 상태로 돌려놓을 수 있다. 따라서 어느 스레드가 항목을 새로 추가하는 작업을 하던 간에 항상 원하는 작업을 제대로 끝마칠 수 있다.  

LinkedQueue.put 메소드는 새로운 항목을 추가하기 전에 먼저 해당하는 큐가 중간 상태인지를 확인한다(단계A). 만약 중간 상태에 있었다면 누군가 다른 스레드에서 해당하는 큐에 값을 추가하고 있었다는 의미이다(C와 D 단계 사이의 상태). 이미 값을 추가하고 있는 다른 스레드가 작업을 마무리할 때까지 기다리기보다 처리해야 할 작업, 즉 꼬리 변수의 참조를 다음 항목으로 넘겨주는 작업을 대신 처리한다(단계 B). 작업을 대신 처리한 이후에도 다른 스레드가 또 값을 추가하기 시작했는지를 다시 확인하고, 만약 그렇다면 고리 변수의 참조를 한 번 더 이동시킨다. 이렇게 꼬리 변수의 참조를 끝까지 이동시켜 큐가 평온한 상태임을 확인한 이후에야 자신이 추가하려던 항목에 대한 작업을 시작한다.  

새로 추가하는 항목을 큐의 끝에 연결시키는 단계 C에서는 CAS 연산을 사용하며, 두 개 이상의 스레드가 동시에 각자의 항목을 큐에 연결시키려 하면 실패하는 스레드가 발생할 수 있다. 하지만 실패하는 경우가 생긴다 해도 큐에 아무런 변경 사항을 가하지도 못했으며 현재 스레드는 꼬리 변수의 참조 값을 다시 읽어서 재시도하면 되기 때문에 큐의 상태에 아무런 문제가 생기지 않는다. 단계 C에서 작업이 성공하면 일단 항목을 연결하는데 성공한 셈이다. 단계 D에서는 두 번재 CAS 연산을 실행하는데 단계 D의 작업은 항목을 새로 추가한 스레드뿐만 아니라 다른 스레드에서도 처리할 수 있기 때문에 일종의 '정리' 작업이라고 볼 수 있겠다. 만약 단계 D의 CAS 연산이 실패한다해도 재시도 할 필요가 없다. 단계 B를 실행하던 다른 스레드에서 정리 작업을 이미 실행했기 때문이다. 어느 스레드건 간에 큐에 값을 추가하기 전에 tail.next 값을 확인해서 정리 작업이 필요한지를 확인하기 때문에 잘 동작한다. 만약 정리 작업이 필요하다면 평온한 상태가 될 때까지 정리 작업을 계속(여러 번 해야 할 수도 있다) 처리하게 된다.  

```java
@ThreadSafe
public class LinkedQueue<E> {
  private static class Node<E> {
    final E item;
    final AtomicReference<Node<E>> next;

    public Node(E item, Node<E> next) {
      this.item = item;
      this.next = new AtomicReference<Node<E>>(next);
    }
  }

  private final Node<E> dummy = new Node<E>(null, null);
  private final AtomicReference<Node<E>> head = new AtomicReference<Node<E>>(dummy);
  private final AtomicReference<Node<E>> tail = new AtomicReference<Node<E>>(dummy);

  public boolean put(E item) {
    Node<E> newNode = new Node<E>(item, null);
    while (true) {
      Node<E> curTail = tail.get();
      Node<E> tailNext = curTail.next.get();
      if (curTail == tail.get()) {
        if (tailNext != null) {
          // 큐는 중간 상태이고, 꼬리 이동
          tail.compareAndSet(curTail, tailNext);
        } else {
          // 평온한 상태에서 항목 추가 시도
          if (curTail.next.compareAndSet(null, newNode)) {
            // 추가 작업 성공, 꼬리 이동 시도
            tail.compareAndSet(curTail, newNode);
            return true;
          }
        }
      }
    }
  }
}
```

##### 15.4.3. 단일 연산 필드 업데이터
<br/>
위의 예제에는 ConcurrentLinkedQueue에서 사용하는 알고리즘의 일부가 소개돼 있지만, 실제로 구현된 내용은 약간 다른 모양을 띠고 있다. 즉 ConcurrentLinkedQueue에서는 각 Node 인스턴스를 단일 연산 참조 클래스로 연결하는 대신 일반적인 volatile 변수를 사용해 연결하고, 연결 구조를 변경할 때는 다음 예제와 같이 리플렉션(reflection) 기반의 AtomicReferenceFieldUpdater 클래스를 사용해 변경한다.  

```java
private class Node<E> {
  private final E item;
  private volatile Node<E> next;

  public Node(E item) {
    this.item = item;
  }
}

private static AtomicReferenceFieldUpdater<Node, Node> nextUpdater = AtomicReferenceFieldUpdater.newUpdater(Node.class, Node.class, "next");
```

단일 연산 필드 업데이터 클래스(Integer, Long, Reference에 해당하는 버전이 준비돼 있다)는 현재 사용 중인 volatile 벼수에 대한 리플렉션 기반의 '뷰'를 나타내며, 따라서 일반 volatile 변수에 대해 CAS 연산을 사용할 수 있도록 해준다. 이런 단일 연산 필드 업데이터 클래스에는 생성 메소드가 없으며, 인스턴스를 생성하려면 생성 메소드를 호출하는 대신 newUpdater라는 팩토리 모세드에 해당하는 클래스와 필드, 즉 변수의 이름을 넘겨서 생성할 수 있다. 필드 업데이터 클래스는 특정 인스턴스와 연결돼있지 않으며, 한 번만 생성하면 지정한 클래스의 모든 인스턴스에 들어 있는 지정 변수의 값을 변경할 때 항상 사용된다. 업데이터 클래스에서 보장하는 연산의 단일성은 일반적인 단일 변수보다 약하다. 다시 말해 업데이터 클래스에 지정한 클래스의 지정 변수가 업데이터 클래스를 통하지 않고 직접 변경되는 경우가 있다면 연산의 단일성을 보장하려면 모든 스레드에서 해당 변수의 값을 변경할 때 항상 compareAndSet 메소드나 기타 산술 연산 메소드를 사용해야만 한다.  

ConcurrentLinkedQueue 클래스에서는 Node 클래스의 next 변수 값을 변경할 대 nextUpdater의 compareAndSet 메소드를 통해 변경하게 돼 있다. 이처럼 약간 돌아가는 듯한 방법을 사용하는 이유는 전적으로성능을 높이기 위함이다. 큐의 연결 노드와 같이 자주 생성하면서 오래 사용하지 않는 클래스가 필요한 경우에는 AtomicReference라는 클래스의 인스턴스를 매번 생성할 때마다 AtomicReference의 인스턴스를 함께 생성해야 할 필요성을 없앨 수 있으므로 추가 작업에 대한 부하를 크게 줄여준다. 어쨋거나 거의 모든 경우에는 일반적인 단일 연산 변수만 사용해도 충분하고, 단일 연산 필드 업데이터 클래스를 사용해야 하는 경우는 몇 군데에 불과하다(단일 연산 필드 업데이터 클래스는 현재 클래스이 직렬화된 형태를 그래도 유지하면서 단일 연산 작업을 수행하고자 하는 경우에도 유용하게 사용할 수 있다).  

##### 15.4.4. ABA 문제
<br/>
ABA 문제는 (주로 가비지 컬렉션이 없는 환경에) 노드를 재사용하는 알고리즘에서 비교 후 치환(compare-and-swap) 연산을 고지식하게 사용하다보면 발생할 수 있는 이상 현상을 말한다. CAS 연산은 "V 변수의 값이 여전히 A인지?"를 확인하고 만약 그렇다면 값을 B로 변경하는 작업을 진행한다. 대부분의 경우에는 이정도의 확인만으로 충분하다. 하지만 간혹 "V 변수의 값이 내가 마지막으로 A 값이라고 확인한 이후에 변경된 적이 있는지?"라는 질문의 답을 알아야 할 경우도 있다. 일부 알고리즘을 사용하다 보면 V 변수의 값이 A에서 B로 변경했다가 다시 A로 변경된 경우 역시 변경 사항이 있었다는 것으로 인식하고 그에 해당하는 재시도 절차를 밟아야 할 필요가 있기도 하다.  

이와 같은 ABA 문제는 연결 노드 객체 대한 메모리 관리 부분을 직접 처리하는 알고리즘을 사용할 때 많이 발생한다. 연결 리스트의 머리 변수가 이전에 확인했던 그 객체를 참조하고 있다는 사실만으로는 해당 리스트의 내용이 변경되지 않았다고 확신할 수 없다. 그렇다고 해서 사용하지 않는 연결 노드를 가비지 컬렉터가 맡아서 처리하도록 하는 것만으로는 ABA 문제를 해결할 수 없으며, 대신 굉장히 쉬운 해결 방법이 있다. 즉 참조 값 하나만 변경하는 것이 아니라 참조와 버전 번호의 두 가지 값을 한거번에 변경하는 방법이다. 버전 번호를 관리하면 A에서 B로 변경됐다가 다시 A로 변경된 경우라고 해도 버전 번호를 보고 변경된 상태라는 점을 알 수 있다. AtomicStampedReference(그리고 이와 비슷한 AtomicMarkableReference) 클래스는 두 개의 값에 대해 조건부 단일 연산 업데이트 기능을 제공한다. AtomicStampedReference 클래스는 객체에 대한 참조와 숫자 값을 함께 ㅂ녀경하며, 버전 번호를 사용해 ABA 문제가 발생하지 않는 참조의 역할을 한다. 실제로 봣을 때, 이론적으로는 버전 번호가 정수형 숫자의 범위 내에서 빙빙 돌게 된다. AtomicMarkableReference 클래스 역시 이와 유사하게 객체에 대한 참조와 불린 값을 함께 변경하며, 일부 알고리즘에서 노드 객체를 그대로 놓아두지만 삭제된 상태임을 표시하는 기능으로 활용하기도 한다. 다수의 하드웨어 프로세서에서는 용량이 두 배인, 다시 말해서 객체에 대한 참조의 정수형 값을 동시에 변경하는 CAS 연산(double-wide CAS, CAS₂ 또는 CASX라고 부름)을 제공하기도 하며, 이런 하드웨어적인 지원을 활요하면 AtomicStampedReference나 AtomicMarkableReference 클래스의 기능을 굉장히 효율적으로 처리할 수 있다. 하지만 자바 6에 포함된 AtomicStampedReference 클래스는 아직 용량이 큰 CAS 연산을 활용하지는 않고 있다(용량이 큰 CAS 연산은 두 개의 서로 관계없는 변수를 대상으로 동작하는 DCAS 연산과는 다르며, 이 책을 쓰는 시점에도 DCAS를 지원한다고 알려진 프로세서는 없다).  

### 요약
<br/>
대기 상태에 들어가지 않는 넌블로킹 알고리즘은 락 대신 비교 후 치환(compare-and=swap)과 같은 저수준의 명령을 활용해 스레드 안전성을 유지하는 알고리즘이다. 이런 저수준의 기능은 특별하게 만들어진 단일 연산 클래스를 통해 사용할 수 있으며, 단일 연산 클래스는 '더 나은 volatile 변수'로서 정수형 변수나 객체에 대한 참조 등을 대상으로 단일 연산 기능을 제공하기도 한다.  

넌블로킹 알고리즘은 설계하고 구현하기는 훨씬 어렵지만 특정 조건에서는 훨씬 나은 확장성을 제공하기도 하고, 가용성 문제를 발생시키지 않는다는 장점이 있다. JVM이나 플랫폼 자체의 라이브러리에서 대기 상태에 들어가지 않는 알고리즘을 적절히 활용하는 범위가 넓어지면서 JVM의 버전이 올라갈 때마다 병렬 프로그램의 성능이 계속해서 나아지고 있다.  

### 16. 자바 메모리 모델
<br/>
#### 16.1. 자바 메모리 모델은 무엇이며, 왜 사용해야 하는가?
<br/>
특정 스레드에서 aVariable이라는 변수에 값을 할당한다고 해보자.  

```java
aVariable = 3;
```

자바 메모리 모델은 "스레드가 aVariable에 할당된 3이란 값을 사용할 수 있으려면 어떤 조건이 돼야 하는가?"에 대한 답을 알고 있다. 굉장히 어이 없는 질문이라고 생각되기도 하겠지만, 동기화 기법을 사용하지 않는 상태라면 특정 스레드가 값이 할당되는 즉시, 심지어는 영원히 3이라는 값을 읽어가지 못하게 하는 여러 가지 상황이 발생할 수 있다. 컴파일러에서 소스코드에 적힌 내용을 명확하게 구현하는 코드를 생성해 내지 못할 가능성도 있고, 변수의 값을 메모리에 저장하는 대신 CPU의 레지스터에 보관할 수도 있다. CPU 프로세서는 프로그램을 순차적으로 실행하거나 또는 병렬로 실행할 수도 있고, 사용하는 캐시의 형태에 따라서 할당된 값이 메모리에 실제 보관되는 시점에 차이가 있기도 하며, CPU 내부의 캐시에 보관된 할당 값이 다른 CPU의 시야에는 보이지 않을 수도 있다. 이런 원인 때문에 적절한 동기화 방법을 사용하지 않았다면, 특정 스레드에서 변수에 할당된 최신 값을 읽어가지 못할 수 있으며 따라서 다른 스레드의 시각으로 보기에 이상한 방향으로 실행될 가능성이 있다.  

단일 스레드로 동작하는 환경에서는 프로그램이 동작하면서 사용했던 여러 가지 기법이 만들어낸 결과가 숨겨져 있고, 반면 그로 인해 전체적인 프로그램의 실행 속도는 살당히 빨리진다. 자바 언어 명세(Java Language Specification)에서는 JVM이 단일 스레드 내부에서는 순차적으로 실행되는 환경과 동일한 순서로 실행된 것처럼 같은 결과를 만들어 내주기만 한다면 여러 가지 기법은 처리 속도를 높이기 위한 최근의 노력에서 중요한 위치를 차지하고 있다는 점에서 괜찮은 부분이기도 하다. 물론 CPU의 클럭 스피드가 높아진 것도 프로그램 실행 속도를 크게 높여줬다, 예를 들어 파이프라인 슈퍼스칼라(pilelined superscalar) 실행 구조라든가 동적인 명령 스케줄링, 모험적인 실행 방법, 섬세한 다중 메모리 캐시 등이 바로 그렇다. 하드웨어 프로세서가 고급화됨에 따라 컴파일러 역시 최적의 실행 방법을 찾아내거나 전역 레지스터 할당 알고리즘과 같은 섬세한 기능을 갖추고 있다. 또한 클럭 스피드를 높이는 것만으로는 적절한 가격에 원하는 만큼 속도를 높이기가 어려워지면서, 프로세스 제조 업체에서는 멀티코어 프로세서 구조로 이동하면서 하드웨어적인 병렬 작업을 통해 속도 향상을 꾀하는 양상을 보이고 있다.  

멀티스레드로 실행되는 환경에서는 성능을 크게 제한하지 않는 한 순차성이 주는 안전성과 높은 성능은 찾아보기 어렵다. 병렬 프로그램이라 하더라도 대부분의 시간은 스레드 내부에서 '각자의 작업'을 처리하기 때문에 스레드 간의 작업 조율 기능에 자원을 많이 낭비하는 일은 별 이득도 없으면서 프로그램의 성능만 떨어뜨리는 결과를 낳기 십상이다. 스레드 간의 작업을 조율하는 데 꼭 필요한 데이터만을 공유해 사용하는 것이 올바른 방법이고, JVM은 동기화 기능을 사용하는 부분에 한해서 프로그램이 스레드 간의 조율을 하고자 한다는 점을 파악할 수 있다.  

JMM은 변수에 저장된 값이 어느 시점부터 다른 스레드의 가시권에 들어가는지에 대해 JVM이 해야만 하는 최소한의 보장만 할 뿐이다. JMM은 예측성에 대한 필요와 함께 높은 성능의 JVM을 다양한 종류의 프로세서 구조에서 동작하도록 해야 한다는 실제적인 요구 사항을 쉽게 구현할 수 있어야 한다는 점의 균형을 맞출 목적으로 설계됐다. 특히 JMM의 일부는 JVM에서 실행되는 프로그램의 성능을 최대한 끌어낼 수 있도록 최신 프로세서와 컴파일러에서 사용하는 여러 기법을 사용하고 있는데, 이런 부분에 약간 어려움이 있을 수도 있다.  

##### 16.1.1. 플랫폼 메모리 모델
<br/>
메모리를 공유하는 멀티프로세서 시스템은 보통 각자의 프로세서 안에 캐시 메모리를 갖고 있으며, 캐시 메모리의 내용은 주기적으로 메인 메모리와 동기화된다. 하드웨어 프로세서 아키텍처는 저마다 다른 캐시 일관성(cache coherence)을 지원한다. 일부 시스템에서는 어느 시점이건 간에 동일한 순간에 같은 메모리 위치에서 각 프로세서가 서로 다른 값을 읽어가는 경우를 허용하기도 한다. 운영체제와 컴파일러와 자바 런타임, 때로는 프로그램까지도 서로 다른 하드웨어에서 제공하는 기능과 스레드 안전성에 대한 차이점을 메울 수 있어야 한다.  

멀티프로세서 시스템에서 각 프로세서가 서로 다른 프로세서가 하는 일을 모두 알 수 있도록 하려면 굉장한 부하를 안고 가야 한다. 대부분의 경우 다른 프로세서가 어떤 일을 하고 있는지에 대한 정보는 별로 필요도 없기 대문에 프로세서는 대부분 성능을 높이고자 캐시 메모리의 일관성을 약간씩 희생하곤 한다. 시스템 구조에서 말하는 메모리 모델(memory model)은 프로그램이 메모리 구조에서 어느 정도의 기능을 사용할 수 있을지에 대한 정보를 제공하고, 메모리의 내용을 서로 공유하고자 할 때 프로세서 간의 작업을 조율하기 위한 특별한 명령어(메모리 배리어(memory barrier) 또는 팬스(fence))로는 어떤 것들이 있으며 어떻게 사용해야 하는지에 대한 정보도 제공한다. 자바 개발자가 서로 다른 하드웨어가 갖고 있는 각자의 메모리 모델을 직접 신경 쓰지 않도록 자바는 스스로의 메모리 모델인 JMM을 구성하고 있으며, JMM과 그 기반이 되는 하드웨어 메모리 모델의 차이점은 메모리 배리어를 적절히 활용하는 방법 등으로 JVM에서 담당해 처리한다.  

프로그램이 실행되는 내용을 예상하기에 가장 간편한 방법은 하드웨어 프로세서에 상관 없이 프로그램 내부에 작성된 코드가 실행되는 단 한 가지의 방법이 존재하며, 프로그램이 실행되는 과정에서 변수에 마지막으로 설정한 값을 어떤 프로세서건 간에 정확하게 읽어낼 수 있다고 가정하는 방법이다. 비현실적이긴 하지만 이처럼 꿈같이 간편한 상태를 순차적 일관성(sequential consistency)이라고 부른다. 소프트웨어 개발자는 무의식적으로 순차적 일관성이 존재한다고 가정해버리는 경우가 많은데, 현재 사용중인 어떤 프로세서도 순차적 일관성을 지원하지 않으며 JMM 역시 지원하지 않는다. 역사적으로 폰 노이만 모델(von Neumann model)이라고 부르는 순차적인 실행 구조는 현대의 멀티프로세서 시스템이 동작하는 모습으로 본다면 명확하지 않게 실행 순서를 추정하는 정도에 해당될 뿐이다.  

메모리를 공유해 사용하는 멀티프로세서 시스템(그리고 컴파일러 역시)에서는 여러 스레드에서 데이터를 공유하는 상황에서 메모리 배리어를 사용하지 않도록 일부러 지정한다면 놀랄만한 문제점이 쏟아질 것이다. 다행스럽게도 자바로 프로그램을 작성하는 과정에서 메모리 배리어를 어디에 어떻게 배치해야 하는지를 고민할 필요는 없다. 단지 프로그램 내부에서 동기화 기법을 적절히 활용해 어느 시점에서 공유된 정보를 사용하는지만 알려주면 된다.  

##### 16.1.2. 재배치
<br/>
JMM은 서로 다른 스레드가 각자의 상황에 맞는 순서로 명령어를 실행할 수 있도록 허용하고 있기 때문에 동기화가 돼 있지 않은 부분을 놓고 실행 순서를 예측하는 일이 훨씬 더 복잡하다. 특정 작업이 지연되거나 다른 순서로 실행되는 것처럼 보이는 문제는 '재배치(reordering)'이라는 용어로 통일해서 표현한다.  

메모리 수준에서의 재배치 현상은 프로그램이 오작동하게 만들기 십상이다. 동기화가 제대로되지 않은 상태에서 재배치될 가능성을 예측하는 일은 너무나 어려우며, 반대로 동기화 방법을 적절하게 사용해 재비치 가능성을 없애는 편이 더 쉽다. 동기화가 잘 된 상태에서는 컴파일러, 런타임, 하드웨어 모두 JMM이 보장하는 가시성(visibility) 수준을 위반하는 쪽으로 메모리 관련 작업을 재배치하지 못하게 된다. 많이 사용되는 프로세서 구조를 놓고 보면 volatile 읽기 작업이 volatile이 아닌 읽기 작업과 비슷한 수준의 성능을 낼 만큼 메모리 모델이 강력하게 구현돼 있다.  

##### 16.1.3. 자바 메모리 모델을 간략하게 설명한다면
<br/>
변수를 읽거나 쓰는 작업, 모니터를 잠그거나 해제하는 작업, 스레드를 시작하거나 끝나기를 기다리는 작업과 같이 여러 가지 작업에 대해 자바 메모리 모델(JMM)을 정의한다. JMM에서는 프로그램 내부의 모든 작업을 대상으로 미리 발생(happens-before)라는 부분 재배치(partial reordering) 연산을 정의하고 있다. < 기호로 표시하는 부분 재배치 연산은 비대칭적(antisymmetric)이고, 반사적(reflexive)이고, 전이적(transitive)이다. 하지만 대상 집합의 임의의 항목 x와 y를 놓고 항상 x<y 이거나 또는 y<x이지는 않다. 부분 재배치는 일상 생활에서도 자주 사용되는 연산이다. 작업 A가 실행된 결과를 작업 B에서 볼 수 있다는 점을 보장하기 위해 (작업 A와 B가 같은 스레드에서 실행되건 서로 다른 스레드에서 실행되건 상관 없이) 작업 A와 B 사이에는 미리 발생 관계가 갖춰져야 한다. 두 개 작업 간에 미리 발생 관계가 갖춰져 있지 않다면 JVM은 원하는 대로 해당 작업을 재배치할 수 있게 된다.  

하나의 변수를 두 개 이상의 스레드에서 읽어가려고 하면서 최소한 하나 이상의 스레드에서 쓰기 작업을 하지만, 쓰기 작업과 읽기 작업 간에 미리 발생 관계가 갖춰져 있지 않은 경우에 데이터 경쟁(data race) 현상이 발생한다. 이와 같은 데이터 경쟁 현상이 발생하지 않는 프로그램을 '올바르게 동기화된 프로그램(correctly synchronized program)'이라고 말한다. 올바르게 동기화된 프로그램은 순차적 일관성을 갖고 있으며, 다시 말해 프로그램 내부의 모든 작업이 고정된 전역 순서(global order)에 따라 실행된다는 것을 의미한다.  

미리 발생 현상에 대한 규칙은 다음과 같다.  

+ 프로그램 순서 규칙  
특정 스레드를 놓고 봤을 때 프로그램된 순서에서 앞서있는 작업은 동일 스레드에서 뒤에 실행되도록 프로그램된 작업보다 미리 발생한다.  

+ 모니터 잠금 규칙  
특정 모니터 잠금 작업이 뒤이어 오는 모든 모니터 잠금 작업보다 미리 발생한다. 특정한 Lock 객체를 잠그거나 해제하는 연산은 암묵적인 락과 동일한 메모리 현상을 보여준다.  

+ volatile 변수 규칙  
volatile 변수에 대한 쓰기 작업은 이후에 따라오는 해당 변수에 대한 모든 읽기 작업보다 미리 발생한다. 단일 연산 변수에 대한 읽기 또는 쓰기 연산은 volatile 변수에 대한 작업과 동일한 메모리 현상을 보여준다.  

+ 스레드 시작 규칙  
특정 스레드에 대한 Thread.start 작업은 시작된 스레드가 갖고 있는 모든 작업보다 미리 발생한다.  

+ 스레드 완료 규칙  
스레드 내부의 모든 작업은 다른 스레드에서 해당 스레드가 완료됐다는 점을 파악하는 시점보다 미리 발생한다. 특정 스레드가 완료됐는지를 판단하는 것은 Thread.join 메소드가 리턴되거나 Thread.isAlive 메소드가 false를 리턴하는지 확인하는 방법을 말한다.  

+ 인터럽트 규칙  
다른 스레드를 대상으로 interrupt 메소드를 호출하는 작업은 인터럽트당한 스레드에서 인터럽트를 당했다는 사실을 파악하는 일보다 미리 발생한다. 인터럽트를 당했다는 사실을 파악하려면 InterruptedException을 받거나 isInterrupted 메소드 또는 interrupted 메소드를 호출하는 방법을 사용할 수 있다.  

+ 완료 메소드(finalizer) 규칙  
특정 객체에 대한 생성 메소드가 완료되는 시점은 완료 메소드가 시작하는 시점보다 미리 발생한다.  

+ 전이성(transitivity)  
A가 B보다 미리 발생하고, B가 C보다 미리 발생한다면, A는 C보다 미리 발생한다.  

작업이 부분적으로만 순서가 정해져 있다고 해도, 동기화 작업(락 확보 및 해제, volatile 변수에 대한 읽기나 쓰기 작업 등)은 항상 완전하게 순서가 정해진 상태이다. 따라서 락을 확보한 이후에 연달아 일어나는 volatile 변수의 값을 읽는 작업에 대해 미리 발생 규칙을 적용하는 일도 충분히 가능하다.  

다음 그림을 보면 일반적인 락을 사용해 동기화된 두 개의 스레드 간에 미리 발생 규칙이 적용되는 모습이 나타나 있다. 스레드 A와 스레드 B의 모든 작업은 프로그램 내부의 규칙에 따라 순서가 정해져 있다. 스레드 A에서 락 M을 해제하면 스레드 B에서 해제된 락 M을 확보하며, 스레드 A에서 락을 해제하기 전에 하도록 돼 있던 모든 작업은 스레드 B에서 락을 확보한 이후에 실행되는 작업보다 먼저 실행되도록 순서가 정해진다. 두 개의 스레드가 서로 다른 락으로 동기화돼 있다면 양쪽 스레드에서 일어나는 작업의 순서에 대해 어떤 보장도 할 수 없다. 즉 양쪽 스레드의 작업 사이에는 미리 발생 관계가 전혀 존재하지 않는다.  

<img src="/assets/images/Java_Concurrency_In_Practice/16-2.jpg" width="100%" height="100%"/>
<br/>

##### 16.1.4. 동기화 피기백
<br/>
코드의 실행 순서를 정하는 면에서 미리 발생 규칙이 갖고 있는 능력의 수준 때문에 현재 사용 중인 동기화 기법의 가시성(visibility)에 얹혀가는 방법, 즉 피기백(piggyback)하는 방법도 있다. 다시 말해 락으로 보호돼 있지 않은 변수에 접근해 사용하는 순서를 정의할 때, 모니터 락이나 volatile 변수 규칙과 같은 여러 가지 순서 규칙에 미리 발생 규칙을 함께 적용해 순서를 정의하는 방법을 말한다. 이런 기법은 명령이 나열된 순서에 굉장히 민감하며 따라서 오류가 발생하기 쉽다. 이런 방법은 ReentrantLock과 같이 성능에 중요한 영향을 미치는 클래스에서 성능을 떨어뜨릴 수 있는 아주 작은 요인까지 완벽하게 제거해야 하는 상황이 오기 전가지는 사용하지 않는 편이 좋다.  

FutureTask 클래스에서 protected로 구현하고 있는 AbstractQueuedSynchronizer의 메소드를 보면 이와 같은 피기백 방법을 사용하는 모습을 볼 수 있다. 알다시피 AQS는 FutureTask가 맡은 작업의 진행 상태, 즉 실행 중, 완료, 취소 등의 여부를 정수형으로 보관한다. FutureTask는 작업의 상태 외에도 완료된 작업의 결과 값 등을 보관한다. 한쪽 스레드에서 set 메소드를 사용해 실행한 결과를 보관하고 다른 스레드에서는 get 메소드를 호출해 결과 값을 가져가려고 한다고 가정해보면, set과 get 작업은 미리 발생 규칙으로 그 순서를 정의할 수 있다. 즉 결과 값을 보관하는 변수를 volatile로 선언하는 것으로도 원하는 결과를 얻을 수 있겠지만, 기존의 동기화 방법을 잘 활용하면 훨씬 적은 자원으로 동일한 효과를 얻을 수 있다.  

FutureTask는 미리 발생 규칙에 따라 tryReleaseShared 메소드의 작업이 tryAcquireShared 메소드보다 항상 먼저 실행되도록, 즉 tryReleaseShared 메소드에서 항상 tryAcquireShared 메소드가 읽어가는 기능을 담당하는 innerSet 메소드와 innerGet 메소드의 코드가 다음 예제에 소개돼 있다. innerSet 메소드는 (결국 tryReleaseShared를 호출하는) releaseShared 메소드를 호출하기 전에 result 변수에 값을 보관하고, innerGet 메소드는 (결국 tryAcquireShared를 호출하게 됨) acquireShared 메소드를 호출한 이후에 result 값을 읽어간다. 이처럼 volatile 변수 규칙에 프로그램 순서 규칙을 함께 적용함으로써 innerSet 메소드에서 result 변수에 값을 쓰는 일이 innerGet 메소드에서 result 변수에 값을 쓰는 일이 innerGet 메소드에서 result 변수의 값을 읽는 작업보다 반드시 먼저 발생하도록 조절하고 있다.  

```java
// FutureTask의 내부 클래스
private final class Sync extends AbstractQueuedSynchronizer {
  private static final int RUNNING = 1, RAN = 2, CANCELLED = 4;
  private V result;
  private Exception exception;

  void innerSet(V v) {
    while (true) {
      int s = getState();
      if (ranOrCancelled(e))
        return;
      if (compareAndSetState(e, RAN))
        break;
    }
    result = v;
    releaseShared(0);
    done();
  }

  V innerGet() throws InterruptedException, ExecutionException {
    acquireSharedInterruptibly(0);
    if (getState() == CANCELLED)
      throw new CancellationException();
    if (exception != null)
      throw new ExecutionException(exception);
    return result;
  }
}
```

이와 같은 방법은 X라는 객체의 값을 공개(publish)할 때 미리 발생 규칙을 따로 적용하기보다는, 다른 목적으로 만들어 사용하고 있는 미리 발생 순서 규칙을 X라는 객체의 가시성을 확보하는 데도 함께 사용하기 때문에 '피기백(piggybacking)'이라고 부른다.  

FutureTask 클래스에서 사용하는 것과 같은 종류의 피기백 방법은 오류가 발생할 가능성이 크기 때문에 대충 사용해서는 안 된다. 어쨌거나 특정 클래스가 자체적인 명세의 일부로써 메소드 사이에서 밀리 발생 규칙을 사용하는 경우와 같은 부분에서는 피기백 방법이 딱 들어맞는 상황도 있다. 예를 들어 BlockingQueue를 사용하는 안전한 공개 기법 역시 피기백의 한 형태라고 볼 수 있다. 큐에 값을 집어 넣는 스레드와 큐에서 값을 꺼내가는 스레드는 BlockingQueue 내부에서 적절한 동기화 구조를 통해 큐에서 값을 뽑아내는 작업이 큐에 값을 넣는 작업보다 미리 발생하도록 보장하고 있기 때문에 안전한 공개 상태에서 동작할 수 있다.  

JDK 라이브러리에 들어 있는 클래스 가운데 미리 발생 관계를 보장하고 있는 클래스로는 다음과 같은 것들이 있다.  

+ 스레드 안전한 컬렉션 클래스에 값을 넣는 일은 해당 컬렉션 클래스에서 값을 뽑아내는 일보다 반드시 미리 발생한다.  

+ CountDownLatch 클래스에서 카운트를 빼는 작업은 await에서 대기하던 메소드가 리턴되는 작업보다 반드시 미리 발생한다.  

+ Semaphore에서 퍼밋을 해제하는 작업은 동일한 Semaphore에서 퍼밋을 확보하는 작업보다 반드시 미리 발생한다.  

+ Future 인스턴스에서 실행하는 작업은 해당하는 Future 인스턴스의 get 메소드가 리턴되기 전에 반드시 미리 발생한다.  

+ Executor 인스턴스에 Runnable이나 Callable을 등록하는 작업은 해당 Runnable이나 Callable의 작업이 시작하기 전에 미리 발생한다.  

+ CyclicBarrier나 Exchange 클래스에 스레드가 도착하는 일은 동일한 배리어나 교환 포인트에서 다른 스레드가 풀려나는 일보다 미리 발생한다. CyclicBarrier에서 배리어 동작을 사용하고 있었다면, 배리어에 도착하는 일이 배리어 동작보다 반드시 밀 발생하고, 배리어 동작은 또한 해당 배리어에서 다른 스레드가 풀려나기 전에 반드시 미리 발생한다.  


#### 16.2. 안전한 공개
<br/>
객체가 안전하지 않게 공개되는 이유는 공유 객체를 공개하는 작업과 다른 스레드에서 공개된 객체를 사용하는 작업 간의 미리 발생 관계를 제대로 적용하지 못했기 때문이다.  

##### 16.2.1. 안전하지 못한 공개
<br/>
미리 발생 관계를 제대로 고려하지 못한 상태에서 재배치 작업이 일어날 수 있다는 가능성을 놓고 보면, 적절한 동기화 구조를 갖추지 못하고 공개된 객체를 두고 다른 스레드에서 부분 구성된 객체(partially constructed object)를 볼 수밖에 없는 원인이 쉽게 설명된다. 새로운 객체를 생성하는 과정에서는 변수, 즉 새로운 객체의 필드에 값을 써 넣는 작업이 필요하다. 이와 비슷하게 객체에 대한 참조를 공개하는 과정에는 또 다른 변수, 즉 새로운 객체 대한 참조에 값을 쓰는 작업이 동반된다. 프로그램상에서 공유된 참조를 공개하는 일이 다른 스레드에서 해당 참조를 읽어가는 일보다 미리 발생하도록 확실하게 해두지 않으면(공개된 객체를 가져다 사용하는 스레드의 입장에서 보면) 새로운 객체에 대한 참조에 값을 쓰는 작업과 객체 내부의 변수에 값을 쓰는 과정에서 재배치가 일어날 수 있다. 이와 같이 재배치가 일어나면 다른 스레드에서 객체 참조는 올바른 최신 참조 값을 사용하지만, 객체 내부의 변수 전체 또는 일부에 대해서는 아직 쓰기 작업이 끝나지 않은 상태의 예전 값을 사용할 가능성이 있다. 바로 부분 구성된 객체라는 현상이 발생하는 셈이다.  

다음 예제에서 볼 수 있는 것처럼 늦은 초기화(lazy initalization) 방법을 올바르게 사용하지 못하면 안전하지 않은 공개 상태에 다다르게 된다. 일단 보기에 여기에서 발생할 수 있는 문제는 경쟁 조건이 있다고 판단된다. 그런데 이를  테면 모든 Resource 인스턴스가 동일하다는 등의 특정 상황에서는 이런 문제점(Resource 인스턴스를 두 개 이상 생성할 가능성이 있다는 문제점도 포함)을 정확하게 파악하지 못하고 그냥 지나칠 수 있다. 더군다나 이와 같은 문제점을 그냥 넘긴다 해도 다른 스레드에서 부분 구성된 인스턴스를 볼 수 있기 때문에 UnsafeLazyInitialization 클래스는 여전히 위험성을 안고 있다.  

```java
@NotThreadSafe
public class UnsafeLazyInitialization {
  private static Resource resource;

  public static Resource getInstance() {
    if (resource == null)
      resource = new Resource(); // 안전하지 않은 공개
    return resource;
  }
}
```

스레드 A에서 처음으로 getInstance 메소드를 호출한다고 해보자. 그러면 resource 변수가 null이라는 상태를 볼 수 있으며, 새로운 Resource 인스턴스를 생성하고, resource 변수에서 새로운 Resource 인스턴스를 참조하도록 설정한다. 나중에 스레드 B가 getInstance 메소드를 호출하면 resource 변수가 이미 null이 아닌 상태라는 점을 알게 되고, 이미 만들어져 있는 Resource 인스턴스를 그대로 사용한다. 일단 보기에는 별 문제가 없어 보이지만, 스레드 A에서 resource 변수에 새로운 참조를 설정하는 작업과 스레드 B에서 resource 변수의 값을 확인하는 작업의 사이에 미리 발생 규칙이 전혀 적용되지 않았다는 문제가 있다. 객체를 공개할 때 데이터 경쟁을 하도록 돼 있으며, 따라서 Resource 클래스가 올바른 상태에 있을 때 스레드 B가 사용하리라는 보장이 없다.  

Resource 클래스의 생성 메소드는 Resource 인스턴스가 생성될 때 내부 변수의 값을 현재 값 대신 초기 값을 재설정하도록 돼 있다(Object 클래스의 생성 메소드에서 그렇게 동작한다). 스레드 A와 B 모두 전혀 동기화 작업이 돼 있지 않기 때문에 스레드 B에서는 스레드 A가 실제로 실행하는 순서 대신 A이라는 다른 순서로 실행되는 모습을 보게 될 수도 있다. 결국 스레드 A에서는 Resource 클래스의 인스턴스를 생성한 이후에 resource 변수에 값을 설정했지만, 스레드 B에서는 resource 변수에 값을 설정하는 일이 Resource 인스턴스를 생성하는 일보다 먼저 실행된 것으로 파악할 수도 있다는 말이다. 그러면 스레드 B는 올바르지 않은 상태일 가능성이 높으데다, 나중에 어느 시점에 올바른 상태가 될지 모르는 부분 구성된 Resource 인스턴스를 보게 된다.  

불변 객체가 아닌 이상, 특정 객체를 공개하는 일이 그 객체를 사용하려는 작업보다 미리 발생하도록 구성돼 있지 않다면 다른 스레드에서 생성한 객체를 사용하는 작업은 안전하지 않다.  

##### 16.2.2. 안전한 공개
<br/>
안전한 공개(safe publication)라는 용어는 객체를 공개하는 작업이 다른 스레드에서 해당 객체에 대한 참조를 가져다 사용하는 작업보다 미리 발생하도록 만들어져 있기 때문에 공개된 객체가 다른 스레드에게 올바른 상태로 보인다는 것을 뜻한다. 스레드 A에서 객체 X를 BlockingQueue에 추가하고 다른 스레드에서 큐의 내용을 변경하지 않으면, 객체 X를 스레드 B에서 뽑아냈을 때, 스레드 B는 스레드 A가 큐에 넣었던 그 상태 그대로의 객체를 사용할 수 있다. BlockingQueue 클래스는 내부적으로 put 작업이 take 작업보다 항상 미리 발생 관계가 항상 보장돼 있다.  

이와 같이 미리 발생 관계가 보장된다는 사실은 안전한 공개에 의해 보장되는 가시성과 실행 순서보다 더 강력한 힘을 갖고 있다. X 객체가 스레드 A와 B에서 안전하게 공개됐다면, 안전한 공개라는 방법을 통해 X 객체의 상태에 대한 가시성은 보장받을 수 있지만 스레드 A가 사용했던 변수의 상태에 대해서는 아무런 보장을 하지 못한다. 하지만 스레드 A에서 X를 큐에 추가하는 일이 스레드 B가 X를 큐에서 꺼내는 작업보다 미리 발생한다고 하면, 스레드 B에서는 X를 스레드 A가 큐에 추가할 시점과 동일한 상태로 볼 수 있을 뿐만 아니라(물론 스레드 A와 B가 아닌 다른 스레드에서 큐의 내용을 변경하지 않았다고 가정한다) 스레드 A가 큐에 넣기 전에 처리했던 모든 작업을 전부 볼 수 있다(역시 A와 B가 아닌 다른 스레드에서 큐의 내용을 변경하지 않았다고 가정함). JMM에서는 최소한 스레드 A가 쓴 최신 값을 스레드 B가 볼 수 있도록 보장해준다. 하지만 그 이후에 쓰는 내용은 볼 수도 있고 보지 못할 수도 있다.  

JMM은 이미 강력한 미리 발생 규칙에 따라 동작하고 있음에도 불구하고 왜 지금까지 &#64;GuardedBy와 안전한 공개 기법에 초점을 맞춰왔을까? 일반적으로 프로그램을 작성할 때는 개별적으로 메모리에 쓰기 작업이 일어난 이후의 가시성을 놓고 안전성을 논하기보다는 객체의 소유권을 넘겨주고 공개하는 작업이 훨씬 적합하기 때문이다. 즉 미리 발생 규칙은 개별적인 메모리 작업의 수준에서 일어나는 순서의 문제를 다룬다. 말하자면 동기화 기법에 대한 어셈블리 언어에 해당하는 셈이다. 반대로 안전한 공개 기법은 일반적인 코드를 작성할 때와 비슷한 수준에서 동작하는 동기화 기법이다.  

##### 16.2.3. 안전한 초기화를 위한 구문
<br/>
생성 작업에 부하가 걸리는 객체는 실제로 해당 객체를 필요로 하는 시점이 올 때까지 초기화하지 않고 기다리는 편이 나은 면도 있지만, 이와 같은 늦은 초기화(lazy initialization) 기법을 잘못 사용하면 어떤 문제가 발생하는지도 잘 알고 있다. UnsafeLazyInitialization 클래스의 문제점은 다음 예제와 같이 getInstance 메소드에 synchronized 키워드를 추가하는 것으로 해결할 수 있다. 더군다나 getInstance 메소드 내부에서 처리하는 작업이 상당히 간결한 편(한 번의 비교 연산과 예측 가능한 분기 연산 정도)이기 대문에 여러 스레드에서 getInstance 메소드를 줄기차게 호룰하지 않는 한 SafeLazyInitialization 클래스에 대한 락에 대해ㅐ서는 경쟁이 그다지 많이 발생하지는 않을 것이고, 따라서 꽤 괜찮은 성능을 내 줄 것이라고 예상할 수 있다.  

static으로 선언된 벼수에 초기화 문장을 함께 기술하는 특별한 방법(또는 static이 아닌 변수의 내용을 static 블록에서 초기화하는 경우도 포함)을 사용하면 스레드 안전성을 추가적으로 보장받을 수 있다. static으로 선언된 초기화 문장은 JVM에서 해당 클래스를 읽어들이고 실제 해당 클래스를 사용하기 전에 실행된다. 이런 초기화 과정에서 JVM이 락을 확보하며 각 스레드에서 해당 클래스가 읽혀져 있는지를 확인하기 위해 락을 다시 확보하게 돼 있다. 따라서 JVM이 락을 확보한 상태에서 메모리에 쓰여진 내용은 모든 스레드가 볼 수 있다. 결국 static 구문에서 초기화하는 객체는 생성될 때나 참조될 때 언제든지 따로 동기화를 맞출 필요가 없다. 대신 이와 같은 내용은 초기화한 객체의 내용이 그대로인 상태를 가정할 때만 성립되며, 반대로 객체의 내용일 변경할 수 있다면 읽기 스레드와 쓰기 스레드가 연달아 객체의 내용을 변경할 때마다 동기화를 맞춰야 변경된 내용을 다른 스레드에서 올바르게 볼 수 있고 데이터에 오류가 발생하는 일도 막을 수 있다.  

```java
@ThreadSafe
public class SafeLazyInitialization {
  private static Resource resource;

  public synchronized static Resource getInstance() {
    if (resource == null)
      resource = new Resource();
    return resource;
  }
}
```

```java
@ThreadSafe
public class EagerInitailization {
  private static Resource resource = new Resource();

  public static Resource getResource() { return resource; }
}
```

위의 예제와 같이 성질 급한 초기화 방법을 사용하면 SafeLazyInitialization 클래스에서 getInstance를 호출할 때마다 매번 처리해야만 했던 synchronized 구문을 제거할 수 있다. 이 방법은 JVM이 사용하는 늦은 클래스 로딩(lazy class loading) 기법과 함께 사용할 수 있으며, 자주 사용하는 코드에 대해서 동기화를 맞추야 할 필요를 줄일 수 있다. 다음 예제에는 오로지 Resource 클래스를 초기화할 목적으로 늦은 초기화 홀더 클래스(lazy initalization holder class) 구문을 적용해 작성한 클래스가 소개돼 있다. JVM은 ResourceHolder 클래스를 실제로 사용하기 전까지는 해당 클래스를 초기화하지 않으며, Resource 클래스 역시 static 초기화 구문에서 초기화하기 때문에 추가적인 동기화기법을 적용할 필요가 없다. 어느 스레드건 간에 처음 getResource 메소드를 호출하면 JVM에서 ResourceHolder 클래스를 읽어들여 초기화하고, ResourceHolder 클래스를 초기화하는 도중에 Resource 클래스 역시 초기화하게 돼 있다.  

```java
@ThreadSafe
public class ResourceFactory {
  private static class ResourceHolder {
    public static Resource resource = new Resource();
  }

  public static Resource getResource() {
    return ResourceHolder.resource;
  }
}
```

##### 16.2.4. 더블 체크 락
<br/>
다음 예제에 소개된 것처럼 악명 높은 피해야 할 패턴인 더블 체크 락(double-checked locking) 패턴에 대해서 소개하지 않고는 병렬 프로그래밍을 다루는 책이라 할 수 없다. 굉장히 초기에 사용하던 JVM은 경쟁이 별로 없는 상태라고 해도 동기화를 맞추려면 성능에 엄청난 영향을 주었다. 그 결과 동기화 기법이 주는 영향을 최소화하고자 하는 여러 가지 기발한(최소한 기발하게 보이는) 방법이 나타나기 시작했다. 일부 괜찮은 방법도 있었고, 안좋은 방법도 있었고, 정말 문제 많은 방법도 있었다. DCL은 정말 문제 많은 방법에 속하는 놈이다.  

다시 말하지만 초창기 JVM의 성능은 굉장히 모자란 부분이 많았기 때문에 실제로 사용하지도 않으면서 자원만 많이 소모하는 기능을 제거하거나 애플리케이션의 시동 시간을 줄이는 등의 목적을 위해 늦은 초기화 기법을 많이 사용했었다. 어쨌거나 늦은 초기화 기법을 올바르게 사용하려면 적절하게 동기화돼 있어야 한다. 히자만 초창기에는 동기화 작업에 시간이 많이 걸렸으며 더 중요한 문제는 충분히 이해하지 못하는 경우가 많았다는 사실이다. 특히 배타적인 실행과 관련한 부분은 그나마 많이 알려져 있었지만, 상태의 가시성에 대한 부분은 거의 알려져 있지 않았다.  

DCL은 자주 사용되는 클래스에 대해 늦은 초기화 작업을 하면서도 동기화와 관련된 자원의 손실을 막을 수 있는 꿩 먹고 알 먹는 방법으로 알려져 왔다. DCL은 먼저 동기화 구문이 없는 상태로 초기화 작업이 필요한지를 확인하고, resource 변수의 값이 null이 아니라면(다시 말해 초기화가 돼 있다면) resource 변수에 참조된 객체를 사용한다. 만약 최기화 작업이 필요하다면 동기화 구문을 사용해 락을 걸고 Resource 객체가 초기화됐는지 다시 한 번 확인하는데, 이렇게 하면 Resource 객체를 초기화하는 작업은 한 번에 하나의 스레드만 가능하긴 하다. 여기에서 가장 자주 사용하는 부분, 즉 이미 만들어진 Resource 인스턴스에 대한 참조를 가져오는 부분은 동기화돼 있지 않았다. 바로 이 부분이 문제인데, 부분 구성된 Resource 인스턴스를 사용하게 될 가능성이 있다.  

DCL이 갖고 있는 더 큰 문제는 동기화돼 있지 않은 상태에서 발생할 수 있는 가장 심각한 문제가 스테일 값(여기에서는 null)을 사용할 가능성이 있는 정도에 불과하다고 추정하고 있다는 점이다. 만약 stale 값을 사용하는 경우가 발생하면 락을 확보한 채로 재시도해서 문제를 해결한다. 히자만 실제 최악의 상황은 추정했던 최악의 상황보다 더 심각하다. 즉 현재 객체에 대한 참조를 제대로 본다 하더라도 객체의 상태를 볼 때 스테일 값을 보게되는 경우, 즉 참조된 객체 내부의 상태가 올바르지 않은 상태인 경우가 생길 수 있다.  

(자바 5.0 이후) 수정된 JMM의 내용을 보면 resource 변수를 volatile로 선언했을 때는 DCL마저 정상적으로 동작한다. 또한 volatile 변수에 대한 읽기 연산은 volatile이 아닌 변수의 읽기 연산보다 자원을 아주 조금 더 사용할 뿐이기 때문에 성능에 미치는 영향도 미미하다.  

```java
@NotThreadSafe
public class DoubleCheckedLocking {
  private static Resource resource;

  public static Resource getInstance() {
    if (rsource == null) {
      synchronized (DoubleCheckedLocking.class) {
        if (resource == null)
          resource = new Resource();
      }
    }
    return resource;
  }
}
```

어쨌거나 DCL이 해결하고자 했던 바(경쟁이 없을 때도 느린 동기화 구문, 시동하는 데 시간이 많이 걸리는 문제)는 이미 시대가 지나면서 대부분 사라졌으며, 더 이상 최적화의 의미를 찾기가 어려워졌다. 하지만 늦은 초기화 홀더 클래스 구문은 DCL보다 훨씬 이해하기도 쉬우면서 동일한 기능을 제공한다.  

#### 16.3. 초기화 안전성
<br/>
초기화 안전성(EagerInitailization safety)을 보장한다는 의미는 올바르게 생성된 분변 객체를 어떤 방법으로건, 심지어는 데이터 경쟁이 발생하는 방법으로 공개하더라도 여러 스레드에서 별다른 동기화 구문 없이 안전하게 사용할 수 있다는 의미이다(예를 들어 UnsafeLazyInitialization 클래스에서 Resource 클래스가 변경 불가능한 객체였다면 UnsafeLazyInitialization 클래스 역시 안전한 방법으로 구현됐다는 의미이다).  

초기화 안전성을 확보하지 못한 상태에서는 변경 불가능하다고 알려진 String과 같은 클래스조차 공개하거나 다른 스레드가 사용하는 과정에서 값이 바뀌는 것처럼 보일 수도 있다. String 객체의  불변성을 기반으로 설계된 보안 아키텍처가 있다고 하면, 초기화 안전성을 확보하지 못하는 경우 악의적인 프로그램이 보안 검증 과정을 통과하도록 하는 보안상의 허점이 되기도 한다.  

초기화 안전성이 확보돼 있다면 완전하게 구성된 객체를 대상으로 해당 객체가 어떻게 공개됐던 간에 생성 메소드가 지정하는 모든 final 변수의 값을 어떤 스레드건 간에 올바르게 읽어갈 수 있다는 점을 보장한다. 또한 완전하게 구성된 객체 내부에 final로 선언된 객체를 거쳐 사용할 수 있는 모든 변수(예를 들어 final로 선언된 배열의 항목 또는 final로 선언된 HashMap 내부에 들어 있는 값 등) 역시 다른 스레드에서 안전하게 볼 수 있다는 점도 보장된다. 이 조건은 생성 중인 객체 내부에 final로 선언된 변수를 통해서만 접근할 수 있는 객체에 한해 적용된다.  

final로 선언된 변수를 갖고 있는 클래스는 초기화 안전성 조건 때문에 해당 인스턴스에 대한 참조를 최초로 생성하는 과정에서 재배치 작업이 일어나지 않는다. 생성 메소드에서 final 변수에 값을 쓰는 작업과 final 변수를 통해 접근 가능한 모든 변수에 값을 쓰는 작업을 생성 메소드가 종료되는 시점에 '고정'된다. 따라서 해당 객체에 대한 참조를 가져간 모든 스레드는 최소한 고정된 상태에 있는 변수의 값은 볼 수 있다. final 변수를 토앻 접근 가능한 변수에 초기화를 위해 쓰기 작업을 하는 경우 생성 메소드에서 고정되는 시점 이후에 쓰기 작어빙 동작한다 해도 역시 재배치 현상이 발생하지 않는다.  

안전하게 초기화한다는 말의 의미는 다음 예제의 SafeStates 클래스와 같이 별다른 동기화도 하지 않고 스레드 안전하지 않은 HashSet을 사용한다해도, 이를 대상으로 안전하지 않은 늦은 초기화 작업을 진행하거나 동기화 구문 없이 SafeStates에 대한 참조를 public static으로 선언된 변수에 선언하는 것으로도 안전하게 공개할 수 있다는 뜻이다.  

```java
@ThreadSafe
public class SafeStates {
  private final Map<String, String> states;

  public SafeStates() {
    states = new HashMap<String, String>();
    states.put("alaska", "AK");
    states.put("alabama", "AL");
    …
    states.put("wyoming", "WY");
  }

  public String getAbbreviation(String s) {
    return states.get(s);
  }
}
```

하지만 SafeStates 클래스에 몇 가지 조그마한 변경 사항이 가해지면 앞서 소개했던 스레드 안전성을 잃게 된다. 예를 들어 states 변수가 final로 선언되지 않았거나, 생성 메소드가 아닌 다른 메소드에서 states 변수의 내용을 변경할 수 있도록 돼 있다면 초기화 안전성이 힘을 잃으면서 동기화구문을 추가로 사용하지 않는 한 SafeStates 클래스를 안전하게 사용할 수 없게 된다. 그리고 SafeStates 클래스에 final로 선언되지 않은 다른 변수가 더 있었다면, final이 아닌 변수에 대해서는 다른 스레드에서 올바르지 않은 값을 보게 될 수도 있다. 또한 생성 메소드가 완료되기 전에 해당 객체를 외부에서 사용할 수 있도록 유출시키는 작업 역시 초기화 안전성을 무너뜨리는 일이다.  

초기화 안전성은 생성 메소드가 완료되는 시점에 final로 선언된 변수와 해당 변수를 거쳐 접근할 수 있는 값에 대해서만 가시성을 보장한다. final로 선언되지 않은 변수나 생성 메소드가 종료된 이후에 변경되는 값에 대해서는 별도의 동기화 구문을 적용해야 가시성을 확보할 수 있다.  

### 요약
<br/>
자바 메모리 모델은 특정 스레드에서 메모리를 대상으로 취하는 작업이 다른 스레드에게 어떻게 보이는지의 여부를 병시하고 있다. 가시성을 보장해주는 연산은 미리 발생이라는 규칙을 토앻 부분적으로 실행 순서가 정렬된 상태를 유지하며, 미리 발생 규칙은 개별적인 메모리 작업이나 동기화 작업의 수준으에서 정의하는 규칙이다. 충분히 동기화되지 않은 상태에서는 공유된 데이터를 여러 스레드에서 사용할 때는 굉장히 이상한 현상이 발생할 수 있다. &#64;GuardedBy나 안전한 공개 등 고수준의 방법을 적용하면 미리 발생 규칙과 같은 저수준의 세밀한 부분까지 신경 쓰지 않는다 해도 스레드 안전성을 보장할 수 있다.  

### 부록 A. 병렬 프로그램을 위한 어노테이션
<br/>
#### A.1. 클래스 어노테이션
<br/>
해당하는 클래스의 스레드 안전성 관련 정보를 제공할 수 있도록 &#64;Immutable, &#64;ThreadSafe, &#64;NotThreadSafe라는 세 가지의 클래스 어노테이션을 사용한다. &#64;Immutable 어노테이션은 해당 클래스가 불변(immutable) 클래스임을 나타내며, 따라서 자동적으로 &#64;ThreadSafe이기도 하다. &#64;NotThreadSafe 어노테이션은 해당 클래스가 스레드 안전성을 확보하지 못하고 있다는 의미이며, 스레드 안전성을 확보했다는 어노테이션을 달지 않은 모든 클래스는 당연하게 &#64;NotThreadSafe이기 때문에 꼭 사용해야만 하는 것은 아니다. 다만 한눈에 알아볼 수 있도록 하려면 &#64;NotThreadSafe 어노테이션을 달아두는 편이 좋다.  

이와 같은 어노테이션은 상대적으로 방해되는 부분이 적으면서 클래스 사용자나 유지보수 담당자 모두에게 이득이 있다. 사용자는 해당 클래스가 스레드 안전성을 확보했는지 단번에 알아볼 수 있고, 유지보수 담당자는 스레드 안전성을 계속해서 유지해야 하는지를 명확하게 알 수 있다. 개발 관련 도구에서는 이런 어노테이션을 유용하게 사용할 수 있다. 정적인 코드 분석 도구는 어노테이션이 달려 있는 코드가 어노테이션의 의미에 맞게 구현되어 있는지, 예를 들어 &#64;Immutable 어노테이션이 달려 있는 클래스가 실제로 변경 불가능한지의 여부 등을 확인할 수 있을 것이다.  

#### A.2. 필드와 메소드 어노테이션
<br/>
앞서 소개한 클래스 어노테이션은 클래스를 대상으로 하는 공개 문서의 일부분이라고 볼 수 있겠다, 해당 클래스의 스레드 안전성 확보 전략의 다른 측면은 대부분 공개 문서의 일부로 사용하기 위한 것이라기보다 유지보수 담당자를 위해 필요한 부분이다.  

락을 사용하는 모든 클래스는 어떤 상태 변수를 어떤 락으로 보호하고 있는지에 대한 설명을 문서에 포함시켜야 한다. 부실한 설계로 인해 스레드 안전성 확보에 실패하는 대부분의 경우를 보면, 애초에는 락을 사용해 안정적으로 상태 변수를 보호하고 있었지만 유지보수 과정에서 상태 변수가 추가되거나 변경되었을 때 해당하는 부분을 락으로 적절하게 보호하지 못했기때문일 수도 있고, 아니면 메소드를 추가하면서 사용하는 상태 변수를 적절하게 동기화하지 못한 원인이 있을 때도 있다. 어느 상태 변수를 어느 락으로 보호하고 있는지에 대한 정보를 문서에 명확하게 표시해두면 이와 같은 종류의 문제점을 막을 수 있는 중요한 자원이 된다.  

&#64;GuardedBy(lock) 어노테이션은 해당 필드나 메소드를 사용하려면 반드시 지정된 락을 확보한 상태에서 사용해야 한다는 점을 의미한다. lock 인자는 해당 필드나 메소드를 사용하려 할 때 반드시 확보해야 할 락을 의미한다. lock 인자로 지정할 수 있는 값에는 아래와 같은 것들이 있다.  

+ &#64;GuardedBy("this") 어노테이션은 해당 필드나 메소드가 들어 있는 객체에 대한 암묵적인 락을 확보해야 함을 뜻한다.  

+ &#64;GuardedBy("fieldName") 어노테이션은 지정된 이름의 필드가 가리키는 객체에 의한 암묵적인 락(Lock 클래스가 아닌 모든 객체)이나 또는 명시적인 Lock 객체를 통해 락을 확보해야 함을 의미한다.  

+ &#64;GuardedBy("ClassName.fieldName") 어노테이션은 &#64;GuardedBy("fieldName")과 동일하지만 지정한 클래스의 static 필드를 대상으로 한다.  

+ &#64;GuardedBy("methodName()") 어노테이션은 지정한 이름의 메소드를 호출한 결과로 받아온 객체에 대한 암묵적인 락을 확보해야 한다는 의미이다.  

+ &#64;GuardedBy("ClassName.class") 어노테이션은 지정한 이름의 클래스 자체에 대한 락을 확보해야 한다는 의미이다.  

&#64;GuardedBy 어노테이션을 사용하면 락을 사용해야 할 상태 변수에 어떤 것이 있는지 한눈에 알아볼 수 있기 때문에 코드를 유지보수하는 입장에서 굉장히 중요한 정보이고, 자동화된 코드 분석 도구에서 잠재적인 스레드 안전성 관련 오류를 찾아내도록 도와줄 수 있는 기본 자료가 된다.

---
title: Kubernetes Patterns
categories:
- Kubernetes
feature_text: |
  ## Kubernetes Patterns
feature_image: "https://picsum.photos/2560/600?image=733"
image: "https://picsum.photos/2560/600?image=733"
---
<style>
	thead td { text-align: center; }
	td { border: 1px solid #444444; }
	.align-center { text-align: center; }
</style>

### 1. 개요
<br/>

#### 1.1. 클라우드 네이티브로 가는 길
<br/>

쿠버네티스 같은 클라우드 네이티브 플랫폼에서 가장 인기 있는 애플리케이션 아키텍처는 마이크로서비스(microservice) 방식이다. 마이크로서비스 방식의 소프트웨어 개발 기술은 운영 복잡성 대신 개발 복잡성을 활용하고 비즈니스 기능의 모듈화를 통해 소프트웨어 복잡성을 해결한다.  

마이크로서비스를 도입하는 움직임의 일환으로, 마이크로서비스를 처음부터 새로 만들거나 모놀리스(monolith)를 마이크로서비스로 분할하는 데 쓰이는 이론과 보완 기술은 상당히 많다. 대부분 이와 같은 실용 사례는 에릭 에반스(Eric Evans)가 쓴 『도메인 주도 설계』(위키북스, 2011), 그리고 바운디드 컨텍스트(bounded contexts)와 애그리깃(aggregate) 개념에 근거한다. 바운디드 컨텍스트 개념은 대형 모델들을 각 컴포넌트로 분리해 다루고, 애그리깃 개념은 바운디드 컨텍스트를 트랜잭션 경계를 갖는 모듈로 그룹화하는 데 도움을 준다. 그러나 이러한 비즈니스 도메인을 고려하는 것 외에도, 마이크로서비스에 기반하든 안 하든 간에 모든 분산 시스템에는 조직, 구조, 런타임 동작(runtime behavior) 등 기술적으로 고려해야 할 사항이 많다.  

컨테이너와 쿠버네티스 같은 컨테이너 오케스트레이터는 분산 애플리케이션 문제를 해결하기 위한 많은 새로운 기본 요소(primitive)와 추상화를 제공한다. 따라서 분산 시스템을 쿠버네티스에 적용할 때 고려해야 할 다양한 옵션에 대해 설명한다.  

컨테이너와 클라우드 네이티브 플랫폼은 분산 시스템에 많은 이점을 주지만, 만약 컨테이너에 넣은 것이 모두 쓰레기라면 대규모 분산 쓰레기만을 얻게 될 것이다. 이상적인 클라우드 네이티브 애플리케이션을 만드는 데 필요한 여러 기술의 조합을 보여준다.  

상위 레벨에서, 저마다 다른 설계 고려사항을 요구하는 클라우드 네이티브 애플리케이션에는 다양한 추상화 레벨이 있다.  

+ 가장 낮은 코드 레벨(code level)에서는, 정의된 모든 변수, 생성한 모든 메소드, 인스턴스화하기로 결정한 모든 클래스가 장기간의 애플리케이션 유지 관리에 중요한 역할을 한다. 어떤 컨테이너 기술, 어떤 오케스트레이션 플랫폼을 사용하든 간에 개발팀과 이들이 만든 아티팩트가 가장 큰 영향을 미칠 것이다. 무엇보다도, 클린 코드를 작성하기 위해 노력하고, 적절한 양의 자동화 테스트를 만들며, 지속적으로 코드를 리팩토링하고, 소프트웨어 장인의 함량을 갖춘 개발자들을 양성하는 것이 중요하다.  

+ 도메인 주도 설계(domain-driven design)란 아키텍처를 가능한 한 현실세계에 가깝게 맞추려는 의도로 비즈니스 관점에서 소프트웨어를 설계하는 방법이다. 도메인 주도 설계 방식이 객체 지향 프로그래밍 언어에는 가장 효과적이지만, 그 밖에도 현실세계 문제에 대한 소프트웨어를 모델링하고 설계하는 좋은 방법들이 많이 있다. 올바른 비즈니스 및 트랜잭션 경계, 사용하기 쉬운 인터페이스, 풍부한 API 등을 갖춘 모델은 향후 성공적인 컨테이너화 및 자동화를 위한 기본이 된다.  

+ 마이크로서비스 아키텍처 방식(microservices architectural style)은 매우 빠르게 발전해 표준이 되었으며, 변화하는 분산 애플리케이션 설계에 대한 소중한 원칙과 실용적인 방법을 제공한다. 이와 같은 원칙을 적용함으로써 우리는 오늘날 최신 소프트웨어에 공통 요구사항인 스케일(scale)과 회복성(resiliency), 변화 속도에 최적화된 구현을 할 수 있다.  

+ 컨테이너(container)는 분산 애플리케이션을 패키징하고 실행하는 표준 방법으로 빠르게 적용되었다. 훌륭한 클라우드 네이티브 요소인 모듈식의 재사용 가능한 컨테이너를 만드는 것이 또 하나의 기본 전제조건이다. 모든 조직에서 개발하는 컨테이너 수가 증가함에 따라 해당 컨테이너들을 더 효과적인 방법과 도구로 관리해야 할 필요성이 대두되고 있다. 클라우드 네이티브는 비교적 새로운 용어로서, 컨테이너화된 대규모 마이크로서비스를 자동화하기 위한 원칙과 패턴, 도구를 설명하는 데 쓰인다.  

이러한 패턴들이 효과를 내려면 무엇보다도 클린 코드와 도메인 주도 설계, 마이크로서비스 패턴, 그 밖의 관련 설계 기법을 적용해 애플리케이션 내부를 잘 설계해야 한다.  

#### 1.2. 분산 기본 요소  
<br/>

새로운 추상화와 기본 요소의 의미를 알아보기 위해 잘 알려진 객체 지향 프로그래밍, 특히 자바와 비교해보겠다. 객체 지향 프로그래밍 세계에는 클래스와 객체, 패키지, 상속, 캡슗화, 다형성(polymorphism) 같은 개념이 있다. 자바 런타임은 객체와 애플리케이션의 수명주기 관리 방법에 대해 특정 기능을 제공하고 보장한다.  

자바 언어와 자바 가상 머신(JVM)은 애플리케이션을 생성하기 위한 로컬 인프로세스(in-process) 빌딩 블록을 제공한다. 쿠버네티스는 멀티 노드와 멀티 프로세스로 퍼져 있는 분산 시스템을 구축하기 위한 새로운 분산 기본 요소와 런타임을 제공함으로써 이 빌딩 블록이라는 널리 알려진 사고방식에 완전히 새로운 관점을 추가한다. 쿠버네티스를 사용하면 전체 애플리케이션 동작을 구현하기 위해 로컬 기본 요소에만 의존하지 않는다.  

분산 애플리케이션 컴포넌트를 만들려면 여전히 객체 지향 구성요소를 사용해야 하지만, 일부 애플리케이션 동작을 위해서는 쿠버네티스의 구성요소를 사용할 수도 있다. 다양한 개발 개념이 로컬과 분산 기본 요소에서 어떻게 다르게 구현되는지를 보여준다.  

<table>
    <thead>
        <tr>
            <th>개념</th>
            <th>로컬 기본 요소</th>
            <th>분산 기본 요소</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td>캡슐화 동작</td>
            <td>클래스</td>
            <td>컨테이너 이미지</td>
        </tr>
        <tr>
            <td>인스턴스화 동작</td>
            <td>객체</td>
            <td>컨테이너</td>
        </tr>
        <tr>
            <td>재사용 단위</td>
            <td>.jar 파일</td>
            <td>컨테이너 이미지</td>
        </tr>
        <tr>
            <td>컴포지션(Composition)</td>
            <td>클래스 A가 클래스 B를 포함</td>
            <td>사이드카 패턴</td>
        </tr>
        <tr>
            <td>상속</td>
            <td>클래스 A가 클래스 B를 확장</td>
            <td>'FROM 부모 이미지'로 만든 컨테이너 이미지</td>
        </tr>
        <tr>
            <td>배포 단위</td>
            <td>.jar/.war/.ear</td>
            <td>파드</td>
        </tr>
        <tr>
            <td>빌드타임/런타임 격리</td>
            <td>모듈, 패키지, 클래스</td>
            <td>네임스페이스, 파드, 컨테이너</td>
        </tr>
        <tr>
            <td>초기화 필요 조건</td>
            <td>생성자</td>
            <td>초기화 컨테이너</td>
        </tr>
        <tr>
            <td>초기화 직후 트리거</td>
            <td>Init 메소드</td>
            <td>postStart</td>
        </tr>
        <tr>
            <td>삭제 직전 트리거</td>
            <td>Destroy 메소드</td>
            <td>preStop</td>
        </tr>
        <tr>
            <td>정리(Cleanup) 절차</td>
            <td>finalize(), 셧다운 훅</td>
            <td>Defer 컨테이너</td>
        </tr>
        <tr>
            <td>비동기 & 병렬 실행</td>
            <td>ThreadPoolExecutor, ForkJoinPool</td>
            <td>잡</td>
        </tr>
        <tr>
            <td>주기적 작업</td>
            <td>Timer, ScheduledExecutorService</td>
            <td>크론잡</td>
        </tr>
        <tr>
            <td>백그라운드 작업</td>
            <td>데몬 스레드</td>
            <td>데몬세트</td>
        </tr>
        <tr>
            <td>설정 관리</td>
            <td>System.getenv(), Properties</td>
            <td>컨피그맵, 시크릿</td>
        </tr>
    </tbody>
</table>
<br/><br/>

Defer(혹은 de-init) 컨테이너는 아직 구현되어 있지 않다. 그러나 쿠버네티스의 향후 버전 기능에 넣기 위해 제안(http://bit.ly/2TegEM7)을 진행 중이다.  

인프로세스(in-process) 기본 요소와 분산 기본 요소는 공통점이 있지만 직접적으로 비교하거나 서로 대체할 수는 없다. 이 둘은 다른 추상화 레벨에서 작동하며 다른 전제조건과 보장이 있다. 일부 기본 요소는 함께 사용되어야 한다. 예를 들어 객체를 생성하기 위해서는 클래스를 사용해야 하고 이 클래스를 컨테이너 이미지에 넣어야 한다. 그러나 쿠버네티스에서의 크론잡(CronJob)같은 기본 요소는 자바의 ExecutorService 기능을 완벽하게 대체할 수 있다.  

##### 1.2.1. 컨테이너  
<br/>

컨테이터(Container)는 쿠버네티스 기반 클라우드 네이티브 애플리케이션의 빌딩 블록이다. 객체 지향 프로그래밍(OOP)과 자바를 비교해 본다면, 컨테이너 이미지는 클래스와 비슷하며 컨테이너는 객체와 비슷하다. 클래스를 확장해 동작을 재사용하고 변경하는 방시과 유사하게, 컨테이너 이미지를 확장해 동작을 재사용하고 변경하는 컨테이너 이미지를 만들 수 있다. 객체를 컴포지션(Composition)할 수 있고  기능을 사용하는 방식과 유사하게, 파드 안에 여러 컨테이너를 넣고 서로 협력하는 컨테이너를 사용함으로써 컨테이너를 컴포지션할 수 있다.  

비유를 이어가 보면, 쿠버네티스는 JVM과 비슷하지만 멀티 호스트로 분산시켜 컨테이너를 실행하고 관리하는 책임을 진다. 초기화 컨테이너(init Container)는 객체 생성자 같은 것이다. 대몬세트는 예를 들면, 자바 가비지 콜렉터 같은 백그라운드로 실행되는 데몬 스레드와 비슷하다. 파드(pod)는 다중으로 실행 중인 객체가 수명주기를 공유하고 서로 직접적으로 접근할 수 있는 스프링 프레임워크에서의 제어 역전(Inversion of Control, IoC) 컨텍스트와 유사하다.  

컨테이너를 사용한다고 병렬화가 더 좋아지는 것은 아니지만, 여기서 요점은 컨테이너가 쿠버네티스에서 기본적인 역할을 하며, 모듈화된 재사용 가능한 단일 목적의 컨테이너 이미지를 만드는 것은 모든 프로젝트는 물론이고 심지어는 컨테이너 에코시스템을 통틀어 장기적인 성공을 거두는 데 기본이 된다는 점이다. 패키징과 격리를 제공하는 컨테이너 이미지의 기술적 특성 외에 컨테이너는 과연 무엇을 나타내며, 분산 애플리케이션 영역에서 컨테이너의 목적은 무엇일까? 컨테이너를 어떻게 바라볼지 몇 가지를 정리해봤다.  

+ 컨테이터 이미지는 하나의 문제를 해결하는 기능 단위다.
+ 컨테이터 이미지는 하나의 팀에 의해 소유되며, 릴리스 주기가 있다.
+ 컨테이터 이미지는 자기 완비적이며, 런타임 의존성을 정의하고 수행한다.
+ 컨테이터 이미지는 불변적이며, 한번 만들어지면 변경되지 않는다. 즉 이미 설정 값이 정해져 있다.
+ 컨테이터 이미지는 런타임 의존성과 자원 요구사항이 정의되어 있다.
+ 컨테이터 이미지에는 기능을 노출시키기 위해 잘 정의된 API가 있다.
+ 컨테이터는 일반적으로 하나의 유닉스 프로세스로 실행된다.
+ 컨테이너는 일회용이며 언제든지 스케일 업(scale up)과 스케일 다운(scale down)을 안전하게 수행할 수 있다.  

이 모든 특징 외에 덧붙이자면, 적절한 컨테이너 이미지는 모듈식이다. 컨테이너 이미지는 다른 환경에서 재사용해 실행할 수 있도록, 또한 다양한 사용 예(use case)에도 적용할 수 있도록 파라미터로 전달할 수 있게 되어 있다. 작고, 모듈식이면서, 재사용 가능한 컨테이너 이미지를 활용하면 마치 프로그래밍 언어의 재사용 가능한 라이브러리처럼 더 특화되고 장기적으로 안정적인 컨테이너 이미지를 만들 수 있다.  

##### 1.2.2. 파드  
<br/>

컨테이너의 특징을 들여다보면, 컨테이너가 마이크로서비스 원칙에 딱 들어맞는다는 사실을 알 수 있다. 컨테이너 이미지는 하나의 기능 단위를 제공하고, 하나의 팀이 관리하며, 독립적인 릴리스 주기를 가지고, 독립된 런타임과 배포를 제공한다. 대부분의 경우 하나의 마이크로서비스는 하나의 컨테이너 이미지에 대응된다.  

그러나 대부분의 클라우드 네이티브 플랫폼은 컨테이너 그룹의 수명주기를 관리하기 위해 또 다른 기본 요소를 제공하는데, 쿠버네티스에서는 이를 파드(Pod)라 부른다. 파드란 컨테이너 그룹의 스케줄링과 배포, 격리된 런타임에 대한 최소 단위다. 파드 안의 모든 컨테이너는 항상 같은 호스트에 스케줄링되고, 호스트 이전이나 스케일을 목적으로 함께 배포되며, 파일시스템, 네트워킹, 프로세스 네임스페이스를 공유할 수도 있다. 이렇게 동시에 적용되는 수명주기 덕분에, 파드 안의 컨테이너는 파일 시스템이나 로컬호스트 혹은 호스트 간 프로세스 통신 메커니즘을 통한 네트워크 통신으로 컨테이너 간의 상호작용을 할 수 있다.  

개발이나 빌드 때의 마이크로서비스는 한 팀에서 개발하고 릴리스하는 컨테이너 이미지에 해당된다. 그러나 런타임 시의 마이크로서비스는 배포와 배치(placement), 스케일의 단위인 파드로 표현된다. 규모나 마이그레이션에 관계없이 컨테이너를 실행하는 유일한 방법은 파드 추상화를 통하는 것이다. 간혹 파드에는 하나 이상의 컨테이너가 포함된다. 일례로 컨테이너화된 마이크로서비스가 런타임 시에 헬퍼 컨테이너를 사용하는 경우를 들 수 있다.  

컨테이너와 파드, 이 둘의 고유한 특성은 마이크로서비스 기반의 애플리케이션을 설계하기 위한 새로운 패턴과 원칙을 제공한다. 잘 설계된 컨테이너의 특성을 알아봤으니 이제는 파드의 특성을 한번 살펴보자.  

+ 파드는 스케줄링의 최소 단위다. 이 말은 스케줄러가 파드에 속한 모든 컨테이너의 요구사항을 만족하는 호스트를 찾으려 시도한다는 뜻이다. 만약 다수의 컨테이너가 포함된 하나의 파드를 생성한다면, 스케줄러는 모든 컨테이너의 요구를 만족할 정도로 충분한 자원을 갖춘 호스트를 찾아야 한다.

+ 파드는 파드에 속한 컨테이너들의 동일 장소 배치(colorcation)를 보장한다. 동일 장소 배치 덕분에 동일한 파드 안의 컨테이너는 서로 상호작용할 수 있는 방법이 또 있다. 가장 일반적인 통신 방법은 데이터 교환을 위해 공유 로컬 파일시스템을 사용하거나 로컬호스트 네트우어크 인터페이스를 사용하는 방법, 또는 고성능 상호작용을 위한 호스트 프로세스 간 통신(IPC, interprocess communication) 메커니즘 등이 있다.

+ 한 파드는 파드 안의 모든 컨테이너가 공유하는 하나의 IP 주소와 이름, 포트 범위를 갖는다. 다시 말해, 병렬로 실행되는 유닉스 프로세스들이 호스트 상의 네트워킹 공간을 공유할 때 주의해야 하는 것과 마찬가지로, 동일한 파드 안에 있는 컨테이너들 또한 포트가 겹치지 않게 조심해서 설정해야 한다는 뜻이다.  

파드는 애플리케이션이 실행되는 쿠버네티스의 가장 작은 단위이며, 직접 파드에 접근하는 일은 없다.  

##### 1.2.3. 서비스  
<br/>

파드는 스케일 업이나 다운, 컨테이너 정상상태(health) 확인 실패나 노드 이전 같은 여러 가지 이유로 언제든지 생성되고 사라질 수 있는 일시적인 자원이다. 파드의 IP 주소는 파드가 노드에 스케줄되고 시작된 이후에만 알 수 있다. 파드가 실행되고 있는 노드가 더 이상 정상상태(health)가 아니라면, 파드는 다른 노드에 다시 스케줄링될 수 있다. 이 말은 파드의 네트워크 주소는 애플리케이션이 실행되는 동안 바뀔 수 있으며, 파드를 찾고 로드 밸런싱하기 위한 또 다른 기본 요소가 필요하다는 사실을 의미한다.  

쿠버네티스 서비스(Service) 역할이 필요한 이유가 여기에 있다. 서비스는 쿠버네티스의 간단하면서도 강력한 또 하나의 추상화로, 서비스 이름을 IP 주소와 포트 번호로 영구히 연결시켜 준다. 그래서 서비스는 애플리케이션에 접근하기 위한, 이름으로 된 진입점이라 할 수 있다. 대부분 사례에서 서비스는 파드 세트에 대한 진입점으로 사용되지만, 모든 경우가 그렇지는 않다. 서비스는 일반적인 기본 요소며, 쿠버네티스 클러스터 외부에서 제공되는 기능들을 가리킬 수도 있다. 따라서 서비스는 서비스 디스커버리(discovery)와 로드 밸런싱으로 사용될 수 있으며, 서비스 컨슈머(consumer)에게 영향을 주지 않으면서 구현 변경과 스케일을 가능하게 해준다.  

##### 1.2.4. 레이블  
<br/>

쿠버네티스에서 제공하는 레이블(label)과 네임스페이스(namespace)라는 2가지 기본 요소는 애플리케이션 개념을 정의하는 데 도움이 된다.  

마이크로서비스가 등장하기 이전의 애플리케이션은 단일 버전 스키마와 단일 릴리스 주기를 갖는 단일 배포 단위에 해당했다. 당시 애플리케이션은 .war나 .ear를 비롯한 패키징 포맷 형태인 하나의 파일이었다. 그러나 이후에 애플리케이션은 마이크로서비스로 나뉘어서, 각기 독립적으로 개발되고, 릴리스되며, 실행되고, 재시작되고, 스케일되기에 이르렀다. 마이크로서비스를 도입하면 통일된 하나의 애플리케이션 개념은 사라지고, 하나의 애플리케이션 레벨로 수행하는 핵심 아티팩트나 동작 또한 더 이상 존재하지 않는다. 그러나 일부 독립 서비스가 애플리케이션에 속함을 나타내야 한다면 레이블을 쓰면 된다. 모놀리스(monolith) 애플리케이션 하나를 3개의 마이크로서비스로, 또 하나는 2개의 마이크로서비스로 분할했다면 가정해보자.  

이렇게 하면 개발과 런타임에 무관한 5개의 파드 정의가 존재하게 되지만(파드 인스턴스는 더 많을 수도 있다), 첫 파드 3개와 또 다른 파드 2개가 각기 다른 애플리케이션을 나타내도록 지정해야 할 수도 있다. 파드는 비즈니스 가치를 제공하기 위해 독립적일 수 있지만 서로 의존적일 수도 있다. 예를 들어 하나의 파드에는 프론트엔트를 책임지는 컨테이너가 있을 수 있고, 또 다른 2개의 파드에는 백엔드 기능을 제공하는 컨테이너가 있을 수 있다. 이 3개의 파드 중 하나라도 다운되면 애플리케이션은 비즈니스 관점에서 쓸모가 없게 된다. 레이블 셀렉터를 사용하면 파드를 세트로 쿼리, 식별하고 하나의 논리적 단위로 관리할 수 있다.  

다음은 레이블을 사용하는 몇 가지 예다.  

+ 레이블은 실행 중인 특정 파드의 인스턴스들을 가리키기 위해 레플리카세트(ReplicaSet)에서 사용된다. 즉 모든 파드 정의에는 스케줄링에서 사용될 고유한 레이블 조합이 있어야 한다는 의미다.

+ 레이블은 스케줄러에서 많이 사용된다. 스케줄러는 파드의 요구사항에 맞는 노드에 파드를 배치하기 위해, 레이블을 사용해 파드를 분산시키기도 하고 함께 노드에 위치시키기도 한다.

+ 레이블은 파드를 논리적 그룹으로 묶어 가리킬 수 있고 그 파드 그룹에 애플리케이션 식별자를 지정할 수 있다.

+ 이와 같은 일반적인 사용 예 외에 레이블은 메타데이터를 저장하는 데도 쓰인다. 어떤 레이블이 사용될지 미리 예측하기는 어렵지만 파드의 모든 중요한 면을 설명할 수 있도록 레이블을 정의해두는 것이 좋다. 예를 들어 논리적인 애플리케이션 그룹, 비즈니스 특성과 중요도, 하드웨어 아키텍처나 위치 설정 같은 특정 런타임 플랫폼 의존성을 가리키는 레이블들을 갖고 있으면 모두 쓸모가 있다. 이후에 이들 레이블은 스케줄러가 더 세분화하여 스케줄링하는 데 사용되거나, 명령 줄(command line)에서 대규모로 매칭되는 파드를 관리하는 데도 사용될 수 있다. 하지만 미리 앞서서 너무 많은 레이블을 추가해서는 안 된다. 필요할 때 나중에 레이블을 추가하면 된다. 어떤 레이블이 사용될지, 그리고 레이블을 삭제하면 어떤 영향이 의도치 않게 일어날지 알아낼 방법이 없기 때문에 레이블을 삭제하는 것은 훨씬 더 위험 부담이 크다.  

##### 1.2.5. 애노테이션  
<br/>

레이블과 유사한 또 다른 기본 요소로는 애노테이션(annotation)이 있다. 레이블처럼 애노테이션도 맵(map) 형태로 구성되지만 사람보다는 기계를 위한 용도로 사용되며, 검색 불가능한 메타데이터(metadata)를 지정하는 데 사용된다.  

애노테이션 정보는 객체를 매칭하거나 쿼리하려는 것이 아니다. 그보다는, 우리가 사용하길 원하는 다양한 도구나 라이브러리에 인식시키기 위해 객체에 추가적인 메타데이터를 넣는 용도로 사용된다. 애노테이션은 주로 빌드 ID, 릴리스 ID, 이미지 정보, 타임스탬프, 깃(Git) 브랜치명, 풀리퀘스트(pull request) 번호, 이미지 해시, 레지스트리 주소, 작성자 이름, 도구 정보 등에 쓰인다. 매칭되는 자원에 대해 어떤 동작을 수행하거나 쿼리를 수행하는 데 레이블이 주로 쓰이는 반면, 애노테이션은 기계가 인식하는 메타데이터를 추가하는 데 사용된다.  

##### 1.2.6. 네임스페이스  
<br/>

또 다른 기본 요소로는 자원을 그룹화해 관리할 수 있는 쿠버네티스 네임스페이스(namespace)를 들 수 있다.  

쿠버네티스 네임스페이스를 활용하면, 멀티 호스트로 구성된 쿠버네티스 클러스터를 논리적 자원 풀(pool)로 나눌 수 있다. 네임스페이스는 쿠버네티스 자원에 대한 영역(scope)을 제공하고, 또한 권한과 정책을 클러스터의 하위 섹션에 적용하는 메커니즘을 제공한다. 네임스페이스의 가장 일반적인 사용 예는 개발 환경, 테스트 환경, 통합 테스트 환경, 운영 환경 같은 다양한 소프트웨어 환경으로 구분하는 것이다. 또한 네임스페이스는 멀티테넌시(multitenancy)를 구현하는 데 쓰이고, 팀의 작업공간이나 각 프로젝트, 심지어는 특정 애플리케이션을 격리하는 데도 사용될 수 있다. 그러나 궁극적으로 어떤 환경을 효과적으로 격리하려면 네임스페이스로는 충분하지 않으며 별도의 클러스터를 만드는 것이 가장 일반적이다. 통상적으로는, 개발, 테스트, 통합테스트 환경 등의 비운영 쿠버네티스 클러스터를 하나 만들고, 성능 테스트와 운영 환경을 위한 또 다른 운영 쿠버네티스 클러스터를 만든다.  

네임스페이스의 특징과 다양한 시나리오에서 어떻게 사용되는지를 한번 알아보자.  

+ 네임스페이스는 쿠버네티스 자원으로서 관리된다.
+ 네임스페이스는 컨테이너, 파드, 서비스, 레플리카세트 등의 자원에 대한 영역을 제공한다. 네임스페이스 내에서 자원 명은 고유해야 하지만, 다른 네임스페이스에서는 동일한 자원 명을 가질 수 있다.
+ 기본적으로 네임스페이스는 자원에 대한 영역을 제공하지만, 이들 자원을 격리할 수는 없으며 하나의 자원이 다른 자원에 접근하는 것도 막을 수는 없다. 예를 들어 파드 IP 주소를 아는 경우라면, 개발 네임스페이스의 파드는 운영 네임스페이스의 파드에 접근할 수 있다. 그러나 원한다면, 네임스페이스별로 진정한 멀티테넌시를 구현할 수 있도록 네트워크 격리를 제공해주는 쿠버네티스 플러그인을 사용할 수 있다.
+ 네임스페이스 자체나 노드, PersistentVolume 같은 자원은 네임스페이스에 속하지 않으며 클러스터 전체에서 유일한 이름을 가져야 한다.
+ 각 쿠버네티스 서비스는 네임스페이스에 속하며, &lt;service-name&gt;.&lt;namespace-name&gt;.svc.cluster.local 형식으로 네임스페이스가 포함된 DNS 주소를 갖는다. 네임스페이스 이름은 해당 네임스페이스에 속한 모든 서비스의 URI에 존재하므로, 네임스페이스 이름을 잘 정하는 것은 매우 중요하다.
+ 리소스쿼터(ResourceQuota)는 네임스페이스당 자원 소모량을 제한할 수 있는 제약조건을 제공한다. 리소스쿼터를 사용하면 클러스터 관리자는 타입별 객체의 수량을 제어할 수 있다. 예를 들어 개발자 네임스페이스에는 컨피그맵(ConfigMap) 5개, 시크릿(Secret) 5개, 서비스 5개, 레플리카세트 5개, PersistentVolumeClaim 5개, 파드 10개만 생성되게 제한할 수 있다.
+ 리소스쿼터는 네임스페이스별로 사용자가 요청하는 총 컴퓨팅 자원량을 제한할 수도 있다. 예를 들어 32GB RAM과 16코어 용량을 갖춘 클러스터에서, 운영 네임스페이스에는 전체 자원의 절반에 해당하는 16GB RAM과 8코어 용량의 자원을, 스테이징(staging) 환경에는 8GB RAM과 4코어 용량을, 개발 용도로는 4GB RAM과 2코어 용량을, 테스트 네임스페이스에는 개발과 동일한 수준의 자원을 할당할 수 있다. 네임스페이스와 리소스쿼터를 이용해 객체 그룹에 자원 제약을 거는 기능은 매우 중요하다.  

#### 1.3. 정리  
<br/>

개발자가 매일 사용하는 쿠버네티스 기본 요소는 더 많다. 예를 들어 컨테이너화된 서비스를 만든다면 쿠버네티스의 모든 장점을 활용할 수 있는 쿠버네티스 객체 컬렉션을 이용할 수 있다. 이런 객체들은 컨테이너화된 서비스를 쿠버네티스에 올리기 위해 애플리케이션 개발자가 사용하는 것뿐임을 잊지 말자. 관리자(administrator)가 사용하는 것으로, 개발자가 플랫폼을 효과적으로 관리하는 데 쓰이는 또 다른 개념도 있다.  

#### 1.4. 참고 자료  
<br/>

+ 컨테이너 기반 애플리케이션 설계 원칙(https://red.ht/2HBKqYI)
+ 12요소 애플리케이션(https://12factor.net)
+ 『도메인 주도 설계: 소프트웨어의 복잡성을 다루는 지혜』(에릭 에반스 지음, 이대엽 옮김, 위키북스, 2011)
+ 컨테이너 베스트 프록티스(http://bit.ly/2TUyNTe)
+ 도커파일(Dockerfile) 작성에 대한 베스트 프랙티스(https://docker.ly/2TFZBaL)
+ 컨테이너 패턴(http://bit.ly/2TFjsH2)
+ 일반적인 컨테이너 이미지 가이드라인(https://red.ht/2u6Ahvo)
+ 파드(https://kubernetes.io/docs/user-guide/pods/)

### 2. 예측 범위 내의 요구사항  
<br/>

공유 클라우드 환경에서 애플리케이션 배포, 관리 및 공용(coexistence)을 성공적으로 수행하려면 애플리케이션 자원 요구사항과 런타임 의존성을 명확히 식별하고 정의해야 한다. 예측 범위 내의 요구사항(Predicate Demands) 패턴이란 하드 런타임(hard runtime) 의존성이나 자원 요구사항과는 상관 없이, 애플리케이션 요구사항을 선언하는 방법에 관한 것이다. 요구사항 선언은 쿠버네티스가 클러스터 내에서 애플리케이션에 적합한 노드를 찾기 위해서 반드시 필요하다.  

#### 2.1. 문제  
<br/>

컨테이너 안에서 애플리케이션을 실행할 수만 있다면, 쿠버네티스는 다른 프로그래밍 언어로 작성된 애플리케이션도 관리할 수 있다. 하지만 언어에 따라 각기 요구사항이 다르게 마련이다. 일반적으로 컴파일된 언어는 더 빠르게 실행되며, JIT(juit-in-time) 런타임이나 인터프리터 언어에 비해 메모리 소모가 낮다. 동일한 범주의 수많은 현대적인 프로그래밍 언어가 자원 소비의 관점에서 자원 요구사항이 비슷하다는 점을 고려할 때, 언어의 종류보다는 도메인이나, 애플리케이션의 비즈니스 로직 혹은 실제 세부적인 구현 사항이 훨씬 더 중요하다.  

컨테이너가 최적의 기능을 수행하는 데 필요한 자원량을 예측하기란 어려워서, 개발자가 테스트를 수행한 후에야 비로소 서비스 구현을 위한 자원 필요량을 알 수 있다. 고정된 CPU 값이나 메모리 소비 프로파일을 가진 서비스도 있고, 스파이크(spike)를 치는 서비스도 있다. 또한 서비스에 데이터를 저장하기 위해 영구적인 스토리지(storage)가 필요하기도 하며, 호스트 서버에 고정된 특정 포트 번호를 사용해야 제대로 작동하는 레거시 서비스도 있다. 이렇듯 모든 애플리케이션 특성을 정의하고 이를 관리 플랫폼으로 전달하는 것은 클라우드 네이티브 애플리케이션의 기본 전제조건이다.  

자원 요구사항 외에도 애플리케이션 런타임은 데이터 스토리지 도는 애플리케이션 설정 같은 플랫폼 관리 기능이 필요하다.  

#### 2.2. 해결책  
<br/>

컨테이너의 런타임 요구사항을 알아야 하는 2가지 중요한 이유가 있다. 첫째, 모든 런타임 의존성이 정의되고 자원 요구사항이 계산되면, 쿠버네티스는 가장 효율적인 하드웨어 사용을 위해 클러스터 내에 컨테이너 실행 위치를 지능적으로 결정할 수 있다. 우선순위가 다른 프로세스들이 자원을 공유하면서 사용하는 환경에서는 사전에 각 프로세스의 요구사항을 알아야 프로세스가 성공적으로 공존할 수 있다. 그러나 지능적인 배치(placement)가 전부는 아니다.  

컨테이너 자원 프로파일이 필수적인 두 번째 이유로 용량 계획을 들 수 있다. 특별한 서비스 요구사항과 총 서비스 수에 근거해 다양한 환경에 대한 용량 계획을 들 수 있다. 특별한 서비스 요구사항과 총 서비스 수에 근거해 다양한 환경에 대한 용량 계획을 세우고, 전체 클러스터에 대한 요구사항을 충족하는 비용 효율이 높은 최적의 호스트 프로파일을 만들 수 있다. 성공적인 클러스터 관리를 위해 서비스 자원 프로파일과 용량 계획은 장기적으로 함께 진행해야 한다.  

자원 프로파일을 좀 더 자세히 살펴보기에 앞서 런타임 의존성을 선언하는 방법을 먼저 알아보자.  

##### 2.2.1. 런타임 의존성  
<br/>

가장 일반적인 런타임 의존성은 애플리케이션 상태를 저장하는 데 쓰이는 파일 스토리지다. 컨테이너 파일 시스템은 일시적이며 컨테이너가 종료되면 삭제된다. 쿠버네티스는 컨테이너 재시작에도 삭제되지 않고 유지되는 다용도의 파드 레벨 스토리지로서 볼륨을 제공한다.  

가장 간단한 볼륨 타입은 emptyDir이며, 파드가 살아 있는 동안만 존재하고 파드를 제거하면 볼륨과 그 안의 내용도 삭제된다. 파드를 다시 시작한 후에도 볼륨을 유지하려면 다른 종류의 스토리지 메커니즘을 지원하는 볼륨이 필요하다. 애플리케이션이 오랫동안 스토리지에 파일을 읽거나 써야 하는 경우라면, 볼륨을 사용하는 컨테이너 정의 안에 명시적으로 의존성을 선언해야 한다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random/generator
    volumeMounts:
    - mountPath: "/logs"
      name: log-volume
  volumes:
  - name: log-volume
    persistentVolumeClaim:
      claimName: random-generator-log
```

스케줄러는 파드가 요청한 볼륨 종류를 판단하며, 이는 파드가 실행될 위치에 영향을 미친다. 만약 클러스터 노드가 제공하지 않는 볼륨을 파드가 필요로 한다면, 파드는 결코 스케줄링되지 않는다. 볼륨은 런타임 의존성의 한 예로서, 파드가 실행될 수 있는 인프라스트럭처가 무엇인지, 그리고 파드가 스케줄링될 수 있는지 여부에 영향을 준다.  

쿠버네티스에 hostPort를 통해서 호스트 시스템의 특정 포트로 컨테이너 포트 노출을 요청할 때도 비슷한 의존성이 발생한다. hostPort를 사용하면 노드에 또 다른 런타임 의존성이 생성되고 파드를 스케줄링할 수 있는 위치가 제한된다. hostPort는 클러스터에서 각 노드에 해당 포트를 예약하고 노드 하나당 최대 하나의 파드만 스케줄링되게 제한한다. 포트 충돌 때문에 쿠버네티스 클러스터 노드 수만큼만 파드를 확장할 수 있다.  

또 다른 타입의 의존성으로는 설정(Configuration)이 있다. 거의 모든 애플리케이션에는 몇 가지 설정 정보가 필요하며 쿠버네티스에서 제공하는 권장 솔루션은 컨피그맵(ConfigMap)이다. 애플리케이션 설정에는 환경 변수를 통한 설정과 파일 시스템을 통한 설정, 2가지 방법이 있다. 2가지 경우 모두 컨피그맵으로 명명된 컨테이너 런타임 의존성을 적용한다. 요청한 모든 컨피그랩이 생성되지 않으면, 컨테이너는 노드에 스케줄링될 수 있지만 시작되지는 않는다.  

컨피그맵과 비슷한 개념으로는 시크릿(Secret)이 있으며, 환경 특화된 설정을 컨테이너에 적용하는 좀 더 안전한 방법을 제공한다. 시크릿을 사용하는 방법은 컨피그맵 사용 방법과 동일하며, 컨테이너부터 네임스페이스까지 시크릿은 컨피그맵과 같은 종류의 의존성이라 할 수 있다.  

스토리지와 포트 번호는 클러스터 노드가 제공하지만, 컨피그맵과 시크릿 객체 생성은 우리가 수행해야 하는 단순한 관리 작업이다. 스토리지와 포트 번호 의존성은 파드가 스케줄링되는 위치를 제한하며, 컨피그맵과 시크릿 의존성은 파드가 시작하는 것을 막을 수도 있다. 이런 의존성을 갖는 컨테이너화된 애플리케이션을 설계할 때는 향후 발생하게 될 런타임 제약사항을 항상 고려해야 한다.  

##### 2.2.2. 자원 프로파일  
<br/>

컨피그맵, 시크릿, 볼륨 같은 컨테이너 의존성을 지정하는 방법은 간단하다. 컨테이너의 자원 요구사항을 알기 위해서는 더 많은 고민과 실험이 필요하다. 쿠버네티스 컨텍스트 내에서의 컴퓨팅 자원(compute resources)이라 함은 컨테이너에 의해 요청되고, 컨테이너에 할당되고, 컨테이너가 소비할 수 있는 무언가로 정의된다. 이들 자원은 압축 가능 자원(compressible resource, 예를 들면 CPU나 네트워크 대역폭처럼 제어 가능한)과 압축 불가능 자원(incompressible resource, 예를 들면 매모리처럼 제어 불가능한)으로 분류된다.  

압축 가능 자원과 압축 불가능 자원은 반드시 구별해야 한다. 만일 컨테이너가 CPU처럼 조절 가능한 압축 가능 자원을 너무 많이 소비한다면 병목현상이 나타난다. 하지만 만약 메모리처럼 압축 불가능 자원을 너무 많이 사용한다면, 그 컨테이너는 죽어버린다(애플리케이션에 할당된 메모리 해제를 요청할 수 있는 방법이 없기 때문이다).  

애플리케이션의 특성과 세부 구현사항에 근거해 필요한 최소 자원량(즉 요청(requests))과 사용량의 증가 한도인 최대 자원량(즉 제한(limits))을 지정해야 한다. 각 컨테이너 정의(definition)에는 요청과 제한의 형태로, 필요한 CPU 양과 메모리 양을 지정할 수 있다. 상위 레벨에서 requests/limits 개념은 소프트/하드 제한과 비슷하다. 예를 들어 자바 애플리케이션에서는 -Xms와 -Xmx 명령 줄 옵션을 사용해서 힙(heap) 사이즈를 지정할 수 있다.  

requests 양(limits를 제외하고)은 스케줄러가 파드를 노드에 배치시킬 때 사용된다. 주어진 파드에 대해, 스케줄러는 해당 파드와 파드 안의 모든 컨테이너 요청 자원량을 합산해 충분히 수용할 용량이 있는 노드들만 고려한다. 그런 의미에서 각 컨테이너의 requests 필드는 파드가 스케줄링될 수 있는 위치인지 아닌지에 영향을 준다.  

requests나 limits 혹은 둘 모두를 기술하느냐에 따라, 플랫폼은 다음과 같이 여러 종류의 서비스 품질(Quality of Service, QoS)를 제공한다.  

+ 최선적(Best-Effort) 파드  
파드가 컨테이너에 대한 요청(requests)과 제한(limits)을 갖고 있지 않다. 이와 같은 파드는 가장 낮은 우선순위로 고려되고, 파드가 위치한 노드의 압축 불가능 자원(incompressible resource)이 전부 사용되어 없어지면 가장 먼저 죽는다.  

+ 확장 가능(Burstable) 파드  
파드가 요청과 제한을 모두 가지고 있지만 값은 다르다(보통 limits는 requests보다 값이 크다). 이런 파드는 최소한의 자원 보장을 받지만 가능한 경우 limits까지 더 많은 자원을 소비하려고 한다. 노드가 압축 불가능 자원에 대한 압박을 받는 경우, 이 파드는 최선적(Best-Effort) 파드가 남아 있지 않다면 죽을 확률이 높다.  

+ 보장(Guaranteed) 파드  
파드가 request와 limit 자원량을 동일하게 갖고 있다. 가장 우선순위가 높은 파드며, 최선적 파드와 확장 가능 파드보다 가장 나중에 죽는다.  

따라서 컨테이너에 대한 자원 값을 정의하거나 생략하면 서비스 품질에 직접적인 영향을 미치며, 또한 자원이 부족한 상태가 되면 파드를 죽이는 결정에도 영향을 미친다. 이런 사항을 고려해 파드 자원 요구사항을 결정해야 한다.  

##### 2.2.3. 파드 우선순위  
<br/>

컨테이너 자원 선언이 파드의 서비스 품질 결정 방법과 자원이 부족한 상황에서 큐블릿(Kubelet)이 파드 내부의 컨테이너를 어떤 우선순위로 죽이는지를 알아봤다. 이와 관련된 기능으로 파드 우선순위(Priority)와 파드 선점(Preemption)이 있다. 파드 우선순위를 사용하면 다른 파드와 비교해 상대적으로 파드의 중요성을 지정할 수 있으며 이는 파드가 스케줄되는 순서에 영향을 준다.  

```yaml
apiVersion: scheduling.k8s.io/v1beta1
kind: PriorityClass
metadata:
  name: high-priority # 우선수위 클래스 객체 이름
value: 1000 # 객체의 우선순위 값
globalDefault: false
description: This is a very high priority Pod class
---
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
  labels:
    env: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
  priorityClassName: high-priority # PriorityClass 자원에서 정의된 값으로, 파드에 적용될 우선순위 클래스
```

여기서는 네임스페이스가 없는 객체로서, 정수 값으로 우선순위를 정의하는 PriorityClass를 생성했다. 이 PriorityClass 이름은 high-priority이고 1,000이라는 우선순위 값을 가진다. 이제 priorityClassName: high-priority라는 이름을 활용해서 파드에 이 우선순위 값을 할당할 수 있다. PriorityClass는 파드 상호 간에 상대적으로 파드의 중요성을 나타내는 메커니즘으로 값이 더 높을수록 더 중요한 파드임을 나타낸다.  

파드 우선순위 기능이 활성화되면 스케줄러가 파드를 노드에 배치하는 순서에 영향을 준다. 먼저, 우선순위 어드미션 컨트롤러(priority admission controller)는 priorityClassName 필드를 사용해 새로운 파드의 우선순위 값을 채운다. 다수의 파드가 배치되기를 기다리는 경우 스케줄러는 보류 중인 파드들이 저장된 큐에서 우선순위가 가장 높은 파드를 맨 처음에 오도록 정렬한다. 스케줄링 큐 안에 우선순위가 높은 보류 중인 파드가 선택되고, 다른 스케줄링 제약사항이 없다면 선택된 그 파드는 바로 스케줄링된다.  

여기에 중요한 포인트가 있다. 파드를 배치하기에 충분한 용량을 가진 노드가 하나도 없다면, 스케줄러는 자원을 확보하고 우선순위가 높은 파드를 배치하기 위해 노드에서 실행되고 있는 우선순위고 낮은 파드를 제거한다. 결론적으로 그 밖의 모든 스케줄링 요구사항이 충족되면 우선순위가 높은 파드는 우선순위가 낮은 파드보다 더 빨리 스케줄링될 수 있다. 이 알고리즘을 통해 클러스터 관리자는 더 중요한 워크로드(workload) 파드를 효과적으로 제어하고 스케줄러가 낮은 우선순위 파드를 축출해 워커노드(worker node)에 공간을 확보함으로써 우선순위가 높은 파드를 먼저 배치할 수 있다. 만약 어떤 파드가 스케줄링될 수 없으면 스케줄러는 또 다른 우선순위가 더 낮은 파드를 계속 배치한다.  

앞에서 설명한 파드 서비스 품질(Pod QoS)과 파드 우선순위(Pod priority) 기능은 서로 연관되지 않고 겹치는 부분이 거의 없는, 직교하는(othogonal) 기능이다. 서비스 품질은 사용 가능한 컴퓨팅 자원이 낮을 때 노드의 안정성을 유지하기 위해 큐블릿에 의해 주로 사용된다. 큐블릿은 파드를 축출(eviction)하기 전에 먼저 서비스 품질을 고려하고 다음에 파드의 PriorityClass를 고려한다. 반면에 스케줄러 축출 로직은 선점 대상을 선택할 때 파드의 서비스 품질을 완전히 무시한다. 스케줄러는 배치를 기다리는 높은 우선순위 파드의 요구를 충족시키는 가장 낮은 우선순위 파드를 고르려고 시도한다.  

어떤 파드가 특정한 우선순위를 가지고 있으면 또 다른 축출된 파드에 예기치 못한 영향을 끼칠 수 있다. 예를 들어 파드가 정상적으로 종료되어도 PodDisruptionBudget이 보장되지 않으면 파드 쿼럼(quorum)에 의존하는 우선순위가 낮은 클러스터 애플리케이션은 깨질 수 있다.  

또 다른 문제는 악의적이거나 알 수 없는 사용자가 가능한 가장 높은 우선순위 파드를 만들어 그 밖의 모든 파드를 축출하는 것이다. 이를 방지하기 위해 리소스쿼터(ResourceQuota)가 PriorityClass를 지원하도록 확장되었으며 일반적으로 선점 또는 축출되어서는 안 되는 중요한 시스템 파드를 위해 더 큰 우선순위 번호가 예약되어 있다.  

정리하자면, 사용자가 지정한 숫자로 어떤 파드를 배치하거나 죽일지에 대한 우선순위를 정하고 스케줄러와 큐블릿에 알려주기 때문에, 파드 우선순위는 주의해서 사용되어야 한다. 어떤 변경사항이라도 많은 파드에 영향을 줄 수 있고 플랫폼이 예측 범위 내의 서비스 레벨 계약(service-level agreements)을 제공하지 못하게 할 수 있다.  

##### 2.2.3. 프로젝트 자원  
<br/>

쿠버네티스는 개발자가 특정 격리 환경에 적합한 애플리케이션을 실행할 수 있는 셀프 서비스 플랫폼이다. 그러나 공유 멀티테넌트 플랫폼에서의 작업은 특정 경계와 일부 사용자가 플랫폼의 모든 자원을 소비하지 못하게 하는 제어 장치가 있어야 한다. 이런 도구 중 하나가 리소스쿼터(ResourceQuota)로, 네임스페이스 안에서 집계된 자원 소비스를 제한하기 위한 제약 조건을 제공한다. 리소스쿼터를 사용하면 클러스터 관리자가 컴퓨팅 자원(CPU나 메모리 등) 전체 사용량과 스토리지 전체 사용량을 제한할 수 있다. 또한 네임스페이스 안에서 생성된 컨피그맵, 시크릿, 파드, 서비스 같은 객체의 총 개수를 제한할 수 있다.  

또 다른 유용한 도구인 리미트레인지(LimitRange)는 각 자원 종류마다 최대 자원량을 설정할 수 있다. 아울러, 여러 자원 종류에 대한 최소 및 최대 허용량과 이 자원에 대한 기본 값도 지정할 수 있다. 또한 오버커밋 레벨(overcommit level)로 알려진 requests와 limits 사이의 비율을 제어할 수 있다.  

리미트레인지는 컨테이너 자원 프로파일을 제어하기에 유용해서 어떤 컨테이너도 클러스터 노드가 제공하는 것보다 큰 자원을 요청하지 못한다. 또한 클러스터 사용자가 노드에 또 다른 컨테이너가 할당될 수 없을 정도의 매우 큰 자원을 사용하는 컨테이너를 생성하는 것을 방지할 수 있다. requests(limits가 아니라)는 스케줄러가 배치할 때 사용하는 컨테이너 기본 특성인 것을 고려하면 LimitRequestRatio로 컨테이너의 요청과 제한 사이의 차이 값을 제어할 수 있다. requests와 limits 차이가 크면, 노드에 오버커밋할 가능성이 크고, 많은 컨테이너가 처음 요청 값보다 더 많은 자원을 동시에 필요로 할 경우 애플리케이션 성능이 낮아질 수 있다.  

##### 2.2.4. 용량 계획  
<br/>

컨테이너가 각기 다른 환경에서 다양한 인스턴스 수에 따라 자원 파일이 서로 다를 수 있다는 점을 고려해보면, 다목적 환경에 대한 용량 계호기은 그리 간단하지 않다. 예를 들어 운영 클러스터가 아닌 환경에서 하드웨어를 최적으로 활용하려면, 최선적(Best-Effort)과 확장 가능(Busstable) 컨테이너를 주로 사용할 수 있다. 이런 동적인 환경에서는 많은 컨테이너가 동시에 시작되고 멈출 수 있으며, 자원 부족으로 플랫폼이 컨테이너를 죽인다고 하더라도 그다지 치명적이지 않다. 운영 환경이 좀 더 안정적이고 예측 가능하길 원한다면, 주로 보장(Guranteed) 컨테이너를 사용하고 약간의 확장 가능 컨테이너를 이용하면 된다. 만약 컨테이너가 죽는다면 그것은 클러스터 용량을 확장하라는 신호일 확률이 높다.  

물론 실제 상황에서는 많은 서비스를 관리해야 하는데 그중 일부는 없어질 서비스도 있고, 일부는 여전히 설계와 개발단계 중인 서비스이기 때문에, 다들 쿠버네티스 같은 플랫폼을 사용하려고 한다. 대상이 계속 바뀐다 하더라도 앞서 설명한 것과 유사한 접근 방식에 근거해, 각 환경별 모든 서비스에 대한 총 필요 자원량을 계산할 수 있다.  

또한, 환경이 달라지면 컨테이너 수도 달라지므로, 오토스케일링(autoscaling), 빌드 잡(build job), 인프라스트럭처 컨테이너 등을 위한 여분을 남겨둬야 할 수도 있다. 이런 정보와 인프라스트럭처 제공회사에 근거해, 필요 자원을 제공하는 가장 비용 효율적인 컴퓨팅 인스턴스를 선택할 수 있다.  

#### 2.3. 정리  
<br/>

컨테이너는 프로세스 격리뿐만 아니라 패키지 포맷으로서도 유용하다. 자원 프로파일이 식별되면, 컨테이너는 성공적인 용량 계획을 위한 구성 요소가 된다. 초기 테스트를 수행해 각 컨테이너에 필요한 자원을 찾고 이를 향후 용량 계획 및 용량 예측의 기본 정보로 사용하라.  

그러나 그보다 더 중요한 점이 있다. 자원 프로파일은 스케줄링하고 관리하는 결정을 돕기 위해 애플리케이션이 쿠버네티스와 통신하는 방법이라는 사실이다. 애플리케이션이 requests나 limits를 제공하지 않으면 쿠버네티스가 할 수 있는 것은 클러스터가 가득 찰 때 그 컨테이너를 불투명 상자로 처리하는 것뿐이다. 따라서 모든 애플리케이션에서는 필수로 이와 같은 자원 선언을 염두에 두고 제공해야 한다.  

#### 2.4. 참고 자료  
<br/>

+ 예측 요구사항 예제(http://bit.ly/2CrT8FJ)
+ 컨피그맵 사용(http://kubernetes.io/docs/user-guide/configmap/)
+ 리소스쿼터(http://kubernetes.io/docs/admin/resourcequota/)
+ 쿠버네티스 베스트 프렉티스: 자원 요청과 제한(http://bit.ly/2ueNUc0)
+ 파드 CPU와 메모리 제한 설정(http://kubernetes.io/docs/admin/limit range/)
+ 자원 부족을 다루는 설정(http://bit.ly/2TKEYKz)
+ 파드 우선순위와 선점(http://bit.ly/2OdBctU6)
+ 쿠버네티스에서 서비스 자원 품질(http://bit.ly/2HGimUq)  

### 3. 선언적 배포  
<br/>

#### 3.1. 문제  
<br/>

셀프 서비스 방식으로 네임스페이스로 격리된 환경을 제공할 수 있으며, 스케줄러를 통해 사용자 개입을 최소화하면서 서비스를 배포할 수 있다. 그러나 마이크로서비스 수가 증가하면 새로운 버전을 지속적으로 업데이트하고 교체하는 것도 부담이 커지게 된다.  

다음 버전으로 서비스 업그레이드는 새로운 버전의 파드를 시작하기, 이전 버전의 파드를 안전하게 중지하기, 새로운 파드가 성공적으로 시작되었는지 대기 및 확인하기, 실패할 경우 이전 버전으로 롤백하기 등의 동작을 포함한다. 이러한 동작은 일부 다운타음은 허용하지만 동시에 여러 서비스 버전을 실행하지 않는 경우, 또는 다운타임은 없지만 업데이트가 진행되는 동안 두 버전의 서비스가 함께 실행되기 때문에 자원 사용이 증가하는 경우, 둘 중 하나의 방식으로 수행된다. 이러한 단계를 수동으로 수행하다 보면 작업자에 의한 오류가 발생할 수 있고, 적합한 스크립트를 만드는 것도 많은 노력이 필요하게 되어, 두 방법 모두 릴리스 프로세스에 병목 현상을 야기한다.  

#### 3.2. 해결책  
<br/>

다행스럽게도 쿠버네티스는 이런 동작들까지도 자동화했다. 디플로이먼트(Deployment) 개념을 이용하면, 애플리케이션 업데이트 방법, 개별 전략 활용, 다양한 업데이트 프로세스 조정 등을 기술(describe)할 수 있다. 팀과 프로젝트에 따라 몇 분에서 몇 달이 걸릴 수 있는 릴리스 주기별로 모든 마이크로서비스 인스턴스에 대해 다수의 디플로이먼트를 고려하고 있다면, 쿠버네티스 자동화로 이런 노고를 절검할 수 있다.  

스케줄러가 효과적으로 역할을 수행하려면 호스트 시스템에 대한 충분한 리소스, 적절한 배치 정책, 적절하게 정의된 자원 프로파일을 포함하는 컨테이너가 필요하다. 마찬가지로, 디플로이머트가 정확히 수행되기 위해서는 컨테이너가 훌륭한 클라우드 네이티브 일원이 되어야 한다. 디플로이먼트의 핵심은 예측 범위 안에서 파드 세트를 시작하고 중지하는 기능이다. 이것이 예상대로 작동하려면, 컨테이너가 (SIGTERM 같은) 수명주기 이벤트를 잘 수신해야 하고, 파드가 성공적으로 실행되었는지를 알려주는 정상상태 확인 종단점을 제공해야 한다.  

컨테이너가 이 두 영역을 정확하게 커버한다면, 플랫폼은 이전 컨테이너를 깨끗하게 종료할 수 있고, 업데이트된 인스턴스를 시작해 이전 인스턴스를 교체할 수 있다. 그런 다음 업데이트 프로세스의 나머지 부분을 선언적 방법으로 정의해서, 미리 정의된 단계와 예상된 결과를 하나의 원자적 작업으로 실행할 수 있다. 컨테이너 업데이트 동작에 대한 옵션을 한번 살펴보자.  

##### 3.2.1. kubectl 을 사용한 명령형 롤링 업데이트는 더 이상 지원되지 않음  
<br/>

쿠버네티스는 처음부터 롤링(roling) 업데이트를 지원해 왔다. 첫 번째 구현은 본질적으로 명령형(imperative)이었다. 즉 kubectl 클라이언트가 각 업데이트 단계에 대한 수행 작업을 서버에 알려준다.  

kubectl rolling-update 명령은 여전히 존재하지만, 이러한 명령형 접근은 다음과 같은 단점으로 인해 더 이상 지원되지 않는다.  

+ kubectl rolling-update는 요청 종료 상태를 보여주지 않으며, 그 대신 시스템을 요청 상태(desired state)로 만드는 명령을 실행한다.

+ 컨테이너와 레플리케이션컨트롤러(ReplicationController)를 교체하기 위한 전체 오케스트레이션 로직이 kubectl에 의해 수행된다. 업데이트 프로세스가 진행되는 동안, kubectl은 그 내부에서 API 서버와 상호작용하고 모니터링하여 고유한 서버측 책임을 클라이언트로 이동시킨다.

+ 시스템을 요청 상태로 만들려면 하나 이상의 명령이 필요할 수도 있다. 이러한 명령은 다른 환경에서도 자동화되고 반복 가능해야 한다.

+ 시간이 지나면 다른 누군가가 당신의 변경사항을 바꿀 수 있다.

+ 서비스가 계속 업그레이드되는 동안, 업데이트 프로세스를 문서화하고 최신으로 유지해야 한다.

+ 배포 내용을 확인하는 유일한 방법은 시스템 상태를 체크하는 것이다. 간혹 현재 시스템 상태는 요청 상태가 아닐 수도 있으므로, 이 경우에는 배포 문서를 이용해 현재 시스템 상태와 연관하여 체크해야 한다/ 이런 점을 보완하기 위해 쿠버네티스 백엔드에 의해 완전히 관리되는 선언적 업데이트(declarative update)를 지원하는 디플로이먼트 자원 객체가 도입되었다.  

##### 3.2.2. 롤링 배포  
<br/>

쿠버네티스에서 애플리케이션을 업데이트하는 선언적인 방법은 디플로이먼트 개념을 활용하는 것이다. 내부적으로 디플로이먼트 추상화를 통해 RollingUpdate(기본값)와 Recreate 같은 전략을 사용해 업데이트 프로세스 동작을 구체화할 수 있다.  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: random-generator
spec:
  replicas: 3 # 3개의 레플리카(replicas) 선언. 롤링 업데이트를 위해서는 2개 이상의 레플리카가 필요하다.
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1 # 업데이트 동안 일시적으로 지정된 레플리카 수에 더해, 실행될 수 있는 파드의 수
      maxUnavailable: 1 # 업데이트 동안 사용 불가능하게 될 수 있는 파드의 수
    selector:
      matchLabels:
        app: random-generator
    template:
      metadata:
        labels:
          app: random-generator
      spec:
        containers:
        - image: k8spatterns/random-generator:1.0
          name: random-generator
          readinessProbe: # 레디니스(Readiness) 점검은 무중단(zero downtime)을 제공하기 위한 롤링 배포에 매우 중요함을 잊어서는 안 된다.
            exec:
              command: [ "stat", "/random-generator-ready" ]
```

RollingUpdate 전략 행동은 업데이트 프로세스 동안 중단이 없음을 보장한다. 내부적으로 이 디플로이먼트 구현은 새로운 레플리카를 생성하고 새로운 컨테이너로 이전 컨테이너를 고려함으로써 비슷한 동작을 수행한다. 여기에서 한 가지 향상된 기능은 새로운 컨테이너 생성 비율을 제어할 수 있는 디플로이먼트를 사용한다는 것이다. 디플로이먼트 객체를 사용하면 maxSurge 및 maxUnavailable 필드를 통해 초과 파드와 사용 가능한 파드 범위를 제어할 수 있다.  

선언적 업데이트를 작동시키기 위한 옵션은 다음과 같다.  

+ kubectl replace로 새로운 버전의 디플로이먼트로 전체 디플로이먼트를 교체한다.
+ 디플로이먼트를 패치(kubectl patch)나 대화식으로 편집(kubectl edit)해서 새로운 버전의 새로운 컨테이너 이미지를 넣는다.
+ kubectl set image 명령으로 디플로이먼트에 새로운 이미지를 넣는다.  

앞서 언급한 명령형 방법으로 서비스를 배포하는 단점을 해결하는 것 외에도 디플로이먼트는 다음과 같은 이점이 있다.  

+ 디플로이먼트는 그 상태가 내부적으로 쿠버네티스에 의해 전적으로 관리되는 쿠버네티스 자원 객체다. 전체 업데이트 프로스세는 클라이언트와 상호작용 없이 서버 측에서 실행된다.
+ 디플로이먼트는 선언적 특성을 통해, 배포에 필요한 단계보다는 배포된 상태가 어떻게 보여야 하는지를 알 수 있다.
+ 디플로이먼트 정의는 운영 환경에 배포되기 전에 다양한 환경에서 테스트된 실행 가능한 객체다.
+ 업데이트 프로세스는 모두 기록되며, 일시 중지 및 계속을 위한 옵션, 이전 버전으로 롤백을 위한 옵션으로 버전이 지정된다.  

##### 3.2.3. 고정 배포  
<br/>

RollingUpdate 전략은 업데이트 프로세스 동안 무중단을 보장하는 유용한 방법이다. 그러나 이런 접근 방법의 부작용은 업데이트 프로세스 동안 두 버전의 컨테이너가 동시에 실행된다는 것이다. 이로 인해 서비스 컨슈머(consumer)에 문제가 발생할 수 있는데, 특히 업데이트 프로세스가 이전 버전과 호환되지 않는 변경 사항을 서비스 API에 도입하고, 클라이언트가 이를 처리할 수 없을 때 발생할 수 있다. 이런 시나리오를 해결하기 위해, Recreate 전략이 있다.  

Recreate 전략은 maxUnavailable 개수를 레플리카(replica)와 똑같은 개수로 설정하는 효과가 있다. 즉 우선적으로 현재 버전의 모든 컨테이너를 죽이고, 이전 버전의 컨테이너가 축출될 때 모든 신규 컨테이너를 동시에 시작함을 의미한다. 이 같은 순차적 작업으로 인해, 이전 버전의 모든 컨테이너가 중지된 상태에서는 다운타임이 발생하며, 들어오는 요청을 처리할 신규 컨테이너는 하나도 없다. 긍정적인 측면으로 보자면, 두 버전의 컨테이너는 동시에 실행되지 않고, 서비스 컨슈머가 한 번에 오직 하나의 버전만 처리 가능하도록 단순화된다.  

##### 3.2.4. 블루-그린 릴리스  
<br/>

블루-그린 배포(Blue-Green deployment)란 다운타임을 최소화하고 위험성을 줄여서 운영 환경에 소프트웨어를 배포하기 위해 사용되는 릴리스 전략이다. 쿠버네티스의 디플로이먼트 추상화는 쿠버네티스가 어떻게 불변 컨테이너를 한 버전에서 다른 버전으로 변환시키는지를 정의한 기본 개념이다. 기타 쿠버네티스 기본 요소와 함께 디플로이먼트 기본 요소를 빌딩 블록처럼 활용해서, 블루-그린 배포의 고급 릴리스 전략을 구현할 수 있다.  

서비스 메시(Service Mesh)나 케이네이티브(Knative) 같은 확장 서비스를 사용하지 않는다면 블루-그린 배포는 수동으로 수행되어야 한다. 기술적으로는 아직 어떤 요청도 처리하지 않은 최신 버전의 컨테이너(이를 그린(green)이라 칭함)로 두 번째 디플로이먼트를 생성해 실행한다. 이 단계에서는, 원래 디플로이먼트의 이전 파드 레플리카(이를 블루(blue)라고 칭함)가 여전히 실행되며 실제 요청을 처리한다.  

일단 새로운 버전의 파드가 정상적이고 실제 요청을 처리할 준비가 되었다는 확신이 들면, 이전 파드 레플리카에서 새로운 레플리카로 트래픽을 전환한다. 쿠버네티스에서 이런 동작은 서비스 셀렉터(Service selector)를 새로운 컨테이너(그린으로 태그됨)로 일치시키는 업데이트에 의해 수행된다. 일단 그린 컨테이너가 모든 트래픽을 처리하면, 블루 컨테이너는 삭제될 수 있고 향후 또다른 블루-그린 배포를 위해 블루 컨테이너 자원이 해제된다.  

블루-그린 접근 방식의 이점은 요청을 처리하는 애플리케이션 버전이 하나뿐이므로 서비스 컨슈머에 의해 동시에 여러 버전이 처리되는 복잡성을 줄여준다는 점이다. 반면, 단점은 블루와 그린 컨테이너가 모두 실행되는 동안 애플리케이션 용량은 2배로 필요하다는 점이다. 또한, 전환하는 동안 장기 실행(long-running) 프로세스 및 데이터베이스 상태 변화로 인해 심각한 문제가 발생할 수 있다.  

##### 3.2.5. 카나리아 릴리스  
<br/>

카나리아 릴리스(Canary release)는 이전 인스턴스의 작은 하위집합만 새로운 인스턴스로 교체함으로써 새로운 버전의 애플리케이션을 운영에 유연하게 배포하는 방식이다. 이 기술은 일부 컨슈머만 업데이트된 버전을 사용하게 함으로써 운영에 새 버전을 도입할 때의 위험을 줄여준다. 새 버전의 서비스와 소수의 사용자 시험군에게 적용된 방식이 만족스럽다면, 비로소 이전 인스턴스를 모두 새로운 버전으로 교체한다.  

쿠버네티스에서 카나리아 릴리스 기술은 카나리아 인스턴스로 쓰이는 작은 레플리카 수를 갖는 신규 컨테이너 버전(주로 디플로이먼트를 사용)에 대해 새로운 레플리카세트를 생성함으로써 구현할 수 있다. 이 단계에서 서비스는 컨슈머 중 일부를 업데이트된 파드 인스턴스로 바로 연결해야 한다. 일단 신규 레플리카세트와 함께 모든 것이 예상대로 작동한다는 확신이 들면, 신규 레플리카세트를 늘리고 이전 레플리카세트를 0개로 줄인다. 이런 방법으로 제어 가능하면서 실제 사용자에 의해 테스트된 서비스를 점점 증가시켜 실행한다.  

#### 3.3. 참고 자료  
<br/>

+ 선언적 배포 예제(http://bit.ly/2Fc6d6J)
+ 롤링 업데이트(http://bit.ly/2r06Ich)
+ 디플로이먼트(http://bit.ly/2q7vR7Y)
+ 디플로이먼트를 사용한 무상태 애플리케이션 실행(http://bit.ly/2XZZhIL)
+ 블루-그린 배포(http://bit.ly/1Gph4FZ)
+ 카나리아 릴리스(https://martinfowler.com/build/CanaryRelease.html)
+ 오픈시프트(OpenShift)를 사용한 데브옵스(https://red.ht/2W7fdAQ)  

### 4. 정상상태 점검  
<br/>

정상상태 점검(Health Probe) 패턴은 애플리케이션이 쿠버네티스와 정상상태 여부를 통신하는 방법에 관한 패턴이다. 완전 자동화에 도달하기 위해서는, 쿠버네티스가 클라우드 네이티브 애플리케이션의 실행 여부와 요청 처리 준비 상태 여부를 감지할 수 있어야 하기 때문에, 클라우드 네이티브 애플리케이션은 애플리케이션 상태를 유추 가능하도록 잘 관측할 수 있어야 한다. 이런 관측은 파드의 수명주기 관리 및 트래픽이 애플리케이션으로 라우팅되는 방식에 영향을 준다.  

#### 4.1. 문제  
<br/>

쿠버네티스는 컨테이너 프로세스 상태를 주기적으로 확인하고 문제가 감지되면 컨테이너를 다시 시작한다. 그러나 실제에서, 프로세스 상태 확인은 애플리케이션 정상상태를 결정하기에 충분하지 않다. 대부분의 경우, 애플리케이션에 행(hang)이 걸리더라도 프로세스는 여전히 실행 중이다. 예를 들어 자바 애플리케이션이 OutOfMemoryError를 던지더라도 여전히 JVM 프로세스는 실행 중일 수 있다. 또는, 애플리케이션이 무한 루프 또는 교착 상태, 어떤 스래싱(threshing, 캐시, 힙, 프로세스)으로 인해 동작을 안할 수도 있다, 이런 상황을 감지하기 위해 쿠버네티스는 애플리케이션의 정상상태(health)를 체크할 수 있는 신뢰할 만한 방법을 필요로 한다. 즉 애플리케이션이 내부적으로 어떻게 작동하는지를 이해하려는 것이 아니라, 애플리케이션이 예상대로 작동 중이며 컨슈머에게 서비스를 제공할 수 있는지 여부를 확인할 방법이 필요한 것이다.  

#### 4.2. 해결책  
<br/>

소프트웨어 업계에서 버그 없는 코드를 작성하는 것은 불가능하다. 게다가 분산 애플리케이션으로 작업할 때는 장애(failure)가 일어날 확률이 훨씬 더 높아진다. 결국, 장애에 대한 세간의 관심은 장애를 피하는 것에서 오류를 감지하고 복구하는 쪽으로 이동했다. 장애에 대한 정의가 천차만별이기 때문에, 장애 감지는 모든 애플리케이션에 동등하게 적용할 만큼 간단한 작업이 아니다. 또한, 수많은 유형의 장애별로 각기 그에 알맞은 장애 조치가 필요하다. 일시적인 장애는 충분한 시간이 주어지면 자체 복구될 수 있으며, 애플리케이션 재시작이 필요한 장애도 있다. 쿠버네티스가 장애를 감지하고 해결하는 데 쓰이는 여러 가지 확인 단계를 살펴보자.  

##### 4.2.1. 프로세스 정상상태 확인  
<br/>

프로세스 정상상태 확인(process health check)은 큐블릿(Kubelet)이 컨테이너 프로세스에 대해 지속적으로 수행하는 가장 간단한 정상상태 확인이다. 컨테이너 프로세스가 실행 중이 아니라면, 점검(probing)은 다시 시작된다. 따라서 또 다른 정상상태 확인을 하지 않더라도, 애플리케이션은 이런 일반적인 확인 단계로 좀 더 견고해진다. 애플리케이션이 모든 종류의 장애를 감지할 수 있고 자기 스스로 종료될 수 있다면, 프로세스 장상상태 확인만으로도 충분하다. 그러나 대부분의 경우 이것만으로는 충분치 않아, 다른 유형의 정상상태 확인이 필요하다.  

##### 2.4.2. 라이브니스 점검  
<br/>

애플리케이션이 교착 상태에 빠지면, 프로세스 정상상태 확인(health check)으로는 여전히 정상으로 간주된다. 이런 종류의 문제와 애플리케이션 비즈니스 로직에 따른 또 다른 형태의 장애를 감지하기 위해 쿠버네티스에는 라이브니스 점검(Liveness probe)이 있다. 라이브니스(생존상태) 점검이란 큐블릿 에이전트가 정기적으로 검사를 수행해 컨테이너가 여전히 정상상태인지 확인하는 과정이다. 일부 장애는 애플리케이션 워치독(watchdog)이 장애 보고를 못할 수 있으므로, 애플리케이션 자체 내부보다는 외부에서 장상상태 확인을 수행하는 것이 중요하다. 장애 조치와 관련해서는, 장애가 감지되는 경우 컨테이너가 재시작되기 때문에 이 라이브니스 점검은 프로세스 정상상태 확인과 유사하다. 그러나 애플리케이션 정상상태 확인 시에 어떤 방법을 쓰는지에 대해서는 다음과 같이 좀 더 융통성을 부여한다.  

+ HTTP 점검은 컨테이너 IP 주소에 대해 HTTP GET 요청을 수행하며 200과 399 사이의 성공적인 HTTP 응답 코드를 기대한다.
+ TCP 소켓 점검은 성공적인 TCP 연결을 가정한다.
+ Exec 점검은 컨테이너 커널 네임스페이스에서 임의의 명령을 실행하고 성공적인 종료 코드(0)를 기대한다.  

HTTP 기반의 라이브니스 점검은 다음 예제에서 볼 수 있다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-liveness-check
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    env:
    - name: DELAY_STARTUP
      value: "20"
    ports:
    - containerPort: 8080
    protocol: TCP
    livenessProbe:
      httpGet: # 정상상태 확인 종단점으로 HTTP 점검
        path: /actuator/health
        port: 8080
      initialDelaySeconds: 30 # 애플리케이션에 준비 시간을 주기 위해 첫 번째 라이브니스 점검을 스행하기 전 30초를 기다림
```

애플리케이션의 특성에 ㄸ라 가장 적합한 방법을 선택할 수 있다. 애플리케이션이 정상상태인지 아닌지 여부를 판별하는 것은 개발자의 구현에 달려 있긴 하지만, 정상상태 확인을 통과하지 못하면 컨테이너가 다시 시작된다는 사실을 기억해야 한다. 컨테이너를 재시작하는 것이 소용없는 경우라면, 근본적인 문제 해결 없이 쿠버네티스가 컨테이너를 재시작하므로 정상상태 확인 실패는 아무런 도움이 되지 않는다.  

##### 2.4.3. 레디니스 점검  
<br/>

라이브니스 점검은 비정상적인 컨테이너를 죽이고 새로운 컨테이너로 대체함으로써 애플리케이션의 정상상태를 유지하는 데 유용하다. 그러나 때로는 컨테이너가 정상상태가 아닐 수 있어, 재시작하는 것도 도움이 안될 때가 있을 수 있다. 가장 일반적인 예는 컨테이너가 여전히 시작 중이고 아직 요청을 처리할 준비가 안 됐을 경우다. 혹은, 컨테이너가 과부하가 걸릴 수도, 지연 시간이 증가할 수도 있으며, 또 다른 부하(load)로부터 잠시 보호받길 원할 수도 있다.  

이런 상황을 위해 쿠베네티스에는 레디니스 점검(readiness probe)이 있다. 레디니스(준비상태) 점검 수행 방법은 라이브니스 점검(HTPP, TCP, Exec)과 동일하지만, 장애 조치는 다르다. 컨테이너를 재시작하지 않으며, 레디니스 점검에 실패하면 컨테이너는 서비스(Service) 엔드포인트(Endpoint)에서 제거되고 새로운 트래픽을 수신할 수 없다. 레디니스 점검은 컨테이너가 서비스 요청을 받기 전에 준비할 수 있는 시간을 갖도록 컨테이너가 준비되는 시점에 신호를 보낸다. 또한 레디니스 점검은 라이브니스 점검처럼 주기적으로 수행되므로 이후 단계에서 트래픽으로부터 서비스를 보호하는 데 유용하다. 다음 예제는 서비스 준비를 갖춘 시점에 애플리케이션이 생성한 파일의 존재 여부를 점검하는 방식으로 구현한 레디니스 점검 코드다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pod-with-readiness-check
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    readinessProbe:
      exec: # 애플리케이션이 서비스 요청을 처리할 준비가 되었음을 나타내는 파일이 존재하는지 확인한다. stat은 파일이 존재하지 않으면 에러를 반환하고 레디니스 점검은 실패한다.
        command: [ "stat", "/var/run/random-generator-ready" ]
```

다시 정리하면, 언제 애플리케이션이 작업을 수행할 준비가 되었는지를 판별하고, 언제 애플리케이션을 그대로 둬야 할지를 결정하는 것은 정상상태 확인 코드를 어떻게 구현하는지에 따라 다르다. 프로세스 정상상태 확인과 라이브니스 점검은 컨테이너를 재시작해 장애를 복구하기 위한 것이지만, 레디니스 점검은 애플리케이션에 준비할 시간을 주고 애플리케이션이 스스로 복구되기를 기다린다. 시그텀(SIGTERM) 신호를 수신한 후에 쿠버네티스는 레디니스 점검 통과 여부와 상관없이 컨테이너가 새로운 요청(예를 들어 컨테이너가 종료될 때)을 받지 못하게 만든다는 사실을 명심하자.  

대부분의 경우, 라이브니스와 레디니스 점검은 동일한 체크를 수행한다. 하지만 레디니스 점검이 있으면 컨테이너에게는 시작할 수 있는 시간이 주어진다. 오직 레디니스 점검을 통과해야만 디플로이먼트가 성공한 것으로 간주된다. 결국, 예를 들면, 이전 버전의 파드는 롤링 업데이트의 일부분으로서 종료될 수 있다.  

라이브니스와 레디니스 점검은 클라우드 네이티브 애플리케이션 자동화의 기본 빌딩 블록이다. 스프링 엑추에이터(Spring actuator), 와일드플라이 스웜(Wild-Fly Swarm) 정상상태 확인, 카라프(Karaf) 정상상태 확인, 자바용 마이크로프로파일 스펙(MicroProfile spec) 등의 애플리케이션 프레임워크는 정상상태 점검 제공을 위한 구현 방법을 제공한다.  

#### 2.5. 정리  
<br/>

완전 자동화를 위해, 클라우드 네이티브 애플리케이션은 관리 플랫폼에게 애플리케이션 정상상태를 읽고, 해석하고, 필요한 경우 조치 활동에 대한 수단을 제공함으로써 관찰 가능해야 한다. 정상상태 확인은 배포, 자가 치유, 스케일 등과 같은 자동화 활동 안에서 기본 역할을 수행한다. 그러나 애플리케이션이 정상상태에 대해 더 많은 가시성을 제공할 수 있는 방법은 더 있다.  

이를 위한 명백하고 오래된 방법은 로깅(logging)이다. 컨테이너가 중요한 이벤트를 시스템 밖으로 보내고, 시스템 에러를 로그로 남기고, 추후 분석을 위해 이 로그들을 중앙에서 수집하는 것이 좋다. 로그는 일반적으로 자동화 활동에 사용되는 것이 아니라 알람을 발생시키고 추가 조사를 하는 데 쓰인다. 로그가 유용한 측면으로는 장애에 대한 사후 분석과 눈에 띄지 않는 오류에 대한 탐지를 꼽을 수 있다.  

표준 스트림에 로그를 남기는 것 외에도, 컨테이너가 종료되는 이유를 /dev/termination-log에 로그로 남기는 것이 좋다. 이 위치는 컨테이너가 영구적으로 사라지기 전에 마지막 상태를 남길 수 있는 장소다.  

컨테이너는 블랙박스처럼 취급되어 애플리케이션을 패키징하고 실행하는 통합된 방법을 제공한다. 그러나 클라우드 네이티브 일원이 되어야 할 컨테이너는 컨테이너 정상상태를 관찰하고 그에 따라 행동하는 런타임 환경에 대한 API를 반드시 제공해야 한다. 이 기능은 컨테이너 업데이트 및 수명주기를 통합된 방식으로 자동화하기 위한 기본 전제조건으로, 시스템의 회복성과 사용자 경험을 향상시킨다. 실제로, 컨테이너화된 애플리케이션은 최소한 정상상태 확인(라이브니스와 레디니스)을 위한 API를 제공해야 한다.  

잘 동작하는 애플리케이션알 하더라도 관리 플랫폼이 오픈트레이싱(OpenTracing)이나 프로메테우스(Prometheus)같은 트레이싱 및 메트릭 수집 라이브러리와 통합하여 컨테이너화된 애플리케이션 상태를 관찰할 수 있도록 또 다른 수단을 제공해야 한다. 애플리케이션을 블랙박스처럼 취급하되, 플랫폼이 애플리케이션을 최상의 방법으로 관찰하고 관리할 수 있도록 필요한 모든 API를 구현해야 한다.  

#### 2.6. 정리  
<br/>

+ 정상상태 점검 예제(http://bit.ly/2Y6wCLG)
+ 라이브니스 레디니스 점검 설정(http://bit.ly/2r096A3)
+ 레디니스와 라이브니스 점검을 사용한 정상상태 확인 설정(http://bit.ly/2HJkoDf)
+ 자원 서비스 품질(http://bit.ly/2HGimUq)
+ 노드제이에스(Node.js) 및 쿠버네티스를 사용한 원활한 종료(http://bit.ly/2udUfo0)
+ 쿠버네티스에서의 고급 정상상태 확인 패턴(https://ahmet.im/blog/advanced-kubernetes-health-checks/)  

### 5. 수명주기 관리  
<br/>

클라우드 네이티브 플랫폼으로 관리되는 컨테이너화된 애플리케이션은 자기 자신의 수명주기를 제어할 수 없으며, 좋은 클라우드 네이티브 일원으로서 자격을 갖추려면 관리 플랫폼에 의해 생성된 이벤트를 받아서 그에 맞춰 수명주기를 조절해야 한다.  

#### 5.1. 문제  
<br/>

컨테이너 상태를 모니터링하는 것 외에도 플랫폼은 때때로 애플리케이션에 명령어를 보내고 애플리케이션이 그 명령어에 반응할 것을 기대한다. 정책 및 외부 요인데 따라 클라우드 네이티브 플랫폼은 자신이 관리하는 애플리케이션에 대해 언제든지 시작이나 중지를 결정할 수 있다. 어떤 이벤트가 중요하고 그 이벤트에 어떻게 반응해야 하는지를 결정하는 것은 컨테이너화된 애플리케이션의 몫이다. 하지만 실제로 이 일은 플랫폼이 애플리케이션에 명령어를 보내고 통신하는 데 사용하는 API가 수행한다. 또한, 애플리케이션은 수명주기 관리로부터 혜택을 얻거나, 이런 서비스가 필요하지 않은 경우엔 수명주기 관리를 무시하기도 한다.  

#### 5.2. 해결책  
<br/>

프로세스 상태 확인만으로는 애플리케이션의 정상상태를 나타내는 데 충분하지 않음을 이미 살펴봤다. 그렇기 때문에 컨테이너 정상상태를 모니터링하려면 다른 API가 있어야 한다. 이와 마찬가지로, 프로세스를 실행하고 중지하는 데에 프로세스 모델만 사용하는 것은 충분치 않다. 현실 세계에서의 애플리케이션은 좀 더 세분화된 상호작용과 수명주기 관리 기능이 필요하다. 실행을 준비하는 데 도움이 필요한 애플리케이션도 있고, 원만하고 깔끔한 종료 절차가 필요한 애플리케이션도 있다. 이와 같은 다양한 사용 사례에서, 일부 이벤트는 플랫폼에 의해 발생되며 원하는 경우 컨테이너가 이벤트를 수신하고 처리할 수 있다.  

애플리케이션의 배포 단위는 파드다. 모두가 아는 바와 같이, 파드는 하나 혹은 그 이상의 컨테이너로 구성된다. 파드 레벨에는 또 다른 구조체로, 컨테이너 수명주기 관리에 도움을 주는 초기화 컨테이너가 있다. 그리고 제안 단계에 머물러 있는 지연 컨테이너(defer-container)도 있다.  

##### 5.2.1. 시그텀 신호  
<br/>

컨테이너가 속한 파드가 종료 중이기 때문이든 단순히 라이브니스 점검이 실패해 컨테이너가 재시작되든 간에, 쿠버네티스가 컨테이너를 멈추기로 결정할 때마다 컨테이너는 시그텀(SIGTERM) 신호를 수신한다. 시그텀이란 쿠버네티스가 갑작스러운 시그킬(SIGKILL) 신호를 보내기 전에 컨테이너가 깨끗하게 종료될 수 있도록 컨테이너를 슬쩍 찔러 보는 것이다. 일단 시그텀 신호를 받으면 애플리케이션은 가능한 한 빨리 멈춰야 한다. 일부 애플리케이션의 경우, 시그텀 신호를 받으면 빨리 종료될 수 있다. 한편, 일부 애플리케이션에서는 진행 중인 요청을 완료하고, 연결을 해제한 다음, 임시 파일을 깨끗이 지워야 할 수도 있는데 이는 좀 오래 걸릴 수도 있다. 어떤 경우는 컨테이너를 깨끗하게 종료하기 위한 좋은 순간은 시그텀 신호에 반응하는 것이다.  

##### 5.2.2. 시그킬 신호  
<br/>

시그텀 신호 수신후에도 컨테이너 프로세스가 종료되지 않는다면 다음에 따라오는 시그킬(SIGKILL) 신호에 의해 강제로 종료된다. 쿠버네티스는 시그킬 신호를 즉시 보내지는 않고, 시그텀 신호가 발생한 후 기본적으로 유예 시간인 30초를 기다린다. 이 유예 시간은 .spec.terminationGracePeriodSeconds 필드를 사용해 개별 파드마다 정의할 수 있지만, 쿠버네티스 명령이 실행되는 동안 재정의될 수도 있으므로 보장할 순 없다. 여기서 목적은 프로세스를 빠르게 시작하고 멈출 수 있는 컨테이너화된 애플리케이션을 임시로 설계하고 구현하는 것이다.  

##### 5.2.3. 시작 후 훅  
<br/>

수명주기 관리를 위해 프로세스 신호만 사용한 것은 다소 제한적이다. 그렇기 때문에 쿠버네티스에서는 postStart와 preStop 같은 추가적인 수명주기 훅을 제공한다. postStart(시작 후) 훅이 포함된 파드 매니페스트는 다음 예제와 같다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: post-start-hook
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    lifecycle:
      postStart:
        exec:
          command: # postStart 명령은 여기서 30초를 기다린다. sleep은 긴 시간 실행되는 시작 코드가 있을 수 있으므로 이에 대한 시뮬레이션일 뿐이다. 또한 여기서는 병렬로 시작하는 주 애플리케이션과 동기화하기 위해 트리고 파일을 사용한다.
          - sh
          - -c
          - sleep 30 && echo "Wake up!" > /tmp/postStart_done
```

postStart 명령은 컨테이너가 생성된 후 주 컨테이너 프로세스와 비동기적으로 실행된다. 많은 애플리케이션 초기화와 준비 로직이 컨테이너 시작 단계의 한 부분으로 구현될 수 있는데도, postStart는 여전히 일부 사용 예에 활용되고 있다. postStart 동작은 블록킹 호출(blocking call)임 postStart 핸들러가 완료될 때까지는 컨테이너 상태가 대기 상태(Waiting)로 남아 있고 파드 상태는 보류 상태(Pending)를 유지한다. postStart의 특성은 주 컨테이너 프로세스의 초기화 시간을 벌기 위해 컨테이너의 시작 상태를 지연시키는 데 사용된다.  

postStart의 또 다른 용도는 파드가 어떤 전제조건을 충족하지 못했을 때 컨테이너가 시작되지 않게 하는 것이다. 예를 들어 postStart 훅이 0이 아닌 종료 코드를 리턴해 에러를 나타내면, 주 컨테이너 프로세스는 쿠버네티스에 의해 죽는다.  

postStart와 preStop 훅이 호출되는 메커니즘은 정상상태 점검 패턴과 유사하며 다음과 같은 핸들러 타입을 지원한다.  

+ exec: 컨테이너 안에서 직접적으로 명령어를 실행함
+ httpGet: 하나의 파드 컨테이너에 의해 공개된 포트로 HTTP GET 요청을 수행함  

postStart의 실행이 보장되지 않으므로 postStart 훅에서 중요한 로직을 실행할 때는 매우 주의해야 한다. 훅은 컨테이너 프로세스와 병렬로 실행되기 때문에 컨테이너가 시작되기 전에 훅이 실행될 수도 있다. 또한 훅은 최소 한 번 실행의 의미로 설계되었으므로, 구현 시에 중복 실행될 수 있음을 염두에 둬야 한다. 또 하나 명심해야 할 점은 플랫폼은 핸들러에 도달하지 못한 실패한 HTTP 요청에 대해선 재시도를 수행하지 않는다는 것이다.  

##### 5.2.4. 종료 전 훅  
<br/>

preStop(종료 전) 훅은 컨테이너가 종료되기 전에 컨테이너로 전송되는 블록킹 호출이다. 시그텀 신호와 동일한 의미를 지니며 컨테이너가 시그텀에 응답하는 것이 불가능할 때 컨테이너를 정상적으로 종료하기 위해 사용해야 한다. 다음 예제에서 preStop 동작은 시그텀 알림에 의해 컨테이너 런타임이 컨테이너 삭제를 호출하여 시그텀 알림을 트리거하기 전에 완료되어야 한다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: pre-stop-hook
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    lifecycle:
      preStop:
        httpGet: # 애플리케이션 내에서 실행중인 /shutdown 종단점을 호출한다.
          port: 8080
          path: /shutdown
```

preStop이 블로킹되거나, 계속 진행중이거나, 실패한 결과가 리턴되더라도, 컨테이너가 삭제되거나 프로세스가 종료되는 상황을 막을 수는 없다. preStop은 블록킹 호출이지만 일정 시간이 지나도 preStop 안끝나면 플랫폼은 컨테이너에 SIGTERM 신호를 호출한다. 그저 preStop은 정상적인 애플리케이션 종료를 위한 시그텀 신호의 편리한 대안일 뿐이다.  

##### 5.2.5. 그 밖의 수명주기 제어  
<br/>

일반 애플리케이션 컨테이너와는 달리 초기화 컨테이너는 순차적으로 실행되고, 완료될 때까지 실행되며, 파드 내의 애플리케이션 컨테이너가 시작되기 전에 실행된다. 이런 특성을 활용하여 파드 레벨 초기화 작업에 초기화 컨테이너를 사용할 수 있다. 수명주기 훅과 초기화 컨테이너는 각각 컨테이너 레벨과 파드 레벨에서 서로 다른 세분성(granularity)으로 작동하며, 서로 바꿔서 사용하거나 훅은 서로 보완해 사용할 수도 있다.  

<table>
  <thead>
    <tr>
      <td>측면</td>
      <td>수명주기 훅</td>
      <td>초기화 컨테이너</td>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>활성화 단계</td>
      <td>컨테이너 수명주기 단계</td>
      <td>파드 수명주기 단계</td>
    </tr>
    <tr>
      <td>시작 단계 동작</td>
      <td>postStart 명령어</td>
      <td>실행될 초기화 컨테이너 목록</td>
    </tr>
    <tr>
      <td>종료 단계 작업</td>
      <td>preStop 명령어</td>
      <td>상응하는 기능이 아직은 없음</td>
    </tr>
    <tr>
      <td>타이밍(Timing) 보장</td>
      <td>postStart 명령은 컨테이너의 ENTRYPOINT와 동시에 실행된다</td>
      <td>애플리케이션 컨테이너가 시작되기 전에 모든 초기화 컨테이너는 성공적으로 종료가 완료되어야 한다</td>
    </tr>
    <tr>
      <td>사용 사례</td>
      <td>컨테이너별로 특화된 중요하지 않은 시작/정리(cleanup) 종료를 실행</td>
      <td>컨테이너를 사용해 워크플로우 같은 순차적 작업을 수행. 작업 실행을 위해 컨테이너를 재사용</td>
    </tr>
  </tbody>
</table>
<br/><br/>

특정한 타이밍 보장을 요구하는 경우를 제외하고 어떤 메커니즘을 사용할지에 대한 엄격한 규칙은 없다. 수명주기 훅과 초기화 컨테이너를 모두 생략하고 컨테이너의 시작이나 종료 명령어의 일부분으로 특정 작업을 수행하기 위해 배시(bash) 스크립트를 사용할 수 있다. 가능하긴 하지만 이런 방법은 스크립트와 컨테이너가 밀접하게 연결되어 유지 관리가 힘들어진다.  

지금까지 설명한 대로, 일부 작업을 수행하는 데 쿠버네티스 수명주기 훅을 사용할 수 있다. 훅은 더 나아가, 초기화 컨테이너를 사용하여 개별 동작을 수행하는 컨테이너를 실행할 수 있다. 수명주기 훅, 초기화 컨테이너의 순으로 이 기능을 사용하는 데 더 많은 노력이 필요하지만, 동시에 더 강력한 보증을 제공하고 재사용을 가능하게 한다.  

쿠버네티스로 관리되는 이점이 있는 애플리케이션을 만들기 위해서는, 컨테이너와 파드 수명주기의 단계 및 활용 가능한 훅을 반드시 잘 이해해야 한다.  

#### 5.3. 정리  
<br/>

클라우드 네이티브 플랫폼이 제공하는 주요 이점 중 하나는 잠재적으로 신뢰할 수 없는 클라우드 인프라스트럭처 위에 애플리케이션을 신뢰성 있게 예측 가능한 범위 내에서 실행하고 확장할 수 있다는 것이다. 클라우드 네이티브 플랫폼은 플랫폼 위에서 실행되는 애플리케이션에 대해 계약과 보장된 기능을 제공한다. 클라우드 네이티브 플랫폼이 제공하는 모든 기능의 이점을 누리고자 이런 보장된 기능을 활용하는 것은 애플리케이션에 이익이 된다. 이와 같은 이벤트를 잘 처리하고 대응함으로써, 사용 중인 서비스에 미치는 영향을 최소화하면서 애플리케이션을 정상적으로 시작하고 종료할 수 있다. 지금 당장은 애플리케이션의 기본 형태로, 즉 컨테이너가 잘 설계된 포직스(POSIX) 프로세스처럼 동작해야 한다. 향후에는 애플리케이션이 스케일 업될 즈음 애플리케이션에 힌트를 주는 이벤트다. 애플리케이션이 멈추는 것을 막기 위해 자원을 해제하도록 요청하는 이벤트가 훨씬 더 많아질지도 모른다. 이제 애플리케이션 수명주기는 사람이 통제하는 것이 아니라 플랫폼에 의해 완전 자동화되어야 한다는 사고 방식의 전환이 필요하다.  

애플리케이션 수명주기 관리 외에도 쿠버네티스 같은 오케스트레이션 플랫폼의 또 다른 큰 역할은 컨테이너를 여러 노드에 분산시키는 것이다.  

#### 5.4. 참고 자료  
<br/>

+ 수명주기 관리 예제(http://bit.ly/2udxws4)
+ 컨테이너 수명주기 훅(http://bit.ly/2Fb38Uk)
+ 컨테이너 수명주기 이벤트를 위한 핸들러 설정(http://bit.ly/2Jn9ANi)
+ 정상적인 종료(http://bit.ly/2TcPnJW)
+ 쿠버네티스에서 파드의 정상적인 종료(http://bit.ly/2CvDQjs)
+ 지연 컨테이너(http://bit.ly/2TegEM7)  

### 6. 자동 배치  
<br/>

자동 배치(Automated Placement) 패턴은 쿠버네티스 스케줄러의 핵심 기능으로서, 컨테이너 자원 요청을 만족하고 스케줄링 정책을 준수하는 노드에 신규 파드를 할당해주는 기능이다.  

#### 6.1. 문제  
<br/>

합리적인 규모의 마이크로서비스 기반 시스템은 수십 또는 수백 개의 격리된 프로세스로 구성된다. 컨테이너와 파드는 패키징과 배포를 위한 좋은 추상화를 제공하지만 적절한 노드에 이런 프로세스들을 배치하는 문제는 해결하지 못한다. 마이크로서비스가 점점 더 많은 수로 늘어나고 있기 때문에, 이를 개별적으로 노드에 할당하고 배치하는 것은 관리하기 어려운 작업이다.  

컨테이너는 컨테이너 서로 간의 의존성, 컨테이너와 노드 간의 의존성, 자원 요구사항, 시간에 따른 변경사항을 가지고 있다. 클러스터에서 사용 가능한 자원은 시간이 지남에 따라, 클러스터를 확장 축소함에 따라, 이미 배치된 컨테이너가 자원을 소비함에 따라 다양하게 변한다. 컨테이너를 배치하는 방법은 가용성, 성능뿐만 아니라 분산 시스템의 용도에도 영향을 준다. 이로인해 노드에 컨테이너를 스케줄링하는 일은 마치 이동 목표물에 과녁을 조준하는 것과도 같다.  

#### 6.2. 해결책  
<br/>

쿠버네티스에서 파드는 스케줄러에 의해 노드에 할당된다. 스케줄링은 다양한 설정이 가능하고, 발전하며 빠르게 변화하고 있다. 쿠버네티스 스케줄러는 강력하고, 시간을 절약해주는 도구다. 전체 쿠버네티스 플랫폼에서 스케줄러는 기본적인 역할을 수행하지만, 다른 쿠버네티스 컴포넌트(API 서버, 큐블릿)과 비슷하게, 하나의 독립 프로세스로 실행될 수 있거나 전혀 사용되지 않을 수 있다.  

상위 레벨에서 보면 쿠버네티스 스케줄러가 수행하는 주요 작업은 API 서버로부터 새로 생성된 파드 정의를 조회하고, 파드를 노드에 할당하는 것이다. 스케줄러는 초기 애플리케이션 배치를 위한 것이든, 스케일 업을 위한 것이든, 애플리케이션이 비정상 노드에서 정상 노드로 이동될 때이든 간에, 스케일 업을 위한 것이든, 애플리케이션이 비정상 노드에서 정상 노드로 이동될 때이든 간에, 모든 파드에 대해 적절한 노드(그런 노드가 있기만 하다면)를 찾는다. 이는 런타임 의존성, 자원 요구사항, 고가용성 가이드 정책 등에 대한 고려를 통해, 그리고 파드의 수평적 확장, 또는 상호 호출 시 성능 및 낮은 지연 시간을 위한 파드들의 가까운 배치 등을 통해 수행된다. 그러나 스케줄러가 스케줄링 작업을 정확하게 수행하고 선언적 배치를 가능하게 하려면 가용한 용량을 확보한 노드와 선언된 자원 프로파일 및 가이드 정책을 갖춘 컨테이너가 필요하다.  

##### 6.2.1. 가용한 노드 자원  
<br/>

먼저, 쿠버네티스 클러스터에는 새로운 파드를 실행하기 위해 충분한 자원 용량을 확보한 노드가 있어야 한다. 모든 노드에는 파를 실행할 수 있는 용량이 있고, 스케줄러는 파드가 요청한 자원의 총합이 할당 가능한 노드의 용량보다 작다는 것을 확인해야 한다. 쿠버네티스 전용 노드만을 고려하면 노드 용량은 다음과 같은 공식으로 계산된다.  

Allocatable [애플리케이션 파드에 대한 용량] = Node Capacity [하나의 노드에 가용한 용량] - Kube-Reserved [큐블릿, 컨테이너 런타임 같은 쿠버네티스 데몬] - System-Reserved [sshd, udev 같은 OS 시스템 데몬]  

OS와 쿠버네티스를 관리하는 시스템 데몬 용도의 자원을 예약해 놓지 않으면, 파드는 노드와 전체 용량을 사용할 때까지 스케줄링될 수 있고, 이로 인해 파드와 시스템 데몬이 서로 자원을 사용하겠다고 경쟁하여 노드에 자원 부족 문제를 발생시킬 수 있다. 또한 쿠버네티스에 의해 관리되지 않는 컨테이너가 노드에서 실행 중이라도, 쿠버네티스에 의한 노드 용량 계산에 반영된다.  

이런 제한에 대한 임시적 해결 방법은 아무 일도 하지 않는 플레이스홀더(placeholder) 파드를 실행하는 것으로, 이 플레이스홀더에는 관리되지 않는 컨테이너의 자원 사용량과 일치하는 CPU와 메모리에 대한 자원 요청만이 있을 뿐이다. 이런 파드는 추적되지 않는 컨테이너의 자원 소비를 나타내거나 예약하기 위해서만 생성되고 스케줄러가 노드에 대한 더 좋은 자원 모델을 구축하는 데 도움을 준다.  

##### 6.2.2. 컨테이너 자원 요구  
<br/>

또 다른 효율적인 파드 배치를 위한 중요한 요구사항은 컨테이너가 런타임 의존성과 자원 요구정의를 갖는 것이다. 그 내용은 컨테이너에 요청(reuqest)과 제한(limit)을 갖는 자원 프로파일과 스토리지 또는 포트 같은 환경 의존성을 선언해야 한다는 것이다. 그래야만 파드가 현명하게 노드에 할당되고 피크 타임 동안 서로 영향 없이 실행될 수 있다.  

##### 6.2.3. 배치(Placement) 정책  
<br/>

마지막은 올바른 필터를 가지거나 특정 애플리케이션 요구에 대해 우선순위 정책에 대한 것이다. 대부분의 사용 예에서는 기본 술어(predicate) 세트와 기본 우선순위 정책이 설정된 세트를 갖는 스케줄러만으로도 충분하다. 스케줄러를 실행할 때 기본 스케줄러 정책을 다른 정책으로 덮어 쓸 수도 있다.  

스케줄러 정책과 사용자정의(custom) 스케줄러는 오직 관리자만이 클러스터 설정으로 정의할 수 있다. 일반 사용자는 미리 정의된 스케줄러만 참조할 수 있다.  

기본 스케줄러 정책을 설정하는 것 외에도 다중 스케줄러를 띄워서 파드를 특정 스케줄러에 지정해 배치할 수도 있다. 스케줄러에 고유한 이름을 주어서 다르게 설정된 또 다른 스케줄러를 띄울 수 있다. 그런 다음 파드를 정의할 때 파드 명세에, .spec.schedulerName 필드 값으로 사용자정의 스케줄러 이름을 추가하면, 파드는 사용자정의 스케줄러에 의해서 스케줄링된다.  

##### 6.2.4. 스케줄링 프로세스  
<br/>

파드는 배치(placement) 정책에 따라 특정 용량을 가진 노드에 할당된다.  

노드에 할당되지 않은 파드가 생성되는 즉시, 곧바로 스케줄러는 할당 가능한 모든 노드 그리고 필터링 정책과 우선순위 정책 세트와 함께 해당 파드를 선택한다. 첫 번째 단계에서, 스케줄러는 필터링 정책을 적용하고, 파드 조건에 기초해 자격 없는 모든 노드를 제거한다. 두 번째 단계에서, 남은 노드는 가중치에 의해 정렬된다. 마지막 단계애서는, 스케줄링 프로세스의 주요 결과인 파드가 노드에 할당된다.  

대부분의 경우 파드부터 노드까지의 할당 작업은 스케줄러에 맡기는 것이 좋으며, 배치(placement) 로직을 세세하게 관리하지 않는 편이 낫다. 그러나 일부 경우에서는 파드를 특정 노드나 노드 그룹에 강제로 할당하길 원할 수 있다. 이러한 할당은 노드 셀렉터(node selector)를 사용해 수행할 수 있다. .spec.nodeSelector는 파드의 필드로서, 파드 실행에 적합한 노드가 되도록(노드에 파드가 스케줄링될 수 있도록) 노드에 레이블로 존재하는 키-값 쌍의 맵이 지정되어 있어야 한다.  

예를 들어 SSD 스토리지나 GPU 가속 하드웨어를 갖는 노드에 파드를 강제로 실행시키려 한다고 가정해보자. 다음 예제의 nodeSelector가 disktype: ssd와 일치하는 파드 정의를 사용하면, disktype=ssd 레이블이 지정된 노드만이 파드 실행에 적합하다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
  nodeSelector:
    disktype: ssd # 노드가 이 파드이 노드로 간주되기 위해 반드시 일치해야 하는 노드 레이블 세트
```

노드에 사용자정의 레이블을 지정하는 것 외에도 모든 노드에 있는 일부 기본 레이블을 이용할 수 있다. 모든 노드는 kubernetes.io/hostname 레이블을 갖고 있으며 파드를 호스트명으로 노드에 배치할 때 사용할 수 있다. OS와 아키텍처, 인스턴스 타입을 나타내는 또 다른 기본 레이블도 배치에 유용하게 사용될 수 있다.  

##### 6.2.5. 노드 어피니티  
<br/>

쿠버네티스는 스케줄링 프로세스를 설정하기 위한 많은 유연한 방법을 지원한다. 그중 한 가지 기능인 노드 어피니티(node affinity)는 앞서 설명한 노드 셀렉터 접근 방식을 일반화한 것으로, 필수(required) 혹은 선호(preferred)라는 규칙을 지정할 수 있다. 필수 규칙은 파드가 노드에 스케줄링되기 위해서 반드시 충족되어야 한다. 반면, 선호 규칙은 반드시 충족되어야 하는 것은 아니면 일치하는 노드의 가중치를 증가시켜 노드가 선택되게 한다. 또한, 노드 어피니티 기능은 In, NotIn, Exists, DoesNotExist, Gt, Lt 같은 연산자를 활용해 언어를 좀 더 잘 표현할 수 있게 만듦으로써, 표현 가능한 제약조건의 종류를 크게 확장한다. 다음 예제는 노드 어피니티 선언 방법을 보여준다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  affinity:
    nodeAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: # 스케줄링 프로세스에서 고려해야 할 노드에 3개 이상의 코어(노드 레이블에 의해 표시된)가 있어야 한다는 엄격한 요구사항이다. 노드의 조건이 변경되어도 스케줄링 동안에 이 규칙이 재적용되지는 않는다.
        nodeSelectorTerms:
        - matchExpressions: # 레이블과 일치한다.
          - key: numberCores
            operator: Gt
            values: [ "3" ]
      preferredDuringSchedulingIgnoredDuringExecution: # 유연한 요구사항으로, 가중치를 갖는 셀렉터 목록이다. 모든 노드에 대해 일치하는 셀렉터들의 모든 가중치 합계를 계산한다. 엄격한 요구사항과 일치하는 한, 값이 가장 큰 노드가 선택된다.
      - weight: 1
        preference:
          matchFields: # 필드(jsonpath로 지정된)와 일치한다. 연산자로 In과 NotIn만 사용 가능하고, 값 목록에는 하나의 값만 허용된다.
          - key: metadata.name
            operator: NotIn
            values: [ "master" ]
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
```

##### 6.2.6. 파드 어퍼니티와 파드 안티어피니티  
<br/>

노드 어퍼니티는 강력한 스케줄링 방법으로서, nodeSelector로는 충분하지 않을 때 선택해야 한다. 이 메커니즘은 레이블이나 필드 일치를 기반으로 파드가 실행될 수 있는 노드를 제한한다. 또한 이 메커니즘은 하나의 파드가 또 다른 파드의 상대적 위치를 지정하는 파드 간의 의존성을 표현할 수 없다. 고가용성을 위해 파드를 분산시키는 방법을 표현하거나, 지연 시간 개선을 위해 파드를 함께 포장하고 배치하는 방법을 표현하기 위해서, 파드 어퍼니티(pod affinity)와 파드 안티어피니티(pod antiaffinity)를 사용한다.  

노드 어퍼니티는 노드 세분성(granularity)으로 작동하지만, 파드 어퍼니티는 노드로 제한되지 않고 다중 토폴로지 레벨에서 규칙을 표현할 수 있다. 다음 예제에서 보듯이, topologyKey 필드와 일치하는 레이블을 사용하면, 노드(node), 랙(rack), 클라우드 제공회사 존(zone), 리전(region) 같은 도메인 규칙과 결합시켜서 더 세분화된 규칙을 적용할 수 있다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  affinity:
    podAffinity:
      requiredDuringSchedulingIgnoredDuringExecution: # 대상 노드에서 실행 중인 또다른 파드와 관련된 파드 배치에 대한 필수 규칙이다.
      - labelSelector: # 함께 배치할 파드를 찾기 위해 레이블 셀렉터다.
          matchLabels:
            confidential: high
        topologyKey: security-zone # confidential=high 레이블을 가진 파드가 실행 중인 노드는 security-zone 레이블이 있어야 한다. 여기에 정의된 파드는 동일한 레이블과 값을 갖는 노드에 스케줄링된다.
    podAntiAffinity: # 파드가 배치되어서는 안 되는 노드를 찾기 위한 안티어피니티 규칙이다.
      preferredDuringSchedulingIgnoredDuringExecution: # 이 파드가 confidentail=none 레이블을 가진 파드가 실행중인 노드에 배치되어서는 안 된다고(그러나 배치될 수도 있다) 설명하는 규칙
      - weight: 100
        podAffinityTerm:
          labelSelector:
            matchLabels:
              confidential: none
          topologyKey: kubernetes.io/hostname
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
```

노드 어피니티와 유사하게, 파드 어피니티와 파드 안티어피니티에 대해 각각 requiredDuringSchedulingIgnoredDuringExecution과 preferredDuringSchedulingIgnoredDuringExecution이라는 엄격한 요구사항과 유연한 요구사항이 있다. 다시 말하면 노드 어피니티와 마찬가지로, 필드 이름에 IgnoredDuringExecution 접미어를 붙였는데, 이는 향후 확장성을 위한 것이다. 현재는 노드의 레이블이 바뀌고 어피니티 규칙이 더 이상 유효하지 않다면 파드가 계속 실행은 되지만, 향후에는 런타임으로 변경되는 것도 고려 대상이다. 그러나 노드 레이블이 바뀌고 스케줄되지 않은 파드가 노드 어피니티 셀렉터와 일치한다면, 이 파드는 해당 노드에 스케줄링된다.  

##### 6.2.7. 테인트와 톨러레이션  
<br/>

파드가 스케줄링되고 실행될 수 있는 위치를 제어하는 더 고급 기능은 테인트(taint)와 톨러레이션(toleration)을 기반으로 한다. 노드 어피니티는 파드가 노드를 선택할 수 있는 속성인 반면, 테인트와 톨러레이션은 그 반대다. 테인트와 톨러레이션은 파드가 스케줄되어야 하는지 혹은 스케줄되지 말아야 하는지를 노드가 제어하게 허용한다. 테인트는 노드의 특성으로서, 노드에 이 값이 존재할 때, 파드가 테인트에 대한 톨러레이션이 없다면 해당 노드에 스케줄링될 수 없다. 그런 의미에서 테인트와 톨러레이션은 기본적으로 스케줄링에는 사용할 수 없는, 노드에 스케줄링을 허용하는 옵트인(opt-in)이라 할 수 있다. 반면에 어피니티 규칙은 실행할 노드를 명시적으로 선택하여 선택되지 않은 노드를 모두 제외하는 옵트아웃(opt-out)이다.  

테인트는 kubectl: kubectl taint nodes master node-role.kubernetes.io/master="true": NoSchedule을 사용해 노드에 추가된다.  

```yaml
apiVersion: v1
kind: Node
metadata:
  name: master
spec: # 파드가 이 테인트를 톨러레이션한 경우를 제외하고는 해당 노드에 스케줄링이 불가능하다고 표시하기 위해 노드의 명세에 테인트를 지정한다.
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/master
```

```yaml
apiVersion: v1
kind: Node
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
  tolerations:
  - key: node-role.kubernetes.io/master # node-role.kubernetes.io/master 키로 테인트된 노드를 톨러레이션(즉 스케줄링을 고려햐여)한다. 운영 클러스터에는 마스터로 파드가 스케줄링되는 것을 막기 위해 마스터 노드에 테인트를 설정한다. 하지만 톨러레이션은 이 파드가 마스터로 스케줄링될 수 있게 허용한다.
    operator: Exists
    effect: NoSchedule # 테인트가 NoSchedule 효과를 지정한 경우에만 톨러레이션이 적용된다. 이 필드는 비워둘 수 있으며, 이 경우 톨러레이션은 모든 효과에 적용된다.
```

노드에 스케줄링을 금지하는 하드(hard) 테인트(effect=NoSchedule), 가능한 한 노드에 스케줄링을 피하게 해주는 소프트(soft) 테인트(effect=PreferNoSchedule), 그리고 이미 실행 중인 파드를 노드로부터 축축할 수 있는 테인트(effect=NoSchedule)가 있다.  

테인트와 톨러레이션은 독점적인 파드 세트를 전용 노드에 배치하는 등의 복잡한 사용 예를 허용하거나, 문제가 있는 노드에 테인트를 지정해 이 노드에 실행 중인 파드를 강제로 축출할 수도 있다.  

애플리케이션의 고가용성과 성능 요구사항에 기초해 배치(placement)에 영향을 줄 수는 있지만, 스케줄러에 많은 제약을 가해서 파드가 더 이상 스케줄링되지 못하고 자원은 너무 많이 남겨지는 상황에는 이르지 않도록 조심해야 한다. 예를 들어 컨테이너의 자원 요구사항이 너무 세부적이거나 노드의 자원이 너무 작다면, 결국 활용되지 않는 노드에 자원이 남게 된다.  

자원 요구사항을 줄여서 컨테이너를 생성하면 이 상황을 개선할 수 있다. 또 다른 해법으로는, 쿠버네티스 디스케줄러(descheduler)를 사용해 노드 조각 모음을 수행하고 활용도를 향상시키는 방법이 있다.  

일단 파드가 노드에 할당되면 스케줄러 작업은 완료된 것이며, 노드의 추가 없이 파드가 삭제되어 다시 생성되지 않는다면 파드의 배치는 변경되지 않는다. 앞서 살펴봤듯이, 시간이 지남에 따라 자원 조각화와 클러스터 자원 활용률은 낮아질 수 있다. 또 다른 잠재적인 문제는, 스케줄러의 결정은 새로운 파드가 스케줄링되는 그 시점이 클러스터에 기초한다는 것이다. 만약 클러스터가 유동적이며 노드의 자원 프로파일이 변하거나 새로운 노드가 추가되더라도, 스케줄러는 이전 파드 배치를 수정하지 않는다. 노드 용량을 변경하는 것 외에도, 노드 레이블을 변경할 수 있지만, 이전 배치 역시 수정되지 않는다.  

이 모든 것은 디스케줄러에 의해 해결될 수 있는 시나리오다. 쿠버네티스 디스케줄러(descheduler)는 클러스터 어드민이 파드를 다시 스케줄링하여 클러스터를 정리하고 조각 모음을 수행하기에 적절한 시기라고 결정할 때마다 항상 작업으로서 실행되는 옵션 기능이다. 디스케줄러는 활성화와 조정이 가능한 또는 비활성화할 수 있는 사전 정의된 정책으로 제공된다. 정책은 파일 형태로 디스케줄러 파드에 전달되며, 현재까지는 다음과 같다.  

+ RemoteDuplicates  
이 전략은 레플리카세트나 디플로이먼트와 관련된 하나의 파드를 하나의 노드에 실행시킨다. 만약 둘 이상의 파드가 있으면 초과 파드는 제거된다. 이 전략은 노드가 비정상상태가 되었을 때, 관리 컨트롤러가 다른 정상 노드에 새로운 파드를 실행시키는 사니리오에 유용하다. 비정상 노드가 복구되어 클러스터에 조인될 때 실행 중인 파드의 수는 원하는 수보다 많아지며, 디스케줄러는 원하는 replicas 개수로 파드의 수를 되돌린다. 노드 상의 중복 파드를 제거하는 것은 파드를 초기 배치한 이후 스케줄링 정책과 클러스터 토폴로지가 변경될 때, 더 많은 노드에 파드를 고르게 분산시키는 데 도움이 된다.

+ LowNodeUtilization  
이 전략을 활용해 활용률이 낮은 노드를 찾고 활용률이 과도하게 높은 노드의 파드를 제거하면, 해당 파드들은 활용률이 낮은 노드에 배치될 것이고, 이로써 자원을 더욱 분산시키고 잘 사용할 수 있다. 활용률이 낮은 노드는 CPU나 메모리, 파드 수가 thresholds 수보다 낮은 노드임을 알 수 있다. 마찬가지로, 활용률이 과도한 노드는 targetThresholds 값보다 큰 값을 갖는 노드다. 이들 값 사이에 있는 노드는 적절하게 활용되며, 이 전략에 영향을 받지 않는다.

+ RemovePodsViolatingInterPodAntiAffinity  
이 전략은 파드 간 안티어피니티 규칙을 위반한 파드를 축출하며, 파드가 노드에 배치된 후에 안티어피니티 규칙이 추가될 때 발생할 수 있다.

+ RemovePodsViolatingNodeAffinity  
이 전략은 노드 어피니티 규칙을 위반한 파드를 축출하기 위한 것이다.  

사용된 정책에 관계 없이 디스케줄러는 다음 파드들을 축출하지 않는다.  

+ scheduler.alpha.kubernetes.io/critical-pod 애노테이션으로 표시된 중요한 파드
+ 레플리카세트, 디플로이먼트, 잡(Job)에 의해 관리되지 않는 파드
+ 데몬세트(DaemonSet)에 의해 관리되지 않는 파드
+ 로컬 스토리지를 갖는 파드
+ 파드를 축출하면 PodDisruptionBudget 규칙을 위반하게 되는 PodDisruptionBudget을 사용한 파드
+ 디스케줄 파드 자신(중요한 파드로 표시하여 가능)  

물론, 모든 축출은 먼저 최선적(Best-Efforts) 파드를 선택한 다음, 확장 가능(Burstable) 파드를 선택하고, 마지막으로 보장(Guaranteed) 파드를 축출 후보로 선택하여 파드의 서비스 품질(Qaulity of Service, QoS) 레벨을 준수한다.  

#### 6.3. 정리  
<br/>

배치(placement)는 되도록 개입을 최소한으로 줄여야 하는 영역이다. 2장 '예측 범위 내의 요구사항'의 가이드라인을 따르고, 컨테이너의 모든 자원 요구사항을 정의한다면, 스케줄러는 이 작업을 수행하고 가능한 한 가장 적절한 노드에 파드를 배치할 것이다. 그러나 자원이 충분하지 않은 경우라면, 원하는 배포 토폴로지로 스케줄러를 조정하는 여러 가지 방법이 있다. 요약하자면, 간단한 방법부터 복잡한 방법까지 다음과 같은 방법으로 파드 스케줄링을 제어한다(이 글을 쓰는 시점에 작성한 이 목록은 쿠버네티스의 릴리스마다 변경된다는 것을 명심하자).  

+ nodeName  
파드를 노드에 연결하는 가장 간단한 형태. nodeName 필드는 이상적으로는 스케줄러에 의해 채워져야 하며, 수동으로 노드에 할당하기보다는 정책에 의해 진행된다. 파드를 노드에 할당하는 것은 파드가 스케줄링될 수 있는 곳을 크게 제한하게 된다. 이렇게 하면 애플리케이션이 실행될 노드를 명시적으로 지정했던 쿠버네티스가 존재하지 않았던 때로 되돌아 간다.

+ nodeSelector  
nodeSelector는 키-값 맵으로 지정한다. 파드가 노드에 실행되려면, 파드에는 노드의 레이블로 지정된 키-값이 있어야 한다. 파드와 노드에 의미 있는 레이블을 지정할 때(어찌 됐듯 해야 할 때), nodeSelector는 스케줄러 선택을 제어하는 가장 간단한 메커니즘 중 하나다.

+ 기본 스케줄링 변경  
기본 스케줄러(default scheduler)는 클러스터 내의 노드에 새로운 파드를 배치하는 역할을 담당하며, 합리적으로 동작한다. 그러나 필요한 경우, 스케줄러의 필터링의 우선순위 정책 목록, 순서, 가중치를 변경(alteration)할 수 있다.

+ 파드 어피니티와 안티어피니티  
파드 어피니티(Pod affinity)와 안티어피니티(antiaffinity) 규칙으로 하나의 파드는 또 다른 파드들에 대한 의존성을 표현할 수 있다. 예를 들어 애플리케이션의 지연 요구사항, 고가용성, 보안 제약조건 등에 대한 의존성을 표현할 수 있다.

+ 노드 어피니티  
노드 어피니티(Node affinity) 규칙으로 파드는 노드에 대한 의존성을 표현할 수 있다. 예를 들어 노드의 하드웨어, 위치 등을 고려한다.

+ 테인트와 톨러레이션  
테인트(taint)와 톨러레이션(toleration)으로 노드는 파드를 스케줄링해야 할지 혹은 하지 말아야 할지를 제어할 수 있다. 예를 들어 파드 그룹에 대한 노드를 지정하거나, 런타임에 파드를 축출할 수 있다. 테인트와 톨러레이션의 장점은 또 있다. 새로운 레이블을 갖는 새로운 노드를 추가해 쿠버네티스 클러스터를 확장하면, 모든 파드에 새로운 레이블을 추가할 필요 없이, 새로운 노드에 배치해야 하는 파드에만 추가하면 된다.

+ 사용자정의 스케줄러  
앞의 방법 중 어느 것도 충분하지 않거나, 복잡한 스케줄링 요구사항이 있다면, 나만의 사용자정의 스케줄러(custom scheduler)를 작성할 수 있다. 사용자정의 스케줄러는 표준 쿠버네티스 스케줄러와 함께, 혹은 스케줄러 대신 실행될 수 있다. 함께 실행하는 방식에서는 표준 쿠버네티스 스케줄러가 스케줄링을 결정할 때 최종 패스(pass)로 호출하는 '스케줄러 확장자(extender)' 프로세스를 사용한다. 이 방식으로 전체 스케줄러를 구현할 필요는 없지만, 노드를 필터링하고 우선순위를 정하기 위해 HTTP API는 제공해야 한다. 나만의 스케줄러를 사용하면, 하드웨어 비용, 네트워크 지연시간, 노드에 파드 할당 시 활용률 향상 같은 쿠버네티스의 외부 요소를 고려할 수 있다는 장점이 있다. 또한, 기본 스케줄러와 함께 여러 개의 사용자정의 스케줄러를 사용할 수 있으며, 각 파드에 사용할 스케줄러를 설정할 수도 있다. 각 스케줄러는 파드 그룹 전용의 다양한 정책을 가질 수 있다.  

지금까지 살펴본 바와 같이 파드 배치를 제어하는 방법은 매우 다양해서, 올바른 방식을 선택하거나 여러 방식을 조합하는 것이 어려울 수도 있다. 컨테이너 자원 프로파일의 크기를 정하고 선언할 것, 파드와 노드에 적절하게 레이블을 지정할 것, 마지막으로, 쿠버네티스 스케줄러에는 개입을 최소한으로 줄이라는 것이다.  

#### 6.4. 정리  
<br/>

+ 자동화된 배치 예제(http://bit.ly/2TTJUMh)
+ 파드를 노드에 할당하기(https://kubernetes.io/docs/user-guide/node-selection/)
+ 노드 배치 및 스케줄링 설명(https://red.ht/2TPlceB)
+ PodDisruptionBudget(https://kubernetes.io/docs/admin/disruptions/)
+ 중요 애드온 파드에 대한 스케줄링 보장(https://kubernetes.io/docs/admin/rescheduler/)
+ 쿠버네티스 스케줄러(http://bit.ly/2Hrq8IJ)
+ 스케줄러 알고리즘(http://bit.ly/2F9Vfi2)
+ 다중 스케줄러 설정(http://bit.ly/2HLv5Fk)
+ 쿠버네티스의 디스케줄러(http://bit.ly/2YMQzYn)
+ 쿠버네티스 클러스터 균형 유지: 고가용성의 비결(http://bit.ly/2zuecKk)
+ 자원 스케줄리에 대해 알고 싶지만 남에게는 묻기 어려웠던 모든 것에 대해(http://bit.ly/2FNkBT9)  

### 7. 배치 잡  
<br/>

배치 잡(Batch Job) 패턴은 독립된 원자 작업단위를 관리하는 데 적합하다. 배치 잡은 잡 추상화를 기초로 하며, 짧은 수명(short-lived) 파드를 분산 환경에서 완료될 때까지 안정적으로 실행한다.  

#### 7.1. 문제  
<br/>

컨테이너 관리와 실행을 위한 쿠버네티스의 주요 기본 요소는 파드다. 다양한 특징을 가진 파드를 생성하는 방법은 여러 가지가 있다.  

+ 순수 파드  
컨테이너를 실행시키기 위해 파드를 수동으로 생성할 수 있다. 그러나 파드가 실행 중인 노드에 장애가 발생하면, 파드는 다시 시작되지 않는다. 개발이나 테스트 목적을 제외하고는 이런 방식의 파드 실행은 권장되지 않는다. 이러한 순수 파드(bare pod) 방식은 비관리 파드(unmanaged Pod)나 꾸밈없는 파드(naked Pod)라고도 불린다.

+ 레플리카세트  
레플리카세트(ReplicaSet) 컨트롤러는 지속적으로 실행되는(예를 들면 웹서버 컨테이너를 실행하기 위한) 파드의 수명주기를 생성하고 관리하는 데 사용된다. 언제라도 실행 중인 파드의 레플리카세트를 지속적으로 유지하고, 지정된 수와 동일한 파드의 수가 가용함을 보장한다.

+ 데몬세트  
데몬세트(DaemonSet) 컨트롤러는 모든 노드에 하나의 파드를 실행하는 컨트롤러다. 일반적으로 모니터링, 로그 수집, 스토리지 컨테이너 등의 플랫폼 기능을 관리하는 데 사용된다.  

이런 파드들의 공통적인 측면은 시간이 지나도 멈추지 않는 장기 실행 프로세스라는 것이다. 그러나 경우에 따라서는 미리 정의된 일정 시간의 작업단위를 안정적으로 실행한 후 컨테이너를 종료하는 것이 필요하다. 이런 작업을 위해 쿠버네티스는 잡(Job)이라는 자원을 제공한다.  

#### 7.2. 해결책  
<br/>

쿠버네티스 잡은 하나 이상의 파드를 생성하고 성공적으로 실행되는 것을 보장하기 때문에 레플리카세트와 비슷하다. 그러나 일단 예측된 파드 수까지 성공적으로 도달하면 잡은 완료된 것으로 간주되고 더 이상 추가로 파드가 시작되지 않는다는 점은 다르다. 잡 정의는 다음 예제와 같다.  

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: random-generator
spec:
  completions: 5 # 잡이 완료되기 위해서는 5개의 파드가 실행되어야 하고, 모두 성공해야 한다.
  parallelism: 2 # 2개의 파드가 병렬로 실행될 수 있다.
  template:
    metadata:
      name: random-generator
    spec:
      restartPolicy: OnFailure # 잡에는 restartPolicy가 필수적으로 지정되어야 한다.
      containers:
      - image: k8spatterns/random-generator:1.0
        name: random-generator
        command: [ "java", "-cp", "/", "RandomRunner", "/numbers.txt", "10000" ]
```

잡과 레플리카세트 정의 사이에서 중요한 차이점 중 하나는 .spec.template.spec.rastartPolicy다. 레플리카세트의 기본 값은 Always인데, 이는 항상 지속적으로 실행되어야 하는 장기 실행 프로세스에 적합하다. 잡에는 Always 값이 허용되지 않으며 OnFailure나 Never 값 중에 하나만 사용 가능하다.  

그럼 왜 성가시게 순수 파드 대신에 파드를 한 번만 실행하는 잡을 생성할까? 잡을 사용하면 다음과 같은 여러 가지 안정성과 확장성을 얻을 수 있다.  

+ 잡은 일시적인 인메모리(in-memory) 작업이 아니라, 클러스터 재시작에도 살아남는 지속된 작업이다.
+ 잡은 완료되고 나면, 삭제되지 않고 추적 목적으로 유지된다. 잡의 일부분으로 생성된 파드도 삭제되지 않으며 검사가 가능하다(예를 들어 컨테이너 로그를 확인하기 위해). 이는 순수 파드도 마찬가지이지만, 오직 restartPolicy: OnFailure일 때만 동일하다.
+ 잡은 여러 번 실행되어야 할 수도 있다. .spec.completions 항목을 사용하면 잡이 완료되기 전에 파드가 성공적으로 끝나야 하는 횟수를 지정할 수 있다.
+ 잡이 여러 번 완료되어야 하는 경우(.spec.completion을 통해), 여러 개의 파드를 동시에 실행해서 잡을 확장할 수도 있다. 이것은 .spec.parallelism 필드에 값을 지정하면 된다.
+ 파드가 실행 중인 동안 노드에 장애가 생기거나 파드가 어떤 이유로 축출되었을 때, 스케줄러는 새로운 정상상태 노드에 파드를 배치하고 재실행한다. 기존의 순수 파드는 또 다른 노드로 결코 옮겨지지 않기 때문에 순수 파드는 실패 상태로 남는다.  

반드시 작업단위의 완료가 보장되어야 하는 시나리오에서는 이 모든 것들이 잡을 매력적으로 만든다.  

잡 동작에서 중요한 역할을 하는 2개의 필드는 다음과 같다.  

+ .spec.completions  
잡을 완료하기 위해 실행되어야 하는 파드의 수를 지정한다.

+ .spec.parallelism  
몇 개의 파드를 동시에 실행할지를 지정한다. 큰 수를 지정한다고 해서 높은 수준의 병렬 처리를 보장하는 것은 아니며, 실제 파드 개수는 요청된 파드 개수보다(예를 들면 한계치, 자원쿼터, 남은 완료 횟수 부족 등의 이유 때문에) 작을 수 있다(어떤 특별한 상황에서는 더 많을 수도 있다). 이 필드를 0으로 설정하면 잡은 효과적으로 일시 중지된다.  

이 두 파라미터를 기반으로 다음과 같은 잡의 종류가 있을 수 있다.  

+ 단일 파드 잡(Single Pod Job)  
이 유형은 .spec.completions 값과 .spec.parallelism 값을 생략하거나 기본 값인 1로 세팅하면 선택된다. 이런 잡은 하나의 파드만 시작하고 파드가 성공적으로 종료(종료 코드 0과 함께)되자마자 완료된다.

+ 고정 완료 횟수 잡(Fixed completion count Job)  
1보다 큰 수를 .spec.completions에 지정하면, 많은 파드가 성공해야 한다. 선택적으로, .spec.parallelism 값을 설정하거나, 값을 생략하여 기본 값인 1로 세팅할 수 있다. .spec.completions 설정 값과 같은 수의 파드가 성공적으로 완료되어야, 이 잡이 완료된 것으로 간주된다. 이 방식은 미리 작업 항목 수를 알고 있을 때 최선의 선택이 되며, 단일 작업 항목에 대한 처리 비용은 전용 파드를 사용하는 것과 같다고 할 수 있다.

+ 작업 큐 잡(Work queue Job)  
.spec.completions 값을 생략하고 .spec.parapllelism를 1보다 큰 정수로 세팅하면, 병렬 잡(parallel job)에 대한 작업 큐(work queue)를 가진다. 작업 큐 잡은 최소한 하나의 파드가 성공적으로 종료되고, 또 다른 모든 파드가 종료될 때 완료된 것으로 간주된다. 이 설정은 파드 서로 간의 협력이 필요하고 조화롭게 끝내기 위해서 각 파드가 무엇을 작업할지를 결정해야 한다. 예를 들어, 고정된 그러나 그 수는 알 수 없는 작업 항목이 큐에 저장된 경우, 병렬 파드가 이들을 하나씩 꺼내어 작업할 수 있다. 큐가 빈 것을 감지한, 그리고 성공적으로 종료된 첫 번째 파드는 잡의 완료를 표시한다. 잡 컨트롤러 또한 또 다른 모든 파드가 종료될 때까지 기다린다. 하나의 파드가 여러 작업 항목을 처리하므로, 이런 잡 유형은 세분화된 작업 항목에 매우 적절하다. 작업 항목당 하나의 파드에 대한 오버헤드가 타당하지 않을 대 말이다.  

처리해야 할 무제한 스트림 작업 항목이 있다면, 레플리카세트 같은 컨트롤러가 이런 작업 항목을 처리하기에 더 적합하다.  

#### 7.3. 정리  
<br/>

잡 추상화는 매우 기초적이지만, 크론잡 등 또 다른 기본 요소의 기반이 되는 매우 근본적인 기본 요소다. 잡은 독립된 작업단위를 믿을 만하고 스케일 가능한 실행 단위로 전환할 수 있게 도와준다. 그러나 잡은 개별적으로 처리 가능한 작업 항목을 잡이나 파드에 매핑하는 방법으로 처리하지는 않는다. 이것은 각 옵션의 장단점을 고려한 후에 결정해야 할 사항이다.  

+ 작업 항목당 하나의 잡(One Job per work item)  
이 옵션은 쿠버네티스 잡을 생성하는 오버헤드가 있으며, 자원을 소모하는 많은 잡을 관리하는 플랫폼을 위한 옵션이다. 이 옵션은 각 작업 항목이 독립적으로 기록, 추적, 확장되어야 하는 복잡한 작업일 경우에 유용하다.

+ 모든 작업 항목에 대해 하나의 잡(One Job for all work items)  
이 옵션은 플랫폼에 의해 개별적으로 추적될 필요가 없고 관리될 필요가 없는 많은 작업 항목에 적합하다. 이 시나리오에서는, 작업 항목은 배치 프레임워크를 통하여 애플리케이션 내에서 관리되어야 한다.  

잡 기본 요소는 작업 항목 스케줄에 대한 최소 기본 사항만 제공한다. 복잡한 구현은 원하는 결과를 얻기 위한 배치(batch) 애플리케이션 프레임워크와 결합해야 한다(예를 들면, 자바 에코 시스템에서는 표준 구현으로 스프링 배치(Spring Batch)와 제이버러(JBeret)가 있다).  

모든 서비스가 항상 실행되어야 하는 것은 아니다. 요청 시 실행되어야 하는 서비스도 있고, 특정 시간에 또는 주기적으로 실행되어야 하는 서비스도 있다. 잡을 사용하면 필요할 때만 파드를 실행할 수 있고, 작업 실행 동안에만 파드를 실행할 수도 있다. 잡은 요청된 용량을 확보하고, 파드 배치 정책을 충족하며, 다른 컨테이너와의 의존성을 고려한 노드에 스케줄링된다. 짧은 수명주기 작업에는 장기 실행 추상화(레플리카세트 등)보다 잡을 사용하면, 플랫폼의 다른 워크로드에 쓸 자원이 절약된다. 이 모든 것을 통해 잡은 고유한 기본 요소가 되고, 쿠버네티스는 다양한 워크로드를 지원하는 플랫폼이 될 수 있다.  

#### 7.4. 참고 자료  
<br/>

+ 배치 잡(batch job) 예제(http://bit.ly/2Jnloz6)
+ 유한 워크로드 완료를 위한 실행(http://bit.ly/2WlZTW2)
+ 확장을 활용한 병렬 처리(http://bit.ly/2Y563GL)
+ 작업 큐를 사용한 대규모 단위 병렬 처리(http://bit.ly/2Y29cqS)
+ 작업 큐를 사용한 소규모 단위 병렬 처리(http://bit.ly/2Obtutr)
+ 메타컨트롤러로 생성한 인덱스 잡(http://bit.ly/2FkjQSA)
+ 자바 배치 처리(batch processing) 프레임워크와 라이브러리(https://github.com/jberet)  

### 8. 주기적 잡  
<br/>

주기적 잡(Periodic Job) 패턴은 시간 차원을 추가하고 작업단위 실행이 시간적 이벤트에 의해 트리거 되도록 하여 배치 잡(Batch Job) 패턴을 확장한다.  

#### 8.1. 문제  
<br/>

마이크로서비스 및 분산 시스템에서는, HTTP와 경량 메시지를 사용하여 실시간 및 이벤트 주도(event-driven) 방식의 애플리케이션 상호작용을 처리하는 경향이 있다. 그러나 소프트웨어 개발의 최신 동향과 관계없이, 잡 스케줄링은 역사가 깊으며 여전히 소프트웨어 개발과 관련이 있다. 주기적 잡은 보통 시스템 유지 보수나 관리 작업을 자동화하기 위해 사용된다. 또한 주기적으로 실행되어야 하는 특정한 작업을 요구하는 비즈니스 애플리케이션과 관련이 있다. 여기서 일반적인 예로든 파일 전송을 통한 B2B(business-to-business) 통합, 데이터베이스 조회를 통한 애플리케이션 통합, 뉴스레터 이메일 전송, 오래된 파일 정리 및 보관 등을 들 수 있다.  

시스템 유지보수 목적을 위해 주기적 잡을 관리하는 전통적인 방식은 크론(Cron), 혹은 특화된 스케줄링 소프트웨어를 사용하는 것이다. 그러나 특화된 소프트웨어는 간단한 사용 예에 활용하기에는 비용이 많이 들며, 단일 서버에서 실행되는 크론 잡은 관리하기가 어렵고 단일 장애 지점(single point of failure)이 될 수도 있다. 그렇기 때문에 개발자들은 스케줄링 측면과 비즈니스 로직 실행 측면 모두를 관리할 수 있는 솔루션을 만들려는 경향이 있다. 예를 들어 자바 환경에서는, 쿼츠(Quartz)와 스프링 배치(Spring Batch)같은 라이브러리와 ScheduledThreadPoolExecutor 클래스로 구현한 사용자정의 라이브러리로 시간적 작업을 실행할 수 있다. 그러나 크론과 마찬가지로, 이런 접근 방식의 여러운 점은 스케줄링이 회복성과 고가용성을 갖추도록 만드는 것인데, 이에는 많은 리소스 사용이 필요하다. 또한 이런 접근 방식을 사용하면, 시간 기반의 잡 스케줄러는 애플리케이션이 일부이므로, 고갸용성 스케줄러를 만들기 위해서는 전체 애플리케이션의 가용성이 높아야 한다. 일반적으로 스케줄러가 포함된 다중 인스턴스의 애플리케이션이 동시에 실행 중이면, 오직 하나의 인스턴스만 활성화되어야 하고 잡을 스케줄링해야 한다. 이것은 리더(leader) 선출 및 또 다른 분산 시스템 문제가 수반된다.  

결국, 하루에 한 번 몇 개의 파일을 복사하는 간단한 서비스로 인해, 멀티 노드, 분산 리더 선출 메커니즘 등이 필요할 수 있다. 쿠버네티스 크론잡 구현은 잘 알려진 크론 형식을 이용한 잡 자원 스케줄을 허용하며, 개발자들이 시간적 스케줄링 측면보다는 수행될 작업을 구현하는 데에 집중하게 함으로써, 이 모든 것을 해결한다.  

#### 8.2. 해결책  
<br/>

크론잡 인스턴스는 유닉스 크론탭(크론 테이블)의 1개 행과 유사하며, 잡의 시간적 측면을 관리한다. 또한 지정된 시간에 주기적으로 잡을 수행하는 것이 가능하다. 샘플 정의에 대해서는 다음 예제를 참조하라.  

```yaml
apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: random-generator
spec:
  # 3분마다
  schedule: "*/3 * * * *" # 3분마다 실행하는 크론 정의
  jobTemplate:
    spec:
      template: # 일반 잡과 동일한 정의를 사용하는 잡 템플릿
        spec:
          containers:
          - image: k8spatterns/random-generator:1.0
            name: random-generator
            command: [ "java", "-cp", "/", "RandomRunner", "/numbers.txt", "10000" ]
          restartPolicy: OnFailure
```

잡 정의 외에도, 크론잡은 시간적 측면을 정의하기 위한 추가 필드가 있다.  

+ .spec.schedule  
잡의 스케줄을 지정하기 위한 크론탭 항목(예를 들면 매시간 실행하는 0 &#42; &#42; &#42; &#42;).

+ .spec.startingDeadlineSeconds  
스케줄 시간이 누락된 경우 작업 시작에 대한 마감 시간(초)을 나타낸다. 일부 사용 예에서 작업은, 특정 시간 내에 실행된 경우에만 유효하고, 늦게 실행된 경우에는 소용없다. 예를 들어 잡이 컴퓨팅 자원 부족이나 의존성 누락으로 요청된 시간에 실행되지 않았다면, 처리 예정인 데이터가 이미 쓸모없어졌기 때문에 실행을 안 하는 것이 더 좋을 수 있다.

+ .spec.concurrencyPolicy  
동일한 크론잡에 의해 생성된 잡을 동시에 실행하는 방법을 정의한다. 기본 동작인 Allow는 이전 잡이 완료되지 않았어도 새로운 잡 인스턴스를 생성한다. 이런 식의 동작을 원하지 않는다면, Forbid 값으로 현재 잡이 완료되지 않았다면 다음 실행을 건너뛰거나, Replace 값으로 현재 실행 중인 잡을 취소하고 새로운 잡을 시작할 수 있다.

+ .spec.suspend  
이미 시작된 실행에는 영향을 주지 않고 모든 후속 실행을 일시 중단하는 필드다.

+ .spec.successfulJobHistoryLimit와 .spec.failedJobHistoryLimit  
감사(audit) 목적으로 완료한 잡과 실패한 잡을 각각 몇 개 유지시켜야 하는지를 지정하는 필드들이다.  

크론잡은 매우 전문화된 기본 요소이고, 작업단위가 시간 차원을 가지는 경우에만 적용된다. 크론잡이 범용 기본요소는 아니지만, 쿠버네티스 기능이 서로의 위에서 어떻게 빌드되고 클라우드 네이티브가 아닌 사용 예를 어떻게 지원하는지에 대한 훌륭한 예시다.  

#### 8.3. 정리  
<br/>

크론잡 컨테이너를 구현할 대 애플리케이션에서는 중복 실행, 미실행, 병렬 실행, 취소 등의 실패 사례를 비롯한 예상치 못한 모든 오류 상황을 고려해야 한다.  

#### 8.4. 참고 자료  
<br/>

+ 주기적 잡 예제(http://bit.ly/2HGXAnh)
+ 크론잡(https://kubernetes.io/docs/concepts/jobs/cron-jobs/)
+ 크론(https://en.wikipedia.org/wiki/Cron)  

### 9. 데몬 서비스  
<br/>

데몬 서비스(Daemon Service) 패턴을 사용하면 대상 노드에 우선 순위가 지정된, 인프라스트럭처 중심의 파드를 배치하고 실행할 수 있다. 데몬 서비스 패턴은 주로 관리자가 쿠버네티스 플랫폼 기능을 향상시키기 위해서 노드에 특화된 파드를 실행하는 데 사용된다.  

#### 9.1. 문제  
<br/>

소프트웨어 시스템에서 데몬(daemon)이라는 개념은 다양한 레벨로 존재한다. 운영 시스템 레벨에서, 데몬은 백그라운드 프로세스로 실행되는 장기 실행(long-running), 자가 회복(self-recovering) 컴퓨터 프로그램이다. 유닉스에서 데몬 이름은 httpd, named, sshd처럼 'd'로 끝난다. 그 외의 운영체제에서는 서비스로 시작된(services-started) 작업과 고스트 잡(ghost job)처럼 다른 용어가 사용된다.  

어떤 이름으로 불리든 간에. 데몬 프로그램은 공통적으로 프로세스로서 실행되고, 일반적으로 모니터, 키보드, 마우스 등과 상호작용하지 않으며, 시스템 부팅 때 실행된다. 애플리케이션 레벨에도 비슷한 개념이 존재한다. 예를 들어 JVM 데몬 스레드는 백그라운드로 실행되고 유저 스레드에 지원 서비스를 제공한다. 이 데몬 스레드는 우선순위가 낮으며, 애플리케이션 수명에 관여하지 않고 백그라운드로 실행되고, 가비지 컬렉션이나 파이널라이제이션(finalization) 등의 작업을 수행한다.  

이와 유사하게, 쿠버네티스에는 데몬세트(DaemonSet)라는 개념이 있다. 쿠버네티스가 다중 노드로 퍼져 있는 분산 플랫폼이고 애플리케이션 파드 관리가 주요 목적이라는 점을 고려하면, 데몬세트는 클러스터 노드에서 실행되고 나머지 클러스터에 대한 일부 백그라운드 기능을 제공하는 파드로 보면 된다.  

#### 9.2. 해결책  
<br/>

레플리카세트(ReplicaSet)와 그 이전에 나온 레플리케이션컨트롤러(ReplicationController)는 몇 개의 파드가 실행되어야 하는지를 책임지는 제어 구조다. 레플리카세트와 레플리케이션컨트롤러는 끊임없이 실행 중인 파드의 개수를 모니터링하고, 요청된 파드 개수와 실제 파드 개수가 일치하는지 확인한다. 그런 면에서 데몬세트도 비슷한 구조이며, 특정 개수의 파드가 항상 실행되도록 책임진다. 파드의 개수를 사용자가 지정하는 것이 아니라, 노드당 1개의 파드가 실행될 때 실행될 노드 수에 따라 총 파드의 수가 달라지기 때문에 특정 개수(a certain number)라고 표현한다. 레플리카세트와 레플리케이션컨트롤러가 데몬세트와 다른 점을 꼽자면, 그 둘은 노드 수와는 상관없이 고가용성이나 사용자 사용량에 대한 애플리케이션 요구사항에 따라 지정된 수의 파드를 실행한다는 점이다.  

반면, 데몬세트에서는 파드를 어느 노드에 실행하고, 몇 개의 인스턴스를 실행할 것인지를 컨슈머 사용량에 의해 결정짓지 않는다. 데몬세트의 주요 목적은 모든 노드 혹은 특정 노드 위에 단일 파드를 계속 실행시키는 것이다. 다음 예제의 데몬세트 정의를 살펴보자.  

```yaml
apiVersion: extensions/v1beta1
kind: DaemonSet
metadata:
  name: random-generator
spec:
  selector:
    matchLabels:
      app: random-generator
  template:
    metadata:
      labels:
        app: random-generator
    spec:
      nodeSelector: # hw-rng 값으로 설정된 feature 레이블을 갖는 노드만 사용한다.
        feature: hw-rng
      containers:
      - image: k8spatterns/random-generator:1.0
        name: random-generator
        command:
        - sh
        - -c
        - >-
          "while true; do
          java -cp / RandomRunner /host_dev/random 100000;
          sleep 30; done"
        volumeMounts: # 데몬세트는 유지보수 작업을 실행하기 위해 노드의 파일시스템 일부를 마운트한다.
        - mountPath: /host_dev
          name: devices
      volumes:
      - name: devices
        hostPath: # 노드 디렉토리에 직접 접근하기 위해 hostPath를 사용한다.
          path: /dev
```

이런 동작들을 보면, 데몬세트의 주요 후보로는 흔히 로그 수집기, 메트릭 익스포터(exporter), 큐브프록시(kube-proxy) 등 클러스터 전반적을 작업을 수행하는 인프라스트럭처와 관련된 프로세스를 들 수 있다. 데몬세트와 레플리카세트를 관리하는 방법에는 다른 점이 많지만, 주요 차이점은 다음과 같다.  

+ 기본적으로 데몬세트는 모든 노드에 하나의 파드가 배치된다. 이것은 nodeSelector 필드를 사용하여 노드 그룹으로 제한하고 제어할 수 있다.
+ 데몬세트로 생성된 파드는 이미 특정 nodeName을 갖고 있다. 그 결과, 데몬세트는 컨테이너를 실행하기 위해 쿠버네티스 스케줄러를 필요로 하지 않는다. 그러므로 쿠버네티스 컴포넌트를 실행하고 관리하기 위해 데몬세트를 사용할 수 있다.
+ 데몬세트로 생성된 파드는 스케줄러가 시작되기 전에 실행될 수 있다. 이를 통해, 스케줄러에 의해 다른 파드들이 노드에 배치되기 전에 데몬세트 파드를 실행할 수 있다.
+ 스케줄러가 사용되지 않기 때문에, 노드의 unschedulable 필드는 데몬세트 컨트롤러와 관련이 없다.
+ 데몬세트로 관리되는 파드는 오직 대상 노드 위에서만 실행되기 때문에, 대부분의 컨트롤러는 이 파드들을 더 높은 우선순위로 별개로 취급한다. 예를 들면 디스케줄러(descheduler)는 이 데몬 파드들을 축출하지 않으며, 클러스터 오토스케일러는 데몬 파드를 분리해 관리한다.  

일반적으로 데몬세트는 모든 노드 혹은 지정된 노드 그룹에 파드를 하나 생성한다. 이를 감안하면, 데몬세트가 관리하는 파드에 접근하는 방법은 여러 가지가 있다.  

+ 서비스  
데몬세트와 동일한 파드 셀렉터를 갖는 서비스(Service)를 생성하고, 임의의 노드에서 로드 밸런싱되는 데몬 파드에 접근하기 위해 서비스를 사용한다.

+ DNS  
모든 파드 IP와 포트를 갖고 있는 DNS로부터 다중 A 레코드를 조회할 수 있는 데몬세트와 동일한 파드 셀렉터를 갖는 헤드리스(headless) 서비스를 생성한다.

+ hostPort를 사용한 NodeIP  
데몬세트 내의 파드는 hostPort를 지정할 수 있으며 노드 IP 주소와 특정 포트를 통해 접근할 수 있다. hostIp와 hostPort, protocol의 결합은 유일해야 하기 때문에 파드가 스케줄링될 수 있는 위치의 수는 제한적이다.

+ 푸시  
데몬세트 파드 내의 애플리케이션은 파드 외부의 잘 알려진 위치나 서비스로 데이터를 푸시(push)할 수 있다. 컨슈머(consumer)는 데몬세트 파드에 접근할 필요가 없다.  

+ 정적 파드  
데몬세트와 비슷한 방식으로 컨테이너를 띄우는 또 다른 방법은 정적 파드(static Pod) 메커니즘을 활용하는 것이다. 쿠버네티스 API 서버와 통신하고 파드 매니페스트를 얻는 것 외에도, 큐블릿(Kubelet)은 로컬 디렉터리로부터 자원 정의를 얻을 수 있다. 이런 방식으로 정의된 파드는 오직 큐블릿에 의해서만 관리되고 하나의 노드에서만 실행된다. API 서비스는 이런 파드를 관찰하지 않으며, 컨트롤러는 없고 파드에 대한 정상상태 확인(health check)을 실행하지 않는다. 큐블릿은 이런 파드를 관찰하고 파드가 죽으면 파드를 재시작한다. 마찬가지로, 큐블릿은 파드 정의 변경에 대해 설정 디렉토리를 주기적으로 확인하고, 이에 따라 파드를 추가 혹은 제거한다.  

정적 파드는 쿠버네티스 시스템 프로세스의 컨테이너화된 버전이나 또 다른 컨테이너들을 따로 분리하는 데 사용할 수 있다. 그러나 데몬세트는 플랫폼의 나머지 부분과 더 잘 통합되기에, 정적 파드보다는 데몬세트 사용을 권장한다.  

#### 9.3. 참고 자료  
<br/>

+ 데몬 서비스 예제(http://bit.ly/2TMX3rc)
+ 데몬세트(http://bit.ly/2r07CWx)
+ 데몬세트에 대한 롤링 업데이트 수행(http://bit.ly/2CAZ13F)
+ 데몬세트와 잡(http://bit.ly/2HLeHof)
+ 정적 파드(https://kubernetes.io/docs/tasks/administer-cluster/static-pod/)  

### 10. 싱글톤 서비스  
<br/>

싱글톤 서비스(Singleton Service) 패턴은 동시에 하나의 애플리케이션 인스턴스만 활성화하지만 고가용성을 보장한다. 이 패턴은 애플리케이션에서 구현할 수 있지만, 쿠버네티스에 모두 위임할 수도 있다.  

#### 10.1. 문제  
<br/>

쿠버네티스가 제공하는 주요 기능 중 하나는 애플리케이션을 손쉽게 확장하는 기능이다. kubectl scale 같은 단일 명령으로, 또는 레플리카세트 같은 컨트롤러 정의를 통한 선언으로, 애플리케이션 부하에 따라, 파드를 동적으로 확장할 수 있다. 동일한 서비스(쿠버네티스 서비스(Service) 자원이 아니라, 파드를 나타내는 분산 애플리케이션 컴포넌트)의 다중 인스턴스를 실행해서 일반적으로 시스템의 처리량과 가용성을 높인다. 서비스 중 하나의 인스턴스가 비정상상태가 되면, 요청 분배자는 이후 요청을 또다른 정상상태 인스턴스로 전달하기 때문에 가용성이 향상된다. 쿠버네티스에서는, 다중 인스턴스는 파드의 레플리카이며, 서비스(Service) 자원은 요청 분배를 책임진다.  

그러나 어떤 경우에는 동시에 하나의 서비스 인스턴스만 실행되어야 할 수도 있다. 예를 들어 서비스 내에서 주기적으로 실행되는 작업이 있고 동일한 서비스의 다중 인스턴스가 있다면, 각각의 인스턴스는 스케줄 간격으로 작업을 실행할 것이고, 하나의 작업만 실행될 거라는 기대와는 다르게 중복 작업이 수행될 것이다. 또 다른 예를 들자면, 특정 자원(파일 시스템 또는 데이터베이스)에 대해 폴링(polling)을 수행하는 서비스로 오직 하나의 인스턴스, 심지어 단일 스레드로 폴링이 수행되기를 원한다. 세 번째 경우는 메시지 브로커로부터 단일 스레드 컨슈머가 순차적으로 메시지를 가져와야 할 때 발생하며, 이 경우에도 싱글톤 서비스여야 한다.  

이와 같은 유사한 서비스 인스턴스 개수(보통은 단 하나만 필요하다)에 대한 제어가 필요하다.  

#### 10.2. 해결책  
<br/>

동일한 파드에 여러 레플리카를 실행하면 모든 서비스의 인스턴스가 활성화된 액티브-액티브(active-active) 토폴로지가 생성된다. 원하는 것은 하나의 인스턴스만 활성화되고, 또 다른 모든 인스턴스는 활성화 대기 중인 액티브-패시브(active-passive)나 마스터-슬레이브(master-slave) 토폴로지다. 기본적으로 이는 애플리케이션 외부 잠금(out-of-application locking)과 애플리케이션 내부 잠금(in-application locking), 이 2가지 레벨로 구현할 수 있다.  

##### 10.2.1. 애플리케이션 외부 잠금  
<br/>

이름에서 알 수 있듯이, 이 메커니즘은 애플리케이션 외부의 관리 프로세스를 사용해서 오직 애플리케이션 하나의 인스턴스만 실행되게 한다. 애플리케이션 자체는 이러한 제약사항을 인식하지 못하고 싱글톤 인스턴스로 실행된다. 이런 관점에서 보면, 스프링 프레임워크 같은 런타임 관리에 의해서 한 번만 인스턴스화되는 자바 클래스와 유사하다. 이 클래스 구현에서는 이것이 싱글톤으로 실행된다거나, 다중 인스턴스 생성을 막는 코드 구성을 포함한다는 것을 알지 못한다.  

쿠버네티스에서 애플리케이션 외부 잠금은 하나의 파드를 시작하는 것으로 구현한다. 하지만 이 작업만으로 싱글톤 파드의 고가용성을 보장하지는 못한다. 여기에 더해, 싱글톤 파드를 고가용성 싱글톤으로 만들어 주는 레플리카세트 같은 컨트롤러로 파드를 백업해야 한다. 이 토폴로지는 정확하게 액티브-패시브는 아니지만(패시브 인스턴스가 없기 때문에), 쿠버네티스가 항상 하나의 파드 인스턴가 실행되도록 보장해주기 때문에 동일한 효과가 난다. 컨트롤러가 정상상태 확인을 수행하고 장애가 나면 파드를 복구하기 때문에, 단일 파드 인스턴스는 고가용성이 된다.  

이 방법에서 주의해야 할 사항은 레플리카 숫자다. 레플리카 숫자 변경을 막아주는 플랫폼 레벨의 메커니즘이 없기 때문에, 실수로 레플리카 숫자를 늘리는 일은 없어야 한다.  

특히 시스템에 문제가 생겼을 때 하나의 인스턴스만 항상 실행된다는 것은 전혀 사실이 아니다. 레플리카세트 같은 쿠버네티스의 기본 요소는 일관성보다 가용성을 선호한다. 즉 가용성이 높고 확장 가능한 분산 시스템을 만들기 위해 의도된 결정이다. 이는 레플리카세트가 레플리카 숫자에 대해 '최대한'이 아닌 '최소한'의 의미를 적용한다는 것을 의미한다. replicas: 1을 사용해 레플리카세트를 싱글톤으로 설정하면, 컨트롤러는 최소한 하나의 인스턴스가 항상 실행 중임을 보장하지만, 간혹 그보다 더 많은 인스턴스가 실행될 수도 있다.  

가장 많이 일어나는 예외적인 상황이라면, 컨트롤러에 의해 관리되는 파드가 실행되고 있는 노드가 비정상상태에서 나머지 쿠버네티스 클러스터와 연결이 끊어질 때 발생하는 경우를 들 수 있다. 이런 경우 레플리카세트 컨트롤러는 연결이 끊어진 노드의 해당 파드가 종료되었는지를 확인하지 않고, 정상적인 노드에 새로운 파드 인스턴스를 시작한다(용량이 충분하다고 가정하고서), 마찬가지로, 레플리카 수를 변경하거나 다른 노드에 파드를 재배치시킬 때, 파드의 수는 일시적으로 요청된 수를 넘어설 수 있다. 이와 같은 일시적인 증가는 확장 가능한 무상태(stateless) 애플리케이션에 필요한 고가용성 보장과 무중단을 목적으로 수행된다.  

싱글톤은 회복과 복구가 가능하지만, 정의상으로 고가용성은 아니다. 싱글톤은 일반적으로 가용성보다는 일관성을 선호한다. 가용성보다 일관성을 선호하고, 엄격하고 싱글통 보장을 제공하는 쿠버네티스 자원은 스테이트풀세트(StatefulSet)다. 레플리카세트가 애플리케이션에 대해 원하는 보장을 제공하지 못하고, 엄격한 싱글톤 요구사항을 요한다면, 바로 스테이트풀세트가 답이 될 수 있다. 스테이트풀세트는 스테이트풀 애플리케이션을 위한 것이고 강력한 싱글톤 보장을 비롯해 많은 기능을 제공하지만 복잡성도 함께 증가시킨다.  

일반적으로 쿠버네티스의 파드 내에 실행되는 싱글톤 애플리케이션은 메시지 브로커나 관계형 데이터베이스, 파일서버, 또는 다른 파드에서 실행되는 시스템들이나 외부 시스템 쪽으로 나가는 연결을 연다. 그러나 때로는 싱글톤 파드가 안으로 들어오는 연결을 받아야 할 때가 있는데, 이는 쿠버네티스상에서 서비스(Service) 자원을 통해 가능하다.  

일반 서비스(type: ClusterIP를 사용하는)는 가상 IP를 생성하고, 셀렉터로 매칭되는 모든 파드 인스턴스에 대해 로드 밸런싱을 수행한다. 그러나 스테이트풀세트로 관리되는 싱글톤 파드는 단 하나의 파드만 소유하며, 지속적인 네트워크 식별이 가능하다. 이런 경우, 헤드리스 서비스(headless Service, type: ClusterIP와 clusterIP: None으로 설정해)를 생성하는 것이 좋다. 헤드리스라고 부르는 이유는 서비스(Service)에 가상 IP 주소가 없고, 큐브프록시(kube-proxy)가 서비스(Service)를 처리하지 않으며, 플랫폼은 프록시를 수행하지 않기 때문이다.  

어쨌듯, 셀렉터를 갖는 헤드리스 서비스는 API 서버 내에 종단점 레코드를 생성하고 매칭되는 파드에 대해 DNS A 레코드를 생성하기 때문에 여전히 유용하다. 이렇게 사용하면 서비스에 대한 DNS 조회는 가상 IP를 반환하지 않고, 대신 파드의 IP 주소를 반환한다. 이로써 서비스 가상 IP를 통하지 않고, 서비스 DNS 레코드를 통해 싱글톤 파드에 직접 접근할 수 있다. 예를 들어 my-singleton이라는 헤드리스 서비스를 생성하면, 파드의 IP 주소에 직접 접근하기 위해 my-singleton.default.svc.cluster.local를 사용할 수 있다.  

요약하면, 엄격하지 않은 싱글톤의 경우에는 하나의 레플리카와 일반 서비스(Service)를 갖는 레플리카세트면 충분하다. 엄격한 싱글톤과 더 좋은 성능의 서비스 디스커버리를 필요로 하는 경우라면 스테이트풀세트와 헤드리스 서비스가 더 적합하다.  

##### 10.2.2. 애플리케이션 내부 잠금  
<br/>

분산 환경에서 서비스 인스턴스를 제어하는 한 가지 방법은 분산 락(distributed lock)을 사용하는 것이다. 서비스 인스턴스 또는 인스턴스 내부의 컴포넌트가 활성화될 때마다 락을 획득하려고 시도할 수 있으며, 성공하면 그 서비스가 활성화된다. 락을 획득하지 못한 다음 순번의 서비스 인스턴스는 대기하며, 현재 활성화된 서비스가 락을 해제할 때까지 지속적으로 락을 획득하기 위해 시도한다.  

대다수 기존 분산 프레임워크는 이 메커니즘을 활용해 고가용성과 회복성을 달성한다. 예를 들어 메시지 브로커인 아파치 액티브엠큐(Apache ActiveMQ)은 데이터 소스가 공유 락을 제공하는 고가용성 액티브-패시브 토폴로지에서 실행될 수 있다. 첫 번째 브로커 인스턴스가 시작하면서 락을 획득해 활성화되면, 다음 순번으로 시작된 어떤 또 다른 인스턴스는 비활성화되어 그 락이 해제될 때까지 기다린다. 이 전략을 통해 단일 활성화 브로커는 장애에 대한 회복력도 갖추게 된다.  

이 전략은 '싱글톤이란 정적 클래스 변수에 저장된 객체 인스턴스'라는 객체지향 세계에서 통용되는 고전적인 싱글톤과 비교해볼 수 있다. 객체 인스턴스 내에서 클래스는 싱글톤의 존재를 알고 있으며, 동일한 프로세스에 대해 다중 인스턴스를 생성하지 못하게 막는 방식으로 클래스가 작성된다. 분산 시스템에서는, 시작된 파드 인스턴스 수와 상관없이 동시에 둘 이상의 액티브 인스턴스를 허용하지 않는 방식으로 컨테이너화된 애플리케이션 자체에 작성되어야 한다. 분산 환경에서 이를 달성하려면, 먼저 아파치 주키퍼(Apaceh Zookeeper), 해시코프의 컨설(HashiCorp's Consul), 레디스(Redis), 엣시디(Etcd)에서 제공하는 것과 같은 분산 락 구현이 필요하다.  

주키퍼를 사용한 일반적인 구현에서는 임시 노드를 사용한다. 이 임시 노드는 클라이언트 세션이 있는 동안만 존재하며, 세션이 종료되는 순간 즉시 삭제된다. 첫 번째로 시작한 서비스 인스턴가 주키퍼 서버에서 세션을 초기화하고, 활성화시킬 임시 노드를 생성한다. 동일한 클러스터의 그 밖의 모든 서비스 인스턴스는 비활성화되고 임시 노드가 해제될 때까지 기다려야 한다. 이는 전체 클러스터 내에 단 하나의 활성화 서비스 인스턴스만 보장하는 주키퍼 기반의 구현 방식이며, 액티브/패시브 장애 조치를 보장한다.  

쿠버네티스에서는 락 기능에 한정된 주키퍼 클러스터를 운영하기보다는, 쿠버네티스 API를 통해 노출되고 마스터 노드에서 실행 중인 엣시디(Etcd) 기능을 사용하는 편이 더 나은 선택이다. 엣시디는 분산 키-값을 저장하고 복제 상태를 유지하기 위해 래프트(Raft) 프로토콜을 사용한다. 가장 중요한 점이라면, 엣시디는 리더 선출 구현을 위해 필요한 빌딩 블록을 제공하며, 몇몇 클라이언트 라이브러리에는 이미 이 기능이 구현되어 있다는 점이다. 예를 들어 아파치 캐멀(Apache Camel)에는 리더 선출과 싱글톤 기능을 제공하는 쿠버네티스 커넥터가 있다. 한 단계 더 나아가, 이 커넥터는 엣시디 API에 직접 접근하는 대신, 쿠버네티스 API를 사용해 컨피그맵을 분산 락으로 사용한다. 또한 이 커넥터는 쿠버네티스 낙관적 잠금 보장(optimistic locking guarantee)을 사용해서, 동시에 하나의 파드만 수정이 가능한 컨피그맵 등의 자원을 편집한다.  

캐멀 구현은 이런 낙관적 잠금 보장을 통해 하나의 캐멀 라우트 인스턴스만 활성화되게 한다. 그리고 다른 모든 인스턴스는 대기하다가, 활성화되기 전에 락을 획득해야 한다. 캐멀 구현은 락의 사용자정의 구현이지만, 다음과 같은 동일한 목적을 달성할 수 있다. 동일한 캐멀 애플리케이션을 가진 여러 파드가 있을 때, 그중 단 하나만 활성화 싱글톤이 되고, 그 밖의 모든 파드는 비활성화 모드로 기다리게 된다.  

주키퍼나 엣시디 혹은 기타 분산 락 구현을 사용한 구현은 다음과 같이 설명할 수 있을 것이다. 애플리케이션의 인스턴스 하나만 리더가 되고 스스로를 활성화시키며, 다른 인스턴스들은 비활성화 상태로 락을 기다린다. 이로써 다중 파드 레플리카가 시작되고 그 모든 레플리카가 정상상태이며 실행 중이라 하더라도, 단 하나의 서비스만 활성화되어 싱글톤으로서 비즈니스 기능을 수행하며, 다른 인스턴스들은 마스터에 장애가 나거나 종료될 경우에 락을 획득하기 위한 대기 상태에 있다.  

##### 10.2.3. 파드 디스럽션 버짓  
<br/>

싱글톤 서비스와 러디 선출은 서비스가 한 번에 실행되는 최대 인스턴스 수를 제한하려고 시도하는 한편, 쿠버네티스의 PodDisruptionBudget 기능은 다소 이와 반대되는 보완적인 기능으로서, 유지 보수를 위해 동시에 다운되는 인스턴스 수를 제한한다.  

여기서 핵심은, PodDisruptionBudget은 일정 수나 일정 비율의 파드가 임의의 한 시점에 노드에서 자발적으로 축출되지 않게 보장한다는 점이다. 여기서 자발적(Voluntary)이란 축출이 특정 시간 동안 지연될 수 있음을 의미한다. 예를 들어 예측할 수 없거나 제어 불가능하게 노드가 비정상상태가 될 때보다는, 클러스터 스케일 다운 혹은 업그레이드나 유지보스를 위해 노드를 드레인(kubectl drain)함으로써 발생하는 경우를 말한다.  

다음 예제에서 PodDisruptionBudget은 셀렉터와 매치되는 파드에 적용되며 항상 2개의 파드가 가용 상태임을 보장한다.  

```yaml
apiVersion: policy/v1beta1
kind: PodDisruptionBudget
metadata:
  name: random-generator
spec:
  selector:
    matchLabels: # 가용한 파드 수를 위한 셀렉터
      app: random-generator
  minAvailable: 2 # 최소 2개의 파드가 가용해야 한다. 또한 80%처럼 비율로 지정해서 매칭되는 파드의 20%만 축출되게 설정할 수도 있다.
```

.spec.minAvailable 외에도, .spec.maxUnavailable을 사용하는 옵션도 있으며, 이는 축출 후에 불가용하게 될 파드의 수를 지정한다. 그러나 두 필드를 모두 함께 사용할 수는 없으며, PodDisruptionBudget은 일반적으로 컨트롤러가 관리하는 파드에만 적용된다. 컨트롤러에 의해 관리되지 않는 파드(순수(bare) 혹은 꾸밈없는(naked) 파드라고 부름)에 대해서는 PodDisruptionBudget 외에 다른 제약사항이 고려되어야 한다.  

PodDisruptionBudget 기능은 쿼럼을 보장하기 위해 항상 실행 중인 최소의 레플리카 수를 요구하는 쿼럼(quorum) 기반 애플리케이션에 유용하다. 혹은 애플리케이션이 전체 인스턴스 수의 특정 비율 아래로는 절대 떨어지지 않아야 하는 중요한 트래픽을 처리할 때도 유용하다.  

#### 10.3. 정리  
<br/>

강력한 싱글톤을 보장해야 하는 경우라면, 레플리카세트의 애플리케이션 외부 잠금 메커니즘에 기댈 수 없다. 쿠버네티스 레플리카세트는 파드에 대해 '최대한 하나'라는 의미를 보장하기보다는 파드의 가용성을 유지하도록 설계되었다. 그 결과, 2개의 파드가 잠시 동시에 실행되는 실패 시나리오가 많다(다시 말해 싱글톤 파드를 실행하는 노드가 나머지 클러스터로부터 분리될 때, 예를 들어 삭제된 파드 인스턴스를 새 인스턴스로 교체할 때 등), 이를 허용할 수 없다면, 스테이트풀세트를 사용하거나 강력한 보증으로 리더 선출 프로세스에 대한 제어를 제공하는 애플리케이션 내부 잠금 옵션을 고려해야 한다. 애플리케이션 내부 잠금 옵션은 실수로 레플리카 수를 변경해 파드가 확장되는 것을 막아준다.  

또 다른 시나리오에서는, 컨테이너화된 애플리케이션의 일부만 싱글톤이어야만 한다. 예를 들어 HTTP 종단점을 제공하는 컨테이너화된 애플리케이션은 다중 인스턴스로 확장하기에 안전할 수 있지만, 폴링 컴포넌트는 싱글톤이어야 한다. 애플리케이션 외부 잠금 방법을 사용하면 전체 서비스를 확장할 수 없다. 또한 결과적으로 싱글톤은 유지하기 위해 배포 단위에서 싱글톤 컴포넌트를 분리해야 하거나(이론적으로 좋으나 항상 실용적이거나 오버헤드 가치가 있는 것은 아니다). 싱글톤이 되어야 하는 컴포넌트만 락을 거는 애플리케이션 내부 잠금 메커니즘을 사용해야 한다. 이를 통해 투명하게 전체 애플리케이션을 확장하고, HTTP 종단점을 확장하며, 또 다른 부분을 액티브-패시브 싱글톤으로 만들 수 있다.  

#### 10.4. 참고 자료  
<br/>

+ 싱글톤 서비스 예제(http://bit.ly/2TKp5nm)
+ 쿠버네티스와 도커를 사용한 간단한 리더 선출(http://bit.ly/2FwUS1a)
+ 고(Go) 클라이언트에서의 리더 선출(http://bit.ly/2UatejW)
+ PodDisruptionBudget 설정(http://bit.ly/2HDKcR3)
+ 쿠버네티스에서의 클러스터 싱글톤 서비스 생성(http://bit.ly/2TKm1HR)
+ 아파치 캐멀 쿠버네티스 커넥터(http://bit.ly/2JoL6mT)  

### 11. 스테이트풀 서비스  
<br/>

분산 스테이트풀(stateful) 애플리케이셔은 퍼시스턴트 식별자, 네트워킹, 스토리지, 순서성(ordinality) 등과 같은 기능이 요구된다.  

#### 11.1. 문제  
<br/>

쿠버네티스 기본 요소로는, 정상상태 확인 및 자원 제한이 적용된 컨테이너, 여러 컨테이너로 구성된 파드, 클러스터 전반의 동적 배치(placement), 배치 잡(batch job), 스케줄된 잡(scheduled job), 싱글톤(singleton) 등을 들 수 있다. 이러한 모든 기본 요소는 공통적인 특징이 있다. 첫째, 관리 대상 애플리케이션을 동일하고 교체 가능하며 대체 가능한 컨테이너로 구성된 스테이트리스(stateless) 애플리케이션으로 취급하며, 둘째, 모두가 12요소 애플리케이션(Twelve-Factor App) 원칙을 준수한다는 사실이다.  

스테이트리스(stateless) 애플리케이션의 배치와 회복성, 스케일 등의 처리하는 플랫폼을 갖춘다면 큰 효과는 있겠지만, 고려해야 할 상당 부분의 워크로드가 스테이트풀 애플리케이션이다. 스테이트풀 애플리케이션의 모든 인스턴스는 고유하며 오래 지속되는 특성을 지닌다.  

실제로는, 확장성이 뛰어난 모든 스테이트리스 서비스 뒤에는 일반적이 데이터 저장소 형태의 스테이트풀 서비스가 있다. 스테이트풀 워크로드에 대한 지원이 부족했던 초창기 쿠버네티스의 해결책은 다음과 같았다. 클라우드 네이티브 모델의 이점을 얻기 위해 스테이트리스 애플리케이션을 쿠버네티스에 배치하고, 퍼블릭 클라우드 또는 온프레미스 하드웨어상의 클러스터 외부에 기존의 비 클라우드 네이티브 메커니즘을 관리되는 스테이트풀 컴포넌트를 배치했다. 모든 기업에 수많은 스테이트풀 워크로드(구형 및 최신)가 있다는 점을 고려할 때, 스테이트풀 워크로드에 대한 지원 부족은 범용 클라우드 네이티브 플랫폼으로 알려진 쿠버네티스에 있어서 상당한 한계점이었다.  

그러면 스테이트풀 애플리케이션의 일반적인 요구사항은 무엇일까? 디플로이먼트를 사용해 아파치 주키퍼(Apache Zookeeper), 몽고디비(MongoDB), 레디스(Redis), MySQL 같은 스테이트풀 애플리케이션을 배포할 수 있을 것이다. 이를 통해 replicas=1로 레플리카세트를 생성해 신뢰성을 확보하고, 서비스(Service)를 활용해 해당 종단점을 디스커버리하며, PersistentVolumeClaim과 PersistentVolume을 해당 상태의 영구적인 저장소로 사용할 수 있을 것이다.  

단일 인스턴스 스테이트풀 애플리케이션의 경우에는 대체로 맞지만, 레플리카세트(ReplicaSet)는 최대한 하나(at-most-one)라는 의미를 보장하지 않으며 레플리카 수는 일시적으로 달라질 수 있기 때문에 전적으로 맞는 망릉 아니다. 이러한 상황은 막심한 피해와 데이터 손실로 이어질 수 있다. 또한 여러 인스턴스로 구성된 분산 스테이트풀 서비스인 경우, 난관에 부닥칠 수도 있다. 다수의 클러스터화된 서비스로 구성된 스테이트풀 애플리케이션은 기본 인프라스트럭처의 다방면 보장이 요구된다. 분산 스테이트풀 애플리케이션에 대해 가장 일반적으로 오랫동안 이어져 온 전제조건 몇 가지를 살펴보겠다.  

##### 11.1.1. 스토리지  
<br/>

레플리카세트의 replicas 수를 손쉽게 늘려, 분산 스테이트풀 애플리케이션으로 만들 수 있다. 하지만 이런 경우, 스토리지 요구사항은 어떻게 정의할까? 일반적으로 앞서 설명한 분산 스테이트풀 애플리케이션에는 모든 인스턴스에 대해 전용의 퍼시스턴트 스토리지가 필요하다. replicas=3이고 PersistentVolumeClaim(이하 PVC) 정의를 갖춘 레플리카세트는 3개의 파드가 모두 동일한 PersistentVolume(이하 PV)에 연결된다. 레플리카세트와 PVC는 인스턴스를 구동시키고 스토리지를 인스턴스가 스케줄된 노드에 배속시키는 반면, 스토리지는 특정 파드 인스턴스만 사용할 수 있는 것이 아니라 모든 파드 인스턴스 간에 공유된다.  

여기서 스테이트풀세트를 쓰지 않고 해결할 수 있는 차선책을 생각해보자면, 애플리케이션 인스턴스가 공유 스토리지를 이용하고, 스토리지를 하위 폴더로 분할해 충돌 업싱 사용하는 애 내(in-app) 메커니즘을 활용하는 것이다. 이 방법은 가능하긴 하지만, 단일 스토리지로 단일 장애 지점을 만든다. 또한 스케일 중에 파드 수가 변경되면 오류가 발생하기 쉬우며, 스케일 중에 데이터 손상이나 손실을 방지하기가 매우 어려울 수도 있다.  

또 다른 차선책은 분산 스테이트풀 애플리케이션의 모든 인스턴스에 대해 별도의 레플리카세트(replicas=1)을 갖는 것이다. 이 시나리오에서 모든 레플리카세트에는 PVC와 전용 스토리지가 있어야 한다. 이 접근 방식의 단점은 수동 작업이 많다는 것이다. 즉 스케일 업하려면 새로운 레플리카세트나 PVC, 서비스(Service) 정의 세트를 만들어야 한다. 또한 이 방법에는 스테이트풀 애플리케이션의 모든 인스턴스를 하나로 관리하는 단일 추상화가 없다.  

##### 11.1.2. 네트워킹  
<br/>

스토리지 요구사항과 유사하게 분산 스테이트풀 애플리케이션에는 안정적인 네트워크 식별자가 필요하다. 스테이트풀 애플리케이션은 애플리케이션 관련 데이터를 스토리지 공간에 저장하는 것 외에도, 호스트네임과 그들의 피어(peer) 연결 세부사항 같은 설정(configuration) 세부사항 또한 저장한다. 즉 레플리카세트의 파드 IP 주소처럼 동적으로 변경되어선 안 되고 예측 가능한 주소로 모든 인스턴스에 도달할 수 있어야 한다. replicas=1로 설정된 레플리카세트별로 서비스(Service)를 생성하는 차선책을 통해 이 요구사항을 해결할 수는 있다. 그러나 이러한 설정을 관리하는 것은 수동 작업이며 애플리케이션 자체가 안정적인 호스트네임에 의존할 수 없다. 왜냐하면 호스트네임은 재시작할 때마다 변경되며 애플리케이션은 여기에 접근한 서비스의 이름을 알지 못하기 때문이다.  

##### 11.1.3. 식별자  
<br/>

클러스터된 스테이트풀 애플리케이션은 수명이 긴 스토리지 및 네트워크 식별자를 가진 모든 인스턴스에 크게 의존한다. 이는 스테이트풀 애플리케이션에서는 모든 인스턴스가 제각기 고유하고 자신의 고유한 식별자를 알고 있으며, 해당 식별자의 주요 구성요소는 오래 지속되는 스토리지와 네트워킹 좌표이기 때문이다. 여기에 인스턴스의 식별자/이름(일부 스테이트풀 애플리케이션은 고유한 퍼시스턴트 이름이 필요함. 쿠버네티스에서는 파드 이름)도 추가될 수 있다. 레플리카세트로 생성된 파드는 임의의 이름을 지니며, 재시작 시 해당 식별자를 유지하지 않을 것이다.  

##### 11.1.4. 순서성  
<br/>

고유하고 오래 지속되는 식별자 외에도, 클러스터된 스테이트풀 애플리케이션의 인스턴스는 인스턴스 컬렉션에서 위치가 고정되어 있다. 이러한 순서는 일반적으로 인스턴스가 스케일 업 및 다운되는 순서에 영향을 준다. 또한, 데이터 배포나 접근, 그리고 잠금, 싱글톤, 마스터 등과 같은 클러스터 내 동작 위치 지정에도 사용될 수 있다.  

##### 11.1.5. 기타 요구사항  
<br/>

안정적이고 오래 지속되는 스토리지, 네트워킹, 식별자, 순서성(ordinality) 등은 클러스터된 스테이트풀 애플리케이션이 공통적으로 요구하는 사항들이다. 이 외에도, 스테이트풀 애플리케이션 관리에는 상황에 따라 그 밖의 많은 특정한 요구사항이 수반되기도 한다. 예를 들어 어떤 애플리케이션에는 쿼럼(quorum) 개념이 있으며, 항상 사용할 수 있는 최소 수의 인스턴스가 필요하다. 어떤 애플리케이션은 순서성에 민감하고, 어떤 애플리케이션은 병렬 디플로이먼트에 적합하다. 중복 인스턴스를 허용하는 애플리케이션도 있지만 허용하지 않는 애플리케이션도 있다. 이러한 모든 사례를 고려해 계획하고 일반적인 메커니즘을 제공하는 것은 불가능한 작업이므로 쿠버네티스는 스테이트풀 애플리케이션 관리를 위한 CustomResourceDefinition과 오퍼레이터(operator)를 생성할 수 있다.  

#### 11.2. 해결책  
<br/>

스테이트풀세트(StatefulSet)가 스테이트풀 애플리케이션 관리에 제공하는 기능을 설명하기 위해 간혹 스테이트풀세트 동작을 레플리카세트 기본 요소와 비교하곤 한다. 레플리카세트는 이미 주지하듯이 쿠버네티스가 스테이트리스 워크로드 실행에 사용하는 것이다. 여러 의미로, 스테이트풀세트는 애완동물(pet)을 관리하기 위한 것이고, 레플리카세트는 가축(cattle)을 관리하기 위한 것이다. '애완동물 대 가축'은 데브옵스(DevOps) 세계에서 유명한(그러나 논쟁의 여지가 있는) 비유로서, 동일하고 교체 가능한 서버를 가축이라 부르고, 개별 관리가 필요하고 대체 불가한 고유한 서버를 애완동물로 취급한다. 마찬가지로 스테이트풀세트(StatefulSet, 처음에는 비유에서 유래해 PetSet였다)는 동일하고 교체 가능한 파드를 관리하는 레플리카세트와 달리, 대체할 수 없는 파드를 관리하도록 설계되었다.  

스테이트풀세트가 어떻게 동작하는지, 그리고 스테이트풀 애플리케이션의 요구를 어떻게 처리하는지에 대해 한번 살펴보자. 다음 예제는 스테이트풀세트를 이용한 난수 생성 서비스다.  

```yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: rg # 스테이트풀세트의 이름은 생성된 노드 이름에서 접두어로 사용된다.
spec:
  serviceName: random-generator # 필수 서비스(Service)를 참조한다.
  replicas: 2 # 스테이트풀세트의 rg-0와 rg-1이라는 2개의 파드 멤버
  selector:
    matchLabels:
      app: random-generator
  template:
    metadata:
      labels:
        app: random-generator
    spec:
      containers:
      - image: k8spatterns/random-generator:1.0
        name: random-generator
        ports:
        - containerPort: 8080
          name: http
        volumeMounts:
        - name: logs
          mountPath: /logs
volumeClaimTemplates: # 각 파드에 대한 PVC를 생성하기 위한 템플릿(파드의 템플릿과 유사함)
- metadata:
    name: logs
  spec:
    accessModes: [ "ReadWriteOnce" ]
    resources:
      requests:
        storage: 10Mi
```

##### 11.2.1. 스토리지  
<br/>

항상 필요한 것은 아니지만 대부분의 스테이트풀 애플리케이션은 상태를 저장하므로 인스턴스마다 전용 퍼시스턴트 스토리지가 필요하다. 쿠버네티스에서 퍼시스턴트 스토리지를 요청하고 파드와 연관시키는 방법은 PV와 PVC를 통하는 것이다. 파드를 생성하는 것과 같은 방식으로, PVC를 생성하기 위해 스테이트풀세트는 volumeClaimTemplates 요소를 사용한다. 이 추가 속성은 persistfulVolumeClaim 요소를 사용하는 레플리카세트와의 주요한 차이점 중 하나다.  

스테이트풀세트는 사전에 정의된 PVC를 참조하는 대신, 파드를 생성할 때 volumeClaimTemplates을 사용하여 PVC를 즉시 생성한다. 이 메커니즘을 통해 모든 파드는 스테이트풀세트의 replicas 수를 변경해, 스케일 업할 때뿐만 아니라 초기 생성 중에 자체 전용 PVC를 얻을 수 있다.  

아마 모두 눈치챘겠지만, PVC는 파드와 함께 생성되고 파드와 관련이 있다고 지금까지 설명하면서도, PV에 대해서는 아무 언급도 하지 않았다. 스테이트풀세트는 어떤 방식으로든 PV를 관리하지 않기 때문이다. 파드를 위한 스토리지는 관리자가 사전에 프로비저닝(provisioning)하거나, 요청된 스토리지 클래스를 기반으로 PV 프로비저너(provisioner)가 요구 시 즉시(on-demand) 프로비저닝하여 스테이트풀 파드에서 사용할 수 있도록 준비되어 있어야 한다.  

여기에서 비대칭 동작을 주목하기 바란다. 스테이트풀세트를 스케일 업(replicas 수 늘리기)하면 새로운 파드와 관련 PVC가 생성된다. 또한 스케일 다운하면 파드가 삭제되지만 PVC(또는 PV)는 삭제되지 않으므로 PV를 재사용하거나 삭제할 수 없으며, 쿠버네티스는 해당 스토리지를 해제할 수 없다. 이 동작은 의도적으로 설계된 것으로, 스테이트풀 애플리케이션의 스토리지가 중요하고 실수로 인한 스케일 다운으로 인해 데이터 손실이 발생하지 않아야 한다는 추정에 의해 만들어졌다. 스테이트풀 애플리케이션이 의도적으로 스케일 다운되고, 데이터가 다른 인스턴스에 복제되고 비워졌다고 확신하는 경우, PVC를 수동으로 삭제하면 이후에 PV 재사용이 가능하다.  

##### 11.2.2. 네트워킹  
<br/>

스테이트풀세트에 의해 생성된 각 파드에는 스테이트풀세트의 이름과, 순서를 나타내는 인덱스(0부터 시작)로 만들어진 고정된 식별자가 있다. 파드 이름은 임의의 접미사가 포함된 레플리카세트의 파드 이름 생성 메커니즘과는 달리, 예측 가능한 형식으로 생성된다. 스케일 가능한 전용 퍼시스턴트 스토리지는 스테이트풀 애플리케이션의 필수 요수며 네트워킹도 마찬가지다.  

다음 예제는 헤드리스(headless) 서비스(Service)를 정의한다. 헤드리스 서비스에서 clusterIP: None은 해당 서비스가 큐브프록시(kube-proxy)에 의해 처리되지 않으며, 클러스터 IP 할당이나 로드 밸런싱이 되지 않음을 의미한다. 그렇다면 왜 헤드리스 서비스가 필요한 걸까?  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: random-generator
spec:
  clusterIP: None # 서비스를 헤드리스 서비스로 선언
  selector:
    app: random-generator
  ports:
  - name: http
    port: 8080
```

레플리카세트를 통해 생성된 스테이트리스 파드들은 동일하다고 가정하며 요청이 어느 파드에 도달하든 상관없다(따라서 일반 서비스(Service)로 로드 밸런싱함). 그러나 스테이트풀 파드는 서로 다르므로 좌표로 특정 파드에 도달해야 한다.  

셀렉터가 있는 헤드리스 서비스(.selector.app == random-generator에 주목하자)는 이를 확실히 가능하게 한다. 이러한 서비스(Service)는 API 서버에 엔드포인트 레코드를 생성하고, 서비스를 지원하는 파드를 직접 가리키는 A 레코드(주소)를 리턴하는 DNS 항목을 생성한다. 간단히 말해 각 파드는 클라이언트가 예측 가능한 방식으로 직접 연결할 수 있는 DNS 항목을 갖는다. 예를 들어 random-generator 서비스가 default 네임스페이스에 속하는 경우, 서비스 이름 앞에 파드의 이름을 붙여 정규화된 도메인 이름인 rg-0.random-generator.default.svc.cluster.local을 통해 rg-0 파드에 도달할 수 있다. 이와 같은 매핑을 통해, 클러스터된 애플리케이션의 도 다른 구성원이나 또 다른 클라이언트가 원하는 경우, 특정 파드에 도달할 수 있다.  

또한 SRV 레코드에 대한 DNS 참조(lookup, 예를 들면 dig SRV random-generator.default.svc.cluster.local을 통해)를 수행해 스테이트풀세트의 관리 서비스(Service)에 등록된 모든 실행 중인 파드를 발견할 수 있다. 이런 메커니즘을 사용하면 어떤 클라이언트 애플리케이션에서도 필요한 경우 동적 클러스터 구성원을 발견할 수 있다. 셀렉터를 기반으로 헤드리스 서비스가 스테이트풀세트에 연결해야 할 뿐만 아니라, 스테이트풀세트는 또한 serviceName: "random-generator"처럼 서비스 이름으로 서비스(Service)에 연결해야 한다.  

반드시 volumeClaimTemplates를 통해 정으된 전용 스토리지일 필요는 없지만, serviceName 필드를 통해 서비스(Service)에 연결하는 것은 필수다. 관리 서비스는 스테이트풀세트가 생성되기 전에 존재해야 하며 세트의 네트워크 식별자를 담당한다. 원하는 경우 스테이트풀 파드에 대한 로드 밸런싱을 제공하는 다른 타입의 서비스를 언제든지 추가로 생성할 수 있다.  

스테이트풀세트는 분산 환경에서 스테이트풀 애플리케이션 관리에 필요한 빌딩 블록 세트와 보장된 동작을 제공한다. 스테이트풀 사용 예에 의미 있는 방식으로 스테이트풀 애플리케이션을 선택하고 사용하는 것은 사용자의 몫이다.  

##### 11.2.3. 식별자  
<br/>

식별자(identity)는 메타 빌딩 블록이며, 여타 모든 스테이트풀 세트 보장은 이것을 기반으로 만들어진다. 스테이트풀세트의 이름을 기반으로, 파드 이름과 식별자를 예측할 수 있다. 그런 다음 해당 식별자를 사용해 PVC의 이름을 지정하고, 헤드리스 서비스를 통해 특정 파드에 도달하는 등의 작업을 수행한다. 파드를 만들어 사용하기에 앞서 모든 파드의 식별자를 예측할 수 있으며, 필요하다면 애플리케이션 자체에서 해당 식별자를 사용할 수 있다.  

##### 11.2.4. 순서성  
<br/>

정의에 따라, 분산 스테이트풀 애플리케이션은 고유하고 고체할 수 없는 여러 인스턴스로 구성된다. 인스턴스의 고유성 외에도 인스턴스화의 순서/위치에 따라 인스턴스가 서로 관련될 수 있으며, 이 경우 순서성(ordinality) 요구사항의 적용된다.  

스테이트풀세트 관점에서, 순서성은 오직 스케일할 때에만 적용된다. 파드는 순서를 나타내는 접미어(0부터 시작)를 붙인 이름을 가지며, 파드 생성 순서는 파드가 스케일 업과 다운(역순으로 n-1dptj 0까지)되는 순서를 정의한다.  

다수의 래플리카로 레플리카세트를 생성하면, 첫 번째 레플리카가 성공적으로 시작될 때까지 기다리지 않고 모든 파드는 함께 스케줄되고 시작된다. 즉 파드가 시작되고 준비될 때 순서는 보장되지 않는다. 이는 레플리카세트를 스케일 다운할 때도 마찬가지다(replicas 수를 변경하거나 삭제함). 레플리카세트에 속하는 모든 파드는 순서와 의존성 없이 동시에 종료되기 시작한다. 이 동작은 더 빠르게 완료할 수도 있겠지만, 이것은 스테이트풀 애플리케이션에 바람직하지 않다. 특히 데이터 분할(partitioning)과 분배(distribution)가 인스턴스 사이에 연관된 경우에는 더욱 그렇다.  

스케일 업 및 다운 중에 적절한 데이터 동기화를 허용하기 위해 스테이트풀세트는 기본으로 순차적 시작 및 종료를 수행한다. 즉 파드는 첫 번째 파드(인덱스 0)에서 시작하고, 해당 파드가 성공적으로 시작된 경우에만 그다음에 스케줄된 파드(인덱스 1)이 시작하고 순차적으로 계속된다. 스케일 다운할 때의 순서는 반대다. 먼저 인덱스가 가장 높은 파드를 종료하고, 성공적으로 종료된 경우에만 다음 하위 인덱스가 종료된다. 이런 순서는 인덱스가 0인 파드가 종료될 때가지 계속된다.  

##### 11.2.5. 기타 기능  
<br/>

스테이트풀세트에는 스테이트풀 애플리케이션의 요구에 맞게 사용자정의(custom)로 지정할 수 있는 또 다른 측면이 있다. 각 스테이트풀 애플리케이션은 고유하며, 스테이트풀세트 모델에 적합하게 만들 경우 세심한 고려가 필요하다. 순조로운 스테이트풀 애플리케이션 적용을 위해 유용하게 쓰일 수 있는 몇 가지 쿠버네티스 기능을 더 살펴보자.  

+ 분할된 업데이트  
이미 실행 중인 스테이트풀 애플리케이션을 업데이트하는 경우(예를 들어 .spec.template 요소를 변경하여), 스테이트풀세트는 나머지 인스턴스에 업데이트를 적용하는 동안 특정 수의 인스턴스가 그대로 유지되도록 보장하는 단계적 롤아웃(예를 들면 카나리아(canary) 릴리스)을 허용한다.  

기본 롤링 업데이트 전략을 사용하면 .spec.updateStrategy.rollingUpdate.partition 번호를 지정해 인스턴스를 분할할 수 있다. 이 파라미터(기본값은 0)는 업데이트를 위해 스테이트풀세트를 분할해야 하는 서수를 나타낸다. 파라미터를 지정하면, 서수 인덱스가 partition보다 크거나 같은 모든 파드는 업데이트되지만 서수가 partition보다 작은 나머지 모든 파드들은 업데이트되지 않는다. 파드가 삭제되는 경우에도 마찬가지여서, 쿠버네티스는 이전 버전으로 파드를 다시 만든다. 이 기능을 사용하면 클러스터된 스테이트풀 애플리케이션을 부분적으로 업데이트(예를 들면 쿼럼 유지)한 다음 partition을 다시 0으로 설정해 나머지 클러스터에 대해 변경 사항을 롤아웃(roll out)할 수 있다.  

+ 병렬 배포  
spec.podManagementPolicy를 Parallel로 설정하면, 스테이트풀세트는 모든 파드를 병렬로 시작하거나 종료한다. 즉 다음 파드로 넘어가기 전에, 파드가 실행되고 준비도거나 혹은 완전히 종료되기를 기다리지 않는다. 스테이트풀 애플리케이션에서 순차적 처리가 필요하지 않다면 이 옵션을 사용해 운영 절차의 속도를 높일 수 있다.

+ 최대한 하나(at-most-one) 보장  
고유성(uniqueness)은 스테이트풀 애플리케이션 인스턴스의 기본 속성 중 하나로, 쿠버네티스는 스테이트풀세트의 두 파드가 동일한 식별자를 갖거나 동일한 PV에 바인딩되지 않게 한다. 반대로 레플리카세트는 인스턴스에 최소한 X개 보장(At-Least-X-Guarantee)을 제공한다. 예를 들어 레플리카가 2로 설정된 레플리카세트는 2개 이상의 인스턴스를 실행 상태로 유지하려고 한다. 경우에 따라 해당 숫자가 더 높아질 가능성이 있더라도, 컨트롤러의 우선순위를 통해 파드 수를 지정된 숫자 아래로 떨어지지 않게 유지한다. 새로운 파드로 교체할 때 이전 파드가 완전히 종료되지 않았다면, 지정된 수 이상의 레플리카가 실행될 수 있다. 또는 NotReady 상태라서 쿠버네티스 노드에 도달할 수 없지만 여전히 파드가 실행 중인 경우에도, 레플리카 수는 더 많아질 수 있다. 이 시나리오에서 레플리카세트의 컨트롤러는 정상적인 노드에서 새로운 파드를 시작해 요청한 것보다 더 많은 파드를 실행할 수 있다. 이것은 최소한 X(At-Least-X)라는 의미 내에서 모두 허용된다.  

반면 스테이트풀세트 컨트롤러는 중복 파드가 없도록 가능한 모든 체크를 수행하므로 최대한 하나(At-Most-One) 보장이 가능하다. 이전 인스턴스가 완전히 종료된 것으로 확인되지 않으면 스테이트풀세트 컨트롤러는 파드를 다시 시작하지 않는다. 노드가 작동하지 않을 때, 쿠버네티스가 파드가 (그리고 모든 노드가) 종료되었음을 확인할 수 없으면, 스테이트풀세트 컨트롤러는 다른 노드에 새로운 파드를 스케줄하지 않는다. 스테이트풀세트의 최대한 하나라는 의미가 이러한 규칙을 결정한다.  

이러한 보장을 무시하고 스테이트풀세트에 중복 파드가 생길 수도 있으므로, 적극적인 개입이 필요하다. 예를 들어 물리적 노드가 여전히 실행 중일 때, API 서버에서 연결할 수 없는 노드 자원 객체를 삭제하면 이 보장은 무시된다. 이러한 작업은 노드가 작동 중지되었거나 전원이 꺼진 것으로 확인되고, 파드 프로세스가 해당 노드에서 실행 중이지 않은 경우에만 수행해야 한다. 또는 kubectl delete pods &#95;&lt;pod&gt;&#95; --grace-period=0 --force를 사용해 파드를 강제로 삭제하면, 파드가 종료되었다는 확인이 큐블릿(Kubelet)에서 오기를 기다리지 않는다. 이 작업을 수행하면 파드는 API 서버에서 즉시 지워지고, 그에 따라 스테이트풀세트 컨트롤러가 대체 파드를 시작함으로써 중복으로 어이질 수 있다.  

#### 11.3. 참고 자료  
<br/>

+ 스테이트풀 서비스 예제(http://bit.ly/2Y7SUN2)
+ 스테이트풀세트 기본(http://bit.ly/2r0boiA)
+ 스테이트풀세트(http://bit.ly/2HGm6oE)
+ 스테이트풀세트로 카산드라(Cassandra) 배포(http://bit.ly/2HBLNXA)
+ 분산 시스템 코디네이터인 주키퍼 실행(http://bit.ly/2JmNPNQ)
+ 헤드리스 서비스(http://bit.ly/2v7Z19P)
+ 스테이트풀세트 파드의 강제 삭제(http://bit.ly/2OeuRrh)
+ 쿠버네티스에서 스테이트풀 애플리케이션의 안전한 스케일 다운(http://bit.ly/2Fk0mgK)
+ 스테이트풀 애플리케이션의 설정 및 배포(http://bit.ly/2UsbkJt)  

### 12. 서비스 디스커버리  
<br/>

서비스 디스커버리(Service Discovery) 패턴은 서비스의 클라이언트가 서비스를 제공하는 인스턴스에 접근할 수 있는 안정적인 종단점(endpoint)을 제공한다. 이를 위해 쿠버네티스는 서비스 컨슈머와 프로듀서가 클러스터 안에 있는지 여부에 따라 여러 메커니즘을 제공한다.  

#### 12.1. 문제  
<br/>

쿠버네티스 워크로드의 더욱 일반적인 사용 예는 대개 클러스터내의 다른 파드나 외부 시스템에서 들어오는 HTTP 연결 형식의 외부 자극을 기다리며 장기간 실행되는 서비스가 있을 경우에 발생한다. 이 경우 서비스 컨슈머에는, 스케줄러에 의해 동적으로 배치되고 때로는 탄력적으로 스케일 업 및 스케일 다운되는 파드를 디스커버리(discovery)하는 메커니즘이 필요하다.  

#### 12.2. 해결책  
<br/>

쿠버네티스가 등장하기 이전에 가장 일반적인 서비스 디스커버리 메커니즘은 클라이언트 측의 디스커버리를 통한 것이었다. 이 구조에서 서비스 컨슈머가 여러 인스턴스로 확장될 수 있는 또 다른 서비스를 호출해야 하는 경우, 서비스 컨슈머는 서비스 인스턴스의 레지스트리를 검토하고 호출할 서비스 에이전트를 선택할 수 있는 디스커버리(discovery) 에이전트르 갖게 됐다. 예를 들어 컨슈머 서비스 내에 내장된 에이전트(예를 들면 주키퍼 클라이언트, 컨설(Consul) 클라이언트, 리본(Ribbon) 등) 또는 프라나(Prana)처럼 동일한 장소에 배치된 여타 프로세스가 레지스트리에서 서비스를 조회했다.  

하지만 쿠버네티스가 등장한 이후부터는 배포, 정상상태 확인, 치유(healing), 자원 격리 등과 같은 분산 시스템에서 비 기능적으로 해야 할 일의 상당수가 플랫폼으로 이동하고 있으며, 서비스 디스커버리 및 로드 밸런싱도 마찬가지다. 서비스 지향 아키텍처(SOA)의 정의를 사용하는 경우라면, 서비스 프로바이더 인스턴스는 서비스 기능을 제공하면서 서비스 레지스트리에 여전히 자신을 등록해야 하며, 서비스 컨슈머는 레지스트리에 있는 정보에 접근해 서비스에 도달해야 한다.  

쿠버네티스 영역에서는, 파드로 구현된 서비스 인스턴스를 동적으로 디스커버리할 수 있도록 서비스 소비자가 고정된 가상 서비스(Service) 종단점을 호출하는 모든 것이 배후에서 일어난다.  

언뜻 보기에 서비스 디스커버리는 단순한 패턴처럼 보일 수 있다. 그러나 서비스 컨슈머(consumer)가 클러스터 내부 또는 외부에 있는지 여부와 서비스 프로바이더(provider)가 클러스터 내부 또는 외부에 있는지에 따라 이 패턴을 구현하는 데 여러 메커니즘을 사용할 수 있다.  

#### 12.2.1. 내부 서비스 디스커버리  
<br/>

쿠버네티스에서 실행해려고 하는 웹 애플리케이션이 있다고 가정해보자. 몇 개의 레플리카로 디플로이먼트(Deployment)를 생성하자마자 스케줄러는 파드를 적절한 노드에 위치시키고 각 파드는 시작하기 전에 할당된 클러스터 IP 주소를 얻는다. 그런데 다른 파드 내의 또 다른 클라이언트 서비스가 해당 웹 애플리케이션 종단점을 사용하려는 경우, 서비스 프로바이더 파드의 IP 주소를 미치 알기는 어렵다.  

이것은 쿠버네티스 서비스(Service) 자원이 다룰 문제다. 서비스 자원은 동일한 기능을 제공하는 파드 모음에 대해 일정하고 안정적인 진입점(entry point)을 제공한다. 서비스(Service)를 생성하는 가장 쉬운 방법은 kubectl expose를 통해 하나의 파드, 또는 디플로이먼트나 레플리카세트(ReplicaSet)의 여러 파드에 대해 서비스를 생성하는 것이다. 이 명령은 clusterIP라는 가상 IP 주소를 생성하고 자원에서 파드 셀렉터와 포트 번호를 가져와서 정의를 생성한다. 하지만 정의를 완전히 제어하기 위해 다음 예제에 표시된 대로 서비스를 수동으로 만든다.  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: random-generator
spec:
  selector: # 파드 레이블과 매핑하는 셀렉터
    app: random-generator
  ports:
  - port: 80 # 해당 서비스에 접속할 수 있는 포트
    targetPort: 8080 # 파드가 기다리는 포트
    protocol: TCP
```

이 예제에서의 정의는 random-generator라는 이름(이 이름은 이후에 디스커버리를 위해 중요함)의 서비스(Service)를 생성한다. 그리고 type: ClusterIP(기본값임)는 포트 80으로 TCP 연결을 받아들이고 셀렉터 app: random-generator로 설정된 모든 파드에 포트 8080으로 라우팅한다. 파드가 언제 또는 어떻게 생성되는지는 고려하지 않고, 매핑하는 모든 파드가 라우팅 대상이 된다.  

여기서 기억해야 할 중요한 점은, 일단 서비스(Service)가 생성되면 쿠버네티스 클러스터 내에서만 접근할 수 있는 clusterIP가 할당되고(이런 이유로 이름이 clusterIP다), 서비스 정의가 존재하는한 IP는 변경되지 않는다는 사실이다. 그러나 클러스터 내 여타 애플리케이션에서 이 동적으로 할당된 clusterIP가 무엇인지 어떻게 알 수 있을까? 다음과 같은 2가지 방법이 있다.  

+ 환경 변수를 통한 디스커버리  
쿠버네티스가 파드를 가동시키면 해당 환경 변수는 해당 시점에 존재하는 모든 서비스(Service)의 세부 정보로 채워진다. 예를 들어 포트 80에서 수신하는 random-generator 서비스는 새로 시작하는 모든 파드에 삽입된다. 해당 파드를 실행하는 애플리케이션은 사용해야 하는 서비스 이름을 알고 있으며 이러한 환경 변수를 읽도록 코딩될 수 있다. 이와 같은 참조(lookup)는 어떠한 언어로 작성된 애플리케이션이든 사용할 수 있는 간단한 메커니즘이며, 개발이나 테스트 목적으로 쿠버네티스 클러스터 외부에서 쉽게 애플리케이션할 수 있다. 이 메커니즘의 주요 문제는 서비스 생성에 대한 시간적 의존성(temporal dependency)이다. 환경 변수는 이미 실행 중인 파드에 삽입할 수 없으므로 서비스 좌표는 해당 서비스가 쿠버네티스에서 생성된 후 시작된 파드에 대해서만 사용할 수 있다. 이를 위해서는 서비스에 의존하는 파드를 시작하기 전에 서비스를 정의해야 한다. 그렇지 않은 경우라면 파드를 다시 시작해야 한다.

+ DNS 참조를 통한 디스커버리  
쿠버네티스는 모든 파드가 사용하도록 자동으로 설정된 DNS 서버를 운영한다. 또한 새 서비스(Service)를 만들면 해당 서비스는 모든 파드에서 사용할 수 있는 새로운 DNS 항목을 자동으로 할당받는다. 클라이언트가 접근하려는 서비스 이름을 알고 있다고 가정하면 random-generator.default.svc.cluster.local과 같은 전체 주소 도메인 네임(FQDN, fully qualified domain name)으로 서비스에 도달할 수 있다. 여기서 random-generator는 서비스 이름이고, default는 네임스페이스 이름이며, svc는 서비스 자원임을 나타내며, cluster.local은 클러스터별 접미어이다. 원하는 경우 클러스터 접미사와 네임스페이스를 생략하고 동일한 네임스페이스에서 서비스에 접근할 수 있다.  

DNS 서버는 서비스(Service)가 정의되는 즉시 DNS 서버가 모든 파드에 대한 모든 서비스를 참조(lookup)할 수 있기 때문에 DNS 디스커버리 메커니즘에는 환경 변수 기반 메커니즘의 단점이 없다. 그러나 비표준이거나 서비스 컨슈머가 알 수 없는 경우에는 사용할 포트 번호를 찾기 위해 여전히 환경 변수를 사용해야 한다.  

다음은 또 다른 타입을 기반으로 생성된 type: ClusterIP 서비스의 몇 가지 상위 레벨 특성이다.  

+ 다중 포트  
단일 서비스(Service) 정의는 여러 소스와 대상 포트를 지원할 수 있다. 즉 파드가 포트 8080의 HTTP와 포트 8443의 HTTPS를 모두 지원하는 경우, 서비스 2개를 따로 정의할 필요가 없다. 단일 서비스는 일례로 80과 443 두 포트 모두를 노출할 수 있다.

+ 세션 어피니티  
새 요청이 있을 경우 서비스(Service)는 기본적으로 파드를 임의로 선택하여 연결하는데, 이는 sessionAffinity: ClientIP를 사용해 변경할 수 있다. 이를 통해 동일한 클라이언트 IP에서 오는 모든 요청에 대해 동일한 파드를 선택한다. 여기에서 명심해야 할 부분은 쿠버네티스 서비스는 L4 전송 계층 로드 밸런싱을 수행하며, 네트워크 패킷을 조사하거나 HTTP 쿠키 기반 세션 어피니티(session affinity)같은 애플리케이션 레벨 로드 밸런싱을 수행할 수 없다는 것이다.

+ 레디니스 점검  
만약 파드에 레디니스(readiness) 체크가 정의되어 있고, 이것이 실패한 경우, 레이블 셀렉터가 해당 파드와 일치하더라도 호출할 서비스(Service) 엔드포인트 목록에서 해당 파드는 제거된다.

+ 가상 IP  
type: ClusterIP로 서비스(Service)를 생성하면 고정 가상 IP 주소를 얻는다. 그러나 해당 IP 주소는 네트워크 인터페이스에 해당하지 않으며 실제로 존재하지 않는다. 모든 노드에서 실행되는 큐브프록시(kube-proxy)는 새로운 서비스를 선택하고 해당 가상 IP로 향하는 네트워크 패킷을 캡처해 선택된 파드 IP로 교체하는 규칙을 노드의 iptables에 업데이트한다. iptables의 규칙은 ICMP 규칙을 추가하지 않고 TCP나 UDP 같은 서비스 정의에 지정된 프로토콜만 추가한다. 결과적으로 서비스(Service)의 IP 주소로 ICMP 프로토콜을 사용하는 핑(ping)을 할 수 없다. 그러나 TCP(HTTP 요청 등)를 통해 서비스 IP 주소에 접근할 수 있다.

+ ClusterIP 선택  
서비스(Service) 생성 중에는 .spec.clusterIP 필드로 사용할 IP를 지정할 수 있다. IP 주소는 유효해야 하며 사전에 정의된 범위 내에 있어야 한다. 딱히 권장하지는 않지만 이 옵션은 특정 IP 주소를 사용하도록 구성된 레거시 애플리케이션을 처리하거나 재사용하려는 기존 DNS 항목이 있는 경우 유용할 수 있다.  

쿠버네티스 서비스(Service)의 type: ClusterIP는 클러스터 내에서만 접근할 수 있다. 셀렉터가 일치하는 파드를 디스커버리하기 위해 사용되며 가장 일반적으로 사용되는 타입이다.  

##### 12.2.2. 수동 서비스 디스커버리  
<br/>

selector를 설정해 서비스(Service)가 생성되면, 쿠버네티스는 종단점 자원 목록에서 selector와 일치하고 준비된 파드 목록을 추적한다. 다음 예제에서는 kubectl get endpoints random-generator로 서비스 대신 생성된 모든 엔드포인트를 확인할 수 있다. 클러스터 내 파드로 연결을 리디렉션(redirection)하는 대신 외부 IP 주소와 포트로 연결을 리디렉션할 수도 있다. 이 방법은 다음 예제처럼 서비스의 selector 정의를 생략하고 엔드포인트 자원을 수동으로 생성해 수행할 수 있다.  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: external-service
spec:
  type: ClusterIP
  ports:
  - protocol: TCP
    ports: 80
```

다음 예제에서는 서비스(Service)와 이름이 같고 대상 IP와 포트를 포함하는 종단점 자원을 정의한다.  

```yaml
apiVersion: v1
kind: Endpoints
metadata:
  name: external-service # 이름은 해당 종단점에 접근하는 서비스(Service)와 일치해야 한다.
subnets:
  - addresses:
    - ip: 1.1.1.1
    - ip: 2.2.2.2
    ports:
    - port: 8080
```

해당 서비스(Service) 또한 클러스터 내에서만 접근할 수 있으며 이전의 서비스와 동일한 방식으로 환경 변수 또는 DNS 참조(lookup)을 통해 사용할 수 있다. 여기서의 차이점은 종단점 목록이 수동으로 유지 관리되며 값은 일반적으로 클러스터 외부의 IP 주소를 가리킨다는 점이다.  

외부 자원에 연결하는 것이 이 메커니즘의 가장 일반적인 용도이지만, 이 방법이 유일한 것은 아니다. 엔드포인트는 파드의 IP 주소를 가질 수 있지만, 또 다른 서비스(Service)의 가상 IP 주소는 가질 수 없다. 서비스의 좋은 점 중 하나는 서비스 IP 주소 변경으로 이어지는 자원 정의를 삭제하지 않고 셀렉터를 추가하거나 제거해 외부 또는 내부 프로바이더를 가리킬 수 있다는 점이다. 따라서 서비스 컨슈머는 처음에 설정된 것과 동일한 서비스 IP 주소를 계속 사용할 수 있으며, 실제 서비스 프로바이더의 구현은 클라이언트에 영향을 주지 않고 온프레미스(on-premises) 환경에서 쿠버네티스로 마이그레이션할 수 있다.  

이 수동 목적지 설정 범주에는 다음 예제에서 볼 수 있듯이 서비스(Service) 타입이 하나 더 있다.  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: database-service
spec:
  type: ExternalName
  externalName: my.database.example.com
  ports:
  - port: 80
```

이 서비스(Service) 정의에도 selector가 없고 타입은 ExternalName이다. 이는 구현 관점에서 중요한 차이점이다. 이 서비스 정의는 DNS만 사용해 externalName이 가리키는 컨텐츠에 매핑된다. IP 주소로 프록시를 통하지 않고 DNS CNAME을 사용해 외부 종단점에 대한 엘리어스(alias)를 생성하는 방법이다. 그러나 기본적으로는, 클러스터 외부에 있는 종단점에 대한 쿠버네티스 추상화를 제공하는 또 다른 방법이다.  

##### 12.2.3. 클러스터 외부의 서비스 디스커버리  
<br/>

쿠버네티스 클러스터는 외부와 연결되어 있다. 즉 파드에서 외부로 연결되어 있고, 또한 파드에서 제공하는 종단점에 접근하기를 원하는 외부 애플리케이션이 있으므로 그 반대도 요구된다. 클러스터 외부에 있는 클라이언트가 파드에 어떻게 접근할 수 있을지 살펴보겠다.  

서비스(Service)를 생성해 클러스터 외부에 노출시키는 첫 번째 방법은 type: NodePort다.  

다음 예제의 정의는 예전 방법처럼, 셀렉터 app: random-generator와 매핑하는 파드를 서빙하고, 가상 IP 주소의 포트 80에서 연결을 받아들이고, 선택된 파드의 포트 8080으로 각 연결을 라우팅하는 서비스(Service)를 만든다. 하지만 이 모든 정의 외에 여기서는 추가로, 모든 노드에 포트 30036을 예약하고 들어오는 연결을 서비스(Service)로 전달한다. 이와 같은 예약 덕분에, 서비스는 모든 노드의 전용 포트를 통한 외붸 접근뿐만 아니라 가상 IP 주소를 통한 내부 접근도 가능해진다.  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: random-generator
spec:
  type: NodePort # 모든 노드에서 포트 공개
  selector:
    app: random-generator
  ports:
  - port: 80
    targetPort: 8080
    nodePort: 30036 # 고정된 포트(사용 가능해야 함)을 직접 지정하지 않으면 임의로 선택된 포트가 지정됨
    protocol: TCP
```

서비스를 노출하는 이 방법은 좋은 접근 방법처럼 보일 수 있지만 단점이 있다. 고유한 특성 몇 가지를 살펴보겠다.  

+ 포트 번호  
nodePortL 30036으로 특정 포트를 직접 선택하는 대신, 쿠버네티스가 특정 범위 내에서 사용 가능한 포트를 직접 선택하게 할 수 있다.

+ 방화벽 규칙  
NodePort는 모든 노드에서 포트를 열기 때문에, 외부 클라이언트가 노드 포트에 접근할 수 있도록 추가 방화벽 규칙을 설정해야 할 수도 있다.

+ 노드 선택  
외부 클라이언트는 클러스터내의 모든 노드에 대한 연결을 열 수 있다. 그러나 해당 노드를 사용할 수 없어서 또 다른 정상적인 노드에 연결하는 것은 클라이언트 애플리케이션이 해야할 일이다. 이를 위해서, 정상적인 노드를 선택하고 대체 작동을 수행하는 로드 밸런서는 노드 앞세 배치하는 것이 좋다.

+ 파드 선택  
클라이언트가 노드 포트를 통해 연결을 열면, 클라이언트는 연결이 열린 동일한 노드 또는 다른 노드에 있을 수 있는 임의 선택된 파드로 라우팅된다. externalTrafficPolicy: Local을 서비스(Service) 정의에 추가해 쿠버네티스가 항상 연결이 열린 노드에서 파드를 선택하게 함으로써 추가 홉(hop)을 피할 수도 있다. 이 옵션이 설정되면 쿠버네티스는 다른 노드에 있는 파드에 연결할 수 없으므로, 이것이 문제가 될 수도 있다. 이를 해결하려면, 모든 노드에 파드를 배포(데몬 서비스 등으로) 하거나 어느 노드에 정상적인 파드가 배포되어 있는지를 클라이언트가 알게 해야 한다.

+ 소스 주소  
다양한 타입의 서비스(Service)로 전송되는 패킷의 소스 주소에는 몇 가지 특성이 있다. 특히 NodePort 타입을 사용하는 경우 클라이언트 주소는 소스 NAT(SNAT)된다. 즉 클라이언트 IP 주소를 포함하는 네트워크 패킷의 소스 IP 주소가 노드의 내부 주소로 대체된다. 예를 들어 클라이언트 애플리케이션의 패킷을 노드 1로 보내면, 소스 주소는 노드 주소로 바뀌고 목적지 주소는 파드 주소로 바뀌어 패킷을 노드 2(파드가 있는 노드)로 전달한다. 이때 파드가 네트워크 패킷을 수신하면, 소스 주는 원래 클라이언트의 주소와 일치하지 않으며 노드 1의 주소와 동일하다. 이를 방지하기 위해 앞서 설명한 대로 externalTrafficPolicy: Local을 설정해서 트래픽을 오직 노드 1에 있는 파드에만 전달할 수 있다.  

외부 클라이언트를 위한 또 다른 서비스 디스커버리 방법은 로드 밸런서를 이용하는 것이다. 모든 노드에서 해당 포트를 열어서 type: ClusterIP를 사용하는 일반 서비스(Service) 상에 type: NodePort 서비스를 구축하는 방법을 살펴봤다. 이 접근법의 제약사항은 클라이언트 애플리케이션이 정상적인 노드를 선택하기 위해 여전히 로드 밸런서가 필요하다는 점이다. LoadBalancer 타입의 서비스(Service)는 이러한 제약을 해소해준다.  

일반적인 서비스(Service)를 만들고 type: NodePort로 모든 노드에서 포트를 여는 것 외에도 클라우드 제공업체의 로드 밸런서를 사용해 외부로 서비스를 노출할 수 있다. 전용 로드 밸런서는 쿠버네티스 클러스터의 게이트웨이 역할을 한다.  

따라서 LoadBalancer 타입의 서비스(Service)는 클라우드 제공업체가 쿠버네티스에 로드 밸런서를 지원하고 프로비저닝한 경우에만 작동한다.  

type: LoadBalancer를 지정해 로드 밸런서가 있는 서비스(Service)를 생성할 수 있다. 그러면 쿠버네티스는 다음 예제에 표시된 것처럼 .spec이나 .status 필드에 IP 주소를 추가한다.  

```yaml
apiVersion: v1
kind: Service
metadata:
  name: random-generator
spec:
  type: LoadBalancer
  clusterIP: 10.0.171.239 # 이용 가능할 때, 쿠버네티스가 clusterIP와 loadBalancerIP를 할당함
  loadBalancerIP: 78.11.24.19
  selector:
    app: random-generator
  ports:
  - port: 80
    targetPort: 8080
    protocol: TCP
status: # status 필드는 쿠버네티스에 의해 관리되며, 인그레스(ingress) IP를 추가함
  loadBalancer:
    ingress:
    - ip: 146.148.47.155
```

이 정의를 적용하면, 외부 클라이언트 애플리케이션은 노드를 선택하고 파드 위치를 찾는 로드 밸런서에 대한 연결을 열 수 있다. 로드 밸런서 프로비저닝이 수행되는 정확한 방법과 서비스 디스커버리는 클라우드 제공업체마다 다르다. 일부 클라우드 제공업체는 로드 밸런서 주소를 정의할 수 있지만 일부는 허용하지 않는다. 또한 일부는 소스 주소를 유지하기 위한 메커니즘을 제공하지만 일부는 로드 밸런서 주소로 이를 대체한다. 따라서 이용 중인 클라우드 제공업체가 제공하는 특정 구현을 확인해야 한다.  

또 다른 타입의 서비스(Service)도 이용 가능하다. 헤디리스 서비스(Service)는 전용 IP 주소를 요청하지 않는다. 서비스의 spec: 섹션에 clusterIP: None을 지정해 헤드리스 서비스를 생성한다. 헤드리스 서비스에 대한 백업 파드는 내부 DNS 서버에 추가되며 스테이트풀세트(StatefulSet)의 서비스를 구현하는 데 가장 유용하다.  

##### 12.2.4. 애플리케이션 계층 서비스 디스커버리  
<br/>

지금까지 설명한 메커니즘과 달리 인그레스(ingress)는 서비스 타입이 아니라 서비스(Service) 앞쪽에 위치하며 스마트 라우터(smart router) 및 클러스터의 진입점(entry point) 역할을 하는 별도의 쿠버네티스 자원이다. 인그레스는 일반적으로 외부에서 접근 가능한 URL, 로드 밸런싱, SSL 종료, 이름 기반 가상 호스팅 등을 통해 서비스(Service)에 대한 HTTP 기반 접근을 제공하지만 그 밖의 특수한 인그레스 구현도 있다.  

인그레스가 작동하려면 클러스터에 하나 이상의 실행 중인 인그레스 컨트롤러가 있어야 한다. 다음 예제는 하나의 서비스(Service)를 노출하는 간단한 인그레스를 보여준다.  

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: random-generator
spec:
  backend:
    serviceName: random-generator
    servicePort: 8080
```

쿠버네티스가 실행되는 인프라스트럭처와 인그레스(ingress) 컨트롤러 구현에 따라, 다음 예제의 정의는 외부에서 접근 가능한 IP 주소를 할당하고 포트 80으로  random-generator 서비스(Service)를 노출한다. 그러나 이것은 각 서비스(Service) 정의별로 외부 IP 주소가 필요한 type: LoadBalancer 서비스와 크게 다르지 않다. 인그레스의 가장 큰 장점이라면, 단일 외부 로드 밸런서와 IP를 재사용해 여러 서비스(Service)를 제공하므로 인프라스트럭처 비용을 절감할 수 있다는 점이다.  

HTTP URI 경로를 기반으로 단일 IP 주소를 여러 서비스(Service)로 라우팅하기 위한 간단한 팬아웃(fan-out) 설정은 다음 예제와 같다.  

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: random-generator
  annotations: nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  roles: # 요청 경로를 기반으로 요청을 보내기 위한 인그레스 컨트롤러 전용 규칙
  - http:
      paths:
      - path: / # /cluster-statu를 제외한 모든 요청을 random-generator 서비스(Service)로 리디렉션
        backend:
          serviceName: random-generator
          servicePort: 8080
      - path: /cluster-status # /cluster-status는 cluster-status 서비스(Service)로 리디렉션
        backend:
          serviceName: cluster-status
          servicePort: 80
```

통상의 인그레스 정의와는 달리 모든 인그레스 컨트롤러는 구현이 다르기 때문에 컨트롤러에는 애노테이션을 통해 전달되는 추가 설정이 필요할 수 있다. 인그레스가 올바르게 설정되었다고 거장하면, 위의 예제의 정의는 로드 밸런서를 프로비저닝하고, 2개의 서로 다른 경로로 2개의 서비스(Service)를 제공하는 단일 외부 IP 주소를 갖는다.  

인그레스(ingress)는 쿠버네티스에 가장 강력하고 동시에 가장 복잡한 서비스 디스커버리 메커니즘이다. 동일한 IP 주소로 여러 서비스(Service)를 노출하고 모든 서비스가 동일한 L7(일반적으로 HTTP) 프로토콜을 사용하는 경우에 가장 유용하다.  

###### 12.2.4.1. 오픈시프트 라우트  
<br/>

레드햇 오픈시프트(Red Hat OpenShift)는 인기있는 쿠버네티스 엔터프라이즈 배포판이다. 쿠버네티스와 완벽하게 호환되는 것 외에도 오픈시프트는 몇 가지 추가 기능을 제공한다. 이러한 기능 중 하나가 라우트(Route)이며 인그레스(Ingress)와 매우 유사하다. 이 둘은 실제로 비슷하기 때문에 차이점을 찾기가 어려울 것이다. 라우트는 쿠버네티스 인그레스(Ingress) 객체가 도입되기 전에 시작되었으므로 일종의 인그레스의 전신으로 간주할 수 있다.  

하지만 라우트는 인그레스 객체와 비교해 다음과 같은 몇 가지 기술적인 차이가 있다.  

+ 라우트는 오픈시프트 통합 HAProxy 로드 밸런서에 의해 경로가 자동으로 선택되므로 추가적인 인그레스 컨트롤러를 설치할 필요가 없다. 하지만 오픈시프트 로드 밸런서에게 빌드를 교체할 수도 있다.
+ 라우트는 서비스(Service)로 향하는 구간에 대해 재 암호화(re-encryption) 또는 패스스루(pass-through)같은 추가적인 TLS 종료(termination) 모드를 사용할 수 있다.
+ 라우트는 트래픽 분할을 위한 다중 가중치 백엔드(backend)를 사용할 수 있다.
+ 라우트는 와일드카드(wildcard) 도메인을 지원한다.  

차이점은 이와 같지만, 원한다면 오픈시프트에서 인그레스를 사용할 수도 있다. 오픈시프트를 사용할 때 무엇을 사용할지 선택할 수 있다.  

#### 12.3. 참고 자료  
<br/>

+ 서비스 디스커버리 예제(http://bit.ly/2TeXzcr)
+ 쿠버네티스 서비스(Service)(http://bit.ly/2q7AbUD)
+ 서비스(Service)와 파드를 위한 DNS(http://bit.ly/2Y5jUwL)
+ 서비스(Service) 디버깅(http://bit.ly/2r0igMX)
+ 소스 IP 사용(https://kubernetes.io/docs/tutorials/services/)
+ 외부 로드밸런서 생성(http://bit.ly/2Gs05Wh)
+ 쿠버네티스 NodePort, LoadBalancer, 인그레스 비교(http://bit.ly/2GrVio2)
+ 인그레스(https://kubernetes.io/docs/concepts/services-networking/ingress/)
+ 쿠버네티스 인그레스 vs 오픈시프트 라우트(https://red.ht/2JDDflo)  

### 13. 자기 인식  
<br/>

#### 13.1. 문제  
<br/>

대부분의 사용 예에서 클라우드 네이티브 애플리케이션은 스테이트리스(stateless)이고, 다른 애플리케이션과 관련된 식별자 없이 간단히 처분할 수 있다. 그러나 때때로 이러한 종류의 애플리케이션조차도 애플리케이션 자체에 대한 정보와 실행 중인 환경에 대한 정보가 필요할 수 있다. 여기에는 파드 이름, 파드 IP 주소, 애플리케이션이 배치된 호스트 이름처럼, 런타임 시에만 파악 가능한 정보가 포함될 수도 있다. 또는 특정 자원 요청 및 제한처럼 파드 레벨에서 정의된 기타 정적 정보나, 런타임 시에 사용자가 변경할 수 있는 애노테이션 및 레이블 같은 일부 동적 정보가 포함될 수도 있다.  

예를 들어 사용자는 컨테이너에서 사용 가능한 자원에 따라 애플리케이션 스레드 풀(thread-pool) 크기를 조정하거나, 가비지(grabage) 수집 알고리즘 또는 메모리 할당을 변경하고 싶을 수 있다. 또, 정보를 로깅(logging)하거나 중앙 서버로 메트릭을 보낼 때 파드 이름과 호스트 이름을 사용하고 싶을 수도 있다. 동일한 네임스페이스의 특정 레이블이 있는 다른 파드를 검색해서 클러스터된 애플리케이션에 결합하고 싶을 수도 있다. 이와 같은 다양한 사용 예에 대해, 쿠버네티스가 제공하는 것이 바로 다운워드(Downward) API다.  

#### 13.2. 해결책  
<br/>

이 같은 요구사항에 더불어, 지금부터 설명할 해결책은 컨테이너에만 국한된 것이 아니라, 자원의 메타데이터가 변경되는 모든 동적 환경에서 일어난다. 예를 들어 아마존 웹서비스(이하 AWS)는 인스턴스 메타데이터와 사용자 데이터 서비스를 제공함으로써, 모든 EC2 인스턴스에서 EC2 인스턴스 자체에 대한 메타데이터를 쿼리해 가져올 수 있다. 마찬가지로 AWS ECS에서는 컨테이너가 컨테이너 클러스터에 대한 정보를 가져오기 위해 쿼리할 수 있는 API를 제공한다.  

쿠버네티스 접근 방식은 그보다 더 훌륭하고 사용하기도 쉽다. 다운워드 API를 사용하면 환경변수 및 파일을 이용해 파드에 대한 메타데이터를 컨테이너 및 클러스터에 전달할 수 있다. 이는 컨피그맵(ConfigMap)과 시크릿(Secret)으로 애플리케이션 관련 데이터를 전달하는 것과 동일한 메커니즘이다. 하지만 다운워드 API를 사용하는 경우 데이터를 사용자가 생성하지는 않는다. 대신, 사용자가 해당하는 키를 지정하면 쿠버네티스는 값을 동적으로 채운다.  

여기서 가장 중요한 점은 다운워드 API를 사용하면 메타데이터가 파드에 삽입되어 로컬에서 사용 가능하다는 점이다. 애플리케이션은 클라이언트를 사용할 필요도, 쿠버네티스 API와 상호작용할 필요도 없으며, 쿠버네티스와 무관하게 유지될 수 있다. 다음 예제에서 환경 변수를 통해 메타데이터를 요청하는 것이 얼마나 쉬운지 한번 살펴보자.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    env:
    - name: POD_IP
      valueFrom:
        fieldRef: # 환경 변수 POD_IP는 파드의 속성으로 설정되며 파드 시작 시 생성됨
          fieldPath: status.podIP
    - name: MEMORY_LIMIT
      valueFrom:
        resourceFieldRef:
          containerName: random-generator # 환경 변수 MEMORY_LIMIT는 컨테이너의 메모리 자원 제한 값으로 설정됨. 실제 제한 선언은 여기에 표시하지 않음
          resource: limits.memory
```

이 예제에서는 fieldRef를 사용하여 파드 레벨 메타 데이터에 접근한다. 표시된 키는 fieldRef.fieldPath에서 환경 변수 및 downwardAPI 볼륨으로 사용할 수 있다.  

+ spec.nodeName: 파드를 호스팅하는 노드 이름
+ status.hostIP: 파드를 호스팅하는 노드 IP 주소
+ metadata.name: 파드 이름
+ metadata.namespace: 파드가 실행중인 네임스페이스
+ status.podIP: 파드 IP 주소
+ spec.serviceAccountName: 파드에서 사용하는 서비스 어카운트(ServiceAccount)
+ metadata.uid: 파드의 고유 ID
+ metadata.labels['key']: 파드의 레이블 키의 값
+ metadata.annotations['key']: 파드의 애노테이션 키의 값  

fieldRef와 마찬가지로 resourceFieldRef를 사용하여 파드에 속한 특정 컨테이너에 대한 메타 데이터에 접근할 수 있다. 해당 메타 데이터는 resourceFieldRef.container로 지정할 수 있는 컨테이너에 특정된다. 환경 변수로 사용되면 기본적으로 현재 컨테이너가 사용된다. resourceFieldRef.resource에서 사용 가능한 키는 다음에 표시되어 있다.  

+ request.cpu: 컨테이너의 CPU 요청
+ limits.cpu: 컨테이너의 CPU 제한
+ request.memory: 컨테이너의 메모리 요청
+ limits.memory: 컨테이너의 메모리 제한  

사용자는 파드가 실행되는 동안 레이블과 애노테이션 같은 특정 메타데이터를 변경할 수 있지만, 파드를 다시 시작하지 않으면 환경 변수에 이러한 변경사항이 반영되지 않는다. 그러나 downwardAPI 볼륨은 레이블과 애노테이션에 대한 업데이트를 반영할 수 있다. 앞서 설명한 각 필드 외에, downwardAPI 볼륨은 metadata.labels과 metadata.annotations 참조로 모든 파드 레이블과 애노테이션을 파일로 캡처할 수 있다. 다음 예제는 이러한 볼륨의 사용 방법을 보여준다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    valumeMounts:
    - name: pod-info # 다운워드 API의 값은 파일로 파드에 마운트될 수 있다.
      mountPath: /pod-info
  volumes:
  - name: pod-info
    downwardAPI:
      items:
      - path: labels # labels파일은 한 줄씩 name=value 형식으로 모든 레이블을 포함한다. 이 파일은 레이블이 변경될 때 업데이트된다.
        fieldRef:
          fieldPath: metadata.labels
      - path: annotations # 레이블과 동일한 형식으로, annotations 파일은 모든 애노테이션을 포함한다.
        fieldRef:
          fieldPath: metadata.annotations
```

볼륨을 사용하면 파드가 실행되는 동안 메타데이터가 변경되는 경우, 볼륨 파일에 반영된다. 그러나 파일 변경을 감지하고 그에 따라 업데이트된 데이터를 읽는 것은 이것을 사용하는 애플리케이션에 항상 달려 있다. 만약 이러한 기능이 애플리케이션에서 구현되지 않은 경우에는 예전과 같은 방법으로 파드를 다시 시작해야 할 것이다.  

#### 13.3. 정리  
<br/>

대부분의 경우 애플리케이션은 자기 인식(self-awareness)이 필요하며 애플리케이션 자체와 실행환경에 대한 정보를 필요로 한다. 쿠버네티스는 자체 분석(introspection) 및 메타데이터 삽입을 위한 비 침입(nonintrusive) 메커니즘을 제공한다. 다운워드(downward) API의 단점 중 하나는 참조할 수 있는 고정된 수의 키를 제공한다는 것이다. 애플리케이션이 여타 자원이나 클러스터 관련 메타데이터 등의 추가 데이터를 필요로 하는 경우에는 API 서버에 쿼리해야 한다.  

동일한 네임스페이스에서 특정 레이블이나 애노테이션을 가진 또 다른 파드를 발견하기 위해, API 서버에 쿼리하는 많은 애플리케이션이 이러한 기법을 사용하고 있다. 그런 다음, 애플리케이션은 발견된 파드와 클러스터를 형성하고, 상태를 동기화하는 등의 작업을 할 수 있다. 이런 기법은 또한, 해당하는 파드를 발견하고 계측을 시작하려는 모니터링 애플리케이션에 의해 사용되기도 한다.  

쿠버네티스 API 서버와 상호작용해서 다운워드 API가 제공하는 것 이상의 많은 자체 참조 정보를 얻을 수 있고 다양한 언어에서 사용할 수 있는 클라이언트 라이브러리가 존재한다.  

#### 13.4. 참고 자료  
<br/>

+ 자기 인식 예제(http://bit.ly/2TYBXpc)
+ 파일을 사용해 파드 정보를 컨테이너에 제공(http://bit.ly/2CoZyFy)
+ 환경 변수를 사용해 파드 정보를 컨테이너에 노출(http://bit.ly/2JpuHPe)
+ 아마존 ECS 컨테이너 에이전트 자체분석(https://amzn.to/2JnVXgX)
+ 인스턴스 메타데이터와 사용자 데이터(http://amzn.to/1Ci0fTl)  

### 14. 초기화 컨테이너  
<br/>

초기화 컨테이너(Init Container) 패턴을 사용하면 초기화 관련 작업에 개별 수명주기를 제공해 메인 애플리케이션 컨테이너와 분리할 수 있다.  

#### 14.1. 문제  
<br/>

초기화(Initialization)는 많은 프로그래밍 언어의 보편적인 관심사 중 하나다. 초기화를 언어의 일부로 간주하는 언어도 있는 한편, 이름 규약과 패턴을 써서 구성체(construct)를 이니셜라이저(Initializer)로 나타내는 언어도 있다. 예를 들어 자바 프로그래밍 언어에서 일부 설정이 필요한 객체를 인스턴스화하기 위해 생성자(constructor, 또는 복잡한 사용 사례를 위한 정적 블록)를 사용한다. 생성자는 객체 내에서 제일 먼저 실행되도록 보장되며, 런타임을 관리해 한 번만 실행되도록 보장된다. 또한 생성자를 사용해 필수 파라미터 같은 필수조건을 검증할 수 있고, 인수 또는 기본값을 사용해 인스턴스 필드를 초기화할 수 있다.  

초기화 컨테이너도 이와 비슷하지만, 클래스 레벨(class level)이 아니고 파드 레벨(Pod level)이다. 파드 안에 주요 애플리케이션을 형성하는 하나 이상의 컨테이너가 있는 경우, 이들 컨테이너는 시작하기 전에 필요조건이 있을 수 있다. 여기에는 파일 시스템, 데이터베이스 스키마 설정, 애플리케이션 시드 데이터(seed data) 설치 등을 위한 특수 권한 설정이 포함될 수도 있다. 또한 이 초기화 로직에는 애플리케이션 이미지에 포함될 수 없는 도구와 라이브러리가 필요할 수 있다. 보안상의 이유로 애플리케이션 이미지에 초기화 작업을 수행할 권한이 없을 수도 있다. 또는 외부 의존성이 충족될 때까지 애플리케이션 시작을 지연시켜야 할 수도 있다. 이러한 모든 사용 예에 적용할 수 있도록, 쿠버네티스는 초기화 작업을 기본 애플리케이션 업무와 분리하는 패턴을 구현하는 데 초기화 컨테이너를 이용한다.  

#### 14.2. 해결책  
<br/>

쿠버네티스의 초기화 컨테이너는 파드 정의의 일부며, 파드의 모든 컨테이너는 두 그룹, 즉 초기화 컨테이너와 애플리케이션 컨테이너로 나뉜다. 모든 초기화 컨테이너는 순차적으로 하나씩 실행되며 애플리케이션 컨테이너가 시작되기 전에 모든 초기화 컨테이너가 성공적으로 종료되어야 한다. 이러한 의미에서 초기화 컨테이너는 객체 초기화를 돕는 자바 클래스의 생성자(constructor)와 같다. 반면에 애플리케이션 컨테이너는 병렬로 실행되며 시작 순서는 임의적이다.  

일반적으로 초기화 컨테이너는 의존성을 위해 파드 시작을 지연시키는 경우를 제외하고는 작아야 하며, 빠르게 실행되고 성공적으로 완료되어야 한다. 의존성이 충족될 때까지 종료되지 않는 경우도 있다. 초기화 컨테이너가 실패하면, 전체 파드는 다시 시작되고(RestartNever로 표시되어 있는 않은 경우), 모든 초기화 컨테이너도 다시 실행된다. 따라서 부작용을 방지하기 위해 초기화 컨테이너를 여러 번 실행하더라도 결과가 달라지지 않도록 구현하는 것이 좋다.  

한편, 초기화 컨테이너는 애플리케이션 컨테이너와 동일한 기능을 가지고 있다. 모든 컨테이너는 동일한 파드의 일부이므로 자원 제한, 볼륨 및 보안 설정을 공유하고 동일한 노드에 배치된다. 반면에 정상상태 확인(health-check)과 자원 처리 시맨틱이 약간 다르다. 파드 시작 프로세스가 애플리케이션 컨테이너로 넘어가기 전에 모든 초기화 컨테이너가 성공적으로 종료되어야 하므로, 초기화 컨테이너에 대한 레디니스 체크는 없다.  

초기화 컨테이너는 스케줄링(scheduling), 오토스케일링(autoscaling), 쿼터(quota) 관리르 위한 파드 자원 요구사항을 계산하는 방식에도 영향을 준다. 파드의 모든 컨테이너를 실행하는 순서가 정해지면 실질적인 파드 레벨 요청 및 제한 값을 다음 두 그룹 중 가장 높은 값이 된다.  

+ 가장 높은 초기화 컨테이너 요청/제한 값
+ 모든 애플리케이션 컨테이너의 요청/제한 값의 합  

결과적으로 자원 요구량이 높은 초기화 컨테이너의 자원 요구량이 낮은 애플리케이션 컨테이너가 있는 경우, 스케줄링에 영향을 주는 파드 레벨 요청 및 제한 값은 초기화 컨테이너의 높은 값을 기반으로 한다. 이 설정은 자원을 효율적으로 사용하지 않는다. 초기화 컨테이너가 짧은 시간 동안 실행되고 나머지 시간 동안 노드에 가용할 용량이 남아 있더라도, 다른 파드에서 이 자원을 사용할 수는 없다.  

또한 초기화 컨테이너를 사용하면 작업을 분리해 컨테이너를 단일 목적으로 유지할 수 있다. 애플리케이션 컨테이너는 애플리케이션 엔지니어가 만들 수 있고 애플리케이션 로직에만 집중할 수 있다. 배포 엔지니어는 초기화 컨테이너를 작성할 수 있고 구성 및 초기화하는 작업에만 집중할 수 있다. 다음 예제에서는 파일을 제공하는 HTTP 서버 기반의 애플리케이션 컨테이너를 보여준다.  

이 컨테이너는 일반적인 HTTP 서비스를 제공하며, 다른 사용 예인 경우에는 제공할 파일이 어디에서 왔을지에 대해선 아무런 가정도 하지 않는다. 동일한 파드에서 초기화 컨테이너는 깃(Git) 클라이언트 기능을 제공하며 깃 리포지토리(Git repo)를 복제한다. 두 컨테이너는 같은 파드 내에 있으므로 동일한 볼륨에 접근하여 데이터를 공유할 수 있다. 즉 복제된 파일을 초기화 컨테이너에서 애플리케이션 컨테이너로 공유할 수 있다.  

다음 예제는 비어 있는 데이터를 볼륨에 복제하는 초기화 컨테이너를 보여준다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: www
  labels:
    app: www
spec:
  initContainers:
  - name: download
    image: axeclbr/git
    command: # 외부 깃(Git) 리포지토리를 마운트된 디렉토리로 복제
    - git
    - clone
    - https://github.com/mdn/beginner-html-site-scripted
    - /var/lib/data
    volumeMounts: # 초기화 컨테이너와 애플리케이션 컨테이너에서 사용하는 공유 볼륨
    - mountPath: /var/lib/data
      name: source
  containers:
  - name: run
    image: docker.io/centos/httpd
    ports:
    - containerPort: 80
    volumeMounts: # 초기화 컨테이너와 애플리케이션 컨테이너에서 사용하는 공유 볼륨
    - mountPath: /var/www/html
      name: source
  volumes: # 데이터를 공유하기 위해 노드에서 사용하는 빈 디렉토리
  - emptyDir: {}
    name: source
```

컨피그맵(ConfigMap) 또는 퍼시스턴트볼륨(PersistentVolume)을 사용해 동일한 효과를 얻을 수 있었지만, 초기화 컨테이너가 어떻게 작동하는지 보여주고자 했다. 이 예제는 기본 컨테이너와 볼륨을 공유하는 초기화 컨테이너의 일반적인 사용 패턴을 보여준다.  

+ 파드 실행 상태 유지  
초기화 컨테이너의 결과를 디버깅하고 싶은 경우, 애플리케이션 컨테이너에 일시적인 sleep 명령을 주게 되면 상황을 조사할 수 있는 시간을 확보할 수 있다. 이 트릭은 초기화 컨테이너가 시작되지 않거나 설정이 잘못됐거나 깨져서 애플리케이션이 시작되지 않는 경우에 특히 유용하다. 파드 선언 내에 다음 명령을 설정하면, kubectl exec -it &lt;Pod&gt; sh로 파드에 들어가서 마운트된 볼륨을 디버그할 수 있는 시간을 벌 수 있다.  

```yaml
command:
- /bin/sh
- "-c"
- "sleep 3600"
```

사이드카(Sidecar)는 HTTP 서버 컨테이너와 깃 컨테이너가 애플리케이션 컨테이너로 나란히 실행되므로 이것을 이용하면 비슷한 효과를 얻을 수 있다. 그러나 사이드카 방식을 사용하면 어떤 컨테이너가 먼저 실행되는지 알 수 있는 방법이 없으며, 사이드카는 컨테이너를 연속적으로 나란히 실행시키기 위한 것이다(깃 동기화 컨테이너가 로컬 폴더를 지속적으로 업데이트한다). 만약 초기화 보장과 지속적인 데이터 업데이트가 모두 필요한 경우라면 사이드카와 초기화 컨테이너를 함께 사용할 수 있다.  

##### 14.2.1. 그 밖의 초기화 기법  
<br/>

초기화 컨테이너는 파드가 시작된 후에 활성화되는 파드 레벨 구성체(construct)다. 초기화 컨테이너 외에도 쿠버네티스 자원을 초기화하는 데 사용되는 몇 가지 기술을 소개한다.  

+ 어드미션 컨트롤러  
어드미션 컨트롤러(admission controller)는 객체가 백본(Etcd)에 저장되기 전에 쿠버네티스 API 서버에 대한 모든 요청을 가로채고 그것을 변경하거나 유효성을 점검할 수 있는 플러그인 세트다. 점검(check)을 적용하고, 자원 사용량을 제한하고, 기본값을 설정하는 컨트롤러들이다. 하지만 모든 컨트롤러는 kube-apiserver 바이너리로 컴파일되며, API 서버가 시작될 때 클러스터 관리자에 의해 구성된다. 이런 플러그인 시스템은 유연성이 매우 낮았어서, 그 때문에 어드미션 웹훅이 쿠버네티스에 추가되었다.

+ 어드미션 웹훅  
어드미션 웹훅(admission webhook)은 웹훅 설정에 정의된 내용과 일치하는 요청에 대해 HTTP 롤백을 수행하는 외부 어드미션 컨트롤러다. 어드미션 웹훅에는 변형 웹훅(mutating webhook)(사용자정의 기본 값을 적용하기 이해 자원을 변경할 수 있음)과 검증 웹훅(validating webhook)(사용자정의 어드미션 정책을 적용해 자원에 대한 요청을 거부할 수 있음)의 2가지 유형이 있다. 이와 같은 외부 컨트롤러 개념을 통해 쿠버네티스 외부에서 어드미션 웹훅을 개발할 수 있고 런타임에 설정할 수 있다.

+ 이니셜라이저  
이니셜라이저(Initializer)는 관리자가 정책을 강제로 적용하거나, 모든 객체의 메타데이터에 저장된 보류 중인 초기화 전 단계의 작업에 기본값을 적용할 때 유용하다. 그리고 사용자정의 이니셜라이저 컨트롤러는 컨트롤러와 똑같은 이름의 작업을 실행한다. 모든 초기화 작업이 완전히 완료된 후에 API 객체가 일반 컨트롤러에 표시된다.

+ 파드프리세트  
파드프리세트(PodPreset)는 다른 어드미션 컨트롤러에 의해 검토되며, 파드가 생성될 때 파드의 레이블과 일치하는 파드프리세트에 지정된 필드를 파드에 삽입할 수 있다. 필드에는 볼륨, 볼륨 마운트, 환경변수 등이 포함된다. 파드프리세트는 파드 생성 시점에 추가적인 런타임 요구사항을 삽입한다. 파드프리세트를 적용할 파드는 레이블 셀렉터(label selector)로 지정한다. 파드프리세트를 사용하면 파드 템플릿 작성자가 여러 파드에서 필요로 하는 반복되는 정보를 자동으로 추가할 수 있다.  

쿠버네티스 자원을 초기화하는 기술들은 많다. 그러나 이들 기술은 생성 시점에 자원을 확인하고 변경하기 때문에 어드미션 웹훅과는 다르다. 예를 들어 이와 같은 기술들을 이용해 초기화 컨테이너가 없는 파드에 초기화 컨테이너를 파드 생성 시점에 삽입할 수 있다. 반면에, 초기화 컨테이너 패턴은 파드를 시작하는 시점에 활성화되고 실행된다. 결국 가장 중요한 차이점을 정리하자면, 초기화 컨테이너는 쿠버네티스에 배포하는 개발자가 사용하지만, 여기서 설명하는 기술들은 관리자가 컨테이너 초기화 프로세스를 제어하고 관리하기 위해 사용한다는 점이다.  

#### 14.3. 정리  
<br/>

그렇다면 파드의 컨테이너를 두 그룹으로 나누는 이유는 무엇일까? 파드에서 초기화가 필요한 경우 바로 애플리케이션 컨테이너를 사용하지 않아야 하는 이유는 무엇일까? 답은 바로, 이 2가지 컨테이너 그룹은 각기 수명주기와 목적, 때론 작성자도 다르기 때문이다.  

초기화 컨테이너는 애플리케이션 컨테이너보다 먼저 실행된다. 더 중요한 사실은, 초기화 컨테이너는 오직 현재 초기화 컨테이너가 성공적으로 완료되었을 때만 다음 단계로 진행한다는 점이다. 이는 모든 초기화 단계들은 이전 단계가 성공적으로 완료되었는지 확인할 수 있다는 의미다. 반대로, 애플리케이션 컨테이너는 병렬로 실행되며 초기화 컨테이너와 동일한 보장을 제공하지 않는다. 이러한 구문을 통해 초기화와 애플리케이션 관련 작업을 분리해서 컨테이너를 만들고 파드를 구성할 수 있다.  

#### 14.4. 참고 자료  
<br/>

+ 초기화 컨테이너 예제(http://bit.ly/2TW7ckN)
+ 초기화 컨테이너(http://bit.ly/2TR7OsD)
+ 파드 초기화 설정(http://bit.ly/2TWMEbL)
+ 자바스크립트의 이니셜라이저 패턴(http://bit.ly/2TYF14G)
+ 스위프트(Swift)의 객체 초기화(https://apple.co/2FdSLPN)
+ 어드미션 컨트롤러 사용(http://bit.ly/2ztKrJM)
+ 동적 어드미션 제어(http://bit.ly/2DwR2Y3)
+ 쿠버네티스 이니셜라이저 작동 방식(http://bit.ly/2TeYz0k)
+ 파드 프리세트(https://kubernetes.io/docs/concepts/workloads/pods/podpreset/)
+ 파드프리세트로 파드에 정보 삽입(http://bit.ly/2Fh7QzV)
+ 쿠버네티스 이니셜라이저 튜토리얼(http://bit.ly/2FfEu4W)  

### 15. 사이드카  
<br/>

사이드카(Sidecar) 컨테이너는 기존 컨테이너의 변경 없이 기능을 확장하고 향상시킨다. 사이드카 패턴은 단일 목적 컨테이너들이 서로 긴밀하게 협력할 수 있게 해주는 기본 컨테이너 패턴 중 하나다.  

#### 15.1. 문제  
<br/>

컨테이너는 개발자와 시스템 관리자가 통합된 방식으로 애플리케이션을 작성, 배포 및 실행하는 데 오늘날 가장 많이 사용되는 패키징 기술이다. 컨테이너는 별도의 런타임, 릴리스 주기, API, 컨테이너를 소유한 팀 등으로 기능 단위를 자연스럽게 구분해 경계를 만든다. 바람직한 컨테이너는 단일 리눅스 프로세스(하나의 문제에 대해서만 잘 해결하는)처럼 동작하며 쉽게 교체할 수 있고 재사용이 가능하다. 치환성(replaceability)과 재사용(reuse)은 기존의 특수한 컨테이너를 활용해 더욱 신속하게 애플리케이션을 작성할 수 있기 때문에 필수다.  

요즘엔 HTTP 호출을 만들 때 클라이언트 라이브러리를 작성하지 않고 기존 라이브러리를 사용한다. 이와 유사하게, 웹 사이트를 서비스할 때도 웹서버용 컨테이너를 만들 필요 없이 기존 서버를 활용할 수 있다. 이 접근 방식을 통해 개발자는 시간을 절약할 수 있고, 더 작은 수의 양질의 컨테이너를 유지 관리하는 환경을 만들 수 있다. 그러나 단일 목적의 재사용 가능한 컨테이너를 활용하려면 컨테이너의 기능을 확장하는 방법과 컨테이너 간에 협업할 수 있는 수단이 필요하다. 사이드카 패턴은 컨테이너가 기존에 존재하는 다른 컨테이너의 기능을 향상시키는 이러한 유형의 협업을 가능하게 만든다.  

#### 15.2. 해결책  
<br/>

엄밀히 말하면 러타임 중에는 파드도 역시 컨테이너다. 하지만 파드의 다른 모든 컨테이너보다 가장 먼저, 일시 중지된 프로세스(말 그대로 pause 명령으로)로 시작되며, 애플리케이션 컨테이너가 파드의 수명주기 동안 상호작용하는 데 사용하는 모든 리눅스 네임스페이스를 유지하는 것 외에는 아무 작업도 하지 않는다. 이런 구현 세부사항 외에 더욱 흥미로운 것은 파드 추상화(Pod abstraction)가 제공하는 특성이다.  

파드는 가장 근본적인 기본 요소로, 여러 클라우드 네이티브 플랫폼에 각기 다른 이름으로 존재하지만 기능은 유사하다. 배포 단위로서 파드는 파드 자신에게 속한 컨테이너들에 대해 특정 런타임 제약을 적용한다. 예를 들어 모든 컨테이너는 동일한 노드에 배치되고 동일한 파드 수명주기를 공유한다. 또한 파드는 컨테이너들이 볼륨을 공유하고 로컬 네트워크 또는 호스트 IPC를 통해 서로 통신할 수 있게 허용한다. 이러한 이유로, 사용자는 컨테이너 그룹을 파드로 만든다. 사이드카(일명 사이드킥(Sidekick)이라고도 함)는 다른 컨테이너의 기능을 확장하고 향상시키기 위해 컨테이너를 파드에 넣는 시나리오를 기술하는(describe) 데 사용된다.  

이 패턴을 설명하기 위한 예제로 HTTP 서버와 깃 동기화 프로그램을 사용해보겠다. HTTP 서버 컨테이너는 HTTP를 통해서 파일을 제공하는 데에만 중점을 두고 파일이 어디서 어떻게 오는지는 모른다. 마찬가지로 깃 동기화 컨테이너의 유일한 목표는 깃 서버에서 로컬 파일 시스템으로 데이터를 동기화하는 것이다. 동기화된 파일이 어떻게 되는지는 상관하지 않으며, 오직 로컬 폴더를 원격 깃 서버와 동기화하는 것에만 관심이 있다. 다음 예제는 HTTP 서버와 깃 동기화 컨테이너가 볼륨을 이용해 파일을 교환하는 파드 정의를 보여준다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: web-app
spec:
  containers:
  - name: app
    image: docker.io/centos/httpd # HTTP로 파일을 제공하는 기본 애플리케이션 컨테이너
    ports:
    - containerPort: 80
    volumeMounts:
    - mountPath: /var/www/html # 사이드카와 기본 애플리케이션 컨테이너 간에 데이터를 교환하기 위해 공유된 장소
      name: git
  - name: poll
    image: axeclbr/git # 깃 서버로부터 데이터를 가져오고 병렬로 실행되는 사이드카 컨테이너
    volumeMounts:
    - mountPath: /var/lib/data # 사이드카와 기본 애플리케이션 컨테이너 간에 데이터를 교환하기 위해 공유된 장소
      name: git
    env:
    - name: GIT_REPO
      value: https://github.com/mdn/beginner-html-site-scripted
    command:
    - "sh"
    - "-c"
    - "git clone $(GIT_REPO) . && watch -n 600 git pull"
    workingDir: /var/lib/data
  volumes:
  - emptyDir: {}
    name: git
```

이 예제는 깃 동기화 프로그램이 콘텐츠를 제공하는 HTTP 서버의 동작을 향상시키고 동기화 상태를 유지하는 방법을 보여준다. 협업하는 두 컨테이너가 똑같이 중요하다고 말할 수도 있겠지만, 사이드카 패턴에서는 기본 컨테이너가 있고 공동 작업을 향상시키는 보조 컨테이너가 있다. 일반적으로 기본 컨테이너는 컨테이너 목록에 나열된 첫 번째 컨테이너이자 기본 컨테이너다(예를 들면 kubectl exec 명령을 실행할 때).  

사이드카 패턴을 통해, 컨테이너는 런타임에 협업이 가능해지며, 그와 동세이 두 컨테이너의 관심을 분리시킴으로써 서로 다른 프로그래밍 언어를 사용하거나 릴리스 주기가 다른 팀들이 각자 별도로 컨테이너를 개발할 수 있다. 이를 통해 HTTP 서버의 치환성과 재사용성을 높여주며, 깃 동기화 프로그램은 기타 애플리케이션이나 또 다른 설정을 적용해 파드의 단일 컨테이너로서 또는 여러 컨테이너와 협업하기 위해 재사용될 수 있다.  

#### 15.3. 참고 자료  
<br/>

+ 사이드카 예제(http://bit.ly/2FqSZUV)
+ 컨테이너 기반 분산 시스템을 위한 디자인 패턴(http://bit.ly/2Odan24)
+ 프라나(Prana): 넷플릭스 PaaS 기반 애플리케이션과 서비스를 위한 사이드카(http://bit.ly/2Y9PRnS)
+ 빈 깡통 전화기(Tin-Can Phone): 레거시 애플리케이션에 권한 부여 및 암호화를 추가하는 패턴(https://www.feval.ca/posts/tincan-phone/)
+ 만능 일시중지 컨테이너(http://bit.ly/2FOYH21)  

### 16. 어댑터  
<br/>

어댑터(Adapter) 패턴은 여러 종류의 다양한 컨테이너화 시스템이 외부에서 사용할 수 있는 표준화되고 정규화된 형식의 통합 인터페이스를 준수하게 한다. 어댑터 패턴은 사이드카의 모든 특성을 상속받지만 애플리케이션에 대해 적합한 접근만 제공한다.  

#### 16.1. 문제  
<br/>

컨테이너를 사용하면 다른 라이브러리와 언어로 작성된 애플리케이션을 통일된 방식으로 패키징하고 실행할 수 있다. 오늘날 팀 조직마다 각기 다른 기술을 사용하고 여러 종류의 컴포넌트로 구성된 분산 시스템을 만드는 일은 흔하다. 이러한 이질성(heterogeneity)으로 인해 코든 컴포넌트가 다른 시스템에 의해 통일된 방식으로 처리되어야 할 때 난관에 부딪칠 수 있다. 이런 경우, 어댑터 패턴은 시스템의 복잡성을 숨기고 시스템에 대한 통합된 접근을 제공한다.  

#### 16.2. 해결책  
<br/>

어댑터 패턴은 예제를 들어 설명하는 것이 가장 좋은 방법이다. 분산 시스템을 성공적으로 실행하고 지원하기 위한 주요 전제조건은 상세한 모니터링과 알람을 제공하는 것이다. 또한 모니터링해야 하는 여러 서비스로 구성된 분산 시스템의 경우, 외부 모니터링 도구를 사용해 모든 서비스의 메트릭(metric) 상태를 주기적으로 체크해 기록할 수 있다.  

그러나 다른 언어로 작성된 서비스에는 동일한 기능이 없을 수도 있으며, 모니터링 도구가 요구하는 형식의 메트릭을 노출하지 못할 수도 있다. 이러한 다양성은 전체 시스템에 대한 통합된 뷰를 요구하는 단일 모니터링 솔루션에서 이기종 애플리케이션을 모니터링하는 데 어려움을 야기한다. 어댑터 패턴을 사용하면 다양한 애플리케이션 컨테이너의 메트릭을 하나의 표준 형식 및 프로토콜로 내보냄으로써 통합된 모니터링 인터페이스를 제공할 수 있다. 어댑터 컨테이너가 로컬로 저장된 메트릭 정보를 모니터링 서버가 이해할 수 있는 외부 형식으로로 변환하는 과정을 보여준다.  

이러한 접근 방식으로 보자면, 기본 애플리케이션 컨테이너 외에 파드의 모든 서비스에는 사용자정의 애플리케이션별로 메트릭을 읽어서, 모니터링 도구가 이해할 수 있는 일반 형식으로 노출하는 별도의 컨테이너가 하나 더 있다. 여기서는 HTTP를 통해 자바 기반 메트릭을 내보내는 어댑터 컨테이너가 하나 있고, HTTP를 통해 파이썬 기반 메트릭을 노출하는 또 다른 파드에 어댑터 컨테이너가 하나 더 있을 것이다. 모니터링 도구에서 모든 메트릭은 공통의 정규화된 형식으로 HTTP를 통해 제공된다.  

샘플 난수 생성 애플리케이션을 다시 참조해 어댑터를 만들어서, 어댑터 패턴을 구체적으로 구현해보자. 적잘하게 구성되면, 어댑터는 난수 생성기를 사용해 난수를 만드는 데 걸린 시간을 보여주는 로그 파일을 작성한다. 하지만 아직은 프로메테우스(Prometheus)를 이용해서 로그 파일의 내용을 모니터링할 수 없다. 왜냐하면 로그 형식이 프로메테우스가 예상하는 형식과 일치하지 않고, 프로메테우스 서버가 그 값을 가져갈 수 있도록 HTTP 종단점을 통해 이 정보를 제공해야 하기 때문이다.  

이 사용 예의 경우에 어댑터가 적합하다. 사이드카 컨테이너는 작은 HTTP 서버를 시작해서, 모든 요청에서 사용자정의 로그 파일을 읽고 이를 프로메테우스가 이해할 수 있는 형식으로 변환한다. 다음 예제는 이러한 어댑터를 사용한 디플로이먼트를 보여준다. 이와 같은 설정을 사용하면, 기본 애플리케이션이 프로메테우스에 대해 알 필요가 없는, 분리된 프로메테우스 모니터링 설정이 가능하다. 깃허브 리포지토리의 전체 예제(https://github.com/k8spatterns)에서 프로메테우스 설치와 함께 이 설정 내용을 볼 수 있다.  

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: random-generator
spec:
  replicas: 1
  selector:
    matchLabels:
      app: random-generator
  template:
    metadata:
      labels:
        app: random-generator
    spec:
      containers:
      - image: k8spatterns/random-generator:1.0 # Port 8080으로 난수 생성 서비스를 제공하는 기본 애플리케이션 컨테이너
        name: random-generator
        env:
        - name: LOG_FILE # 난수 생성에 대한 시간 정보가 포함된 로그 파일의 경로
          value: /logs/random.log
        ports:
        - containerPort: 8080
          protocol: TCP
        volumeMounts: # 프로메테우스 어댑터 컨테이너와 공유되는 디렉토리
        - mountPath: /logs
          name: log-volume
      # -------------------------------------------
      - image: k8spatterns/random-generator-exporter # Port 9889로 내보내는 프로메테우스 익스포터(exporter) 이미지
        name: prometheus-adapter
        env:
        - name: LOG_FILE # 기본 애플리케이션이 로깅하는 로그 파일의 경로
          value: /logs/random.log
        ports:
        - containerPort: 9889
          protocol: TCP
        volumeMounts: # 공유 볼륨은 어댑터 컨테이너에도 마운트됨
        - mountPath: /logs
          name: log-volume
      volumes:
      - name: log-volume
        emptyDir: {} # 파일은 노드의 파일 시스템에서 emptyDir 볼륨을 통해 공유됨
```

어댑터 패턴의 또 다른 용도는 로깅(logging)이다. 각 컨테이너들은 각자 다른 형식과 새부 수준으로 정보를 기록할 수 있다. 어댑터는 중앙집중식 로그 애그리게이터(aggregator)에서 로그를 수집할 수 있도록 자기 인식 패턴을 사용해 해당 정보를 정규화하고 정리하며 맥락 관련 정보를 보강한다.  

#### 15.3. 정리  
<br/>

어댑터는 사이드카 패턴을 특수화한 것으로, 통합 인터페이스 뒤에 복잡성을 숨기고 이기종 시스템에 대한 리버스 프록시(Reverse proxy) 역할을 한다. 일반적인 사이드카 패턴과 구분되는 명료한 이름을 붙이면, 어댑터 패턴의 목적을 좀 더 정확하게 전달할 수 있다.  

#### 15.4. 참고 자료  
<br/>

+ 어댑터 예제(http://bit.ly/2HvFF3Y)
+ 분산 시스템 툴킷: 모듈식 분산 시스템 설계를 위한 컨테이너 패턴(http://bit.ly/2U2iWD9)  

### 17. 앰배서더  
<br/>

앰배서더(Ambassador) 패턴은 복잡성을 숨기고 파드 외부의 서비스에 접근하는 통합 인터페이스를 제공하는 사이드카의 특수 패턴이다.  

#### 17.1. 문제  
<br/>

컨테이너화 서비스는 단독으로 존재하지 않으며, 동적으로 변경되는 주소, 클러스터된 서비스 인스턴스의 로드 밸런싱, 신뢰할 수 없는 프로토콜, 어려운 데이터 형식 등으로 인해 여타 서비스에 신뢰할 수 있는 방법으로 도달하기 어려운 경우가 많다. 컨테이너는 되도록 단일 목적으로 사용해야 하며 재사용할 수 있어야 한다. 그러나 비즈니스 기능을 제공하고 특수한 방식으로 외부 서비스를 사용하는 컨테이너는 한 가지 이상의 역할을 처리해야 한다.  

외부 서비스를 사용하려면 컨테이너에는 넣고 싶지 않은 특별한 서비스 검색 라이브러리가 필요할 수도 있다. 또는 서로 다른 종류의 서비스 검색 라이브러리와 메소드를 사용해 서로 다른 종류의 서비스를 교환하고 싶을 수도 있다. 앰배서더 패턴의 목적은 이처럼 외부에서 여러 서비스에 접근할 수 있도록 로직을 추상화하고 분리하는 것이다.  

#### 17.2. 해결책  
<br/>

앰배서더 패턴을 설명하기 위해 애플리케이션 캐시(cache)를 이용할 것이다. 개발 환경에서 로컬 캐시에 접근하는 것은 간단하게 설정할 수 있지만 운영 환경에서는 캐시의 각 샤드(shard)에 연결할 수 있는 클라이언트 설정이 필요할 수 있다. 또 다른 예로는 레지스트리(registry)에서 서비스를 찾고 클라이언트 측의 서비스 검색을 수행해 해당 서비스를 이용하는 것이다. 세 번째 예로는 HTTP 같은 신뢰할 수 없는 프로토콜을 통해 서비스를 이용해야 하므로, 애플리케이션을 보호하려면 회로 차단기(circuit-breaker) 로직을 사용하고, 타임아웃을 설정하며, 재시도 수행 등의 작업을 해야 한다.  

이러한 모든 경우에, 외부 서비스 접근의 복잡성을 숨기고 로컬호스트를 통해 기본 애플리케이션 컨테이너에 대한 간단한 뷰 및 접근을 제공하는 앰배서더 컨테이너를 사용할 수 있다.  

앰배서더 패턴의 장점은 사이드카 패턴의 장점과 유사하며, 컨테이너를 단일 목적으로 재사용할 수 있다. 앰배서더 패턴을 통해 애플리케이션 컨테이너는 비즈니스 로직에 중점을 두고 외부 서비스 이용에 대한 책임과 세부사항을 다른 특수 컨테이너에 위임할 수 있다. 또한 다른 애플리케이션 컨테이너와 결합 가능한, 특수하고 재사용 가능한 앰배서더 컨테이너를 만들 수 있다.  

다음 예제는 REST 서비스와 병렬로 실행되는 앰배서더를 보여준다. 응답을 리턴하기 전에 REST 서비스는 생성된 데이터를 고정 URL인 http://localhost:9009로 전송해서, 생성된 데이터를 로깅한다. 앰배서더 프로세스는 이 포트를 통해 수신하고 데이터를 처리한다. 이 예제에서는 데이터를 콘솔에서만 출력하지만, 데이터를 전체 로깅 인프라스트럭처로 전달해서 좀 더 정교한 작업을 수행할 수도 있다. REST 서비스의 경우, 로그 데이터에 무슨 일이 일어나든 문제가 되지 않으며 기본 컨테이너를 건드리지 않고 파드를 다시 설정해 앰배서더를 쉽게 교체할 수 있다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
  labels:
    app: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0 # 난수 생성을 위한 REST 서비스를 제공하는 기본 애플리케이션 컨테이너
    name: main
    env:
    - name: LOG_URL # 로컬호스트를 통해 앰배서더와 통신하기 위한 URL
      value: http://localhost:9009
    ports:
    - containerPort: 8080
      protocol: TCP
  - image: k8spatterns/random-generator-log-ambassador # 병렬로 실행되고 포트 9009로 수신하는 앰배서더(해당 포트는 파드 외부로 노출하지 않음)
    name: ambassador
```

#### 17.3. 정리  
<br/>

좀 더 고수준에서 바라보자면 앰배서더 패턴은 사이드카 패턴의 일종이다. 앰배서더와 사이드카의 주요 차이점이라면, 앰배서더는 추가 기능으로 기본 애플리케이션을 보강시키지 않는다는 점을 들 수 있다. 그 대신 앰배서더는 외부 영역에 대한 스마트 프록시 역할을 할 뿐이다. 여기서 유래해 '대리인'이라는 뜻의 앰배서더라는 이름이 붙여졌다(앰배서더 패턴은 간혹 프록시 패턴이라고도 부른다).  

#### 17.4. 참고 자료  
<br/>

+ 앰배서더 패턴(http://bit.ly/2FpjBFS)
+ 앰배서더 패턴을 사용해 서비스를 동적으로 설정하는 방법(http://bit.ly/2HxGIOG)
+ 엣시디(Etcd)로 구동되는 앰배서더(Ambassador)와의 동적인 도커 링크(http://bit.ly/2TQ1uBO)
+ 앰배서더 컨테이너를 통한 연결(https://docker.ly/2UdTGKc)
+ CoreOS 앰배서더 패턴 수정(http://bit.ly/2Ju4zmb)  

### 18. EnvVar 설정  
<br/>

#### 18.1. 문제  
<br/>

모든 주요 애플리케이션에는 데이터 소스, 외부 서비스, 운영 수준의 튜닝(tuning) 등에 접근하기 위한 설정이 필요하다. 12요소 애플리케이션 방법론에 나와 있듯이 애플리케이션 내에서 설정을 하드 코딩하는 것은 좋지 않다. 대신 애플리케이션을 빌드한 후에도 변경할 수 있도록 설정을 외부화 해야 한다. 이는 불변 애플리케이션 아티팩트(artifact)의 공유를 허용하고 촉진하므로, 컨테이너화된 애플리케이션에 더 많은 가치가 부여된다. 그러면 어떻게 해야 이러한 것들을 컨테이너화 영역에서 잘 이루어지게 할 수 있을까?  

#### 18.2. 해결책  
<br/>

12요소 애플리케이션 방법론에서는 애플리케이션 설정을 저장하기 위해 환경 변수를 사용할 것을 권장한다. 이 방법은 간단하며 모든 환경과 플랫폼에서 작동한다. 모든 운영 체제에는 환경 변수를 정의하는 방법과 환경 변수를 애플리케이션에 전파하는 방법이 있으며, 모든 프로그래밍 언어를 통해 이러한 환경 변수에 쉽게 접근할 수 있다. 환경 변수는 전반적으로 적용 가능하다. 환경 변수를 사용할 때 일반적인 사용 패턴은 빌드 타입에 하드 코딩된 기본값을 정의하는 것으로, 이렇게 하면 런타임에 덮어쓸 수 있다.  

쿠버네티스의 경우 이러한 유형의 환경 변수는 디플로이먼트(Deployment) 또는 레플리카세트(ReplicaSet)같은 컨트롤러와 파드 명세(Specification)에서 직접 설정할 수 있다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    env:
    - name: LOG_FILE
      value: /tmp/random.log # 리터럴(literal) 값을 가진 EnvVar
    - name: PATTERN
      valueFrom:
        configMapKeyRef: # 컨피그맵(ConfigMap)을 참조하는 EnvVar
          name: random-generator-config # 컨피그맵 이름
          key: pattern # EnvVar 값을 찾기 위한 컨피그맵 내의 키
    - name: SEED
      valueFrom:
        secretKeyRef: # 시크릿(Secret)을 참조하는 EnvVar(참조 방법은 컨피그맵과 동일)
          name: random-generator-secret
          key: seed
```

이러한 파드 템플릿에서는 값을 환경 변수(LOG&#95;FILE 등)에 직접 설정할 수 있을 뿐만 아니라 쿠버네티스 시크릿(민감한 데이터) 및 컨피그맵(민감하지 않은 설정)에 위임할 수도 있다. 컨피그맵과 시크릿을 이용한 간접 처리 방식은 환경 변수를 파드 정의와 독립적으로 관리할 수 있다는 장점이 있다.  

이 예제에서 SEED 변수는 시크릿 자원에서 가져온다. 이는 시크릿을 제대로 사용한 예이긴 하지만, 환경 변수가 안전하지 않다는 사실도 중요하다. 민감하고 읽기 쉬운 정보를 환경 변수에 넣으면 이 정보는 쉽게 읽을 수 있지만, 로그로 유출될 수도 있다.  

##### 18.2.1. 기본 값  
<br/>

기본 값(default value)은 존재하지 않을 수도 있는 설정 파라미터 값을 선택해야 하는 부담을 덜어주므로 매우 편리하다. 또한 설정보다는 관례(Convention Over Configuration, CoC) 개발방식에서도 중요한 역할을 한다. 그러나 기본 값이 항상 좋은 선택은 아니다. 때로는 진화하는 애플리케이션의 안티패턴(antipattern)이 될 수도 있다.  

기본 값을 소급해서 변경하는 것은 매우 어려운 작업이다. 그 이유는 첫째, 기본값 변경은 코드 내에서 기본 값을 바꾸는 것을 의미하므로 다시 빌드해야 한다. 둘째, 기본 값에 의존하게 되면(관례적으로든 의식적으로든) 기본값이 변경될 때마다 애플리케이션 사용자가 변경 사항을 전달받아 호출 코드도 수정해야 한다.  

하지만 맨 처음에 기본 값을 얻기는 어렵기 때문에, 기본 값을 변경하는 것은 종종 쓸모가 있다. 기본 값 변경을 주요 변경(major change)으로 간주하는 것이 필수다. 만약 시맨틱 버전 관리를 사용하는 경우라렴 기본 값의 변경으로 주요 버전 번호를 적절하게 변경할 수 있다. 만약 주어진 기본 값이 잘못 설정되었다면, 사용자가 설정 값을 제공하지 않은 경우 기본 값을 모두 제거하고 오류를 발생시키는 편이 훨씬 낫다. 이렇게 하면, 애플리케이션이 예상치 못한 다른 작업을 조용히 수행하는 대신, 적어도 초기에 오류를 띄우고 확실히 중단할 것이다.  

이러한 모든 문제를 고려해서 할당한 기본 값이 오랫동안 지속될 것이라고 90% 확신할 수 없는 경우라면 처음부터 기본 값을 피하는 것이 가장 좋은 방법이다. 암호 또는 데이터베이스 연결 파라미터는 환경에 깊이 의존하고 간혹 예측하기도 어려우므로, 기본 값을 제공하지 않는 것이 좋다. 또한 기본 값을 사용하지 않으면 설정 정보가 명시적으로 제공되어야 하므로, 이로써 문서의 역할도 한다.  

#### 18.3. 정리  
<br/>

환경 변수는 보편적으로 적용 가능하기 때문에 다양한 수준에서 변수를 설정할 수 있다. 이 옵션을 사용하면 설정 저의가 단편화되고 해당 환경변수가 어디에 설정되어 있는지 찾기가 어렵다. 모든 환경 변수가 정의된 중심 지점이 없으면 설정 문제를 디버깅하기가 어렵다.  

환경 변수의 또 다른 단점은 애플리케이션을 시작하기 전에만 변수를 설정할 수 있으며 나중에는 변경할 수 없다는 것이다. 즉 런타임에 애플리케이션을 조장하기 위해 설정 '핫(hot)'을 변경할 수 없다는 단점이 있다. 그러나 이것이 설정에 대해 불변(immutability)을 부여하기 때문에, 많은 사람은 이점이라 생각한다. 여기서 불변성이란 실행 중인 애플리케이션 컨테이너를 없애고 새로운 복사본을 수정된 설정으로 시작함을 뜻하며, 롤링 업데이트 같은 원활한 디플로이먼트 전략에 유용하다. 이로써 항상 정의되고 잘 알려진 설정 상태로 유지할 수 있다.  

#### 18.4. 참고 자료  
<br/>

+ EnvVar 설정 예제(http://bit.ly/2YcUtJC)
+ 12요소 애플리케이션(https://12factor.net/config)
+ 불변 서버(https://martinfowler.com/bliki/ImmutableServer.html)
+ 설정 값 세트를 사용하기 위한 스프링 부트 프로파일(http://bit.ly/2YcSKUE)  

### 19. 설정 자원  
<br/>

쿠버네티스에서는 주기적이며 기밀이 요구되는 데이터를 위한 고유한 설정 자원(configuration resource)을 제공하므로, 애플리케이션 수명주기에서 설정 수명주기를 분리할 수 있다.  

#### 19.1. 문제  
<br/>

가급적 모든 설정 데이터를 한 장소에 저장하고 다양한 자원 정의 파일에 분산시키지 않는 것이 좋지만, 전체 설정 파일의 내용을 하나의 환경 변수에 넣는 것은 좋은 방법이 아니다. 따라서 쿠버네티스 설정 자원이 제공하는 추가적인 간접접근(indirection)을 이용하면 유연성을 향상시킬 수 있다.  

#### 19.2. 해결책  
<br/>

쿠버네니스에서는 단순한 환경 변수보다 훨씬 유용한 전용 설정 자원을 제공한다. 이런 설정 자원으로, 범용을 위한 컨피그맵(ConfigMap), 그리고 민감한 데이터를 위한 시크릿(Secret) 객체가 있다.  

둘 다 키-값 쌍의 저장 및 관리 기능을 제공하므로 모두 동일한 방식으로 사용할 수 있다. 컨피그맵에 대한 설명 대부분은 시크릿에도 그대로 적용할 수 있다. 실제 데이터 인코딩(시크릿에서는 Base64) 외에 컨피그맵과 시크릿 사용에서 기술적 차이는 없다.  

데이터가 있는 컨피그맵을 생성하면 컨피그맵의 키를 2가지 방법으로 사용할 수 있다.  

+ 환경 변수에 대한 참조로, 환경 변수의 이름이 키(key)로 사용된다.
+ 파드에 마운트된 볼륨에 매핑된 파일로, 파일 이름이 키로 사용된다.  

쿠버네티스 API를 통해 컨피그맵을 업데이트하면 마운트된 컨피그맵 볼륨의 파일이 업데이트된다. 따라서 애플리케이션이 설정 파일의 핫 리로드(hot reload)를 지원하는 경우 이러한 업데이트의 이점을 즉시 얻을 수 있다. 그러나 환경 변수로 컨피그맵 항목을 사용하면 프로세스가 시작된 후에 환경 변수를 변경할 수 없으므로 업데이트가 반영되지 않는다.  

컨피그맵과 시크릿 외에 다른 대안은 외부 볼륨에 직접 설정을 저장하고 마운트하는 것이다.  

다음 예제는 컨피그맵 사용에 중점을 두지만 시크릿에도 사용할 수 있다. 한 가지 큰 차이점은 시크릿 값은 Base64로 인코딩되어야 한다는 점이다. 컨피그맵 자원은 다음 예제처럼 데이터 섹션에 키-값 쌍을 포함한다.  

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: random-generator-config
data:
  PATTERN: Configuration Resource # 컨피그맵(ConfigMap)은 환경 변수와 마운트된 파일로 접근할 수 있다. 마운트된 파일로 사용될 때 EnvVar 사용과 올바른 파일 이름을 나타내려면 컨피그맵에서 대문자로 된 키를 사용하는 것이 좋다.
  application.properties: |
    # 난수 생성기 설정
    log.file=/tmp/generator.log
    server.port=7070
  EXTRA_OPTIONS: "high-secure,native"
  SEED: "432576345"
```

이 예제에서 컨피그맵은 스프링 부트 application.properties처럼 전체 설정 파일의 내용을 전달할 수 있음을 또한 알 수 있다. 중요한 사용 예의 경우 이 코드 부분이 상당히 커질 수도 있다!  

전체 자원 디스크립터(descriptor)를 수동으로 생성하는 대신, kubectl을 사용해 컨피그맵 또는 시크릿도 생성할 수 있다. 위의 예제에 상응하는 kubectl 명령은 다음 예제와 같다.  

```sh
kubectl create cm spring-boot-config --from-literal=JAVA_OPTIONS=-Djava.security.egd=file:/dev/urandom --from-file=application.properties
```

이 컨피그맵은 다양한 위치(다음 예제처럼 환경 변수가 정의된 모든 위치)에서 읽을 수 있다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - env:
    - name: PATTERN
      valueFrom:
        configMapKeyRef:
          name: random-generator-config
          key: PATTERN
....
```

컨피그맵에 환경 변수로 사용할 항목이 많은 경우, 특정 구문을 사용하면 많은 입력을 저장할 수 있다. 위의 예제의 env: 코드부처럼 각 항목을 개별적으로 지정하지 않고 envFrom:을 사용하면 유효한 환경 변수로 사용할 수 있는 키가 있는 모든 컨피그맵 항목을 노출시킬 수 있다. 다음 예제처럼 해당 환경 변수에는 접두어(CONFIG&#95;)를 붙일 수 있다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
    envFrom: # 환경 변수 이름으로 사용할 모든 키를 컨피그맵 random-generator-config에서 가져온다.
    - configMapRef:
        name: random-generator-config
      prefix: CONFIG_ # 적합한 모든 컨피그맵 키 앞에 CONFIG_ 접두어를 붙인다.
```

컨피그맵과 마찬가지로 시크릿도 항목당 또는 모든 항목에 환경 변수로 사용될 수도 있다. 컨피그맵 대신 시크릿에 접근하려면 configMapKeyRef를 secretKeyRef로 바꾸자.  

볼륨으로 사용하면, 파일 이름으로 정의된 키를 비롯한 전체 컨피그맵이 볼륨에 삽입된다.  

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: random-generator
spec:
  containers:
  - image: k8spatterns/random-generator:1.0
    name: random-generator
    volumeMouts:
    - name: config-volume
      mountPath: /config
  volumes:
  - name: config-volume
    configMap: # 컨피그맵 자원 볼륨에는 맵(map)의 키(key)인 파일 이름과 맵의 값(value)인 파일 내용으로 구성된 파일 수만큼의 항목이 포함된다.
      name: random-generator-config
```

볼륨 선언에 특성을 추가해서, 설정 데이터 매핑을 더욱 세밀하게 조정할 수 있다. 모든 항목을 파일로 매핑하는 대신, 노출해야 하는 모든 키와 사용 가능한 파일 이름을 개별적으로 선택할 수 있다.  

쿠버네티스를 이용해 설정을 저장하는 또 다른 방법은 gitRepo 볼륨을 사용하는 것이다. 이 타입의 볼륨은 파드에 비어 있는(empty) 디렉토리를 마운트하고 깃 리포지토리를 복제한다. 깃에 설정을 저장하면 버전 관리 및 감사(auditing)를 무료로 받을 수 있다는 장점이 있다. 그러나 gitRepo 볼륨은 쿠버네티스 자원이 아니므로 깃 리포지토리에 대한 외부 접근이 필요하며, 클러스터 외부에 있을 수도 있으므로 별도로 모니터링하고 관리해야 한다. 복제(cloning)와 마운트는 파드가 시작할 때 일어나며, 로컬에 복제된 리포지토리는 변경사항이 자동 업데이트되지 않는다. 이 볼륨은 초기화 컨테이너를 사용해 설정(configuration)을 공유 로컬 볼륨에 복사하는 불변 설정(Immutable Configuration)과 유사하게 작동한다.  

사실, gitRepo 타입의 볼륨은 더 이상 사용을 권장하지 않으며, 지금은 깃뿐 아니라 다른 데이터 소스도 지원하는 초기화 컨테이너 기반 솔루션 사용을 권장한다. 따라서 두 가지 모두 외부 시스템에서 설정 데이터를 검색해 볼륨에 저장할 수 있지만, 사전에 정의된 gitRepo 볼륨보다는 좀 더 유연한 초기화 컨테이너 방법을 이용하자.  

##### 19.2.1. 시크릿은 얼마나 안전한가?  
<br/>

시크릿(Secret)은 데이터를 Base64로 인코딩해 가지고 있다가 환경 변수가 마운트된 볼륨으로 파드에 전달하기 전에 디코딩한다. 간혹 시크릿을 보안 기능으로 착각하기도 하지만, Base64 인코딩은 암호화 기법이 아니며 보안 측면에서 평문 텍스트와 동일하다. Base64 인코딩으로 바이너리 데이터를 저장하는 시크릿이 왜 컨피그맵보다 안전하다고 말할까? 시크릿에는 보안을 유지하기 위한 다양한 구현 세부사항이 많다. 이 영역에서 끊임없이 개선이 이뤄지는 중이지만, 현재 시점에서 주요한 구현 세부사항은 다음과 같다.  

+ 시크릿은 자신에게 접근하는 파드가 실행 중인 노드에만 배포된다.
+ 노드에서 시크릿은 tmpfs의 메모리에 저장되며, 실제 스토리지에는 기록되지 않고 파드가 제거될 때 함께 제거된다.
+ 서크릿은 엣시디(Etcd)에 암호화된 형태로 저장된다.  

이러한 주요 구현 세부사항에 상관없이, 루트 사용자로 시크릿(Secret)에 접근하거나, 파드를 생성해서 시크릿을 마운트하는 방법도 있다. 컨피그맵을 비롯한 자원에서처럼 롤 기반 접근 제어(RBAC, role-based access control)를 시크릿에 적용하고, 미리 정의한 서비스 계정이 있는 특정 파드만 읽게 할 수 있다. 그러나 네임스페이스에서 파드를 만들 수 있는 사용자는 파드를 생성하는 것으로 해당 네임스페이스 내에서 더 큰 권한을 가질 수 있다. 그 사용자는 권한이 더 커진 서비스 계정으로 파드를 실행할 수 있으며, 여전히 시크릿을 읽을 수 있다. 네임스페이스에서 파드 생성 접근 권한이 있는 사용자나 컨트롤러는 해당 네임스페이스 내의 모든 서비스 계정을 흉내낼 수 있고, 모든 시크릿과 컨피그맵에 접근할 수 있다. 따라서 민감한 정보는 애플리케이션 레벨에서 추가 암호화가 필요하다.  

#### 19.3. 정리  
<br/>

컨피그맵과 시크릿을 사용하면 쿠버네티스 API로 관리할 수 있는 전용 자원 객체에 설정 정보를 저장할 수 있다. 컨피그맵와 시크릿을 사용하는 가장 큰 장점은 설정 데이터의 정의(definition)와 사용(usage)을 분리할 수 있다는 점이다. 정의와 사용의 분리를 통해 설정을 독립적으로 사용해 개개체를 관리할 수 있다.  

컨피그맵과 시크릿의 또 다른 이점으로는 플랫폼의 고유한 기능이라는 점이다. 사용자저의 구성체(construct)는 필요하지 않는다.  

그러나 이러한 설정 자원에는 제약이 있다. 1MB 크기의 시크릿 제한 때문에 대규모 데이터를 임의로 저장할 수 없으며, 비 설정 애플리케이션 데이터에는 그다지 적합하지 않다는 점이다. 바이너리 데이터를 시크릿에 저장할 수도 있지만, Base64로 인코딩되어야 하므로 약 700kb의 데이터만 사용할 수 있다.  

실제로 쿠버네티스 클러스터는 네임스페이스당 또는 프로젝트당 사용할 수 있는 컨피그맵 수에 대해 개별 할당량(quota)을 지정하므로 컨피그맵을 무분별하게 사용해서는 안 된다.  

#### 19.4. 참고 자료  
<br/>

+ 설정 자원 예제(http://bit.ly/2YeGymi)
+ 컨피그맵 공식 문서(http://bit.ly/2Cs59uQ)
+ 시크릿 공식 문서(https://kubernetes.io/docs/concepts/configuration/secret/)
+ 안정적으로 시크릿 데이터 암호화하기(http://bit.ly/2ORsavt)
+ 시크릿을 이용해 기밀 정보를 안전하게 배포하기(http://bit.ly/2FfcvCn)
+ gitRepo 볼륨(http://bit.ly/2HxuGqO)
+ 컨피그맵 크기 제한(http://bit.ly/2UkHRRy)  

### 20. 불변 설정  
<br/>

불변 설정(Immutable Configuration) 패턴은 데이터를 변경 불가능한 컨테이너 이미지에 패키징하고 런타임에 설정 컨테이너를 애플리케이션에 링크한다. 불변 설정 패턴을 사용하면, 변경 불가능하고 버전이 지정된 설정 데이터를 이용할 수 있을 뿐만 아니라, 환경 변수나 컨피그맵에 저장된 설정 데이터의 크기 제한을 해결할 수 있다.  

#### 20.1. 문제  
<br/>

환경 변수를 통해 컨테이너 기반 애플리케이션을 손쉽게 설정할 수 있다. 환경 변수는 사용하기도 쉽고 범용적으로 지원되지만, 환경 변수 개수가 특정한 임계 값을 초과하면 관리하기가 어려워진다.  

이와 같은 복잡성은 설정 자원 패턴을 사용해 어느 정도 처리할 수 있다. 그러나 이러한 모든 패턴으로더 설정 데이터 자체에 불변성(immutability)이 강제되지는 않는다. 여기서 불변성이란 설정 데이터가 항상 정의된 상태로 유지되는 것을 보장하기 위해 애플리케이션이 시작된 후에는 설정을 변경할 수 없음을 의미한다. 또한, 불변 설정은 버전 관리가 가능하고 변경 제어 정차를 따를 수 있다.  

#### 20.2. 해결책  
<br/>

이 같은 문제를 해결하기 위해, 단일 컨테이너 데이터로서 배포 가능한 단일 패시브(passive) 데이터 이미지에 모든 환경별 설정 데이터를 넣을 수 있다. 런타임에 애플리케이션 데이터 이미지에서 설정을 추출할 수 있도록 애플리케이션과 데이터 이미지는 서로 연결되어 있다. 이 접근 방식을 사용하면 다양한 환경에 대해 서로 다른 설정 데이터 이미지를 쉽게 만들 수 있고, 이미지는 특정 환경에 대한 모든 설정 정보를 통합하고 다른 컨테이너 이미지처럼 버전을 지정할 수 있다. 데이터 이미지는 데이터만 들어 있는 간단한 컨테이너 이미지이기 때문에 만들기가 수월하다. 문제는 시작할 때 연결하는 단계인데, 플랫폼에 따라 다양한 접근 방식을 사용할 수 있다.  

##### 20.2.1. 도커 볼륨  
<br/>

쿠버네티스를 살펴보기 전에 한걸음 물러섯 바닐라(vanilla) 도커를 생각해보자. 도커에서 컨테이너는 컨테이너의 데이터를 볼륨(volume)으로 노출할 수 있다. 도커파일(Dockerfile)에서 VOLUME 지시자를 사용하면, 이후에 공유할 수 있는 디렉토리를 지정할 수 있다. 컨테이너를 시작할 때 컨테이너 내의 디렉토리 내용은 공유 디렉토리로 복사된다. 이 같은 볼륨 연결(volume linking)은 전용 설정 컨테이너의 설정 정보를 또 다른 애플리케이션 컨테이너와 공유하는 좋은 방법이다.  

이제 예제는 한번 살펴보자. 개발 환경을 위해 개발자 설정이 있고 볼륨 /config를 생성하는 도커 이미지를 만든다. 다음 예제와 같이 Dockerfile-config를 사용해 이러한 이미지를 만들 수 있다.  

```sh
FROM scratch
ADD app-dev.properties /config/app.properties # 지정된 속성 추가
VOLUME /config # 볼륨 생성 및 볼륨에 속성 복사
```

이제 다음 예제의 도커 CLI로 이미지와 도커 컨테이너를 만든다.  

```sh
docker build -t k8spatterns/config-dev-image:1.0.1 -f Dockerfile-config
docker create --name config-dev k8spatterns/config-dev-image:1.0.1 .
```

마지막 단계는 애플리케이션 컨테이너를 시작하고 설정 컨테이너에 연결하는 것이다.  

```sh
docker run --volumes-from config-dev k8spatterns/welcome-servlet:1.0
```

애플리케이션 이미지는 설정 컨테이너에 의해 노출되는 볼륨 /config 디렉토리 내에 설정 파일이 있어야 한다. 이 애플리케이션을 개발 환경에서 운영 환경으로 이동할 때는 애플리케이션 이미지 자체를 변경하지 않고, 시작 명령만 변경하면 된다. 즉 다음 예제처럼 애플리케이션 컨테이너를 운영 설정 컨테이너와 볼륨으로 연결하기만 하면 된다.  

```sh
docker build -t k8spatterns/config-prod-image:1.0.1 -f Dockerfile-config
docker create --name config-prod k8spatterns/config-prod-image:1.0.1 .
docker run --volumes-from config-prod k8spatterns/welcome-servlet:1.0
```

##### 20.2.2. 쿠버네티스 초기화 컨테이너  
<br/>

쿠버네티에서, 파드 내의 볼륨 공유는 이러한 종류의 설정이나 애플리케이션 컨테이너 연결에 딱 들어맞는다. 그러나 현재 쿠버네티스는 컨테이너 볼륨을 지원하지 않으므로, 도커 볼륨 연결 기술을 쿠버네티스에 적용할 수가 없다. 그동안의 길게 이어진 논의와 더불어, 이 같은 기능 구현에 따르는 복잡성에 비해 한정된 이점을 고려하면, 컨테이너 볼륨을 지원하기까진 꽤 많은 시간이 걸릴 것이다.  

따라서 컨테이너가 (외부) 볼륨을 공유할 수는 있어도, 컨테이너 내에 있는 디렉토리는 아직 공유할 수 없다. 쿠버네티스에서 불변 설정 컨테이너를 사용하려면, 파드를 시작할 때 비어 있는 공유 볼륨을 초기화할 수 있는 초기화 컨테이너 패턴을 사용할 수 있다.  

도커에서는 도커 볼륨으로 설정 데이터를 공유만 하면 되므로, 설정 도커 이미지는 운영체제 파일이 없는 비어 있는 도커 이미지인 scratch를 기본 이미지로 한다. 그러나 쿠버네티스 초기화 컨테이너는 기본 이미지에서 설정 데이터를 공유 파드 볼륨에 복사해야 하는 작업이 필요하다. busybox는 작지만 이러한 작업을 위한 유닉스 cp 명령을 사용할 수 있으므로 기본 이미지로 적합하다.  

그렇다면 설정을 통한 공유 볼륨 초기화는 어떻게 이뤄지는지 예제를 한번 살펴보자. 먼저, 다음 예제같이 도커파일(Dockerfile)을 사용해 설정 이미지를 다시 만들어야 한다.  

```sh
FROM busybox
ADD dev.properties /config-src/demo.properties
ENTRYPOINT [ "sh", "-c", "cp /config-src/* $1", "--" ] # 와일드카드(wildcard) 처리를 위한 셸(shell) 사용
```

바닐라(vanilla) 도커 사례와의 유일한 차이점은 다른 기본 이미지를 사용한 것, 그리고 도커 이미지가 시작될 때 인수(argument)로 제공된 디렉토리에 속성 파일을 복사하는 ENTRYPOINT를 추가한 것이다. 이 이미지는 이제 디플로이먼트의 .template.spec 내의 초기화 컨테이너에서 참조할 수 있다.  

```yaml
initContainers:
- image: k8spatterns/config-dev:1
  name: init
  args:
  - "/config"
  volumeMounts:
  - mountPath: "/config"
    name: config-directory
containers:
- image: k8spatterns/demo:1
  name: demo
  ports:
  - containerPort: 8080
    name: http
    protocol: TCP
  volumeMounts:
  - mountPath: "/config"
    name: config-directory
volumes:
  - name: config-directory
    emptyDir: {}
```

디플로이먼트의 파드 템플릿 명세에는 볼륨 1개와 컨테이너 2개가 있다.  

+ 볼륨 config-directory는 emptyDir 타입이므로 해당 파드를 호스팅하는 노드에서 비어 있는 디렉토리로 생성된다.
+ 파드를 시작할 때 쿠버네티스가 호출하는 초기화 컨테이너는 방금 만든 이미지로 형셩되며, 이미지의 ENTRYPOINT에서 사용되는 /config를 인수로 설정한다. 이 인수를 통해 초기화 컨테이너는 자신의 내용을 지정된 디렉토리에 복사한다. /config 디렉토리는 config-directory 볼륨에서 마운트된다.
+ 애플리케이션 컨테이너는 config-directory 볼륨을 마운트해서, 초기화 컨테이너가 복사한 설정에 접근한다.  

개발 환경에서 운영 환경으로 설정을 변경하려면, 초기화 컨테이너의 이미지만 교체하면 된다. YAML 정의를 변경하거나 kubectl로 업데이트해서 이를 수행할 수 있다. 그러나 각 환경에 대한 자원 디스크립터(descriptor)를 수정하는 것은 이상적이지 않다. 쿠버네티스의 엔터프라이즈 배포판인 레드햇 오픈시프트(Red Hat OpenShift)를 사용하는 경우라면, 오픈시프트 템플릿으로 이 문제를 해결할 수 있다. 오픈시프트 템플릿은 단일 템플릿으로 각 환경별로 각기 다른 자원 디스크립터를 만들 수 있다.  

##### 20.2.3. 오픈시프트 템플릿  
<br/>

템플릿은 파라미터로 표시되는 일반적인 자원 디스크립터다. 다음 예제에서 볼 수 있듯이 설정 이미지를 파라미터로 쉽게 사용할 수 있다.  

```yaml
apiVersion: v1
kind: Template
metadata:
  name: demo
parameters:
  - name: CONFIG_IMAGE # 템플릿 파라미터 CONFIG_IMAGE 선언
    description: Name of configuration image
    value: k8spatterns/config-dev:1
objects:
- apiVersion: v1
  kind: DeploymentConfig
    // ....
    spec:
      template:
        metadata:
          // ....
          spec:
            initContainers:
            - name: init
              image: $(CONFIG_IMAGE) # 템플릿 파라미터 사용
              args: [ "/config" ]
              volumeMounts:
              - mountPath: /config
                name: config-directory
            containers:
            - image: k8spatterns/deom:1
              // ....
              volumeMounts:
              - mountPath: /config
                name: config-directory
          volumes:
          - name: config-directory
            emptyDir: {}
```

초기화 컨테이너 선언에서 참조하는 파라미터 CONFIG&#95;IMAGE를 빠르게 인식할 수 있도록 위의 예제에서는 전체 디스크립터 중 일부만 발췌했다. 오픈시프트 클러스터에서 이 템플릿을 만들면 다음 예제처럼 oc를 호출해 인스턴스를 생성할 수 있다.  

```sh
oc new-app demo -p CONFIG_IMAGE=k8spatterns/config-prod:
```

이 예제를 실행하기 위한 자세한 설명과 전체 디플로이먼트 디스크립터는 깃허브 예제 링크(https://github.com/k8spatterns/examples)에서 볼 수 있다.  

#### 20.3. 정리  
<br/>

불변 설정 패턴에 데이터 컨테이너를 사용하는 것은 다소 복잡하다. 그러나 이 패턴에는 몇 가지 고유한 장점이 있다.  

+ 환경별 설정은 컨테이너 안에 있으므로, 여타 컨테이너 이미지처럼 버전을 지정할 수 있다.
+ 이런 방식으로 생성된 설정은 컨테이너 레지스트리를 통해 배포될 수 있고, 클러스터에 접근하지 않아도 설정을 확인할 수 있다.
+ 컨테이너 이미지 안에 있는 설정을 직접 변경할 수는 없다. 설정을 변경하려면 버전을 업데이트한 새로운 컨테이너 이미지가 필요하다.
+ 설정 데이터 이미지는 설정 데이터가 너무 복잡해 환경 변수나 컨피그맵에 넣을 수 없을 때 유용하다. 임의의 대규모 설정 데이터를 수용할 수 있기 때문이다.  

한편, 불변 설정 패턴의 단점은 다음과 같다.  

+ 레지스트리를 통해 추가 컨테이너 이미지를 빌드하고 배포해야 하므로, 복잡성이 더 높다.
+ 민감한 설정 데이터를 처리하는 보안 문제에는 아무런 대책이 없다.
+ 쿠버네티스의 경우 별도의 초기화 컨테이너 처리가 필요하므로, 환경에 따라 다른 디플로이먼트 객체를 관리해야 한다.  

따라서 전반적으로 이러한 접근 방식이 실제로 필요한지 신중하게 판단해야 한다. 딱히 불변성이 필요치 않다면, 간단한 컨피그맵만으로도 충분하다.  

#### 20.4. 참고 자료  
<br/>

+ 불변 설정 예제(http://bit.ly/2HL95dp)
+ --volumes-from을 쿠버네티스에서 구현하는 방법(http://bit.ly/2YbRhhy)
+ 기능(feature) 요청: 쿠버네티스의 이미지 볼륨(http://bit.ly/2Wf0pjt)
+ docker-flexvol: 도커 볼륨을 지원하는 쿠버네티스 드라이버(https://github.com/dims/docker-flexvol)
+ 오픈시프트 템플릿(https://red.ht/2Ohh7vO)  

### 21. 설정 템플릿  
<br/>

설정 템플릿(Configuation Template) 패턴을 사용하면 애플리케이션을 시작할 때 크고 복잡한 설정을 만들고 처리할 수 있다. 생성된 설정은 대상 환경별로 설정 템플릿을 파라미터로 가공하여 반영한다.  

#### 21.1. 문제  
<br/>

컨피그맵이나 시크릿의 모든 값의 합은 1MB(기본 백엔드 저장소인 엣시디(Etcd)에 부여된 제한 값)를 넘을 수 없으므로 설정의 크기 또한 고려해야 할 사항이다.  

대규모 설정 파일의 내용은 일반적으로 실행환경이 달라도 큰 차이가 없다. 즉 여러 환경에서 동일한 데이터가 존재하기 때문에 컨피그맵에서 반복(duplication)과 중복(redundancy)이 많이 발생한다. 설정 템플릿 패턴은 이와 같은 특정한 사용 사례의 문제를 해결한다.  

#### 21.2. 해결책  
<br/>

반복을 줄이려면 데이터베이스 연결 파라미터 등의 상이한 설정 값만 컨피그맵에 저장하거나, 환경 변수에 직접 저장하는 편이 좋다. 컨테이너를 시작할 때 이 값들은 설정 템플릿으로 처리되어 전체 설정 파일(제이보스 와일드플라이(JBoss WildFly) standalone.xml 파일 등)로 생성된다. 애플리케이션 초기화 시점에 템플릿을 처리하기 위해 틸러(Tiller, 루비 언어)나 곰플릿(Gomplatem, 고 언어) 등의 많은 도구가 있다. 환경 변수 또는 마운트된 볼륨에서 가져온 데이터로 채워져서 컨피그맵으로 처리되는 설정 템플릿의 예다.  

애플리케이션이 시작되기 전에 완전히 처리된 설정 파일은 여타 설정 파일처럼 직접 사용할 수 있는 위치에 저장된다.  

런타임에 이러한 처리를 수행하는 2가지 기술은 다음과 같다.  

+ 템플릿 처리기(processor)를 ENTRYPOINT의 일부로 도커파일(Dockerfile)에 추가해서, 템플릿 처리를 컨테이너 이미지에 포함시킬 수 있다. 여기에서 진입점(entrypoint)은 일반적으로 템플릿 처리를 수행한 다음 애플리케이션을 시작하는 스크립트다. 템플릿의 파라미터는 환경 변수에서 가져온다.

+ 쿠버네티스의 경우 초기화를 수행하는 더 좋은 방법은 템플릿 처리기가 실행되고 파드의 애플리케이션 컨테이너에 대한 설정을 만드는 초기화 컨테이너를 사용하는 것이다.  

쿠버네티스의 경우에는 컨피그맵을 템플릿 파라미터로 사용할 수 있으므로, 초기화 컨테이너 접근 방식이 가장 좋다.  

애플리케이션의 파드 정의는 최소한 2개의 컨테이너, 즉 템플릿 처리를 위한 초기화 컨테이너와 애플리케이션 컨테이너를 위한 컨테이너로 구성된다. 초기화 컨테이너에는 템플릿 처리기뿐만 아니라 설정 템플릿 자체도 포함되어 있다. 컨테이너들 외에도 이 파드는 2개의 볼륨을 정의한다. 여기서 2가지 볼륨이란 컨피그맵으로 처리되는 템플릿 파라미터를 위한 볼륨, 그리고 처리된 템플릿을 초기화 컨테이너와 애플리케이션 컨테이너 간에 공유하는 데 사용되는 emptyDir 볼륨을 말한다.  

이 설정을 사용하면 파드를 시작할때 다음과 같은 순서로 단계가 수행된다.  

(1) 초기화 컨테이너가 시작되고 템플릿 처리기를 실행한다. 템플릿 처리가는 이미지에서 템플릿을, 그리고 마운트된 컨피그맵 볼륨에서 템플릿 파라미터를 가져와서 emptyDir 볼륨에 그 결과를 저장한다.  
(2) 초기화 컨테이너가 완료되면 애플리케이션 컨테이너가 시작되고, emptyDir 볼륨에서 설정 파일을 읽어 들인다.  

다음 예제는 개발 환경과 운영 환경의 2가지 환경에서 초기화 컨테이너를 사용해 전체 와일드플라이(Wildfly) 설정 파일 세트를 관리한다. 개발 환경과 운영 환경은 매우 유사해서 약간의 차이만 있는데, 이 예제에서는 로깅이 수행되는 방식만 다르다. 각 로그 행(log line)은 각각 DEVELOPMENT:와 PRODUCTION:으로 미리 고정되어 있다.  

예제 깃허브 리포지토리(http://bit.ly/2TKUHZY)에서 자세한 설치 방법과 함께 전체 예제를 볼 수 있다.(여기서는 주요 개념만 보여주므로, 기술적 세부사항은 소스 리포지토리를 참조하기 바란다.)  

다음 예제의 로그 패턴은 standalone.xml에 저장되며, 고(Go) 템플릿 구문을 사용해 파라미터화한다.  

여기서는 곰플릿(Gomplate)을 템플릿 처리기로 사용한다. 해당 처리기는 작성될 템플릿 파라미터를 참조하기 위해 데이터 소스 개념을 사용한다. 여기서 데이터 소스는 초기화 컨테이너에 컨피그맵으로 마운트된 볼륨에서 가져온다. 컨피그맵은 실제 형식이 추출되며, logFormat 키가 있는 단일 항목을 포함한다.  

이 템플릿을 사용해 초기화 컨테이너의 도커 이미지를 만들 수 있다. 이미지 k8spatterns/example-configuration-template-init의 도커파일(Dockerfile)은 매우 간단하다.  

기본 이미지 k8spatterns/gomplate에는 기본적으로 다음 디렉토리를 사용하는 템플릿 처리기와 진입점(entry point) 스크립트가 있다.  

+ /in에는 파라미터로 표시된 standalone.xml을 포함하는 와일드플라이 설정 템플릿이 있으며 모두 이미지에 직접 추가된다.
+ /params에는 YAML 파일인 곰플릿(Gomplate) 데이터 소스를 조회하는 데 사용된다. 이 디렉토리는 컨피그맵으로 파드 볼륨에 마운트된다.
+ /out은 처리된 파일이 저장되는 디렉토리다. 이 디렉토리는 와일드플라이 애플리케이션 컨테이너에 마운트되며 설정을 위해 사용된다.  

예제의 2번째 구성 요소는 파라미터가 저장된 컨피그맵이다. 다음 예제에서는 키-값 쌍이 있는 간단한 파일만 사용한다.  

```
logFormat: "DEVELOPMENT: %-5p %s%e%n"
```

wildfly-parameters라는 이름의 컨피그맵에는 config.yml 키에서 참조하는 YAML 형식의 데이터가 포함되어 있으며 초기화 컨테이너가 이 컨피그맵을 선택한다. 마지막으로 와일드플라이 서버의 디플로이먼트 자원이 필요하다.  

```yaml
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  labels:
    example: cm-template
  name: wildfly-cm-template
spec:
  replicas: 1
  template:
    metadata:
      labels:
        example: cm-template
    spec:
      initContainers:
      - image: k8spatterns/example-config-cm-template-init # 설정 템플릿이 포함된 이미지
        name: init
        volumeMounts:
        - mountPath: "/params" # 파라미터는 컨피그맵 wildfly-parameters으로 마운트된다.
          name: wildfly-parameters
        - mountPath: "/out" # 처리된 템플릿을 작성하기 위한 대상 디렉토리다. 비어 있는(empty) 볼륨으로 마운트된다.
          name: wildfly-config
      containers:
      - image: jboss/wildfly:10.1.0.Final
        name: server
        command:
        - "/opt/jboss/wildfly/bin/standalone.sh"
        - "-Djboss.server.config.dir=/config"
        ports:
        - containerPort: 8080
          name: http
          protocol: TCP
        volumeMounts:
        - mountPath: "/config" # 생성된 전체 설정 파일이 저장된 디렉토리는 /config로 마운트된다.
          name: wildfly-config
      volumes: # 파라미터의 컨피그맵을 위한 볼륨 선언과 처리된 설정을 공유하는 데 사용되는 빈(empty) 디렉토리를 위한 볼륨 선언이다.
      - name: wildfly-parameters
        configMap:
          name: wildfly-parameters
      - name: wildfly-config
        emptyDir: {}
```

이 선언은 매우 중요하므로 좀 더 자세히 살펴보자. 디플로이먼트 명세에는 초기화 컨테이너, 애플리케이션 컨테이너, 2개의 내부 파드 볼륨으로 구성된 파드가 포함되어 있다.  

+ 첫 번째 볼륨인 wildfly-parameters는 동일한 이름의 컨피그맵을 포함한다(즉 파라미터 값이 있는 config.yaml이라는 파일이 포함됨).
+ 다른 볼륨은 초기에는 빈(empty) 디렉토리며, 초기화 컨테이너와 와일드플라이 컨테이너 간에 공유된다.  

이 디플로이먼트를 시작하면 다음과 같이 수행된다.  

+ 초기화 컨테이너가 생성되고 해당 명령이 실행된다. 초기화 컨테이너는 컨피그맵 볼륨에서 config.yml을 가져온 후 이 컨테이너의 /in 디렉토리에서 템플릿을 채우고 처리된 파일을 /out 디렉토리에 저장한다. /out 디렉토리는 볼륨 wildfly-config가 마운트된 위치다.

+ 초기화 컨테이너가 완료되면 와일드플라이 서버는 /config 디렉토리에서 전체 설정을 조회할 수 있는 옵션으로 시작한다. /config는 처리된 템플릿 파일이 있는 공유 볼륨 wildfly-config이다.  

개발 환경에서 운영 환경으로 전환할 때 디플로이먼트 자원 디스크립터를 변경할 필요가 없다는 점이 중요하다. 템플릿 파라미터가 있는 컨피그맵만 변경하면 된다.  

이 방법을 사용하면, 중복되는 대규모 설정 파일들을 복사하고 유지 관리하지 않고도 DRY 설정을 쉽게 만들 수 있다. 예를 들어 모든 환경에서 와일드플라이 설정이 변경되면, 초기화 컨테이너의 템플릿 파일 하나만 업데이트하면 된다. 이 방식은 설정 변경에 따른 위험이 없으므로, 유지 관리에 상당히 유리하다.  

+ 볼륨 디버깅 팁  
설정 템플릿 패턴에서처럼 파드와 볼륨으로 작업할 때, 예상대로 작동하지 않는 경우에은 명확한 디버깅 방법이 없다. 따라서 처리된 템플릿을 검사하려면 해당 노드에서 emptyDir 볼륨의 내용이 포함되어 있는 /var/lib/kubelet/pods/{podid}/volumes/kubernetes.io~empty-dir/ 디렉토리를 확인하자. 파드가 실행 중일 때는 kubectl exec로 파드로 들어가서, 생성된 파일이 있는지 해당 디렉토리를 검사하라.  

#### 21.3. 정리  
<br/>

설정 템플릿 패턴은 설정 자원을 기반으로 하며, 복잡하지만 유사한 설정의 여러 환경에서 애플리케이션을 운영해야 할 때 특히 적합하다. 그러나 설정 템플릿을 사용한 구성은 더 복잡하고 잘못될 수 있는 부분도 더 많으므로, 많은 설정 데이터가 필요한 애플리케이션인 경우에만 사용해야 한다. 간혹 이러한 애플리케이션은 극히 일부분만 해당 환경에 의존적인, 상당히 많은 설정 데이터를 요구한다. 전체 설정을 환경별 컨피그맵에 직접 복사하더라도, 처음에는 작동하지만 시간이 지남에 따라 환경별로 다양하게 변경될 것이기 때문에, 해당 설정의 유지 관리가 부담이 될 수 있다. 이러한 상황에는 템플릿 접근 방식이 가장 적합하다.  

#### 21.4. 참고 자료  
<br/>

+ 설정 템플릿 예제(http://bit.ly/2TKUHZY)
+ 틸러(Tiller) 템플릿 엔진(https://github.com/markround/tiller)
+ 곰플릿(Gomplate)(https://github.com/hairyhenderson/gomplate)
+ 고(Go) 템플릿 구문(https://golang.org/pkg/html/template/)  

### 22. 컨트롤러  
<br/>

컨트롤러(Controller) 패턴은 쿠버네티스 세트를 능동적으로 모니터링하고 요청한 상태로 유지 관리한다. 쿠버네티스의 중심부는 선언된 대상 상태와 함께 현재 애플리케이션 상태를 정기적으로 주시(watch)하고 레컨사일(reconcile)하는 컨트롤러 컬렉션으로 구성된다.  

#### 22.1. 문제  
<br/>

쿠버네티스는 많은 기능을 기본으로 제공하는 정교하고 포괄적인 플랫폼이다. 하지만 범용 오케스트레이션 플랫폼이면서도 모든 애플리케이션 사용 예를 다루지는 않는다. 다행스럽게도, 입증된 쿠버네티스 빌딩 블록 위에서 특정 사용 예를 잘 구현할 수 있는 본연의 확장 기능을 제공한다. 변경이나 중단 없이 어떻게 쿠버네티스를 확장할지, 그리고 쿠버네티스 확장 기능을 사용자정의 사용 예에 어떻게 적용할지에 대해, 이 시점에서 아마 의문의 생길 것이다.  

설계상 쿠버네티스는 선언적인 자원 중심 API를 기반으로 한다. 선언적(declarative)이란 정확히 무슨 의미일까? 명령적(imperative) 접근 방식과 달리, 선언적 접근 방식은 쿠버네티스에게 어떻게 작동해야 하는지를 알려주기보다는, 대상 상태가 어떻게 보여야 하는지를 기술(describe)한다. 일례로 디플로이먼트를 확장할 경우, 쿠버네티스에 '새로운 파드 생성'을 지시해 능동적으로 새 파드를 만들지 않는다. 대신, 쿠버네티스 API를 통해 디플로이먼트 자원의 replicas 속성을 요청한 수로 변경한다.  

그렇다면 새로운 파드는 어떻게 만들까? 이것은 컨트롤러에 의해 내부적으로 수행된다. 자원 상태가 변경(디플로이먼트의 replicas 속성 값 변경처럼)될 때마다 쿠버네티스는 이벤트를 생성해 이해관계가 있는 모든 리스너(listener)에게 브로드캐스트(broadcast)한다. 이러한 리소너는 새로운 자원을 수정, 삭제, 생성함으로 반응할 수 있고, 이를 통해 파드 생성 이벤느 등의 또 다른 이벤트가 생성된다. 또한, 이러한 이벤트는 여타 컨트롤러에 의해 다시 선택되어, 특정 작업을 수행할 수도 있다.  

이러한 전체 절차를 상태 레컨실리에이션(state reconciliation)이라고 한다. 여기서 대상 상태(요청한 레플리카의 수)가 현재 상태(실제 실행 중인 인스턴스)와 다르며 컨트롤러가 다시 요청한 상태가 되도록 레컨사일(reconcile)하는 작업을 수행한다. 이 관점에서 보면 쿠버네티스는 기본적으로 분산 상태 관리자다. 쿠버네티스에 컴포넌트 인스턴스의 요청 상태를 선언하고, 변경 사항이 있을 경우 쿠버네티스는 해당 상태를 유지하려고 시도한다. 그럼 코드를 수정하지 않고 어떻게 이 레컨실리에이션 절차에 연결하고 특정 요구에 맞는 사용자정의 컨트롤러를 만들 수 있을까?  

#### 22.2. 해결책  
<br/>

쿠버네티스에는 레플리카세트(ReplicaSet), 데몬세트(DaemonSet), 스테이트풀세트(StatefulSet), 디플로이먼트(Deployment), 서비스(Service) 같은 표준 쿠버네티스 자원을 관리하는 내장 컨트롤러 컬렉션이 제공된다. 이러한 컨트롤러는 컨트롤러 관리자의 일부로 실행되며 마스터 노드에 (독립형 프로세스 또는 파드로) 배포된다. 컨트롤러는 서로 영향을 주지 않고 독립적이다. 또한, 실제 상태(actual state)와 요청 상태(desired state)에 대한 자원을 모니터링하고 실제 상태를 요청한 상태에 가깝게 만들기 위해, 무한 루프로 레컨실리에이션(reconciliation)을 실행한다.  

하지만, 이렇게 바로 사용 가능한 컨트롤러 외에도, 쿠버네티스 이벤트 중심 아키텍처를 통해 기본적으로 여타 사용자정의 컨트롤러를 플러그인(plug in)으로 사용할 수 있다. 사용자정의 컨트롤러는 내부 컨트롤러와 동일한 방식으로, 동작(behavior) 상태 변경 이벤트에 별도의 기능을 추가할 수 있다. 컨트롤러의 일반적인 특징은 시스템의 이벤트에 빠르게 반응해 특정 작업을 수행한다는 것이다. 레컨실리에이션 절차는 다음과 같은 주요 단계로 구성된다.  

+ 관측(observe): 관측하고 있는 자원이 변경될 때 쿠버네티스가 배포하는 이벤트를 주시(watch)하여 실제 상태(actual state)를 찾는다.
+ 분석(analyze): 실제 상태와 요청한 상태의 차이를 알아낸다.
+ 실행(act): 실제 상태가 요청한 상태로 구동되도록 작업을 수행한다.  

예를 들어 레플리카세트 컨트롤러는 레플리카세트 자원 변경을 주시(watch)해서, 구동해야 하는 파드의 개수를 분석(analyze)하고, API 서버에 파드 정의를 제출하는 작업을 실행(act)한다. 그런 다음, 쿠버네티스의 백엔드는 노드에서 요청된 파드를 구동한다. 컨트롤러는 현재 상태를 관측하고 API 서버를 호출하여 대상 상태에 가깝게 변경한다(필요한 경우).  

컨트롤러는 쿠버네티스의 컨트롤 플레인(control plane)에 포함되어 있으며, 일찍이 플랫폼에 맞춤형 동작을 제공할 수 있다는 점이 명백해졌다. 또한, 컨트롤러는 플랫폼을 확장하고 복잡한 애플리케이션 수명주기 관리를 가능하게 하는 표준 메커니즘이 되었고, 그 결과 오퍼레이터(Operator)라는 더욱 정교한 차세대 컨트롤러가 탄생했다. 발전적인 측면과 복잡성의 관점에서, 능동적인 레컨실리에이션 컴포넌트는 다음과 같은 두 그룹으로 분류할 수 있다.  

+ 컨트롤러  
표준 쿠버네티스 자원을 모니터링하고 작동하는 간단한 레컨실리에이션 프로세스로, 컨트롤러는 플랫폼 동작을 향상시키고 새로운 플랫폼 기능을 추가한다.

+ 오퍼레이터  
오퍼레이터 패턴의 핵심인 CustomResourcedDefinition(CRD)와 연동하는 정교한 레컨실리에이션 프로세스로, 일반적으로 오퍼레이터는 복잡한 애플리케이션 도메인 로직을 캡슐화하고 전체 애플리케이션 수명주기를 관리한다.  

여러 개의 컨트롤러가 동일한 자원에 동시에 작동하는 것을 피하기 위해, 컨트롤러는 싱글톤 서비스(Singleton Service) 패턴을 사용한다. 대부분의 컨트롤러는 디플로이먼트로 배포되지만, 쿠버네티스는 자원 객체가 변경될 때 동시성 문제를 방지하기 위해 자원 수준에서 비선점 잠금(optimistic locking)을 사용하기 때문에, 하나의 레플리카만 사용한다. 결국 컨트롤러는 백그라운드에서 영구적으로 실행되는 애플리케이션에 지나지 않는다.  

쿠버네티스 자체가 고(Go) 언어로 작성되었고, 쿠버네티스에 접근하기 위한 클라이언트 라이브러리도 고 언어로 작성되었기 때문에, 대다수 컨트롤러도 고 언어로 작성되었다. 하지만 컨트롤러는 쿠버네티스 API 서버로 요청을 보내는 어떠한 프로그래밍 언어로 작성하든 상관없다. 다음 예제에서 일반 셸 스크립트(shell script)로 작성한 컨트롤러를 소개해보겠다.  

가장 간단한 종류의 컨트롤러는 쿠버네티스가 자원을 관리하는 방식을 확장한다. 이러한 컨트롤러는 쿠버네티스 내부 컨트롤러처럼 표준 쿠버네티스 자원으로 작동하고, 쿠버네티스 내부 컨트롤러와 유사한 처리를 한다. 다만 쿠버네티스 내부 컨트롤러는 사용자에게 보이지 않는다. 컨트롤러는 자원 정의를 평가하고, 조건부로 일부 작업을 수행한다. 컨트롤러는 자원 정의의 모든 필드를 모니터링하고 처리할 수 있지만, 메타데이터와 컨피그맵이 이러한 컨트롤러의 목적에 가장 적합하다. 다음은 컨트롤러 데이터를 저장할 위치를 선택할 때 고려해야 할 몇 가지 사항이다.  

+ 레이블  
자원 메타데이터의 일부인 레이블(Label)은 모든 컨트롤러에서 주시(watch)할 수 있으며 백엔드 데이터베이스에 인덱싱되어 쿼리로 효율적으로 검색할 수 있다. 셀렉터 등의 기능이 필요할 때(예를 들면 서비스(Service) 또는 디플로이먼트의 파드와 연결할 때) 레이블을 사용해야 한다. 레이블은 오직 영/숫자 이름과 제한된 값만 사용 가능하다. 레이블에 허용되는 구문과 문자 집합은 쿠버네티스 문서를 참조하자.  

+ 애노테이션  
애노테이션(annotation)은 레이블을 대체할 수 있는 탁월한 대안으로, 만약 레이블 값이 구문 제한을 벗어난다면, 레이블 대신 애노테이션을 사용해야 한다. 애노테이션은 색인화되지 않으므로 컨트롤러 쿼리에서 키로 사용되지 않는 비 식별(nonidentifying) 정보에 애노테이션을 사용한다. 임의의 메타 데이터에 레이블이 아닌 애노테이션을 사용해도, 내부 쿠버네티스 성능에는 부정적인 영향을 미치지 않는다.  

+ 컨피그맵  
때로는 컨트롤러에 레이블이나 애노테이션에 잘 맞지 않는 추가 정보가 필요한 경우, 컨피그맵(ConfigMap)을 사용해 대상 상태 정의를 저장할 수 있으며, 이러한 컨피그맵은 컨트롤러에서 주시(watch)하고 읽어 들인다. CRD는 사용자정의 대상 상태 명세를 설계하는 데 훨씬 더 적합하고 일반 컨피그맵보다 권장되지만, CRD를 등록하려면 클러스터 수준 권한이 높아야 한다. 이러한 권한이 없는 경우에는, CRD대신 컨피그맵을 사용하는 것이 가장 좋다.  

다음은 컨트롤러 패턴을 샘플 구현으로 공부할 수 있는 합리적이고 간단한 예제 컨트롤러다.  

+ jenkins-x/exposecontroller  
이 컨트롤러(http://bit.ly/2Ushlpy)는 서비스(Service) 정의를 주시(watch)하고 메타데이터에서 expose라는 이름의 애노테이션을 발견하면, 컨트롤러는 서비스(Service)에 대한 외부 접근을 위해 인그레스(Ingress) 객체를 자동으로 노출한다. 또한 누군가가 서비스를 제거하면, 컨트롤러도 인그레스 객체를 제거한다.

+ fabric8/configmapcontroller  
컨피그맵 객체의 변경을 주시(watch)하고 연관된 디플로이먼트(Deployment)의 롤링 업그레이드를 수행하는 컨트롤러(http://bit.ly/2uJ2FnI)다. 컨피그맵을 볼 수 없고 동적으로 새로운 설정으로 업데이트할 수 없는 애플리케이션에 이 컨트롤러를 사용할 수 있다. 특히, 파드가 컨피그맵을 환경 변수로 사용하는 경우거나, 재시작하지 않고 애플리케이션 신속하고 안정적으로 자체 업데이트를 할 수 없을 경우에 적합하다.

+ 컨트롤러 리눅스 업데이트 오퍼레이터  
노드에서 특정 애노테이션이 발견될 때 쿠버네티스 노드를 재부팅하는 컨트롤러(http://bit.ly/2uFcNgX)다.  

이제, 단일 셸 스크립트로 구성되며 컨피그맵 자원의 변경사항을 쿠버네티스 API로 주시하는 컨트롤러에 대한 구체적인 예제를 살펴보자. k8spatterns.io/podDeleteSelector로 컨피그맵에 애노테이션을 삽입하면, 지정된 애노테이션 값으로 선택된 모든 파드는 컨피그맵이 변경될 때 삭제된다. 가령 이러한 파드가 디플로이먼트나 레플리카세트 같은 상위 수준의 자원으로 관리되고 있다면, 파드는 변경된 설정으로 다시 시작된다.  

예를 들어 다음 예제의 컨피그맵은 컨트롤러에서 변경사항을 모니터링하고, 값이 webapp인 app 레이블이 있는 모든 파드를 다시 시작한다. 다음 예제의 컨피그맵은 웹 애플리케이션에서 환영 메시지를 띄우는 데 사용된다.  

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: webapp-config
  annotations:
    k8spatterns.io/podDeleteSelector: "app=webapp" # 컨트롤러에서 재시작할 애플리케이션 파드를 찾기 위한 셀렉터로 사용된 애노테이션
data:
  message: "Welcome to Kubernetes Patterns !"
```

컨트롤러 셸 스크립트는 이제 이 컨피그맵을 평가한다. 이 코드의 전체  소스는 예제 깃 리포지토리에서 찾을 수 있다. 요컨대, 컨트롤러는 API 서버가 푸시(push)하는 수명주기 이벤트를 관측하기 위해 끝없는 HTTP 응답 스트림을 받는 행잉(hanging) GET HTTP 요청을 시작한다. 변경된 컨피그맵이 위의 예제에서 설정한 애노테이션을 전달하는지 여부를 감지하기 위해, 일반 JSON 객체 형식으로 된 이벤트를 분석한다. 이벤트가 도착하면 애노테이션 값으로 제공된 셀렉터와 일치하는 모든 파드를 삭제한다. 컨트롤러 작동 방식을 자세히 살펴보자.  

이 컨트롤러의 주요 부분은 레컨실리에이션 루프(loop)이며, 이는 다음 예제에 표시된 것처럼 컨피그맵 수명주기 이벤트를 수신한다.  

```sh
namespace=${WATCH_NAMESPACE:-default} # 주시(watch)할 네임스페이스(설정하지 않으면 default)

base=http://localhost:8001 # 동일한 파드에서 실행되는 앰배서더 프록시를 통해 쿠버네티스 API에 접근
ns=namespace/$namespace

curl -N -s $base/api/v1/${ns}/configmaps?watch=true | \
while read -r event # 컨피그맵 이벤트를 주시하기 위한 루프
do
  # ...
done
```

환경 변수 WATCH&#95;NAMESPACE는 컨트롤러가 컨피그맵 업데이트를 주시(watch)해야 하는 네임스페이스를 지정한다. 컨트롤러 자체의 디플로이먼트 디스크립터(descriptor)에서 이 변수를 설정할 수 있다. 다음 예제에서는 다운워드(Downward) API를 이용해서, 컨트롤러 디플로이먼트의 일부인 다음 예제에 설정된 것처럼 컨트롤러를 배포한 네임스페이스를 모니터링한다.  

```yaml
env:
  - name: WATCH_NAMESPACE
    valueFrom:
      fieldRef:
        fieldPath: metadata.namespace
```

이 네임스페이스로 컨트롤러 스크립트는 쿠버네티스 API 종단점에 대한 URL을 구성해 컨피그맵을 주시한다.  

위의 예제의 watch=true 쿼리 파라미터를 주목해보자. 이 파라미터는 HTTP 연결을 닫지 않고, 이벤트가 발생하자마자 이벤트를 응답 채널로 보내도록 API 서버에 설정한다(행잉(hanging) GET이나 Comet은 이런 기술에 대한 또 다른 이름이다). 처리할 단일 항목으로 각 이벤트가 도착하면 루프는 모든 개별 이벤트를 읽는다.  

보다시피, 컨트롤러는 로컬호스트를 통해 쿠버네티스 API 서버에 접속한다. 이 스크립트를 쿠버네티스 API 마스터 노드에 직접 배포하지 않는데, 어떻게 스크립트에서 로컬호스트를 사용할 수 있을까? 이미 짐작했겠지만, 또 다른 패턴이 있다. 로컬호스트에서 포트 8001을 노출하고 이를 실제 쿠버네티스 서비스(Service)로 프록시하는 앰배서더 컨트롤러와 함께 이 스크립트를 파드에 배포한다.  

물론 이 방식으로 이벤트를 주시(watch)하는 것은 안정적이지 않다. 연결은 언제든지 중단될 수 있으므로 루프를 다시 시작할 방법이 있어야 한다. 또한 이벤트를 놓칠 수 있으므로 운영 등급의 컨트롤러는 이벤트를 주시할 뿐만 아니라 때때로 전체 현재 상태에 대해 API 서버에 쿼리해 이를 새로운 기반으로 사용해야 한다. 하지만 예제는 패턴을 보여주기 위한 목적이므로, 이 정도면 충분하다. 다음 예제의 로직은 루프 내에서 수행된다.  

```sh
curl -N -s $base/api/v1/${ns}/configmaps?watch=true | \
while read -r event
do
  type=$(echo "$event" | jq -r '.type') # 이벤트에서 타입과 컨피그맵 이름을 추출함
  config_map=$(echo "$event" | jq -r '.object.metadata.name')
  annotations=$(echo "$event" | jq -r '.object.metadata.annotations')

  if [ "$annotations" != "null" ]; then
    selector=$(echo $annotations | jq -r "to_entries | .[] | select(.key == \"k8spatterns.io/podDeleteSelector\") | .value | @uri") # 컨피그맵에서 k8spatterns.io/podDeleteSelector 키에 해당하는 모든 애노테이션을 추출함.
  fi

  if [ $type = "MODIFIED" ] && [ -n "$selector" ]; then # 이벤트에 컨피그맵 업데이트가 명시되어 있고 애노테이션이 첨부된 경우, 이 레이블 셀렉터와 일치하는 모드 파드를 찾음
    pods=$(curl -s $base/api/v1/${ns}/pods?labelSelector=$selector | jq -r .items[].metadata.name)

    for pod in $pods; do # 셀렉터와 일치하는 모든 파드를 삭제
      curl -s -X DELETE $base/api/v1/${ns}/pods/$pod
    done
  fi
done
```

먼저, 스크립트는 컨피그맵에 발생한 동작이 무엇인지 명시하는 이벤트 타입을 추출한다. 컨피그맵을 가져온 후 jq를 사용해 애노테이션을 얻는다. jq(https://stedolan.github.io/jq/)는 명령줄에서 JSON 문서를 파싱(parsing)하는 유용한 도구로서, 스크립트는 스크립트가 실행 중인 컨테이너에서 jq를 사용할 수 있다고 가정한다.  

컨피그맵에 애노테이션이 있는 경우, 더욱 복잡한 jq 쿼리를 사용해 애노테이션 k8spatterns.io/podDeleteSelector를 체크한다. 이 쿼리의 목적은 다음 단계에서 애노테이션 쿼리 값을 API 쿼리 옵션에서 사용할 수 있는 파드 셀렉터로 변환하는 것이다. 애노테이션 k8spatterns.io/podDeleteSelector: "app=webapp"은 파드 셀렉터러로 사용되는 app%3Dwebapp로 변환된다. 이 변환은 jq로 수행되며 이러한 추출 작동 방식에 대해서는 다음 '일부 jq 푸(Fu)' 박스에서 설명한다.  

스크립트가 selector를 추출할 수 있는 경우, 이를 사용해 삭제할 파드를 직접 선택할 수 있다. 먼저 셀렉터와 일치하는 모든 파드를 찾은 다음, 직접 API를 호출해 하나씩 삭제한다.  

여기서 소개한 셸 스크립트 기반 컨트롤러는 당연히 운영 등급은 아니지만(예를 들면 이벤트 루프는 언제든 중지될 수 있음) 많은 상용구 코드 없이도 기본 개념을 잘 보여준다.  

나머지 작업은 자원 객체와 컨테이너 이미지 생성에 관한 것이다. 컨트롤러 스크립트 자체는 컨피그맵 config-watcher-controller에 저장되며, 필요할 경우 나중에 쉽게 변경할 수 있다.  

디플로이먼트를 사용해 2개의 컨테이너로 구성된 컨트롤러의 파드를 생성한다.  

+ 로컬호스트 포트 8001에 쿠버네티스 API를 노출하는 쿠버네티스 API 앰배서더 컨테이너 1개, 이미지 k8spatterns/kubeapi-proxy에는 로컬 kubectl이 설치되어 있고, 마운트된 올바른 CA와 토큰으로 kubectl proxy를 실행하는 일파인(Alpine) 리눅스다. 원래 버전인 kubectl-proxy 『쿠버네티스 인 액션』(에이콘 출판, 2020) 책에서 이 프록시를 처음 소개한 마르코 룩시(Marko Luksa)가 작성했다.

+ 방금 만든 컨피그맵에 포함된 스크립트를 실행하는 기본 컨테이너다. curl과 jq가 설치된 알파인 기본 이미지를 사용한다.  

예제 깃 리포지토리(http://bit.ly/2WaPjMD)에서 k8spatterns/kubeapi-proxy와 k8spatterns/curl-jq 이미지에 대한 도커파일을 볼 수 있다.  

이제 파드에 대한 이미지가 준비되었으므로 마지막 단계는 디플로이먼트를 사용해 컨트롤러를 배포하는 것이다. 다음 예제에서 디플로이먼트의 주요 부분을 볼 수 있다(전체 버전은 예제 리포지토리에서 찾을 수 있음).  

```yaml
apiVersion: apps/v1
kind: Deployment
# ....
spec:
  template:
    # ....
    spec:
      serviceAccountName: config-watcher-controller # 이벤트를 주시(watch)할 수 있고 파드를 다시 시작할 수 있는 권한을 가진 서비스어카운트(ServiceAccount)
      containers:
      - name: kubeapi-proxy # 로컬호스트를 쿠버네티스 API로 프록시하는 앰배서더 컨테이너
        image: k8spatterns/kubeapi-proxy
      - name: config-watcher # 모든 도구를 가지고 있으며 컨트롤러 스크립트를 마운트하는 기본 컨테이너
        image: k8spatterns/curl-jq
        # ...
        command:
        - "sh"
        - "/watcher/config-watcher-controller.sh"
        volumeMounts: # 컨트롤러 스크립트를 호출하는 시작 명령
        - mountPath: "/watcher"
          name: config-watcher-controller
      volumes:
      - name: config-watcher-controller # 컨피그맵 기반 볼륨을 기본 파드에 마운트
        configMap:
          name: config-watcher-controller
```

모두 알겠지만, 이전에 만든 컨피그맵에서 config-watcher-controller 스크립트를 마운트하고 이것을 기본 컨테이너의 시작 명령으로 직접 사용한다. 간단하게 하기 위해, 자원 제한 선언 뿐만 아니라 라이브니스(liveness)와 레디니스(readiness) 체크를 생략했다. 또한, 컨피그맵을 모니터링할 수 있는 config-watcher-controller 서비스어카운트(ServiceAccount)가 필요하다. 전체 보안 설정에 대해서는 예제 리포지토리를 참조하기 바란다.  

이제 컨트롤러가 작동하는지 확인해보자. 이를 위해 환경 변수의 값을 유일한 컨텐트(content)로 제공하는 간단한 웹 서버를 사용하고 있다. 기본 이미지는 일반 nc(넷캣(netcat))를 사용해 컨텐트를 제공한다. 이 이미지의 도커파일은 예제 리포지토리에서 찾을 수 있다.  

```yaml
apiVersion: v1
kind: ConfigMap # 제공할 데이터를 저장하기 위한 컨피그맵
metadata:
  name: webapp-config
  annotations:
    k8spatterns.io/podDeleteSelector: "app=webapp" # 웹앱의 파드를 재시작하게 만드는 애노테이션
data:
  message: "Welcome to Kubernetes Patterns !" # 웹앱에서 HTTP 응답으로 사용되는 메시지
--
apiVersion: apps/v1 # 웹앱 디플로이먼트
kind: Deployment
# ...
spec:
  # ...
  template:
    spec:
      containers:
      - name: app
        image: k8spatterns/mini-http-server # 넷캣(netcat)으로 HTTP를 제공하기 위한 단순한 이미지
        ports:
        - containerPort: 8080
        env:
        - name: MESSAGE # 주시(watch)한 컨피그맵에서 가져와서 HTTP 응답 본문으로 사용되는 환경 변수
          valueFrom:
            configMapKeyRef:
              name: webapp-config
              key: message
```

이것으로 일반 셸 스크립트로 구현된 컨피그맵 컨트롤러의 예제를 마친다. 분명한 것은, 실제 시나리오의 경우 이러한 종류의 컨트롤러를 실제 프로그래밍 언어로 작성해 더 나은 오류 처리 기능과 기타 고급 기능을 제공할 수 있다는 점이다.  

##### 22.2.1. 일부 jq 푸(Fu)
<br/>

jq는 컨피그맵의 k8spatterns.io/podDeleteSelector 애노테이션 값을 추출하고 이것을 파드 셀렉터로 변환한다. 이것은 유용한 JSON 명령 줄 도구지만 일부 개념은 약간 혼동될 수 있다. 표현식의 작동 방식을 자세히 살펴보겠다.  

```sh
selector=$(echo $annotations | jq -r "to_entries | .[] | select(.key == \"k8spatterns.io/podDeleteSelector\") | .value | @uri")
```

+ $annotations은 애노테이션 이름을 속성으로 하여 모든 애노테이션을 JSON 객체로 가진다.
+ to&#95;entries를 사용하면 { "a": "b"} 같은 JSON 객체를 { "key": "a", "value": "b" } 등의 항목이 있는 배열로 변환한다. 자세한 내용은 jq 문서를 참조하자(http://bit.ly/2FnPVsw).
+ .[]는 배열 항목을 개별적으로 선택한다.
+ 이러한 항목들 중에서 일치하는 키가 있는 항목만 선택한다. 이 필터에서 일치 항목은 0개 또는 1개만 존재할 수 있다.
+ 마지막으로, 값(.value)을 추출하고 이것을 URI의 일부로 사용할 수 있도록 @url로 변환한다.  

이 표현식은 다음과 같은 JSON 구조를  

```json
{
  "k8spatterns.io/pattern": "Controller",
  "k8spatterns.io/podDeleteSelector": "app=webapp"
}
```

셀렉터 app%3Dwebapp로 변환한다.  

#### 22.3. 정리  
<br/>

요약하자면, 컨트롤러는 관심 객체의 요청한 상태와 실제 상태를 알기 위해 관심 객체를 모니터링하는 능동적인 레컨실리에이션(reconciliation) 프로세스다. 그런 다음, 현재 상태를 요청한 상태와 비슷하게 변경하기 위해 지시를 보낸다. 쿠버네티스는 이 메커니즘을 내부 컨트롤러에서 사용하며, 사용자정의 컨트롤러에서 동일한 메커니즘을 재사용할 수도 있다.  

쿠버네티스 아키텍처의 고도 모듈화 및 이벤트 중심 특성으로 인해 컨트롤러 패턴을 적용할 수 있다. 쿠버네티스 아키텍처는 당연히 확장 포인트인 컨트롤러에 비결합/비동기 방식을 제공한다. 여기에서 중요한 이점은 쿠버네티스 자체와 모든 확장 사이에 정확한 기술적 겨예가 있다는 것이다. 그러나 컨트롤러의 비동기 특성의 한 가지 문제로, 이벤트 흐름이 항상 단순하지 않기 때문에 간혹 디버그하기가 어렵다는 점을 들 수 있다. 그 때문에, 컨트롤러에서 특정 상황을 검사하기 위해 모든 것을 멈추는 중단점을 쉽게 설정할 수가 없다.  

#### 22.4. 참고 자료  
<br/>

+ 컨트롤러 예제(http://bit.ly/2TWw6AW)
+ 컨트롤러 작성하기(http://bit.ly/2HKlIWc)
+ 파이썬으로 사용자정의 컨트롤러 작성하기(https://red.ht/2HxC85a)
+ 쿠버네티스 컨트롤러 심층 분석(http://bit.ly/2ULdC3t)
+ 서비스 노출 컨트롤러(https://github.com/jenkins-x/exposecontroller)
+ 컨피그맵 컨트롤러(https://github.com/fabric8io/configmapcontroller)
+ 사용자정의 컨트롤러 작성하기(http://bit.ly/2TYgo9b)
+ 쿠버네티스 사용자정의 컨트롤러 작성하기(http://bit.ly/2CslrS4)
+ 컨투어(Contour) 인그레스 컨트롤러(https://github.com/heptio/contour)
+ 앱컨트롤러(AppController)(https://github.com/Mirantis/k8s-AppController)
+ 레이블(Label)에 허용되는 문자(http://bit.ly/2Q0td0M)
+ Kubectl-Proxy(http://bit.ly/2FgearB)  

### 23. 오퍼레이터  
<br/>

오퍼레이터(Operator)는 CRD를 사용해 특정 애플리케이션에 대한 운영 지식을 알고리즘 및 자동화된 형식으로 캡슐화는 컨트롤러이며, 오퍼레이터 패턴을 사용하면 컨트롤러 패턴을 확장해 유연성과 표현력을 높일 수 있다.  

#### 23.1. 문제  
<br/>

간혹 쿠버네티스 플랫폼에 추가 도메인 객체가 필요한 새로운 개념을 추가하고 싶을 때가 있다. 예를 들어 프로메테우스(Prometheus)를 모니터링 솔루션으로 선택하고, 이를 쿠버네티스의 모니터링 기능으로 잘 정의된 방식으로 추가하려고 한다고 가정해보자. 이 때 다른 쿠버네티스의 자원을 정의하는 방식과 유사하게, 모니터링 설정을 비롯한 모든 배포 새부정보를 기술하는 프로메테우스 자원이 있다면 좋을 것이다. 또한, 어떤 서비스를 모니터링해야 하는지 지정할 수 있는 자원도 있으면 좋을 것이다(예를 들어 레이블 셀렉터).  

이러한 상황은 확실히 CustomResourceDefinition(이하 CRD)가 매우 유용한 사용 예로, CRD를 통해 쿠버네티스 클러스터에 사용자정의 자원을 추가하고 이것을 고유 자원처럼 사용해 쿠버네티스 API를 확장할 수 있다. 사용자정의 자원과 이러한 자원으로 동작하는 컨트롤러는 오퍼레이터 패턴이 된다.  

지미 젤린스키(Jimmy Zelinskie)의 다음 인용문(http://bit.ly/2Fjlx1h)은 오퍼레이터의 특성윽 가장 잘 설명한다.  

> 오퍼레이터는 쿠버네티스와 쿠버네티스 이외의 것들을 둘 다 이해하는 쿠버네티스 컨트롤러다. 이 두 영역에 대한 지식을 결합함으로써, 두 도메인을 모두 이해하는 실제 운영자가 필요한 작업을 자동화할 수 있다.  

#### 23.2. 해결책  
<br/>

##### 23.2.1. 사용자저의 자원 정의  
<br/>

CRD를 사용하면, 쿠버네티스 플랫폼에서 특정 도메인 컨셉트를 관리하기 위한 목적으로 쿠버네티스를 확장할 수 있다. 쿠버네티스 API를 통해 다른 자원과 유사하게 사용자정의 자우언을 관리하고, 최종적으로 백엔드 저장소 엣시디(Etcd) 등에 저장한다. CRD의 전신은 서드파티리소스(ThirdPartyResources)였다.  

앞서 언급한 시나리오에서는 프로메테우스가 쿠버네티스에 실제로 매끄럽게 통합될 수 있도록 CoreOS 프로메테우스 오퍼레이터에 의해 새로운 사용자정의 자원으로 구현되었다. 다음 예제와 같이 프로메테우스 CRD는 정의되며, 다음 예제에서는 CRD로 사용 가능한 대부분의 필드를 설명한다.  

```yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: prometheus.monitoring.coreos.com # 이름
spec:
  group: monitoring.coreos.com # CRD가 속한 API 그룹
  names:
    kind: Prometheus # 해당 자원의 인스턴스를 식별하기 위한 kind
    plural: prometheuses # 객체의 목록을 지정하기 위해 복수의 형태로 생성하는 이름 지정 규칙
  scope: Namespaced # 자원이 클러스터 전체에서 생성될 수 있는지, 아니면 특정 네임스페이스에서만 가능한지 여부를 지정
  version: v1 # CRD 버전
  validation:
    openAPIV3Schema: .... # 유효성 검사를 위한 OpenAPI V3 스키마(여기엔 표시하지 않음)
```

쿠버네티스가 사용자정의 자원의 유효성을 검사할 수 있도록 OpenAPI V3 스키마(schema)를 지정할 수 있다. 간단한 사용 예의 이 스키마를 생략할 수 있지만 운영 등급 CRD의 경우는 설정 오류를 조기에 감지할 수 있도록 스키마를 제공해야 한다.  

또한, 쿠버네티스는 spec 필드의 subresources를 이용해 CRD에 다음과 같은 2가지 하위자원을 지정할 수 있게 허용한다.  

+ scale  
이 속성을 사용하면 CRD에서 레플리카 수를 관리하는 방법을 지정할 수 있다. 이 필드로 사용자정의 자원의 요청한 레플리카 수가 지정된 JSON 경로를 선언할 수 있다. 여기서 경로는 실행중인 레플리카의 실제 수가 저장된 경로와 사용자정의 자원 인스턴스를 찾는 데 사용되는 레이블 셀렉터가 있는 경로다. 여기서 사용되는 레이블 셀렉터는 필수로 지정해야 하는 것은 아니지만, HorizontalPodAutoscaler와 함께 사용자정의 자원을 사용한다면 필수로 지정되어야 한다.

+ status  
이 속성을 설정하면 상태 변경만 가능한 새로운 API 호출을 이용할 수 있다. 이 API 호출은 개별적으로 보장되며 컨트롤러 외부에서 상태를 업데이트할 수 있다. 반면에, 사용자정의 자원을 전체적으로 업데이트하면 표준 쿠버네티스 자원과 같이 status 섹션이 무시된다.  

다음 예제는 일반 파드에서는 이용할 수 있는 하위자원(subresources) 경로를 보여준다.  

```yaml
kind: CustomResourceDefinition
# ...
spec:
  subresources:
    status: {}
    scale:
      specReplicasPath: .spec.replicas # 선언된 레플리카 수에 대한 JSON 경로
      statusReplicasPath: .status.replicas # 실행 중인 레플리카 수에 대한 JSON 경로
      labelSelectorPath: .status.labelSelector # 실행 중인 레플리카 수를 요청하기 위한 레이블 셀렉터의 JSON 경로
```

CRD가 정의되면 다음 예제와 같은 자원을 쉽게 생성할 수 있다.  

```yaml
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: prometheus
spec:
  serviceMonitorSelector:
    matchLabels:
      team: frontend
  resources:
    requests:
      memory: 400Mi
```

metadata: 섹션의 형식 및 유효성 검사 규칙은 여타 쿠버네티스 자원과 동일하다. .spec:은 CRD 관련 콘텐츠를 포함하며 쿠버네티스는 CRD에 지정된 유효성 검사 규칙에 따라 유효성을 검사한다.  

활성화된 컴포넌트 없이 사용자정의 자원만으로는 그다지 유용하지 않다. 이들에게 의미를 부여하기 위해서는 이러한 자원의 수명주기를 주시(watch)하고 자원 내의 선언에 따라 동작하는 컨트롤러가 필요하다.  

##### 23.2.2. 컨트롤러와 오퍼레이터 분류  
<br/>

오퍼레이터를 만들어 보기 전에 컨트롤러, 오퍼레이터 그리고 특히 CRD에 대해 몇 가지 분류를 살펴보겠다. 오퍼레이터의 동작에 따라 크게 다음과 같이 분류할 수 있다.  

+ 설치 CRD  
쿠버네티스 플랫폼에서 애플리케이션을 설치하고 운영하는 데 사용된다. 일반적인 예로 프로메테우스 자체를 설치하고 관리하는 프로메테우스 CRD가 있다.

+ 애플리케이션 CRD  
반면에 애플리케이션 CRD는 애플리케이션별 도메인 컨셉트를 나타낼 때 사용한다. 이러한 종류의 CRD를 이용해 애플리케이션별 도메인 동작을 쿠버네티스와 결합하는 등 애플리케이션과 쿠버네티스를 긴밀하게 통합할 수 있다. 예를 들어 서비스모니터(ServiceMonitor) CRD는 프로메테우스 오퍼레이터가 프로메테우스 서버에서 메트릭을 수집할 특정 쿠버네티스 서비스(Service)를 등록하기 위해 사용하고, 프로메테우스 오퍼레이터는 프로메테우스 서버 설정을 그에 따라 조정한다.  

오퍼레이터(Operator)는 이 사례의 프로메테우스 오퍼레이터처럼 다른 종류의 CRD로 동작할 수 있다는 점에 유의하자. 두 가지 CRD 범주 사이의 경계는 사실 명확하지 않다.  

컨트롤러와 오퍼레이터의 분류에서 오퍼레이터는 CRD를 사용하는 컨트롤러와 상속 관계(is-a)다. 그러나 이 구분 또한 편차가 있기 때문에 다소 모호하다.  

예를 들어 CRD를 대체해서 컨피그맵(ConfigMap)을 사용하는 컨트롤러가 있는데, 이 방법은 기본 쿠버네티스 자원이 충분하지 않지만 CRD를 생성할 수 없는 시나리오에서 의미가 있다. 이 경우, 컨피그맵은 컨피그맵 컨텐츠 안의 도메인 로직을 캡슐화할 수 있으므로 훌륭한 대안이다. 일반 컨피그맵을 사용하면, 사용자에게 CRD 등록을 위한 클러스터 관리자 권한이 없어도 된다는 장점이 있다. 특정 클러스터 설정에서는 CRD 등록이 불가능하다(예를 들어 오픈시프트 온라인(OpenShift Online) 같은 공용 클러스터에서 실행할 때 등).  

CRD를 특정 도메인 설정으로 사용하는 일반 컨피그맵으로 교체할 때에도 관측-분석-실행(Observe-Analyze-Act) 개념을 사용할 수 있다. 단점은 CRD를 찾기 위한 kubectl get 같은 필수 툴을 사용할 수 없다는 점이다. API 서버 수준의 유효성 검사와 API 버전 관리 기능이 없기 때문이다. 또한 컨피그맵의 status: 필드를 모델링하는 방법에는 제한이 있지만, CRD는 사용자가 원하는 대로 상태 모델을 정의할 수 있다.  

CRD의 또 다른 장점은 CRD의 종류에 따라 개별 조정이 가능한 세분화된 권한 모델이 있다는 것이다. 같은 네임스페이스의 컨피그맵들은 동일한 권한 설정을 공유하기 때문에, 모든 도메인 설정이 컨피그맵에 캡슐화되어 있을 경우엔 이러한 RBAC(role-based access control, 역할 기반 접근 제어) 보안이 불가능하다.  

구현 관점에서는, 바닐라(vanilla) 쿠버네티스 객체만 사용하도록 제한해서 컨트롤러를 구현하는지, 또는 컨트롤러가 사용자정의 자원을 관리하는지 여부가 중요하다. 전자의 경우는 이미 쿠버네티스 클라이언트 라이브러리로 모든 타입을 사용할 수 있다. CRD의 경우는 사전에 정의된 타입 정보가 없으므로, CRD 자원을 관리하기 위해 스키마가 없는(schemaless) 접근 방식을 사용하거나 CRD 정의에 포함된 OpenAPI 스키마를 기반으로 자체적으로 사용자저의 타입을 정의할 수 있다. 타입이 정의된 CRD에 대한 자원은 사용되는 클라이언트 라이브러리와 프레임워크에 따라 다르다.  

오퍼레이터에는 더욱 발전된 쿠버네티스 확장 후크(hook) 옵션이 있다. 쿠버네티스 관리형 CRD가 해당 도메인을 표현하기에 충분하지 않은 경우, 자체 애그리게이션(Aggregation) 계층으로 쿠버네티스 API를 확장할 수 있다. 사용자정의로 구현된 APIService 자원을 쿠버네티스 API의 새로운 URL 경로로 추가할 수 있다.  

custom-api-server라는 이름의 지정된 서비스(Service)에 연결하고, 그 뒤에 파드가 서비스되려면, 다음 예제같은 자원을 사용하면 된다.  

```yaml
apiVersion: apiregistration.k8s.io/v1beta1
kind: APIService
metadata:
  name: v1alpha1.sample-api.k8spatterns.io
spec:
  group: sample-api.k8spatterns.io
  service:
    name: custom-api-server
  version: v1alpha1
```

서비스(Service)와 파드 구현 외에도 파드를 실행시킬 수 있는 서비스어카운트(ServiceAccount)를 설정하기 위한 추가 보안 설정이 필요하다.  

일단 설정되면, API 서버 https://&lt;api 서버 ip&gt;/apis/sample-api.k8spatterns.io/v1alpha1/namespaces/&lt;ns&gt;/...에 대한 모든 요청은 사용자저의 서비스(Service) 구현으로 보내진다. 사용자정의 서비스 구현은 해당 API를 통해 관리되는 자원을 유지하고 요청을 처리한다. 이 방법은 쿠버네티스 자체에서 전적으로 사용자정의 자원을 관리하는 CRD 사례와는 다르다.  

사용자정의 API 서버를 사용하면 자원 수명주기 이벤트를 주시(watch)할 수 있는 자유도가 높아지지만, 훨씬 더 많은 로직을 구현해야 하므로 일반적인 사용 예의 경우는 일반 CRD를 다루는 오퍼레이터로도 충분하다.  

또한 API 서버 애그리게이션 구현에 도움이 되는 API 서버 빌더(http://bit.ly/2JIhHEl) 라이브러리를 사용할 수도 있다.  

이제 CRD를 사용해 오퍼레이터를 개발하고 배포하는 방법을 살펴보겠다.  

##### 23.2.3. 오퍼레이터 개발과 배포  
<br/>

오퍼레이터 작성에 도움이 되는 3가지 주요 프로젝트는 다음과 같다.  

+ CoreOS 오퍼레이터 프레임워크
+ 쿠버네티스 자체의 SIG API 머시너리(Machinery)에서 개발된 큐브빌더(Kubebuilder)
+ 구글 클라우드 플랫폼의 메타컨트롤러(Metacontroller)  

곧 이들 프로젝트에 대해 매우 간략하게 다룰 것이지만, 아직 모두 미성숙 단계이고 시간이 지남에 따라 변하거나 합병될 수도 있다는 것을 명심하기 바란다.  

###### 23.2.3.1. 오퍼레이터 프레임워크  
<br/>

오퍼레이터 프레임워크는 고 언어(Golang) 기반 오퍼레이터 개발을 위한 광범위한 지원을 제공하며 여러 하위 컴포넌트들 제공한다.  

+ 오퍼레이터 SDK는 쿠버네티스 클러스터에 접근하고 오퍼레이터 프로젝트를 시작하기 위한 기본 코드를 생성하는 고수준의 API를 제공한다.
+ 오퍼레이터 수명주기 관리자(Operator Lifecycle Manager)는 오퍼레이터와 CRD의 릴리스, 업데이트 등을 관리한다. 즉 '오퍼레이터의 오퍼레이터'라고 생각하면 된다.
+ 오퍼레이터 미터링(Operator Metering)은 오퍼레이터에 대해 기록하는 기능을 제공한다.  

여기에서는 여전히 진화하고 있는 오퍼레이터 SDK에 대해 자세히 설명하지 않지만 오퍼레이터 수명주기 관리자(이하 OLM)는 오퍼레이터를 사용할 때 특별히 유용한 기능을 제공한다. CRD의 한 가지 문제는 해당 자원을 클러스터 전체에만 등록할 수 있으므로 클러스터 관리자 권한이 필요하다는 것이다. 일반적인 쿠버네티스 사용자는 접근 권한이 부여된 네임스페이스의 모든 측면을 관리할 수 있지만, 클러스터 관리자와의 상호작용 없이 오퍼레이터를 사용할 수는 없다.  

이러한 상호작용을 간소화하기 위해, OLM은 CRD 설치 권한이 있는 서비스 계정으로 백그라운드에서 실행되는 클러스터 서비스다. ClusterServiceVersion(이하 CSV)이라는 전용 CRD가 OLM과 함께 등록되어 해당 오퍼레이터와 관련된 CRD 정의에 대한 참조와 오퍼레이터 디플로이먼트(Deployment)를 지정할 수 있다. 이러한 CSV를 생성하자마자, OLM의 한 부분은 CRD를 비롯한 모든 종속된 CRD가 등록되기를 기다린다. 그리고 OLM은 CSV에 지정된 오퍼레이터를 배포한다. 권한이 없는 사용자를 대신해, OLM의 또 다른 부분을 사용해 해당 CRD를 등록할 수 있다. 이 방법은 일반 클러스터 사용자가 오퍼레이터를 설치할 수 있는 훌륭한 방법이다.  

###### 23.2.3.2. 큐브빌더  
<br/>

+ SIG(Special Interest Group)  
쿠버네티스 커뮤니티가 기능 영역을 구성하는 방법으로 쿠버네티스 깃허브 리포지토리(https://github.com/kubernetes-sigs)에서 현재 SIG 목록을 찾을 수 있다.  

큐브빌더(Kubebuilder)는 포괄적인 문서가 포함된 SIG API 머시너리(Machinery)의 프로젝트다. 오퍼레이터 SDK와 마찬가지로 고 언어 프로젝트의 기본 코드 생성을 지원하고 하나의 프로젝트 내에서 여러 CRD 관리를 지원한다.  

오퍼레이터 프레임워크와 약간의 차이는 큐브빌더는 쿠버네티스 API와 직접 작동하지만 오퍼레이터 SDK는 표준 API 위에 약간의 추상화를 추가하여 사용하기가 더 쉽다는 점이다(히자만 부가적인 기능이 큐브빌더보다 부족하다).  

오퍼레이터의 설치와 수명주기 관리에 대한 지원은 오퍼레이터 프레임워크의 OLM만큼 정교하지 않다. 그러나 두 프로젝트 모두 상당히 겹치므로 결국 어떤 방식으로든 수렴될 것이다.  

###### 23.2.3.3. 메타컨트롤러  
<br/>

메타컨트롤러(Metacontroller)는 사용자정의 컨트롤러 작성의 공통 부분을 캡슐화하는 API를 이용해 쿠버네티스를 확장한다는 점에서 방금 다룬 두 가지 오퍼레이터 구축 프레임워크와는 매우 다르다. 하드 코딩되어 있지 않고 메타컨트롤러에 특정된 CRD를 통해 동적으로 정의된 여러 컨트롤러를 실행함으로써, 쿠버네티스 컨트롤러 관리자(Kubernetes Controller Manager)와 유사하게 작동한다. 즉 실제 컨트롤러 로직을 제공하는 서비스를 호출하는 위임 컨트롤러(delegation controller)다.  

메타컨트롤러를 설명하는 또 다른 방법은 선언적인 동작(declaration behavior)이다. CRD를 사용하면 쿠버네티스 API에 새로운 타입(type)을 저장할 수 있지만, 메타컨트롤러를 사용하면 표준 자원 또는 사용자정의 자원의 동작을 선언적으로 쉽게 정의할 수 있다.  

메타컨트롤러를 통해 컨트롤러를 정의할 때는, 컨트롤러 고유의 비즈니스 로직만 포함하는 기능을 제공해야 한다. 메타컨트롤러는 쿠버네티스 API와의 모든 상호작용을 처리하고, 사용자를 대신해 레컨실리에이션(reconciliation) 루프를 실행하며, 웹후크(webhook)를 통해 함수를 호출한다. 웹후크는 CRD 이벤트를 기술하는 잘 정의된 페이로드(payload)와 함께 호출된다. 함수가 값을 반환함에 따라, 컨트롤러 함수를 대신해 생성(또는 삭제)해야 하는 쿠버네티스 자원의 정의를 반환한다.  

이러한 위임을 통해, HTTP와 JSON을 잘 이해하는 어떤 언어로든, 그리고 쿠버네티스 API 또는 클라이언트 라이브러리에 종속되지 않는 모든 언어로 함수를 작성할 수 있다. 이 함수는 쿠버네티스상에서, 또는 외부 '서비스로서의 함수(Function-as-a-Service)' 프로바이더에서 또는 그 밖의 장소에서 호스팅할 수 있다.  

여기에서 더 많은 세부사항을 다룰 수는 없지만, 사용 예에 간단한 자동화나 오케스트레이션으로 쿠버네티스를 확장하고 사용자정의를 포함시키려 하고, 추가 기능(functionality)이 필요하지 않는 경우라면, 특히 고(Go) 이외의 언어로 비즈니스 로직을 구현하려는 경우일 때는, 메타컨트롤러를 신중히 고려해볼 필요가 있다.  

#### 23.3. 예제  
<br/>

이제 구체적인 오퍼레이터 예제를 한번 살펴보자. 컨피그와처(ConfigWatcer) 타입의 CRD를 적용한다. 해당 CRD의 인스턴스는 주시(watch)할 컨피그맵에 대한 참조와 이 컨피그맵이 변경될 때 재시작할 파드를 지정한다. 이 접근 방식을 사용하면 트리거 애노테이션을 추가하기 위해 컨피그맵 자체를 수정할 필요가 없으므로, 파드에 대한 컨피그맵의 의존성이 제거된다. 또한, 컨트롤러 예제의 간단한 애노테이션 기반 접근 방식은 오직 컨피그맵으로만 단일 애플리케이션에 연결할 수 있는 반면에, CRD를 사용하면 컨피그맵과 파드를 임의로 조합할 수 있다.  

컨피그와처 사용자정의 자원은 다음 예제와 같다.  

```yaml
kind: ConfigWatcher
apiVersion: k8spatterns.io/v1
metadata:
  name: webapp-config-watcher
spec:
  configMap: webapp-config # 주시(watch)하기 위한 컨피그맵(ConfigMap) 참조
  podSelector: # 재시작할 파드를 결정하는 레이블 셀렉터
    app: webapp
```

이 정의에서, configMap 속성은 주시할 ConfigMap의 이름을 참조한다. podSelector 필드는 재시작 할 파드를 식별하는 레이블과 해당 값의 모음이다.  

다음 예제처럼 CRD를 사용해 컨피그와처 사용자저의 자원의 타입을 정의할 수 있다.  

```yaml
apiVersion: apiextensions.k8s.io/v1beta1
kind: CustomResourceDefinition
metadata:
  name: configwatchers.k8spatterns.io
spec:
  scope: Namespaced # 네임스페이스에 연결
  group: k8spatterns.io # 전용 API 그룹
  version: v1 # 초기 버전
  names:
    kind: ConfigWatcher # 컨피그와처(ConfigWatcher) CRD의 고유한 kind 
    singular: configwatcher # kubectl 같은 툴에서 사용할 차원의 레이블
    plural: configwatchers
  validation:
    openAPIV3Schema: # CRD에 대한 OpenAPI V3 스키마 명세
      properties:
        configMap:
          type: string
          description: "Name of the ConfigMap"
        podSelector:
          type: object
          description: "Label selector for Pods"
          additionalProperties:
            type: string
```

오퍼레이터가 해당 타입의 사용자정의 자원을 관리할 수 있으려면 오퍼레이터의 디플로이먼트에 적절한 권한이 있는 서비스어카운트(ServiceAccount)를 붙여야 한다. 이 작업을 위해, 다음 예제에서 전용 롤(Role)을 소개한다. 해당 롤은 롤바인딩(RoleBinding)에서 서비스어카운트에서 연결하기 위해 이후에 사용된다.  

```yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: config-watcher-crd
roles:
- apiGroups:
  - k8spatterns.io
  resources:
  - configwatchers
  - configwatchers/finalizers
  verbs: [ get, list, create, update, delete, deletecollection, watch ]
```

이러한 CRD를 사용하여 사용자정의 자원을 정의할 수 있다.  

CRD 자원을 적절히 활용하기 위해 CRD 자원을 평가하고 컨피그맵이 변경될 때 파드 재시작을 트리거(trigger)하는 컨트롤러를 구현해야 한다. 여기서 컨트롤러 예제의 스크립트를 확장해서 이벤트 루프를 컨트롤러 스크립트에 적용해보겠다.  

컨피그맵 업데이트의 경우 특정 애노테이션을 확인하는 대신 kind: ConfigWatcher의 모든 자원에 대해 쿼리를 수행하고 수정된 컨피그맵이 configMap: 값으로 포함되어 있는지 확인한다. 다음 예제는 레컨실리에이션(reconciliation) 루프를 보여준다. 다음 예제는 해당 오퍼레이터 설치에 대한 자세한 설명도 포함하고 있는 깃 리포지토리를 참조하기 바란다.  

```sh
curl -N -s $base/api/v1/${ns}/configmaps?watch=true | \ # 지정된 네임스페이스의 컨피그맵 변경을 주시(watch)하는 와치 스트림을 시작
while read -r event
do
  type=$(echo "$event" | jq -r '.type')
  if [ $type = "MODIFIED" ]; then # MODIFIED 이벤트만 확인
    
    watch_url="$base/apis/k8spatterns.io/v1/${ns}/configwatchers"

    config_map=$(echo "$event" | jq -r '.object.metadata.name')

    watcher_list=$(curl -s $watch_url | jq -r '.items[]') # 설치된 모든 컨피그와처(ConfigWatcher) 사용자정의 자원의 목록을 가져옴

    watchers=$(echo $watcher_list | jq -r "select(.spec.configMap == \"$config_map\") | .metadata.name") # 목록에서 해당 컨피그맵을 참조하는 모든 컨피그와처(ConfigWatcher) 항목을 추출

    for watcher in watchers; do # 찾아낸 모든 컨피그와처(ConfigWatcher)을 이용해 셀렉터로 설정된 파드를 삭제한다. 여기에서는 명료성을 위해 레이블 셀렉터 산출과 파드 삭제에 대한 로직은 생략했다. 전체 구현에 대해서는 깃 리포지토리의 예제 코드를 참조하기 바란다.
      label_selector=$(extract_label_selector $watcher)
      delete_pods_with_selector "$label_selector"
    done
  fi
done
```

여기 컨트롤러 예제의 해당 컨트롤러는 깃 리포지토리 예제에서 제공되는 샘플 웹 애플리케이션으로 테스트할 수 있다. 해당 디플로이먼트와의 유일한 차이점은, 여기에서는 애플리케이션 설정에 애노테이션이 없는 컨피그맵을 사용한다는 것이다.  

예제에서 소개한 셸 스크립트 기반 오퍼레이터는 상당히 기능적이지만, 매우 단순하며 예외나 오류 사례를 다루지 않는다. 실제 환경에서 쓰이는, 훨씬 더 흥미로운 운영 등급 예제도 많다. '실셰계의 뛰어난 오퍼레이터(Awesome Operators in the Wild)'(http://bit.ly/2Ucjs0J) 글에는 실제 오퍼레이터 목록이 있다. 이전에 프로메테우스(Prometheus) 오퍼레이터가 어떻게 프로메테우스 설치를 관리하는지에 대해서는 앞서 이미 살펴봤다. 또 다른 고 언어(Golang) 기반 오퍼레이터는 엣시디(Etcd) 키-값 저장소를 관리하고 데이터베이스 백업 및 복원 등의 운영 작업을 자동화하는 엣시디 오퍼레이터다.  

자바 프로그래밍 언어로 작성된 오퍼레이터로는, 쿠버네티스상에서 아파치 카프카 같은 복잡한 메시징 시스템을 관리하는 스트럼지(Strimzi) 오퍼레이터가 훌륭하다. JVM 오퍼레이터 툴킷도 뛰어난 자바 기반 오퍼레이터로서, 그루비(Groovy)나 코틀린(Kotlin) 같은 자바와 JVM기반 언어에서 오퍼레이터를 생성하기 위한 토대를 제공하며 예제 집합도 있다.  

#### 23.4. 정리  
<br/>

오퍼레이터를 사용하기에 앞서, 사용 예를 신중하게 검토해서 쿠버네티스 패러다임에 맞는지부터 결정해야 한다.  

대부분은 표준 자원으로 작업하는 일반 컨트롤러로 충분하다. 일반 컨트롤러는 CRD를 등록하기 위한 클러스터 관리자 권한이 없다는 장점이 있지만, 보안이나 유효성 검사에는 취약하다.  

오퍼레이터는 잘 반응하는 컨트롤러를 이용해 자원을 처리하는 선언적인 쿠버네티스 방식에 알맞는 사용자정의 도메인 로직을 모델링하는 데 적합하다. 좀 더 구체적으로 설명하자면, 다음 상황 중 하나에 해당하는 경우 애플리케이션 도메인에 CRD와 함께 오퍼레이터를 사용하는 것이 좋다.  

+ kubectl 같은 기존 쿠버네티스 툴에 긴밀하게 통합되기를 원하는 경우
+ 애플리케이션을 처음부터 설계할 수 있는 사전 작업 없이 애플리케이션을 처음부터 설계해야 하는 그린필드(greenfield) 프로젝트를 진행 중인 경우
+ 자원 경로, API 그룹, API 버전 관리, 특히 네임스페이스 같은 쿠버네티스 사상의 혜택을 누리고 싶은 경우
+ 주시(watch), 인증, 롤(role) 기반 권한 부여, 메타데이터의 셀렉터 등을 사용하여 API에 접근하기 위한, 우수한 클라이언트 지원을 원하는 경우  

사용자정의 사용 예가 이러한 기준에 알맞지만, 좀 더 유연하게 사용자정의 자원을 구현하고 유지하고 싶은 경우라면 사용자정의 API 서버 사용을 고려해보기 바란다. 그러나 쿠버네티스 확장 포인트를 모든 것을 해결할 수 있는 만능이 될 거라 생각해서는 안 된다.  

사용 예가 선언적이지 않은 경우, 관리할 데이터가 쿠버네티스 자원 모델에 맞지 않은 경우, 플랫폼에 긴밀하게 통합할 필요가 없는 등의 경우라면, 별도의 API를 작성해 종래의 서비스(Service)나 인그레스(ingress) 객체로 노출하는 편이 좋을 것이다.  

쿠버네티스 공식 문서(http://bit.ly/2FrfyJ6)에는 컨트롤러, 오퍼레이터, API 애그리게이션, 사용자정의 API 구현 등을 언제 사용하면 좋은지에 대해 기술되어 있다.  

#### 23.5. 참고 자료  
<br/>

+ 오퍼레이터 예제(http://bit.ly/2HvfIkV)
+ 오퍼레이터 프레임워크(http://bit.ly/2CKLYNl)
+ OpenAPI V3(http://bit.ly/2Tluk82)
+ 큐브빌더(Kubebuilder)(http://bit.ly/2I8w9mz)
+ 쿠버네티스 클라이언트 라이브러리(http://bit.ly/2Sh1XYk)
+ 메타컨트롤러(Metacontroller)(https://metacontroller.app/)
+ JVM 오퍼레이터 툴킷(https://github.com/jvm-operators)
+ CustomResourceDefinition으로 쿠버네티스 확장(http://bit.ly/2uk6Iq5)
+ 실세계의 뛰어난 오퍼레이터(Awesome Operators in the Wild)(http://bit.ly/2Ucjs0J)
+ 사용자정의 자원과 API 서버 애그리게이션(http://bit.ly/2FrfR6I)
+ 큐브빌더, 오퍼레이터 프레임워크, 메타컨트롤러 비교(http://bit.ly/2FpO4Ug)
+ TPR(Third Party Resource)은 사라지고, 쿠버네티스 1.7에 CRD 적용(http://bit.ly/2FQnCSA)
+ 사용자정의 자원을 위한 코드 생성(https://red.ht/2HIS9Er)
+ 고(Go)로 작성된 간단한 오퍼레이터(http://bit.ly/2UppsTN)
+ 프로메테우스 오퍼레이터(http://bit.ly/2HICRjT)
+ Etcd 오퍼레이터(http://bit.ly/2JTz8SK)
+ Memhog 오퍼레이터(https://github.com/secat/memhog-operator)  

### 24. 탄력적 스케일  
<br/>

탄력적 스케일(Elastic Scale) 패턴은 다양한 관점에서 애플리케이션을 스케일링(scaling)한다. 즉 파드 레플리카 수를 조정해 수평 스케일링을 하고, 파드에 대한 자원 요구사항을 조정하는 수직 스케일링을 하며, 클러스터 노드 수를 변경해 클러스터 자체를 스케일링한다.  

#### 24.1. 문제  
<br/>

쿠버네티스는 선언적으로 표현된 요청한 상태를 유지함으로써 수많은 불변 컨테이너로 구성된 분산 애플리케이션의 오케스트레이션 및 관리를 자동화한다. 그러나 지속적으로 자주 변하는 대다수 워크로드의 특성으로 인해, 요청한 상태가 어떻게 보여야 하는지를 파악하기란 쉬운 일이 아니다. 컨테이너가 얼마나 많은 자원을 요구할지, 그리고 주어진 시간에 서비스 수준을 적절히 충족시키기 위해 서비스에 얼마나 많은 레플리카가 필요할지를 정확히 알아내려면, 많은 시간과 노력이 필요하다. 다행히도 쿠버네티스를 사용하면 컨테이너의 자원, 요청한 서비스 레플리카 수, 클러스터의 노드 수 등을 쉽게 변경할 수 있다. 이러한 변경은 수동으로 수행되거나, 특정 규칙이 주어지면 완전히 자동화된 방식으로 수행될 수 있다.  

쿠버네티스는 고정된 파드와 클러스터 설정을 유지할 수 있을 뿐만 아니라, 외부 로드 및 용량관련 이벤트를 모니터링하며, 현재 상태를 분석하고, 원하는 성능에 맞게 자체 스케일할 수 있다. 이러한 종류의 관측을 통해 쿠버네티스는, 기대요인이 아닌 실제 사용 메트릭 기반의 견고한 특성을 얻고 조정한다.  

#### 24.2. 해결책  
<br/>

애플리케이션을 스케일링하는 2가지 주요 접근 방법으로는 수평 스케일링(horizontal scaling)과 수직 스케일링(vertical scaling)이 있다. 쿠버네티스에서 수평으로 스케일하는 것은 파드 레플리카를 더 많이 만드는 것과 같다. 수직으로 스케일하는 것은 파드가 관리하는 컨테이너를 실행하는 데 더 많은 자원을 제공함을 의미한다. 설명으로는 간단해 보이지만, 다른 서비스와 클러스터 자체에 영향을 미치지 않도록 공유 클라우드 플랫폼에서 오토스케일링(autoscaling)을 위한 애플리케이션 설정을 생성하려면 상당한 시행착오를 거쳐야 한다. 쿠버네티스는 애플리케이션에 가장 적합한 설정을 찾기 위한 다양한 기능과 기술을 제공하므로, 여기에서 간단히 살펴보겠다.  

##### 24.2.1. 수동 수평 스케일링  
<br/>

수동 수평 스케일링(manual horizontal scaling)이라는 이름에서 알 수 있듯이 수동 스케일링 접근 방식으로 쿠버네티스에 명령을 내리는 사람 운영자를 기반으로 한다. 오토스케일링이 없는 경우나, 장기간에 걸쳐 느리게 변화하는 로드를 처리하는 애플리케이션의 최적의 설정을 점진적으로 디스커버리하고 튜닝할 때, 수동 수평 스케일링 방식을 사용할 수 있다. 수동 접근방식의 장점은 반응에 의한 변경이 아니라, 앞서서 선행된다는 점이다. 예상되는 애플리케이션 로드나 적절한 시점을 알고 있으면, 오토스케일링으로 이미 증가된 로드에 반응하지 않고 미리 스케일할 수 있다. 예를 들어 다음과 같은 2가지 방식으로 수동 스케일링을 수행할 수 있다.  

###### 24.2.1.1. 명령형 스케일링  
<br/>

레플리카세트와 같은 컨트롤러는 특정 개수의 파드 인스턴가 항상 실행 상태에 있게 한다. 따라서 파드 스케일링은 요청한 레플리카 수로 변경만 하면 되므로 매우 간단하다. random-generator라는 이름의 디플로이먼트가 있다면, 다음 예제에 표시된 것처럼 하나의 명령을 써서 4개의 인스턴스로 스케일할 수 있다.  

```sh
kubectl scale random-generator --replicas=4
```

이렇게 변경한 다음, 레플리카세트는 스케일 업(scale up)할 추가 파드를 만들거나, 요청한 것보다 많은 파드가 있는 경우에는 스케일 다운(scale down)하여 파드를 삭제할 수도 있다.  

###### 24.2.1.2. 선언적 스케일링  
<br/>

스케일 명령을 사용하는 것은 손쉬울 뿐 아니라 긴급한 상황에 신속하게 대응하기에도 적합하지만, 클러스터 외부에서는 이 설정이 유지되지 않는다. 일반적으로 모든 쿠버네티스 애플리케이션은 레플리카 수를 비롯한 자원 정의를 소스 컨트롤 시스템에 저장한다. 원래 정의에서 레플리카세트를 다시 생성하면 레플리카 수가 이전의 수로 다시 변경된다. 이렇게 설정이 변경되는 것을 피하고 변경을 백포팅(backporting)하기 위한 운영 프로세스를 도입하려면, 레플리카세트 또는 여타 정의에서 요청한 수의 레플리카를 선언적으로 변경하고 변경사항을 다음 예제처럼 쿠버네티스에 적용하는 편이 낫다.  

```sh
kubectl apply -f random-generator-deployment.yaml
```

레플리카세트, 디플로이먼트, 스테이트풀세트 같은 여러 파드를 관리하는 자원을 스케일할 수도 있다. 퍼시스턴트 스토리지를 사용하는 스테이트풀세트를 스케일할 때는 비대칭 동작에 주목하자. 스테이트풀세트에 .spec.volumeClaimTemplates 항목이 있는 경우에는 스케일링하는 동안 PersistentVolumeClaim(이하 PVC)가 생성되지만, 스케일 다운(scale down)할 때는 스토리지가 삭제되는 것을 방지하기 위해 PVC가 삭제되지 않는다.  

스케일 가능하지만 다른 네이밍 규칙을 따르는 쿠버네티스 자원은 바로 잡(Job) 자원이다. .spec.replicas 대신 .spec.parallelism 필드를 변경해 동일한 파드의 여러 인스턴스를 동시에 실행하도록 잡을 스케일할 수 있다. 더 많은 처리 단위로 증가된 용량은 단일 논리 단위로 작동하므로 의미적으로 동일하다.  

자원 필드를 기술하는 경우, JSON 경로 표기법을 사용한다. 예를 들어 .spec.replicas는 자원의 spec 섹션의 replicas 필드를 가리킨다.  

수동 스케일 방식(명령형과 선언적)은 모두 사람이 애플리케이션 로드의 변화를 관측 또는 예상하고, 스케일할 크기를 결정해, 클러스터에 적용하기를 기대한다. 자동 스케일 방식과 효과는 동일하지만, 자주 변경되고 지속적인 적응이 필요한 동적 워크로드(dynamic workload) 패턴에는 적합하지 않다. 이제 스케일링 결정 자체를 자동화하는 방법을 살펴보겠다.  

##### 24.2.2. 수평 파드 오토스케일링  
<br/>

대부분의 워크로드는 시간이 지남에 따라 변화하는 동적 특성 때문에, 고정된 스케일링 설정을 적용하기가 어렵다. 그러나 쿠버네티스 같은 클라우드 네이티브 기술을 사용하면 변화하는 로드에 따라 적응하는 애플리케이션을 만들 수 있다. 쿠버네티스의 오토스케일링을 통해, 고정되어 있지 않으면서도 다른 로드를 충분히 처리할 수 있는 용량을 보장하는 다양한 애플리케이션 용량을 정의할 수 있다. 이러한 동작을 가능케 하는 가장 간단한 방법은 HorizontalPodAutoscaler(이하 HPA)를 사용해 파드의 수를 수평으로 스케일하는 것이다.  

다음 예제의 명령을 사용하면 random-generator 디플로이먼트를 위한 HPA를 만들 수 있다. HPA가 효력을 지니려면, CPU에 대한 .spec.resources.requests 제한을 디플로이먼트에 선언하는 것이 중요하다. 또 다른 요구사항으로는, 클러스터 전체의 자원 사용량을 데이터를 집계하는 메트릭 서버를 활성화하는 것이다.  

```sh
kubectl autoscale deployment random-generator --cpu-percent=50 --min=1 --max=5
```

이 명령으로, 다음 예제의 HPA 정의를 생성한다.  

```yaml
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: random-generator
spec:
  minReplicas: 1 # 항상 실행해야 하는 최소 파드의 수
  maxReplicas: 5 # HPA가 스케일 업(scale up)할 수 있는 최대 파드의 수
  scaleTargetRef: # 해당 HAP와 연동해야 하는 객체에 대한 참조
    apiVersion: extensions/v1beta1
    kind: Deployment
    name: random-generator
  metrics:
  - resource:
      name: cpu
      target:
        averageUtilization: 50 # 파드에 요청된 CPU 자원에서 원하는(desired) CPU 사용량의 백분율. 예를 들어 파드에 200m의 .spec.resources.requests.cpu가 있는 경우, 평균 100m 이상의 CPU(=50%)를 사용하면 스케일 업이 발생한다.
        type: Utilization
    type: Resource
```

위의 예제에서는 자원의 API 버전 v2beta2를 사용해 HPA를 구성한다. 이 버전은 현재 개발 중이며 기능상 버전 v1의 상위 집합이다. 버전 v2를 사용하면 메모리 사용량이나 애플리케이션별 사용자정의 메트릭 같은 v1의 CPU 사용량보다 훨씬 많은 기준을 사용할 수 있다. kubectl get hpa.v2beta2.autoscaling -o yaml을 사용해서, kubectl autoscale로 만든 v1 HPA 자원을 v2 자원으로 쉽게 변환할 수 있다.  

해당 정의는 1~5개의 파드 인스턴스를 유지하면서 파드의 .spec.resources.requests 선언에서 지정된 CPU 자원의 약 50%에 해당하는 평균 파드 CPU 사용량이 유지되도록 HPA 컨트롤러에 지시한다. 디플로이먼트, 레플리카세트, 스테이트풀세트 같은 scale 하위 자원을 지원하는 모든 자원에 이러한 HPA를 적용할 수는 있지만, 부작용을 고려해야 한다. 디플로이먼트는 업데이트할 때 HPA 정의를 복사하지 않고 새로운 레플리카세트를 만든다. 디플로이먼트에서 관리하는 레플리카세트에 HPA를 적용하면, 새로운 레플리카세트에 복사되지 않고 손실되어버릴 것이다. 그러므로 HPA를 더 높은 수준인 디플로이먼트 추상화에 적용해서, HPA를 유지하고 새로운 레플리카세트 버전에 적용해야 한다.  

이제 HPA가 어떻게 사람 운영자를 대체해 오토스케일링을 보장할지 살펴보겠다. 높은 수준에서 HPA 컨트롤러는 다음 단계를 지속적으로 수행한다.  

(1) HPA 정의에 따라 스케일링될 파드에 대한 메트릭을 가져온다. 파드에서 직접 메트릭을 읽지 않고 집계된 메트릭을(설정을 했다면 사용자정의 메트릭이나 외부 메트릭도) 제공하는 쿠버네티스 메트릭 API에서 메트릭을 읽는다. 파드 레벨 자원 메트릭은 메트릭 API에서 가져오고 다른 모든 메트릭은 쿠버네티스의 사용자정의 메트릭 API에서 가져온다.  

(2) 현재 메트릭 값과 목표하는 요청한 메트릭 값을 기반으로 필요한 레플리카 수를 계산한다. 수식으로 간단히 정리하면 다음과 같다.  

요청한 레플리카 수 = [현재 레플리카 수 × 현재 메트릭 값 / 요청한 메트릭 값]  

예를 들어 지정된 CPU 자원 요청 값 중에서 90%가 현재 CPU 사용량 메트릭 값이고 50%는 요청한 값이 단일 파드가 있는 경우, 레플리카 수는 [1 × 90 / 50]=2가 된다. 실제 구현은 실행 중인 여러 개의 파드 인스턴스를 고려해야 하고, 여러 메트릭 타입을 다뤄야 하며, 다양한 예외 사례와 변동하는 값까지도 고려해야 하기 때문에 더욱 복잡하다. 예를 들어 여러 메트릭이 지정된 경우, HPA는 각 메트릭을 개별적으로 평가한 후 가장 큰 값을 제안한다. 계산이 모두 끝나면 최종 출력은 측정 값을 '요청한 임계 값' 아래로 유지하는 '요청한 레플리카 수'를 나타내는 정수가 된다.  

오토스케일링된 자원의 replicas 필드는 계산된 숫자로 업데이트되며, 여타 컨트롤러는 새롭게 요청된 상태를 얻고 유지하기 위해 약간의 작업을 수행한다.  

오토스케일링은 여전히 빠르게 발전 중인 많은 하위레벨의 세부사항이 있는 쿠버네티스 영역이다. 각 세부사항은 오토스케일리의 전체 동작에 큰 영향을 줄 수 있다.  

대체로, 다음과 같은 메트릭 타입이 있다.  

+ 표준 메트릭  
표준 메트릭은 Resource와 동일하게 .spec.metrics.resource[:].type으로 선언되며 CPU와 메모리 같은 자원 사용량 메트릭을 나타낸다. 또한, 표준 메트릭은 포괄적이며, 모든 클러스터의 모든 컨테이너에서 동일한 이름으로 사용할 수 있다. 백분율이나 절댓값으로 지정할 수 있다. 두 경우에서 모두 값은 보장된 자원량을 기준으로 하며, limits 값이 아닌 컨테이너 자원 requests 값이다. 표준 메트릭은 일반적으로 메트릭 서버나 힙스터(Heapster) 컴포넌트에서 제공하는 가장 사용하기 쉬운 메트릭 타입으로 클러스터 애드온(addon)으로 실행할 수 있다.

+ 사용자정의 메트릭  
사용자정의 메트릭은 Object나 Pod과 동일하게 .spec.metrics.resource[:].type으로 선언되며, 클러스터별로 각기 다른 고급 클러스터 모니터링 설정이 필요하다. 파드 타입의 사용자정의 메트릭은 이름에서 알 수 있듯이 특정 파드 메트릭을 기술하지만, 객체 타입은 모든 여타 객체를 기술할 수 있다. 사용자정의 메트릭은 custom.metrics.k8s.io API 경로의 애그리깃(Aggregated) API 서버로 수행되고 프로메테우스(Prometheus), 데이터독(Datadog), 마이크로소프트 애저(Azure), 구글 스택드라이버(Stackdriver) 등의 다양한 메트릭 어댑터에서 제공된다.  

+ 외부 메트릭  
외부 메트릭은 쿠버네티스 클러스터의 일부가 아닌 자원을 기술하는 메트릭이다. 예를 들어 클라우드 기반 큐 서비스로 메시지를 이용하는 파드가 있을 수 있다. 주로 이러한 시나리오에서 큐 크기에 따라 컨슈머 파드의 수를 스케일하고 싶을 때, 사용자정의 메트릭과 유사하게 외부 메트릭 플러그인이 외부 메트릭을 적용한다.  

오토스케일링을 올바르게 수행하는 것은 쉽지 않으며 어느 정도의 실험과 튜닝이 필요한다. 다음은 HPA를 설정할 때 고려해야 할 몇 가지 주요 영역이다.  

+ 메트릭 선택  
오토스케일링과 관련해 가장 중요한 것은 어떤 메트릭을 사용할지 결정하는 것이다. HPA를 유용하게 사용하려면 메트릭 값과 파드 레플리카 수 사이에 직접적인 상관관계가 있어야 한다. 예를 들어, 선택한 메트릭이 초당 요청(Queries-per-Second)(HTTP 초당 요청 같은) 타입인 경우, 파드 수를 늘리면 쿼리가 더 많은 파드로 전송되므로, 평균 쿼리 수는 줄어 든다. 쿼리의 양과 CPU 사용량 간에 직접적인 상관관계가 있으므로, 메트릭이 CPU 사용량인 경우에도 마찬가지다(쿼리 수가 증가하면 CPU 사용량이 증가함). 하지만 메모리 소모량은 그렇지 않다. 만약 서비스가 특정 양의 메모리를 소비할 때 애플리케이션이 클러스터링되어 있지 않고, 여타 인스턴스들을 인지하지 못하며, 메모리를 분산시키고 해제하는 메커니즘을 가지고 있지 않다면, 더 많은 파드 인스턴스를 시작한다고 해도 메모리가 줄어들지 않을 것이다. 메모리가 해제되어 메트릭에 반영되지 않으면, HPA는 상위 레플리카 임계 값에 도달할 때까지 메모리를 줄이기 위해 점점 더 많은 파드를 생성할 것이고, 이것은 원하는 동작이 아닐 것이다. 따라서 파드 수와 직접 관련된 (가급적 선형적인) 메트릭을 선택해야 한다.

+ 스래싱(thrashing) 방지  
로드가 안정적이지 않을 때, 변동이 심한 레플리카 수를 야기하는 모순된 결정으로 인해 조급한 실행을 하는 것을 피하기 위해, HPA는 다양한 기술을 적용하고 있다. 예를 들어 스케일 업 중에 HPA는 파드가 초기화될 때 높은 CPU 사용량 샘플릉 무시함으로써 로드 증가에 대해 순조로운 반응을 보장한다. 스케일 다운으로 인해 짧은 시간 동안 사용량이 줄어도는 것을 방지하기 위해, 스케일 다운 중에 컨트롤러는 설정 가능한 시간대 동안의 모든 스케일 권장 사항을 고려해서, 시간대 안에서 가장 높은 권장사항을 선택한다. 이 모든 것이 HPA를 임의의 메트릭 변동에 더욱 안정적이게 만든다.

+ 지연된 반응  
메트릭 값을 기반으로 스케일을 발생시키는 것은 여러 쿠버네티스 컴포넌트를 포함하는 다단계 절차다. 먼저, cAdvisor(컨테이너 어드바이저) 에이전트는 큐블릿에 쓰일 메트릭을 주기적으로 수집하고, 메트릭 서버는 큐블릿에서 정기적으로 메트릭을 수집한다. HPA 컨트롤러 루프는 정기적으로 실행되고, 수집된 메트릭을 분석한다. HPA 스케일링 공식은 변동/스래싱(thrashing)을 방지하기 위해 약간의 지연된 반응을 도입한다. 이 모든 동작은 원인과 스케일링 반응 사이의 지연으로 축적된다. 해당 파라미터를 튜닝하여 더 많은 지연을 적용하면 HPA의 응답성이 떨어지고, 지연을 줄이면 플랫폼의 로드가 증가하며 스래싱도 증가한다. 쿠버네티스를 자원과 성능에 균형을 유지하며 쿠버네티스를 설정하기 위한 연구가 계속되고 있다.  

###### 24.2.2.1. 케이네이티브 서빙  
<br/>

케이네이티브 서빙(Knative Serving)을 활용하면, 더욱 고급의 수평 스케일링 기술을 가능하다. 이러한 고급 기능에는 서비스(Service)를 지원하는 파드 세트를 0으로 스케일 다운하고 수신 요청과 같은 특정 트리거가 발생할 때만 스케일 업할 수 있는 '0으로 스케일(scale-to-zero)'이 포함된다. 이를 위해 케이네이티브는 서비스 메시 이스티오(Istio) 위에 구축되어 파드에 투명한 내부 프록시 서비스를 제공한다. 케이네이티브 서빙은 쿠버네티스 표준 메커니즘을 뛰어넘는 더 빠르고 유연한 수평 스케일링 동작을 위한 서비리스 프레임워크의 기반을 제공한다.  

케이네이티브 서빙은 아직 개발 초기 단계인 프로젝트이다.  

##### 24.2.3. 수직 파드 오토스케일링  
<br/>

수평 스케일링은 특히 스테이트리스(stateless) 서비스에 지장을 덜 주기 때문에 수직 스케일링보다 선호된다. 스테이트풀(stateful) 서비스에 수직 스케일이 선호될 것 같지만 그렇지는 않고, 수직 스케일이 유용한 경우는 실제 로드 패턴 기반으로 서비스의 실제 자원 요구를 튜닝할 때다. 시간이 흘러 로드가 변경되면, 왜 파드의 올바른 레플리카 수를 찾는 것이 어렵고 불가능한지에 대해 앞에서 살펴봤다. 수직 스케일링에서도 컨테이너에 대한 올바른 requests와 limits를 찾을 대 이와 같은 문제가 있다. 쿠버네티스 수직 파드 오토스케일러(Vertical Pod Autoscaler, 이하 VPA)는 실제 사용 피드백을 기반으로 자원을 조정 및 할당하는 프로세스를 자동화함으로써 이러한 과제를 해결하는 것을 목표로 한다.  

파드의 모든 컨테이너는 CPU와 메모리 requests를 지정할 수 있으며, 이는 파드가 스케줄 되는 위치에 영향을 준다. 즉 파드의 자원 requests와 limits는 파드가 스케줄러와 약정함으로써 특정한 양의 자원을 보장하거나, 자원이 보장되지 않으면 파드가 스케줄되지 못하게 한다. 메모리 요청을 너무 낮게 설정하면 노드가 더욱 꽉 채워지기 때문에, 메모리 압박으로 인해 메모리 부족 오류(out of memory)나 워크로드 축출이 발생할 수 있다. CPU limits가 너무 낮으면, CPU 고갈이나 성능이 저하된 워크로드가 발생할 수 있다. 반면에 너무 많은 자원 requests를 지정하면, 불필요한 용량이 할당되어 자원이 낭비된다. 클러스터 사용률과 수평 스케일링의 호율성에 영향을 미치므로, 가능한 한 정확하게 자원을 요청하는 것이 중요하다. VPA가 이를 해결하는 데 어떻게 도움이 되는 한번 살펴보자.  

다음 예제에서는 VPA와 메트릭 서버가 설치된 클러스터에서 VPA 정의를 사용하는 파드의 수직 오토스케일링 실례를 보여준다.  

```yaml
apiVersion: poc.autoscaling.k8s.io/v1alpha1
kind: VerticalPodAutoscaler
metadata:
  name: random-generator-vpa
spec:
  selector:
    matchLabels: # 관리할 파드를 식별하기 위한 레이블 셀렉터
      app: random-generator
  updatePolicy:
    updateMode: "Off" # VPA의 변경사항 적용 방법에 대한 업데이트 정책
```

VPA 정의에는 다음과 같은 주요 부분이 있다.  

+ 레이블 셀렉터  
처리해야 할 파드를 식별하여 스케일할 대상을 지정한다.

+ 업데이트 정책  
VPA가 변경 사항을 적용하는 방법을 제어한다. Initial 모드에서는 파드 생성 시간 동안만 자원 요청을 할당할 수 있고 이후에는 할당할 수 없다. 기본 Auto 모드에서는 생성 시 파드에 자원을 할당할 수 있을 뿐만 아니라, 추가적으로 파드를 축출하고 다시 스케줄링하는 등 파드의 수명주기 동안 업데이트할 수 있다. Off 값은 파드에 대한 자동 변경을 비활성화하지만, 자원 값 제안을 받는다. 이것은 컨테이너의 올바른 크기를 발견하기 위한 일종의 드라이 런(dry run)으로서, 직접 적용하지는 않는다.  

VPA 정의에는 VPA가 권장 자원을 계산(컨테이너당 자원 상환 및 하한을 설정하는 등)하는 방법에 영향을 주는 자원 정책을 넣을 수 있다.  

설정된 .spec.updatePolicy.updateMode에 따라, VPA에는 별개의 시스템 컴포넌트가 포함된다. 추천자(recommender), 어드미션 플러그인(admission plugin), 업데이터(updater)의 3가지 VPA 컴포넌트는 모두 독립적이며, 대체 구현으로 교체할 수 있다. 지능적으로 추천하는 추천자 모듈은 구글의 보그(Borg) 시스템에서 착안되었다. 현재 구현에서는 특정 기간(기본적으로 8일) 동안 로드 중에, 컨테이너의 실제 자원 사용량을 분석하고 히스토그램을 생성하여 해당 기간 동안 높은 백분위 값을 선택한다. 메트릭 외에도 자원을 고려하고, 축출과 OutOfMemory 이벤트 같은 특별한 메모리 관련 파드 이벤트도 고려한다.  

위의 예제에서는 .spec.updatePolicy.updateMode를 Off로 선택했지만, 스케일된 파드에서 각각 다른 다른 레벨의 잠재적인 중단을 초래하는 2가지 옵션이 더 있다. 무중단에서 시작해 좀 더 중단을 초래하는 순서로 updateMode의 다른 값이 어떻게 작동하는지 한번 살펴보자.  

+ updateMode: Off  
VPA 추천자(recommeder)는 파드 메트릭과 이벤트를 수집한 후 권장사항(recommedation)을 생성하고 VPA 권장사항은 항상 VPA 자우너의 status 섹션에 저장된다. Off는 여기까지만 진행한다. 즉 VPA 추천자는 분삭하고 권장사항을 생성하지만, 파드에는 적용되지 않는다. 이 모드는 변경사항을 적용하지 않고 중단 없이 파드 자원 소비에 대한 통찰력을 얻을 대 유용하다. 이 정보를 바탕으로 사용자가 원하는 경우 결정을 내린다.

+ updateMode: Initial  
이 모드에서 VPA는 한 단계 더 나아간다. 추천자 컴포넌트가 수행하는 작업 외에도, VPA 어드미션 플러그인을 활성화하여 새로 생성된 파드에만 권장사항을 적용한다. 예를 들어 파드가 HPA를 통해 수동으로 스케일되거나, 디플로이먼트에 의해 업데이트되거나, 어떤 이유로 축출된 후 재시작된 경우, 파드의 자원 요청 값은 VPA 어드미션 컨트롤러에 의해 업데이트된다.  

VPA 어드미션 컨트롤러는 VPA 레이블 셀렉터와 일치하는 새로운 파드의 requests를 오버라이드(override)하는 변형(mutating) 어드미션 플러그인이다. 이 모드는 실행 중인 파드를 재시작하지 않지만, 새로 생성된 파드의 자원 요청을 변경하므로 부분적으로는 중단된다. 이는 새로운 파드를 스케줄할 위치를 결정하는 데 영향을 줄 수 있다. 또한 권장된 자원 요청을 적용한 후, 파드가 다른 노드로 스케줄되어 예기치 않은 결과 발생할 수도 있다. 또는 클러스터에 용량이 충분하지 않으면, 파드가 노드에 스케줄되지 않을 수도 있다.  

+ updateMode: Auto  
앞에서 설명한 권장사항 생성과 새로 생성된 파드를 위한 적용 외에도, 해당 모드에서 VPA는 업데이트된 컴포넌트를 활성화한다. 해당 컴포넌트는 레이블 셀렉터와 일치하는, 실행 중인 파드를 축출한다. 축출 후 VPA 어드미션 플러그인 컴포넌트가 파드를 재생성하여 자원 요청을 업데이트한다. 따라서 해당 접근 방식은 모든 파드를 재시작해서 권장 사항을 강제 적용하여 앞에서 설명한 예기치 않은 스케줄 문제를 일으킬 수 있으므로 가장 많은 중단을 일으킨다.  

쿠버네티스는 불변의 파드 spec 정의로 불변의 컨테이너를 관리하도록 설계되었다. 이렇게 하면 수평 스케일은 간단해지지만, 스케줄리에 영향을 미치고 서버스 중단을 초래할 수 있는 파드 삭제와 재생성을 요청하는 등 수직 스케일에는 많은 문제가 생긴다. 파드가 스케일 다운되고 중단 없이 이미 할당된 자원을 해재하려는 경우에도 마찬가지다.  

VPA와 HPA를 함께 사용하는 것은 또 다른 우려 사항이다. 이러한 오토스케일러(autoscaler)가 아직가진 서로를 인식하지 않아 원치 않는 동작이 발생될 수 있기 때문이다. 예를 들어 HPA가 CPU와 메모리 같은 자원 메트릭을 사용하고 VPA도 동일한 값에 영향을 주는 경우, 수직과 수평으로 동시에 이중 스케일된 파드가 생길 수 있다(이중 조정됨).  

VPA는 아직 베타 버전이며 향후 활발히 사용되면 변경될 가능성도 있다. 그러나 VPA는 자원 소비를 크게 향상시킬 가능성이 있는 기능이다.  

##### 24.2.4. 클러스터 오토스케일링  
<br/>

클라우드 컴퓨팅의 신조 중 하나는 사용한 만큼만 지불하는 자원 소비다. 필요할 때 필요한 만큼만 클라우드 서비스를 사용할 수 있다. CA는 쿠버네티스가 실행 중인 클라우드 제공업체와 긴밀히 소통하여, 사용량이 많을 때는 추가 노드를 요청하거나 사용량이 적을 때는 유휴 노드를 종료해 인프라스트럭처 비용을 낮춘다. HPA와 VPA가 파드 레벨의 스케일을 수행하고 클러스터 내 서비스 용량의 탄력성을 보장하는 반면, CA는 클러스터 용량의 탄력성을 보장하기 위해 노드 확장성을 제공한다.  

CA는 쿠버네티스 애드온(addon)으로 구동되어야 하고 최소 노드 수와 최대 노드 수가 설정되어야 한다. 또한 CA는 쿠버네티스 클러스터가 클라우드 컴퓨팅 인프라스트럭처상에서 실행 중일 때만 기능을 수행할 수 있다. 클라우드 컴퓨팅 인프라스트럭처상에서는 노드가 필요에 따라 프로비저닝되거나 폐기될 수 있으며, AWS, 마이크로소프트 애저, 구글 컴퓨트 엔진 같은 쿠버네티스 CA를 지원한다.  

+ 클러스터 API  
모든 주요 클라우드 제공업체는 쿠버네티스 CA를 지원한다. 그러나 각 클라우드 제공업체가 쿠버네티스 CA 지원을 위해 앞다퉈 자사 플러그인을 만듦으로써, 벤더 고착(vendor lock-in) 현상과 제공업체마다 상이한 CA 지원이 야기되었다. 다행히 클러스터 API 쿠버네티스 프로젝트가 있어서, 클러스터 생성, 구성, 관리 등을 위한 API를 제공한다. AWS, 애저, GCE, 브이스피어(vSphere), 오픈스택 등 모든 주요 퍼블릭 및 프라이빗 클라우드 제공업체가 클러스터 API를 지원한다. 또한 클러스터 API는 CA가 온프레미스 쿠버네티스 설치를 가능하게 한다. 클러스터 API의 핵심은 백그라운드에서 실행되는 머신 컨트롤러로서, 쿠버매틱(Kubermatic) machine-controller나 오픈시프트의 machine-API-operator 같은 여러 독립적인 구현이 이미 존재한다. 향후 클러스터 API는 클러스터 오토스케일링의 중추가 될 수도 있으므로 주시할 필요가 있다.  

CA는 클러스터에 새 노드를 추가하거나 클러스터에서 노드를 제거하는 등의 2가지 작업을 주로 한다. 이러한 작업이 어떻게 수행되는지 한번 살펴보자.  

###### 24.2.4.1. 새로운 노드 추가(스케일 업)
<br/>

부하가 변동하는(낮, 주말, 연휴 기간 등은 분주한 시간대이고 그 밖의 시간대는 부하가 훨씬 적음) 애플리케이션의 경우 이러한 요구를 충족하기 위해 상황에 따라 다르게 용량을 설정해야 한다. 클라우드 제공업체로부터 가장 부하가 많은 시간대 기준으로 고정 용량을 구매할 수도 있지만, 이렇게 하면 부하가 적은 시간대에도 비용을 지불해야 하므로 클라우드 컴퓨팅의 이점이 줄어든다. 이런 경우에 CA를 적용하면 정말 유용하다.  

파드가 수동이나 HPA 또는 VPA를 통해 수평이나 수직으로 스케일될 때, 요청된 CPU나 메모리를 충족시키기에 충분한 용량의 노드에 레플리카를 할당해야 한다. 만약 클러스터에 파드의 모든 요구사항을 충족시키는 용량을 갖춘 노드가 없다면, 파드는 스케줄 불가(unschedulable)로 표시되고 해당 노드가 발견될 때까지 대기 상태를 유지한다. CA는 이러한 파드를 모니터링해 새로운 노드 추가가 파드의 요구를 충족시키는지 여부를 확인하고, 만약 충족시킨다면, CA는 클러스터 크기를 조정하고 대기 중인 파드를 수용한다.  

CA는 임의의 노드로 클러스터를 확장할 수 없다. 즉 클러스터가 실행 중인 사용 가능한 노드 그룹에서 노드를 선택해야 한다. 노드 그룹의 모든 시스템이 동일한 용량과 동일한 레이블릉 가지며, 로컬 매니페스트 파일 또는 데몬세트에 의해 지정된 동일한 파드를 실행한다고 가정한다. 이 가정은 새로운 노드에 의해 얼마나 많은 여분의 파드 용량이 클러스터에 추가될지 CA가 추정하기 위해 필요하다.  

대기 중인 파드의 요구를 여러 노드 그룹이 충족시키는 경우, CA는 익스팬더(expander)라고 부르는 별도의 방책으로 노드 그룹을 선택하도록 설정할 수 있다. 익스팬더는 최소 비용, 최소 자원 낭비, 최대 수의 파드 수용, 무작위 방식 등으로 우선순위를 설정해서 추가 노드로 노드 그룹을 확장할 수 있다. 노드 선택이 성공적으로 마무리되면, 곧바로 클라우드 제공업체는 새로운 머신을 프로비저닝해서 API 서버에 등록해야 한다. 여기서 새로운 머신은, 대기 중인 파드를 호스팅할 새로운 쿠버네티스 노드가 된다.

###### 24.2.4.2. 노드 제거(스케일 다운)  
<br/>

서비스 중단 없이 파드 또는 노드를 스케일다운하는 것은 늘 복잡하고 많은 체크가 요구된다. 스케일 업할 필요가 없으며 노드가 필요 없다고 식별되면, CA는 스케일 다운을 수행한다. 노드가 다음과 같은 주요 조건을 충족하는 경우에는 스케일 다운할 수 있다.  

+ 절반이 넘는 노드 용량이 사용되고 있지 않은 경우, 즉 노드에 있는 모든 파드의 요청된 모든 CPU와 메모리의 합이 노드 할당 가능 자원 용량의 50% 미만인 경우다.

+ 노드상의 이동 가능한 모든 파드(로컬에서 매니페스트 파일로 실행되지 않은 파드 또는 데몬세트로 생성된 파드)를 다른 노드에 배치할 수 있는 경우, 이를 판명하기 위해 CA는 스케줄링 시뮬레이션을 수행하고, 축출될 모든 파드의 사후 위치를 확인한다. 여전히 스케줄러가 결정하는 파드의 최종 위치는 각기 다를 수 있지만, 시뮬레이션을 통해 파드에 쓰일 여분의 용량이 있음을 확인할 수 있다.

+ 애노테이션으로 노드를 스케일 다운되지 않게 제외하는 등의 노드 삭제를 방지할 여타 이유가 없는 경우

+ PodDiruptionBudget이 있는 파드, 로컬 저장소가 있는 파드, 축출을 방지하는 애노테이션이 있는 파드, 컨트롤러 없이 생성된 파드, 시스템 파드 등과 같이, 이동 불가능한 파드가 없는 경우  

다른 노드에서 시작할 수 없는 파드가 삭제되는 일이 결코 없도록, 이와 같은 사항을 모두 확인해야 한다. 이와 같은 모든 상황이 잠시(기본 값은 10분) 지속되면 노드는 삭제 대상이 된다. 노드를 스케줄 불가(unschedulable)로 표시하고 모든 파드를 다른 노드로 이동시킴으로써, 노드는 삭제된다.  

파드와 노드를 스케일 하는 것은 분리되어 있지만 상호 보안적인 절차이다. HPA나 VPA는 사용량 메트릭 및 이벤트를 분석하고 파드를 스케일 할 수 있다. 클러스터 용량이 부족하면 CA가 기능하여 용량이 늘어난다. 배치 잡(batch job), 반복 작업, 지속적인 통합 테스트, 일시적인 용량 증가가 필요한 기타 피크(Peak) 작업 등으로 인해 클러스터 로드에 불규칙성이 있는 경우에도 CA가 유용하다. CA는 용량을 늘리고 줄일 수 있으며 클라우드 인프라스트럭처 비용을 크게 절감할 수 있다.  

##### 24.2.5. 스케일링 레벨  
<br/>

사람 운영자가 여기에 나열된 대부분의 작업을 수동으로 수행할 수는 있겠지만, 이는 클라우드 네이티브 방식이라 할 수 없다. 대규모 분산 시스템 관리를 위해서는, 반복적인 작업의 자동화가 반드시 필요하다. 스케일링을 자동화하고, 사람 운영자는 쿠버네티스 오퍼레이터로 자동화할 수 없는 작업에 주력하는 접근방식이 바람직하다.  

###### 24.2.5.1. 애플리케이션 튜닝  
<br/>

가장 세부적인 레벨로는 애플리케이션 튜닝 기술이 있다. 쿠버네티스 관련 작동(activity)이 아니지만, 가장 먼저 취할 수 있는 조치는 컨테이너에서 실행 중인 애플리케이션을 튜닝하여 할당된 자원을 최대한 잘 사용하는 것이다. 이 작업은 서비스가 스케일될 때마다 수행되는 것이 아니라, 운영에 들어가기 바로 전에 수행해야 한다. 예를 들어 자바 런타임의 경우는, 컨테이너가 가져갈 가용 CPU의 일부분을 최대로 사용하기 위한 적절한 크기의 스레드 풀(thread pool)로 튜닝하는 것이다. 그런 다음 힙(heap), 넌힙(nonheap), 스레드 스택 크기 등의 각 메모리 영역을 튜닝하는 것이다. 이러한 값 조정은 일반적으로 코드 변경이 아니라 설정 변경을 통해 수행된다.  

컨테이너 네이티브(container-native) 애플리케이션은 공유된 전체 노드 용량이 아닌 할당된 컨테이너 자원을 기반으로, 스레드 수에 대한 올바른 기본 값과 애플리케이션의 메모리 크기를 계산할 수 있는 시작 스크립트를 사용한다. 이러한 스크립트를 사용하는 것이 첫 번째 단계다. 한 단계 더 나아가 넷플릭스의 적응형 동시성 제한(Adaptive Concurrency Limits) 라이브러리 등의 라이브러리나 기술을 사용하면 애플리케이션에서 자체 프로파일링 및 조정을 통해 동시성 제한을 동적으로 계산할 수 있다. 이는 수동으로 서비스를 튜닝할 필요가 없는 일종의 앱 내(in-app) 오토스케일이다.  

애플리케이션을 튜닝하면 코드를 변경할 때처럼 역효과가 발생할 수 있으므로, 어느 정도 테스트를 거쳐야 한다. 예를 들어 애플리케이션의 힙 크기를 변경하면, OutOfMemory 오류로 인해 애플리케이션이 종료될 수 있으며 수평 스케일은 도움이 되지 않는다. 반면에, 애플리케이션이 컨테이너에 할당된 자원을 제대로 소비하지 않으면, 파드를 수평 또는 수직으로 확장하거나 더 많은 노드를 프로비저닝하는 것은 별 효과가 없을 것이다. 그래서 이 레벨에서 스케일을 튜닝하면 다른 모든 스케일링 방법에 영향을 미치고 지장을 줄 수 있지만, 최적의 애플리케이션 동작을 얻으려면 한 번 이상 수행해야 한다.  

###### 24.2.5.2. 수직 파드 오토스케일링  
<br/>

애플리케이션이 컨테이너 자원을 효과적으로 소비한다고 가정하면, 다음 단계는 컨테이너에 올바른 자원 요청 및 제한을 설정하는 것이다. 실제 소비에 의해 유도된 최적의 값을 발견하고 적용하는 프로세스를 VPA가 어떻게 자동화하는지 방법을 살펴봤다. 여기서 중요한 문제는 쿠버네티스가 아무런 사전 준비 없이 파드를 삭제되고 생성시킨다는 점이다. 이로 인해 예상치 못한 서비스 중단 시간이 발생하기도 한다. 자원이 부족한 컨테이너에 더 많은 자원을 할당하면, 파드는 스케줄 불가(unschedulable) 상태가 되거나 여타 인스턴스의 로드를 훨씬 증가시킬 수도 있다. 컨테이너 자원을 증가시키면, 증가된 자원을 최대한 잘 사용하기 위한 튜닝이 필요할 수도 있다.  

###### 24.2.5.3. 수평 파드 오토스케일링  
<br/>

애플리케이션 튜닝과 수직 파드 오토스케일링 기술은 모두, 레플리카 수를 변경하지 않고 튜닝함으로써 기존 파드의 성능이 향상시키고자 하는 일종의 수직 스케일링이다. 이제 설명할 2가지 기술은 수평 스케일링의 일종이다. 파드 명세(spec)를 건드리지 않지만 파드와 노드 수를 변경한다. 이 접근방식을 통해 역효과나 중단이 발생할 가능성은 낮아지고 자동화도 더욱 손쉬워진다. HPA는 현재 가장 널리 사용되는 스케일링 형식이다. 처음에는 CPU나 메모리 메트릭 자원만으로 최소한의 기능만 제공했지만, 이제는 사용자정의 및 외부 메트릭을 사용해 좀 더 고도의 스케일링 사용 예가 가능해졌다.  

만약 애플리케이션 튜닝과 수직 파드 오토스케일링을 일단 수행해 올바른 애플리케이션 설정 값을 찾고, 컨테이너의 자원 소비를 결정했다면, HPA를 활성화해서 애플리케이션을 자원 요구 변화에 적응시킬 수 있다.  

###### 24.2.5.4. 클러스터 오토스케일링  
<br/>

HPA와 VPA에 설명된 스케일링 기술은 클러스터 용량 한도 내에서만 탄력성을 제공한다. 쿠버네티스 클러스터에 충분한 여유가 있는 경우에만 기술을 적용할 수 있다. 클러스터 오토스케일러(이하 CA)는 클러스터 용량 레벨에서 유연성을 제공한다. CA는 여타 스케일링 방법을 보완하는 방법이지만, 완전히 분리되어 있다. CA는 추가 용량 요구이 왜 필요한지, 왜 용량이 다 사용되지 않았는지, 워크로드 프로파일을 변경하는 주체가 사람이지 오토스케일러인지 등에는 전혀 관심이 없다. CA는 필요한 용량을 확보하기 위해 클러스터를 확장하거나, 일부 자원을 절약하기 위해 클러스터를 축소할 수 있다.  

#### 24.3. 정리  
<br/>

탄력성(elasticity)을 비롯한 다양한 스케일링 기술은 활발히 발전하고 있는 쿠버네티스 영역이다. HPA는 최근 적절한 메트릭 지원을 추가했으며, VPA는 여전히 실험 단계 중에 있다. 또한 서버리스(serverless) 프로그래밍 모델이 대중화됨에 따라 '0으로 스케일(scale-to-zero)'하는 것과 빠른 스케일링이 우선순위가 됐다. 케이네이티브 서빙(Knative serving)은 쿠버네티스 애드온으로서, '0으로 스케일'에 대한 기반을 제공한다. 케이네이티브와 기본 서비스 메시는 빠르게 발전하는 중이며, 매우 흥미롭고 새로운 클라우드 네이티브 기본 요소를 도입하고 있다.  

분산 시스템의 원하는 상태 명세(state specification)가 주어지면, 쿠버네티스는 이를 생성하고 유지할 수 있다. 또한 지속적으로 모니터링 및 자가 치유(self-healing)를 수행하고, 현재 상태를 요청한 상태와 일치시켜서, 신뢰성과 장애에 대한 회복성을 시스템에 부여한다. 오늘날 많은 애플리케이션이 회복 가능하고 안정적인 시스템을 충분히 갖추고 있지만, 쿠버네티스는 여기서 한걸음 더 나아간다. 작지만 올바르게 구성된 쿠버네티스 시스템은 로드가 과중해질 경우 갑자기 중단되는 것이 아니라 파드와 노드를 확장한다. 이렇듯 외부 스트레스 요인에 직면하는 경우, 시스템은 취약하고 불안정해지기보다는 쿠버네티스의 견고한 기능을 통해 크기를 더 늘리고 강력해진다.  

#### 24.4. 참고 자료  
<br/>

+ 탄력적 스케일의 예(http://bit.ly/2HwQa6V)
+ 수직 파드 오토스케일링에서 파드를 적정 크기로 조정하기(http://bit.ly/2WInN9l)
+ 쿠버네티스 오토스케일링 101(http://bit.ly/2U0XoGa)
+ 수평 파드 오토스케일러(http://bit.ly/2r08Row)
+ HPA 알고리즘(http://bit.ly/2Fh35Xb)
+ 수평 파드 오토스케일러에 대한 자세한 설명(http://bit.ly/2FIUSRH)
+ 쿠버네티스 메트릭 API와 클라이언트(https://github.com/kubernetes/metrics/)
+ 수직 파드 오토스케일링(http://bit.ly/2Fixzbn)
+ 수직 파드 오토스케일링 설정하기(http://bit.ly/2HyI0eb)
+ 수직 파드 오토스케일러 제안(http://bit.ly/2OfAOnW)
+ 수직 파드 오토스케일러 깃허브 리포지토리(http://bit.ly/2BDnAMZ)
+ 쿠버네티스 클러스터 오토스케일링(http://bit.ly/2TkNQI9)
+ 적응형 동시성 제한(http://bit.ly/2JuXxxx)
+ 클러스터 오토스케일러 FAQ(http://bit.ly/2Cum0NH)
+ 클러스터 API(http://bit.ly/2D133T9)
+ 쿠버매틱 Macine-Controller(http://bit.ly/2VeTqae)
+ 오픈시프트 Machine-API-Operator(http://bit.ly/2uI7TzP)
+ 케이네이티브(Knative)(https://cloud.google.com/knative/)
+ 케이네이티브: 서버리스 서비스 제공하기(https://red.ht/2HvenKZ)
+ 케이네이티브 튜토리얼(http://bit.ly/2HT70x9)  

### 25. 이미지 빌더  
<br/>

이미지 빌더(Image Builder) 패턴은 클러스터 내에서 컨테이너 이미지를 빌도하는 것이 왜 합리적인지, 그리고 쿠버네티스 내에서 이미지를 빌드하기 위해 현재 어떤 기술이 존재하는지 설명한다.  

#### 25.1. 문제  
<br/>

애플리케이션 자체를 빌드하는 것은 어떨까? 일반적인 방식은 클러스터 외부에서 컨테이너 이미지를 빌드하고, 레지스트리로 푸시(push)한 다음, 쿠버네티스 디플로이먼트 디스크립터(descriptor)에서 참조하는 것이다. 그러나 클러스터 내에 애플리케이션을 빌드하는 것은 몇 가지 장점이 있다.  

회사 정책이 허용한다면 모든 것에 대해 클러스터는 하나만 갖추는 편이 유리하다. 한곳에서 애플리케이션을 빌드하고 실행하면, 유지 관리 비용을 크기 줄일 수 있으며, 또한 용량 계획을 간소화하고 플랫폼 자원 오버헤드를 줄일 수 있다.  

일반적으로 이미지를 빌드하는 데는 젠킨스(Jenkins) 같은 지속적 통합(Continuous Integration, 이하 CI) 시스템이 쓰인다. CI 시스템을 사용해 빌드를 하면, 빌드 작업을 위한 여유 컴퓨팅 자원을 효율적으로 찾는 스케줄링 문제가 생긴다. 쿠버네티스의 핵심은 이러한 종류의 스케줄링 문제에 가장 적합하고 매우 정교한 스케줄러다.  

이미지 빌드에서 컨테이너 실행으로 전환하는 지속적 전달(Continuous Delivery, 이하 CD)로 이동할 때, 만약 빌드가 동일한 클러스터 내에서 발생한 것이라면 두 단계 모두 동일한 인프라스트럭처를 공유하고 쉽게 전환할 수 있다. 예를 들어 모든 애플리케이션에 사용되는 기본 이미지에서 새로운 보안 취약점이 발견되었다고 가정해보자. 팀에서 이 문제를 해결하고 나면 곧바로, 이 기본 이미지에 의존하는 모든 애플리케이션 이미지를 다시 빌드하고 실행 중인 애플리케이션을 새 이미지로 업데이트해야 한다. 이미지 빌더 패턴으로 구현하면 클러스터는 이미지 빌드와 실행 중인 애플리케이션 배포를 모두 알고 있기 때문에, 기본 이미지가 변경되면 자동으로 재배치를 실행할 수 있다.  

플랫폼에서 이미지를 빌드할 때의 이점을 살펴봤으니, 이제 쿠버네티스 클러스터에서 이미지를 생성하는 데 어떤 기술이 쓰이는지 알아보자.  

##### 25.1.1. 데몬리스 빌드  
<br/>

쿠버네티스 내에서 빌드가 수행될 때 클러스터는 빌드 프로세스를 완전히 제어할 수 있지만, 빌드가 더 이상 격리된 환경에서 실행되지 않기 때문에 더 높은 보안 표준이 필요하다. 클러스터에서 빌드를 수행하려면 루트 권한 없이 실행해야 한다. 다행히도 오늘날에는 높은 권한 없이 작동하는 소위 데몬리스 빌드(daemonless build) 등의 많은 방법으로 이를 해결할 수 있다.  

도커는 탁월한 사용자 경험 덕분에 컨테이너 기술을 대중에게 전파하는 데 큰 성공을 거뒀다. 도커는 클라이언트 서버 아키텍처를 기반으로 하는데, 이 클라이언트 서버 아키텍처에서 도커 데몬은 백그라운드에서 실행되고, REST API를 통해 클라이언트의 지시를 받는다. 안타깝게도, 신뢰할 수 없는 프로세스를 실행하면 컨테이너에서 벗어나서 침입자가 전체 호스트를 제어할 수 있으므로 보안 위험이 발생한다. 이 문제는 컨테이너를 실행할 때뿐만 아니라 컨테이너를 빌드할 때도 불거진다. 도커 데몬이 임의의 명령을 실행할 때 컨테이너 내에서 빌드가 발생하기 때문이다.  

대부분의 많은 프로젝트는 공격을 받을 수 있는 노출 부분을 줄이기 위해, 루트 권한이 없어도 도커 빌드가 가능하도록 만들어졌다. 그중 일부는 지브(Jib)같이 빌드 중에 명령 실행을 금하기도 하며, 다른 기술을 사용하는 툴도 있다. 가장 뛰어난 데몬리스 이미지 빌드 도구로는 img, buildah, 카니코(Kaniko)를 들 수 있다. 또한 S2I 시스템은 루트 권한없이 이미지 빌드를 수행한다.  

#### 25.2. 해결책  
<br/>

쿠버네티스 클러스터에서 이미지를 빌드하는 가장 오래되고 완전한 방법 중 하나는 오픈시프트 빌드 서브시스템으로, 이를 활용해 여러 가지 방법으로 이미지를 만들 수 있다. 지원되는 기술 중 하나는 소위 빌더 이미지를 사용해 독단적으로 빌드를 수행하는 방식인 소스 투 이미지(Soruce-to-Image, 이하 S2I)다.  

클러스터 내부 빌드를 수행하는 또 다른 방법은 케이네이티브(Knative) 빌드다. 쿠버네티스와 서비스 메시 이스티오(Istio)에서 작동하며 서비리스 워크로드의 구축, 배포, 관리를 위한 플랫폼인 케이네이티브(https://cloud.google.com/knative/)의 주요 부분 중 하나다. 케이네이티브는 여전히 매우 초기 단계의 프로젝트로서, 빠르게 발전하고 있다.  

##### 25.2.1. 오픈시프트 빌드  
<br/>

레드햇 오픈시프트는 쿠버네티스의 엔터프라이즈 배포판으로서, 쿠버네티스가 지원하는 모든 기능을 지원할 뿐만 아니라 통합 컨테이너 이미지 레지스트리, 싱글 사인온(single sign-on) 지원, 새로운 사용자 인터페이스 등의 몇 가지 엔터프라이즈 관련 기능을 추가하고 쿠버네티스에 네이티브 이미지 빌드 기능도 추가한다. OKD(https://www.okd.io/)(예전에는 오픈시프트 오리진(Origin)이라 알려진 기능)는 모든 오픈시프트 기능이 포함된 업스트림 오픈 소스 커뮤니티 에디션 배포판이다.  

오픈시프트 빌드는 쿠버네티스가 관리하는 이미지를 직접 빌드하는 최초의 클러스터 통합 방식으로, 이미지 빌드를 위한 다양한 방법을 지원한다.  

+ 소스 투 이미지(S2I): 애플리케이션의 소스 코드를 가져와서 언어별 S2I 빌더 이미지를 사용해 실행 가능한 아티팩트를 생성한 후 이미지를 통합 레지스트리로 푸시(push)한다.  
+ 도커 빌드: 도커파일(Dockerfile)과 컨텍스트 디렉토리를 사용하고 도커 데몬처럼 이미지를 생성한다.
+ 파이프라인 빌드: 사용자가 젠킨스 파이프라인을 설정할 수 있게 허용함으로써, 내부적으로 관리되는 젠킨스 서버의 작업을 빌드하기 위한 빌드를 매핑(mapping)한다.
+ 사용자정의 빌드: 이미지 생성을 완전히 제어할 수 있는 방법이다. 사용자정의 빌드를 이용하면, 빌드 컨테이너 내에서 이미지를 직접 생성해 레지스트리로 푸시해야 한다.  

빌드를 수행하기 위한 입력은 다음과 같은 다양한 소스에서 가져올 수 있다.  

+ 깃(Git): 소소를 가져오기 위한 원격 URL로 명시된 리포지토리
+ 도커파일(Dockerfile): 직접적으로 빌드 설정 자원의 일부가 저장된 도커파일
+ 이미지: 현재 빌드를 위해 파일을 추출할 여타 컨테이너 이미지. 해당 소스 타입은 체인 빌드(chained build)를 허용한다.
+ 시크릿(Secret): 빌드에 기밀 정보를 제공하기 위한 자원
+ 바이너리(Binary): 외부의 모든 입력을 제공하는 소스. 빌드를 시작할 때 바이너리 입력을 제공해야 한다.  

사용할 입력 소스를 어떻게 선택할지는 빌드 방법에 따라 다르다. 바이너리와 깃(Git)은 상화 배타적인 소스 타입이다. 그 밖의 모든 소스는 결합하거나 별도로 사용할 수 있다.  

모든 빌드 정보는 BuildConfig라는 중앙 자원 객체에 정의된다. BuildConfig 자원을 클러스터에 직접 적용하거나 kubectl에 상응하는 오픈시프트 CLI 도구인 oc를 사용해 생성할 수 있다. oc는 빌드를 정의하고 트리거하기 위한 빌드 고유의 명령을 지원한다.  

BuildConfig를 살펴보기에 앞서, 오픈시프트에 특정한 2가지 추가적인 개념을 먼저 이해해야 한다.  

ImageStream은 하나 이상의 컨테이너 이미지를 참조하는 오픈시프트 자원이다. 이것은 별도의 태그를 가진 여러 이미지를 포함하는 도커 리포지토리와 약간 유사하다. 오픈시프트는 ImageStream(리포지토리)이 ImageStream 태그(태그된 이미지)에 대한 참조 목록을 가질 수 있도록 실제 태그된 이미지를 ImageStreamTag 자원에 매핑한다. 이렇게 추가 추상화가 필요한 이유는 무엇일까? 레지스트리에 ImageStreamTag를 위한 이미지가 업데이트될 때 오픈시프트가 이벤트를 발생시킬 수 있기 때문이다. 이미지는 빌드 중이거나 이미지가 오픈시프트 내부 레지스트리로 푸시될 때 생성된다. 이렇게 하면 빌드 또는 배포에서 이러한 이벤트를 수신하고 새 빌드를 트리거하거나 배포를 시작할 수 있다.  

ImageStream을 배포에 연결하기 위해 오픈시프트는 쿠버네티스 디플로이먼트 자원 대신 컨테이너 이미지 참조만 직접 사용할 수 있는 DeploymentConfig 자원을 사용한다. 하지만 ImageStream을 사용하지 않을 경우 오픈시프트에서 바닐라(vanilla) 디플로이먼트 자원을 계속 사용할 수 있다.  

또 하나는, 이벤트에 대한 일종이 리스너(listener)로 간주할 수 있는 트리고(trigger)다. 가능한 트리거는 imageChange이며 ImageStreamTag 변경으로 인해 발생된 이벤트에 반응한다. 일종의 반응으로서, 예를 들어 이러한 트리거를 통해 또 다른 이미지를 다시 빌드하거나, 해당 이미지를 사용해 파드를 재배포할 수 있다. 오픈시프트 공식 문서(https://red.ht/2FrDIDj)에서 imageChange 트리거를 비롯해, 트리거에 대한 더욱 상세한 내용과 사용 가능한 트리거 종류를 확인할 수 있다.  

###### 25.2.1.1. 소스 투 이미지  
<br/>

S2I(Source-to-Image) 빌더 이미지가 어떤 형태인지 간단히 살펴보겠다. S2I 빌더 이미지는 S2I 스크립트 세트를 포함하는 표준 컨테이너 이미지이며 다음과 같은 2가지 필수 명령을 제공해야 한다.  

+ assemble  
빌드가 시작될 때 호출되는 스크립트다. 해당 스크립트가 하는 일은 설정된 입력 중 하나에서 제공하는 소스를 가져와서 필요한 경우 컴파일한 후 최종 아티팩트를 적절한 위치에 복사하는 것이다.

+ run  
이미지의 진입점(entry point)으로 사용된다. 오픈시프트는 이미지를 배포할 때 해당 스크립트를 호출한다. run 스크립트는 생성된 아티팩트를 사용해 애플리케이션 서비스를 제공한다.  

다른 옵션으로, 후속 빌드 실행에서 assemble 스크립트가 이용할 이른바 증분 빌드를 위해 아티팩트를 저장하는 사용법 메시지를 제공하는 스크립트나, 안전성 검사를 추가하는 스크립트를 작성할 수도 있다.  

S2I 빌드에는 빌더 이미지와 소스 입력이라는 2가지 요소가 있다. 2가지 모두, 빌드가 시작될 대 트리거 이벤트가 수신되었거나 수동으로 시작하게 되면 S2I 빌드 시스템에 의해 결합된다. 예를 들어 소스 코드를 컴파일하여 빌드 이미지가 완료되면 컨테이너가 이미지로 커밋되고 설정된 ImageStreamTag로 푸시된다. 이 이미지에는 컴파일과 미리 준비된 아티팩트가 포함되어 있으며, 이미지의 run 스크립트가 진입점(entry point)으로 설정된다.  

다음 예제는 자바 S2I 이미지를 이용한 간단한 자바 S2I 빌드를 보여준다. 이 자바 S2I 빌드는 소스와 빌더 이미지를 가져와서 ImageStreamTag로 푸시되는 출력 이미지를 생성한다. 또한 이 빌드는 oc start-build를 통해 수동으로 시작하거나 빌더 이미지가 변경될 때 자동으로 시작할 수 있다.  

```yaml
apiVersion: v1
kind: BuildConfig
metadata:
  name: random-generator-build
spec:
  source: # 가져올 소스 코드에 대한 참조. 이 경우 깃허브에서 소스 코드를 가져온다.
    git:
      uri: https://github.com/k8spatterns/random-generator
  startegy: # sourceStrategy가 S2I 모드로 전환되고 빌더 이미지는 도커 허브에서 직접 가져온다.
    sourceStrategy:
      from:
        kind: DockerImage
        name: fabric8/s2i-java
  output: # 생성된 이미지로 업데이트 할 ImageStreamTag로서, assemble 스크립트가 실행된 후 커밋된 빌더 컨테이너다.
    to:
      kind: ImageStreamTag
      name: random-generator-build:latest
  triggers: # 빌더 이미지가 업데이트되면 자동으로 재빌드한다.
  - type: ImageChange
```

S2I 애플리케이션 이미지를 생성하는 견고한 메커니즘이며, 빌드 프로세스가 신뢰할 수 있는 빌더 이미지에 의해 완전히 통제되므로 일반 도커 빌드보다 안전한다. 그러나 이 방법에도 몇 가지 단점이 있다.  

복잡한 애플리케이션의 경우, 특히 빌드에서 많은 의존성을 로드해야 하는 경우 S2I가 느려질 수 있다. 최적화가 안된 S2I는 모든 빌드에 대한 모든 의존성을 새로 로드한다. 메이븐(Maven)으로 빌드된 자바 애플리케이션의 경우, 로컬 빌드를 수행할 때처럼 캐싱(caching)이 없다. 인터넷을 반복해서 다운로드하지 않으려면, 캐시 역할을 하는 클러스터 내부 메이븐 리포지토리를 설정하는 것이 좋다. 그런 다음, 원격 리포지토리에서 아티팩트를 다운로드하는 대신, 해당 공통 리포지토리에서 접근하도록 빌더 이미지를 설정해야 한다.  

빌드 시간을 줄이는 또 다른 방법은 S2I와 함께 증분 빌드(incremental build)를 사용해 이전 S2I 빌드에서 생성했거나 다운로드한 아티팩트를 재사용하는 것이다. 그러나 많은 양의 데이터가 이전에 생성된 이미지에서 현재 빌드 컨테이너로 복사되므로, 일반적으로 의존성을 유지하는 클러스터 로컬 프록시를 사용하는 것보다 성능상으로 별로 나을 것이 없다.  

S2I의 또 다른 단점은 생성된 이미지에도 전체 빌드 환경이 포함되어 있다는 점이다. 이로 인해 애플리케이션 이미지의 크기가 커질 뿐만 아니라, 빌더 도구도 취약해질 수 있어서 공격을 받을 수 있는 노출 부분이 증가된다.  

메이븐 등의 불필요한 빌더 도구를 제거하기 위해, 오픈시프트에서는 체인 빌드(chained build)를 제공한다. 체인 빌드는 S2I 빌드의 결과를 가져와 가벼운 런타임 이미지를 만든다.  

+ 도커 빌드  
오픈시프트는 클러스터 내에서 직접 도커 빌드(docker build)를 지원한다. 빌드 컨테이너에 도커 데몬의 소켓을 직접 마운트하면 도커 빌드가 작동되는데, 이것은 docker build를 할 때 사용된다. 도커 빌드의 소스는 도커파일(Dockerfile)과 컨텍스트가 저장된 디렉토리다. 임의의 이미지를 참조하는 Image 소스를 사용할 수도 있고, 도커 빌드 컨텍스트 디렉토리에 복사할 수 있는 파일에서 얻은 Image 소스를 사용할 수도 있다. 도커 빌드는 트리거와 함께 체인 빌드에 사용될 수 있다. 또는 표준 다단계 도커파일을 사용해 빌드 부분과 런타임 부분을 분리할 수 있다. 예제 리포지토리(http://bit.ly/2CxnnuX)에는 체인 빌드와 동일한 이미지를 생성하고 전체적으로 작동하는 다단계 도커 빌드 예제가 포함되어 있다.  

+ 체인 빌드  
체인 빌드는 첫 S2I 빌드로 구성되며 이진 실행 파일과 같은 런타임 아티팩트를 생성한다. 그런 다음, 일반적으로 두 번째 빌드인 도커 빌드에서 생성된 이미지가 해당 아티팩트를 선택한다.  

다음 예제는 위의 예제에서 생성된 JAR 파일을 사용하는 두 번째 빌드 설정의 셋업을 보여준다. 최종적으로 ImageStream으로 푸시(push)되는 random-generator-runtime 이미지는 DeploymentConfig에서 애플리케이션을 실행시키는 데 사용될 수 있다.  

S2I 빌드 결과를 모니터링하는 다음 예제에서 사용된 트리거에 유의하자. 이 트리거는 S2I 빌드를 실행할 때마다 런타임 이미지를 다시 빌드하여 두 ImageStream을 항상 동기화시킨다.  

```yaml
apiVersion: v1
kind: BuildConfig
metadata:
  name: runtime
spec:
  source:
    images:
    - from: # 이미지 소스는 S2I 빌드 실행 결과가 포함된 ImageStream을 참조하고 컴파일된 JAR 아카이브가 포함된 이미지 내의 디렉토리를 선택한다.
        kind: ImageStreamTag
        name: random-generator-build:latest
      paths:
      - sourcePath: /deployments/.
        destinationDir: "."
    dockerfile: |- # S2I 빌드가 생성한 ImageStream에서 JAR 아카이브를 복사하는 도커 빌드를 위한 도커파일 소스
      FROM openjdk:8-alphine
      COPY *.jar /
      CMD java -jar /*.jar
  strategy: # strategy는 도커 빌드를 선택한다.
    type: Docker
  output: # JAR 아카이브를 컴파일하기 위한 S2I가 성공적으로 실행된 후, S2I 결과 ImageStream이 변경되면 자동으로 다시 빌드한다.
    to:
      kind: ImageStreamTag
      name: random-generator:latest
  triggers: # 이미지 업데이트를 위한 리스너를 등록하고 새 이미지가 ImageStream에 추가되면 다시 배포한다.
  - imageChange:
      automatic: true
      from:
        kind: ImageStreamTag
        name: random-generator-build:latest
    type: ImageChange
```

예제 리포지토리(http://bit.ly/2CxnnuX)에서 설치 가이드가 포함된 전체 예제를 찾을 수 있다. 앞서 언급했듯이 탁월한 S2I 모드가 있는 오픈시프트 빌드는 오픈시프트의 특별한 기능을 사용하는 쿠버네티스 클러스터 내에서 컨테이너 이미지를 안전하게 빌드하는 가장 오래되고 완전한 방법 중 하나다. 이제 바닐라 쿠버네티스 클러스터 내에서 컨테이너 이미지를 빌드하는 또 다른 방법을 살펴보겠다.  

##### 25.2.2. 케이네이티브 빌드  
<br/>

케이네이티브 빌드는 현재 중단(deprecated) 프로젝트이다. 텍톤 파이프라인(Tekton Pipelines)으로 대체되었다.  

구글은 쿠버네티스에 고급 애플리케이션 관련 기능을 제공하기 위해 2018년 케이네이티브(Knative) 프로젝트를 시작했다. 케이네이티브의 기반은 이스티오(Istio, https://istio.io/) 같은 서비스 메시(service mesh)로, 트래픽 관리, 관측성, 보안 등을 위한 인프라스트럭처 서비스를 기본으로 제공한다. 서비스 메시는 인프라스트럭처 관련 기능이 있는 사이드카(Sidecar)를 이용해 애플리케이션을 계측한다. 서비스 메시 외에도 케이네이티브는 주로 애플리케이션 개발자를 대상으로 하는 추가 서비스를 제공한다.  

+ 케이네이티브 서빙  
서비스로서 함수(Function-as-a-Service) 플랫폼에서 활용할 수 있는 애플리케이션 서비스를 위한 '0으로 스케일(scale-to-zero)'을 지원한다. 케이네이티브 서빙(Knative serving)은 탄력적 스케일 패턴과 기본 서비스 메시 지원을 함께 사용하여 0에서부터 무작위의 많은 레플리카까지 스케일할 수 있다.

+ 케이네이티브 이벤팅  
케이네이티브 이벤팅(Knative eventing)은 채널을 통해 소스에서 싱크(sink)로 이벤트를 전달하는 메커니즘이다. 이벤트는 싱크로 사용되는 서비스를 0에서부터 확장할 수 있다.

+ 케이네이티브 빌드  
쿠버네티스 클러스터 내에서 애플리케이션 소스코드를 컨테이너 이미지로 컴파일한다. 후속 프로젝트는 텍톤 파이프라인(Tekton Pipelines)으로, 나중에는 케이네이티브 빌드를 대체할 것이다.  

이스티오와 케이네이티브는 모두 오퍼레이터 패턴으로 구현되며, CRD를 사용하여 세심히 관리되는 도메인 자원을 선언한다. 케이네이티브의 이미지 빌더 패턴 구현에 대한 내용이므로, 이제부터 케이네이티브 빌드에 대해 좀 더 자세히 살펴보겠다.  

케이네이티브 빌드는 주로 도구(tool) 개발자를 대상으로 하며, 최종 사용자에게 UI와 완벽한 빌드 환경을 제공한다. 케이네이티브 빌드 프로젝트는 빠르게 진행되고 있으며, 텍톤 파이프라인 같은 후속 프로젝트로 대체될 수도 있지만 원리는 동일하게 유지될 가능성이 높다. 그렇지만, 일부 세부사항은 나중에 변경될 수 있으므로, 최신 케이네이티브 버전과 프로젝트가 계속 업데이트되는 예제 코드(http://bit.ly/2CxnnuX)를 참조하기 바란다.  

케이네이티브는 기존 CI/CD 솔루션과 통합할 수 있는 빌딩 블록을 제공하도록 설계되었으며, 컨테이너 이미지 자체를 빌드하기 위한 CI/CD 솔루션은 아니다. 시간이 지남에 다라 점점 더 많은 솔루션이 등장할 것으로 예상되지만, 여기서는 빌딩 블록들을 한번 살펴보자.  

###### 25.2.2.1. 단순 빌드  
<br/>

빌드 CRD는 케이네이티브 빌드의 핵심 요소다. 빌드 CRD는 케이네이티브 빌드 오퍼레이터가 빌드를 위해 수행해야 하는 구체적인 단계를 정의한다. 다음 예제에서 보여주는 주요 구성요소는 다음과 같다.  

+ source 명세는 애플리케이션의 소스 코드 위치를 가리킨다. 소스는 이 예제에서와 같이 깃 리포지토리일 수도 있고, 구글 클라우드 스토리지 등의 다른 원격 위치일 수도 있으며, 빌드 오퍼레이터가 소스를 추출할 수 있는 임의의 컨테이너일 수도 있다.

+ steps는 소스 코드를 실행 가능한 컨테이너 이미지로 변환하는 데 필요하다. 각 스텝(step)은 스텝을 수행하기 위해 빌더 이미지를 참조한다. 모든 스텝은 /workspace에 마운트된 볼륨에 접근할 수 있다. 이 볼륨은 소스코드가 저장되어 있을 뿐만 아니라, 스텝 간에 데이터를 공유할 때도 사용된다.  

다음 예제에서 소스는 깃허브에서 호스팅하는 간단한 자바 프로젝트이고 메이븐으로 빌드되었다. 빌더 이미지는 자바와 메이븐을 포함하는 컨테이너 이미지다. 지브(Jib)(https://github.com/GoogleContainerTools/jib)는 도커 데몬 없이 이미지를 빌드하고 레지스트리로 푸시하기 위해 사용된다.  

```yaml
apiVersion: build.knative.dev/v1alpha1
kind: Build
metadata:
  name: random-generator-build-jib # 빌드 객체의 이름
spec:
  source: # 깃허브 URL을 포함하는 소스 명세
    git:
      url: https://github.com/k8spatterns/random-generator.git
      revision: master
  steps: # 하나 이상의 빌드 스텝(step)
  - name: build-and-push
    image: gcr.io/cloud-builders/mvn # 이 빌드 스텝에서 사용하는 자바와 메이븐이 포함된 이미지
    args: # 빌더 컨테이너에 제공된 인수(argument)로서, 메이븐이 jib-maven-plugin을 통해 컨테이너 이미지를 컴파일, 생성, 푸시 등을 하도록 트리거한다.
    - compile
    - com.google.cloud.tools:jib-maven-plugin:build
    - -Djib.to.image=registry/k8spatterns/random-generator
    workingDir: /workspace # /workspace 디렉토리는 모든 빌드 스텝(step)에 공유되고 마운트된다.
```

케이네이티브 빌드 오퍼레이터가 빌드를 수행하는 방법을 이해하기 위해, 간략히 내부 구조를 살펴보는 것도 흥미로울 것이다.  

빌드 CRD는 빌드 스텝(step)을 포함하는 파드가 된다. 빌드 스텝은 순차적으로 호출되는 일련의 초기화 컨테이너로 변환된다. 첫 번째 초기화 컨테이너는 무조건 생성된다. 이 예제에서는 외부 리포지토리와 연동하기 위해 인증 정보를 초기화하는 첫 번째 초기화 컨테이너와 깃허브에서 소스를 체크 아웃(check out)하기 위한 두 번째 초기화 컨테이너가 있다. 나머지 초기화 컨테이너는 선언된 빌더 이미지로 지정된 스텝이다. 모든 초기화 컨테이너가 완료되면 기본 컨테이너는 아무 작업도 수행하지 않으므로, 파드는 초기화 단계 후에 중지된다.  

###### 25.2.2.2. 빌드 템플릿  
<br/>

다음 예제는 단일 빌드 스텝만 포함하지만 일반적인 빌드는 여러 스텝으로 구성된다. 빌드템플릿(BuildTemplate) 사용자정의 자원으로, 유사한 빌드에 대해 동일한 스텝을 재사용할 수 있다.  

다음 예제는 3단계로 구성된 빌드 템플릿을 보여준다.  

(1) mvn package로 자바 JVM 파일을 생성한다.  
(2) 해당 JAR 파일을 컨테이너 이미지에 복사하고 java -jar로 컨테이너 이미지를 시작하는 도커파일(Dockerfile)을 생성한다.  
(3) 카니코(Kaniko)를 사용해 빌더 이미지로 컨테이너 이미지를 생성하고 푸시한다. 카니코는 구글에서 만든 도구로서, 사용자 공간(user space)에서 실행되며 로컬 도커 데몬을 사용한 컨테이너 내부의 도커파일(Dockerfile)에서 컨테이너 이미지를 빌드하는 데 쓰인다.  

템플릿은 템플릿을 사용할 때 채워지는 플레이스홀더(placeholder) 같은 파라미터를 지원한다는 점을 제외하면 빌드와 거의 동일하다. 다음 예제에서 IMAGE는 생성할 대상 이미지를 저정하는 데 필요한 단일 파라미터다.  

```yaml
apiVersion: build.knative.dev/v1alpha1
kind: BuildTemplate
metadata:
  name: maven-kaniko
spec:
  parameters:
  - name: IMAGE # 템플릿을 사용할 때 제공할 파라미터 목록
    description: The name of the image to create and push
  steps:
  - name: maven-build # 메이븐을 사용하여 자바 애플리케이션을 컴파일하고 패키징하는 스텝(step)
    image: gcr.io/cloud-builders/mvn
    args:
    - package
    workingDir: /workspace
  - name: prepare-docker-context # 생성된 JAR 파일을 복사하고 시작하는 도커파일(Dockerfile)을 생성하는 스텝. 자세한 내용은 이 코드에서는 생략되었으니, 예제 깃허브 리포지토리를 참고하기 바란다.
    image: alphine
    command: [ .... ]
  - name: image-build-and-push # 도커 이미지를 빌드하고 푸시하기 위해 카니코를 호출하는 스텝
    image: gcr.io/kaniko-project/executor
    args:
    - --context=/workspace
    - --destination=${IMAGE} # destination은 제공된 템플릿 파라미터 ${IMAGE}를 사용한다.
```

그런 다음, 빌드에서 해당 템플릿을 사용할 수 있고, 다음 예제처럼 스텝 목록 대신 템플릿 이름을 지정한다. 예제에서 보듯이, 생성할 애플리케이션 컨테이너 이미지의 이름만 지정하면 되고 작성한 다단계 빌드를 쉽게 재사용해 다른 애플리케이션을 만들 수 있다.  

```yaml
apiVersion: build.knative.dev/v1alpha1
kind: Build
metadata:
  name: random-generator-build-chained
spec:
  source: # 소스 코드를 어디에서 가져올지에 대한 소스 명세
    git:
      url: https://github.com/k8spatterns/random-generator.git
      revision: master
  template: # 위의 예제에서 정의한 템플릿에 대한 참조
    name: maven-kaniko
    arguments:
    - name: IMAGE # 파라미터로 템플릿에 채워진 이미지 명세
      value: registry:80/k8spatterns/random-generator
```

케이네이티브 빌드 템플릿 리포지토리(https://github.com/knative/build-template)에서 사전에 정의된 많은 템플릿을 찾을 수 있다.  

#### 25.3. 참고 자료  
<br/>

+ 이미지빌더(ImageBuilder) 예제(http://bit.ly/2FpCkkL)
+ 지브(Jib)(https://github.com/GoogleContainerTools/jib)
+ Img(https://github.com/genuinetools/img)
+ Buildah(https://github.com/prodjectatomic/buildah)
+ 카니코(Kaniko)(https://github.com/GoogleContainerTools/kaniko)
+ 오픈시프트 빌드 설계 문서(http://bit.ly/2HILD0E)
+ 다단계 도커파일(http://bit.ly/2YfUY63)
+ S2I 빌드 체인(https://red.ht/2JqzIw9)
+ 소스 투 이미지(Source-to-Image, S2I) 명세(https://github.com/openshift/source-to-image)
+ 증분(Incremental) S2I 빌드(https://red.ht/2TSGxp9)
+ 케이네이티브(https://cloud.google.com/knative/)
+ 쿠버네티스 클러스터에서 케이네이티브 빌드로 컨테이너 이미지 빌드하기(http://bit.ly/2YJuZUC)
+ 케이네이티브 빌드(https://github.com/knative/build)
+ 텍톤(Tekton) 파이프라인(https://github.com/knative/build-pipeline)
+ Knctl 소개: 케이네이티브를 사용하는 간단한 방법(https://ibm.co/2Hwnmvw)
+ 케이네이티브 빌드 튜토리얼(http://bit.ly/2OewLZb)
+ 케이네이티브 빌드 템플릿(https://github.com/knative/build-templates)